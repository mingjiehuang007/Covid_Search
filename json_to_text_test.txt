paper_id	title	authors	abstract	body_text
paper_id= ffe02373554797a80890d0be4c0d5aca804b546b	title= Emerging therapies for COVID-19 pneumonia	authors= Denise  Battaglini;Chiara  Robba;Lorenzo  Ball;Fernanda  Ferreira Cruz;Pedro Leme Silva;Paolo  Pelosi;Patricia  Rieken;Macedo  Rocco;	abstract= 	body_text= On 11 March 2020, the World Health Organization declared coronavirus disease 2019 (COVID-19) a pandemic. In recent months, with the spread of COVID-19, there has been growing interest in preventive and therapeutic treatments, but no effective strategies for either purpose have been confirmed by the available randomized controlled trials. Although future clinical trials may renew hope [1] , management of COVID-19 is still exclusively supportive. The mechanism of infection with the causative agent of COVID-10, severe acute respiratory syndrome coronavirus-2 (SARS-CoV-2), requires binding of viral spike (S) proteins to the angiotensinconverting enzyme-2 (ACE2) receptor, a process facilitated by transmembrane serine protease 2 (TMPRSS2) and followed by downregulation of ACE2 [2] . The viral membrane then fuses with the host cell, and the viral ribonucleic acid (RNA) is taken up via endocytosis. Within the host cell, it undergoes translation, proteolysis, RNA synthesis, new assembly, and exocytosis to further infect other cells. Secondary hemophagocytic lymphohistiocytosis, with a characteristic 'cytokine storm' component, has been recently proposed as a severe hyperinflammatory complication of COVID-19, driven by viral immune response [3] . Moreover, many COVID-19 patients suffer from abnormal coagulation, which has been associated with poor prognosis [4] . Recent advances have focused on a multi-target therapeutic strategy, taking inspiration from previous outbreaks of coronavirus infections, namely SARS and the Middle East respiratory syndrome (MERS). Nevertheless, no specific drugs are available for the treatment of either infection nor for COVID-19. With the current paucity of data, therapeutic choices that usually would never been adopted before appraising their utility is now being employed in an empiric and presumptive manner, moving away from core tenets of evidence-based medicine. Effective therapies are urgently needed. Renin-angiotensin-aldosterone system (RAAS) inhibitors [2] have been proposed because they bind to ACE2. Data from Wuhan reported that a huge number of COVID-19 patients present with hypertension and RAAS inhibitors as daily antihypertensive therapy [2] . However, preclinical studies demonstrated that RAAS inhibitors may upregulate ACE2 expression, raising concerns as to their efficacy in COVID-19 ( Figure 1 ) [2] . Conversely, abrupt withdrawal of RASS inhibitors in high-risk patients may result in clinical instability and adverse outcome. Clinical trials to determine whether the use of RASS inhibitors may affect the outcome in COVID-19 are ongoing (Table 1 ) [2] . Chloroquine and hydroxychloroquine have broad-spectrum antiviral activity in vitro by inhibiting virus entry into host cells through the inhibition of glycosylation, proteolysis, and endosomal acidification of host receptors. Both drugs also have immunomodulatory effects, attenuating cytokine production [5] . Very recently, the U.S. Food and Drug Administration (FDA) reviewed the available literature, as well as data from the Adverse Event Reporting System Database and the American Association of Poison Control Centers National Poison Data System, suggesting that these drugs should be used only very cautiously in patients at increased risk of heart rhythm problems, especially considering the limitation of their use to hospital settings [5] . On May 7, an observational study of 1376 consecutive patients who received hydroxychloroquine was published in the New England Journal of Medicine. The study concluded that hydroxychloroquine did not influence the time from enrollment to intubation or death [6] , implying limited clinical utility and perhaps detrimental effects. Thus, we recommend that physicians not use hydroxychloroquine until new data from randomized-controlled trials become available (Table 1) . Antiretroviral agents, such as protease inhibitors and integrase strand transfer inhibitors, have also been suggested for the treatment of COVID-19. The use of lopinavir/ritonavir did not result in benefits in terms of viral clearance or mortality rate in a randomized-controlled study of 199 COVID-19 patients. Randomization was made 13 days after symptom onset, suggesting that delayed initiation of therapy was probably associated with the inefficacy of treatment [7] . Other antiretrovirals are still undergoing preclinical testing, with possible evidence of efficacy against SARS-CoV-2 [1] . Favipiravir, a purine nucleoside leading to inaccurate viral RNA synthesis, has recently been approved for clinical trials to treat COVID-19 [8] . Remdesivir, a nucleotide analogue, although was developed as a therapeutic agent for Ebola and Marburg virus infections with interesting results, preliminary data in COVID-19 did not show clinical benefit [9] ; phase III clinical trials are ongoing (Table 1 ). In addition to various antiviral drugs already on the market, there are also several smallmolecule compounds currently in research and development which have shown significant inhibitory effects on many key proteins from similar coronaviruses, such as SARS-CoV and MERS-CoV. These drug candidates mostly inhibit viral enzymes, including proteases and components of RNAdependent RNA polymerase (RdRp). Since the 3C-like proteinase (3CL pro ) has a high level of sequence homology between SARS-CoV and SARS-CoV-2, inhibitors tested against SARS-CoV 3CL pro may also be active against SARS-CoV-2 [10] . Because of their ability to interfere with viral replication, interferons and interferon fusion proteins have been utilized as therapeutic agents for the treatment of viral infections for more than 20 years. Finally, several RNA therapies have been developed, and, in addition to directly targeting the virus, antisense oligonucleotides could be used to target disease-related proteins involved in the inflammatory process. Convalescent plasma has also been shown to reduce viral load and improve outcome in COVID-19 patients [11] . The use of corticosteroids to modulate the inflammatory response in COVID-19 remains controversial. Based on an expert consensus [12] [13] [14] , their use should be carefully weighed, according to clinical evidence: in patients already taking corticosteroids for chronic diseases, increased use should be evaluated with caution, with doses not exceed 0.5-1 mg/kg/day (methylprednisolone or equivalent) and for no longer than 7 days. The safety and efficacy of methylprednisolone in COVID-19 patients have been investigated in the clinical setting (Table 1) . Interleukin (IL)-6 plays a crucial role in the inflammatory response, showing both anti-and pro-inflammatory effects on cells that express the IL-6 receptor (IL-6R), such as macrophages, neutrophils, and T-cells. Tocilizumab, a recombinant humanized monoclonal antibody, is designed to bind IL-6R. In 2017, it was approved for the treatment of cytokine release syndrome, which has prompted its possible use for COVID-19 in patients with higher levels of IL-6; efficacy remains unclear and requires confirmation [3] . Anakinra, an IL-1 receptor inhibitor, may likewise induce a short-term reversible suppression of inflammation. However, both of these drugs might increase the risk of superinfections, making their clinical use for COVID-19 questionable [3] . Bevacizumab, a human monoclonal antibody against vascular endothelial growth factor, has been studied in COVID-19 patients to reduce lung edema [15] . Immunoglobulin therapy is considered another potential treatment for COVID-19 patients. Two small case series (five 2. Viral RNA is released, and transduction begins in the ribosome. 3. This is followed by proteolysis (may be inhibited by lopinavir/ritonavir), which in turn is followed by translation and replication (RNA polymerases; may be inhibited by favipiravir and remdesivir). 5. Then, the ribosome synthesizes nucleocapsid proteins, which will encase RNA, and the viral envelope is assembled with the aid of the endoplasmic reticulum and Golgi apparatus. The now-complete virions are released, ready to infect other cells. 7. In parallel, a local and systemic immune response is now activated, and hypercoagulability ensues (modulated by corticosteroids and heparin). The IL-6 receptor can be inhibited by tocilizumab, and the IL-1 receptor, by anakinra. Bevacizumab acts by inhibiting vascular endothelial growth factor (VEGF). Convalescent plasma reduces viral load. Observational data [20] RAAS: renin-angiotensin-aldosterone system; ACE2: angiotensin-converting enzyme 2; G6PD: glucose-6-phosphate-dehydrogenase; IL6: interleukin 6; IL1: interleukin 1; HIV: human immunodeficiency virus. and three, respectively) reported the use of immunoglobulins extracted from convalescent patients and the use of intravenous immunoglobulin for the treatment of SARS-CoV-2, respectively [1] . Mesenchymal stromal cells (MSCs) have demonstrated a good safety profile and possible efficacy in patients with acute respiratory distress syndrome (ARDS), but whether these therapies are effective for treating COVID-19 remain unknown. Few studies have been published [16] and several others are ongoing. COVID-19 patients with different degrees of severity were treated with MSCs or placebo, and no adverse events were observed. However, further studies on dose, dosing strategies, and timing of administration are needed. COVID-19 patients may also develop severe coagulopathy. During SARS-CoV-2 infection, alveolar macrophages and T cells, among other immune cells, are recruited and activated in an uncontrolled immune response leading to a cytokine storm, characterized by elevated serum levels of interleukin-1β, tumor necrosis factor (TNF)-α, and IL-6, which foster thrombin activity [4] . Heparin is commonly administered to these patients. IL-1β and TNF-α target the glycocalyx via activation of metalloproteinases and heparanase, consequently reducing the content of proteoglycans (e.g., syndecan-1) and heparan sulfate, respectively. Glycocalyx shedding is linked to endothelial barrier rupture, which facilitates the increase of vascular permeability, leukocyte trafficking, and microthrombi. Heparin inhibits thrombin activity, protects endothelial cells from oxidative stress, and may modulate the cytokine storm [17] . Taken together, this evidence suggests that biologics have the potential to broaden the spectrum of treatment options for coronavirus-induced diseases. Prior knowledge and experience with the SARS and MERS outbreaks provide potential strategies for developing new target-specific therapeutic agents. However, COVID-19 has peculiar characteristics, and no strategies have demonstrated clear benefit to date; costs are also a concern. Table 1 and Figure 1 summarize the current studies ongoing, and the mechanisms implicated in the treatment of COVID-19. In conclusion, the main mechanism involved in the pathogenesis of COVID-19 appears to be activation of the inflammatory cascade with a massive pro-coagulable state, which can lead to rapid deterioration including potentially fatal secondary conditions (e.g., thromboembolic events, superinfections). In our opinion, 'common' drugs such as steroids and heparin should be used routinely to modulate inflammation and coagulability. The possibility of superinfection is a concern with corticosteroid therapy, while increased risk of bleeding may be seen with unfractionated or low-molecular-weight heparin. Strict monitoring of inflammatory (C-reactive protein, lymphocyte count, IL-6, ferritin, procalcitonin, etc.) and coagulation parameters (antithrombin III, prothrombin, D-dimer, fibrinogen, international normalized ratio, activated partial thromboplastin time, activated factor X) must be enforced and used to guide daily therapeutic management. The pandemic of novel coronavirus disease 2019 (COVID-19) has posed significant therapeutic challenges. High fatality rates have been observed in COVID-19 patients who require mechanical ventilation. Therefore, there has been growing interest in the development of new therapies capable of improving outcomes. At present, there is no evidence from randomizedcontrolled trials to support any therapeutic strategy as effective for COVID-19. Extensive studies have evaluated different potential strategies; however, to date, no treatment had improved outcomes or reduced fatality rates. Several drugs, including RAAS inhibitors (losartan, recombinant soluble ACE2), aminoquinolines (hydroxychloroquine/chloroquine), antivirals (remdesivir, favipiravir), immunomodulators (steroids, antiinterleukin agents, intravenous immunoglobulin, mesenchymal stromal cells), and antithrombotics (heparin) have been studied. These studies have had several limitations, which seem to be associated with: 1) a lack of understanding of the pathophysiological mechanisms of SARS-CoV-2-induced pneumonia; 2) the difficulty of translating in vitro findings into clinical practice; and 3) the lack of proper clinical study designs to elucidate optimal dosage, timing, and population. SARS-CoV-2 is still a poorly known virus, and a better understanding of its transmission mechanisms, clinical spectrum of disease, diagnosis, and fatality rates is urgently needed. Future investigations should focus on new drugs targeting the pathophysiological pathways implicated in COVID-19. Moreover, several therapies which have been investigated may lead to more adverse effects than benefits. Evidence suggests there are different phenotypes of COVID-19, potentially distinguishable through distinct biomarkers, which may respond differently to treatments. Therefore, personalized approaches to COVID-19 management should be recommended, including the use of 'common' drugs such as heparin and steroids as part of daily clinical management. While we await the development of an effective vaccine, clinical trials should help define the best strategy to treat COVID-19 [18, 19, 20] .  
paper_id= ffe04775867d268ef18a29f13138e407b6aa8ea5	title= BMC Infectious Diseases Clinical and epidemiological predictors of transmission in Severe Acute Respiratory Syndrome (SARS)	authors= Mark Ic Chen;Angela  Lp Chow;Arul  Earnest;Nam  Hoe;  Leong;Yee Sin Leo;	abstract= Background: Only a minority of probable SARS cases caused transmission. We assess if any epidemiological or clinical factors in SARS index patients were associated with increased probability of transmission. We used epidemiological and clinical data on probable SARS patients admitted to Tan Tock Seng Hospital. Using a case-control approach, index patients who had probable SARS who subsequently transmitted the disease to at least one other patient were analysed as "cases" against patients with no transmission as "controls", using multivariate logistic regression analysis. Results: 98 index patients were available for analysis (22 with transmission, 76 with no transmission). Covariates positively associated with transmission in univariate analysis at p < 0.05 included delay to isolation (Day 7 of illness or later), admission to a non-isolation facility, preexisting chronic respiratory disease and immunosuppressive disease, need for oxygen, shortness of breath, vomiting, and higher lactate dehydrogenase levels and higher neutrophil counts. In the multivariate analysis, only three factors were significant: delay to isolation, admission to a nonisolation facility and higher lactate dehydrogenase levels of >650 IU/L (OR 6.4, 23.8 and 4.7 respectively). Clinical and epidemiological factors can help us to explain why transmission was observed in some instances but not in others. 	body_text= Severe Acute Respiratory Syndrome (SARS) was the first emerging infectious disease of the new century with epidemic potential. First recognized on 26 Feb 2003, SARS spread rapidly and resulted in 8098 reported cases and 774 deaths in close to 30 countries [1] . While there was no endemic transmission in the majority of these countries, explosive outbreaks were observed in China, Hong Kong, Taiwan, Canada, Vietnam and Singapore. Ongoing research points to an existing animal reservoir for the virus [2, 3] , and future epidemics may hence sporadically emerge from this source [4] . A key feature in the epidemiology of SARS is the widespread variation in the number of secondary infections caused by each potentially infectious case. While multiple secondary infections were traced to single individuals in several super-spreading events [5] [6] [7] , the majority of infected individuals did not cause any secondary infections [7] . A recent paper showed that the efficiency of outbreak control measures could be greatly enhanced if there were predictive methods for identifying infectious individuals [8] . However, a review by Yu and Sung noted that the key risk factors for transmission remain largely unknown [9] . While two previous papers attempted to identify risk factors for onward transmission from index patients [10, 11] , both studies were restricted to household contacts, and neither accounted for factors such as clinical presentation, immune status and disease severity in the index patient, all of which have been suspected to play a role in disease transmission [9] . In this study, we analysed epidemiological and clinical data on probable SARS patients admitted to Tan Tock Seng Hospital. We attempted to identify if any of these factors could explain if individual index patients had transmitted the disease by the time they were detected and isolated, and developed a simple model for explaining the variability in secondary transmission. We used data collected during the outbreak of SARS in Singapore from 1 March to 31 May 2003. Diagnosis of probable SARS was based on the standard WHO case definitions dated 1 st May 2003 [12] . Briefly, it required a patient to have fever, respiratory symptoms and a significant contact history, in the presence of a chest radiograph consistent with pneumonia or adult respiratory distress syndrome, or laboratory assays diagnostic of SARS. We defined an index patient as any individual who had been established, during contact tracing investigations, to have come into effective contact with susceptible individuals, be it in healthcare, household or other social settings. This study included all individuals who were part of the chain of transmission within Tan Tock Seng Hospital -i.e. all index patients causing secondary infections within (TTSH), as well as all individuals exposed in TTSH who, in the course of follow-up surveillance and contact tracing, were found to have contracted SARS, and hence also became index patients. Index patients within TTSH were identified from outbreak investigations by on-site epidemiologists [13] . The hospital contact tracing team was also updated on individuals exposed in TTSH who were subsequently confirmed to be infected. This information was obtained, either when these infected individuals returned to us for clinical assessment, or through the Ministry of Health in the event that these infected individuals presented elsewhere. The team then collated epidemiological information on dates of onset, admission and isolation for these index patients, and verified with other healthcare institutions and the Ministry of Health if these index patients had in turn caused any secondary transmission in either nosocomial or community settings. Secondary infections were linked to an index patient if either of the following criteria were met: -there was effective contact between the index patient and the secondarily infected patient, either as recalled by the index patient or the secondarily infected patient, or documented in medical notes (eg. of physical examination of an index patient by a healthcare worker who became secondarily infected) -there were movement records indicating that effective contact could have occurred between an index patient and a secondarily infected patient, with the same movement records indicating that no other index patients could be an alternative source of infection Eg. an index patient sharing the same room/cubicle as a secondarily infected patient, even if neither patient could give a history of direct contact with each other, as long as no other infectious SARS cases were detected in the same general area In situations where a secondary infection could be linked to more than one index case, the first index patient with whom the secondary infection came into effective contact with was considered to be the source of the infection. Effective contact was defined as conversation or physical contact within a 1 metre distance, where the index patient had to already be symptomatic from SARS, and the onset of symptoms for the secondarily infected patient had to be 2 to 10 days after the exposure which presumptively led to the infection. The above criteria were then used to classify all index patients as to whether the index patient had transmitted SARS to one or more other individuals, which was our outcome of interest. In the case-control analysis, index patients who caused at least one secondary infection were analysed as "cases"; all index patients who did not cause any secondary infections were analysed as "controls". Exposure variables of interest included the demographic, epidemiological and clinical features associated with individual index patients. Epidemiological variables were the context of the exposure to SARS for that index patient, the date of illness onset relative to the implementation of per-sonal protective equipment (PPE) across the entire hospital on 22 nd Mar 2003, delayed isolation, and admission to a non-isolation ward. We defined delayed isolation as admission to isolation wards on Day 7 of illness or later, as an exploratory analysis of the data showed a sharp rise in the proportion causing secondary infections from Day 7 (Figure 1 ). Isolation wards were facilities where visitors were disallowed, and where healthcare staff wore specialized personal protective equipment while managing patients. All other wards are hereafter referred to as nonisolation wards. This included general wards and intensive care units (ICU) and high dependency (HD) wards where isolation precautions had not been instituted by the time the index patient for that ward was admitted. Clinical signs and symptoms, as well as laboratory and radiological investigations, were obtained from a clinical database maintained for all probable SARS patients admitted to TTSH. Index patients were coded on whether key clinical signs and symptoms were absent or present on the date the patient was moved to isolation. For laboratory investigations, we used the results of tests carried out closest in time to the date of isolation. Co-morbid chronic illnesses were grouped as those with possible impact on respiratory function (pre-existing ischemic heart disease/ congestive cardiac failure, chronic obstructive pulmonary disease), and those that could suppress the immune status (diabetes mellitus, chronic renal failure, malignancies, chronic immunosuppressive therapies). We then identified factors significantly associated with secondary transmission through a univariate analysis. In the multivariate analyses, we started from the most significant factor identified in the univariate analysis, and then performed a forward step-wise regression, using the likelihood ratio test to see if inclusion of the next covariate significantly improved the fit of the multivariate model. Odds ratio and their 95% confidence intervals are presented as estimates of the effect sizes. The final multivariate model was then used to model the probability of transmission for combinations of individual patient characteristics. All statistical analyses were performed using Stata V9.0 (Stata Corp, Tex) for Windows, with the level of significance set at 5%. Ethical approval for the clinical database and its subsequent use in this study was granted by the TTSH Hospital SARS Clinical Management Workgroup and the institutional representative for the hospital.  In all, 106 individuals satisfied our study inclusion criteria of being part of the chain of transmission within TTSH. Epidemiological investigations identified secondary transmission in 24 index patients, including 4 which were involved in super-spreading events; 10 of the 24 index patients caused secondary transmission within TTSH itself, while another 14 index patients caused secondary transmission in other healthcare institutions and/or the community. Two index patients with secondary transmission, and six index patients without secondary transmission, had to be excluded due to missing information on key variables. In all, 22 index patients who transmitted the disease were analysed as "cases", against 76 index patients with no identifiable secondary transmission serving as the "controls". Table 1 presents the characteristics of our study population. In all, 58% were infected in the course of their duty as healthcare workers. Another 10% were infected while admitted as inpatients of TTSH for unrelated medical conditions and, with the exception of the imported index patient, the remainder was infected while visiting friends and relatives in TTSH. 90% of our cases eventually developed radiographic abnormalities, and positive confirmatory tests for SARS-CoV infection were available for 93% of the study patients. Median values for the laboratory investigations fell within the accepted reference ranges; for all subsequent analysis, we compared index patients with the most extreme quartile of results against all other index patients. Table 2 compares index patients with and without transmission. None of the demographic characteristics were significantly associated with increased risk of transmission. However, index patients originally infected as inpatients were more likely to transmit the disease. Delay to isolation and admission to a non-isolation facility were both strongly associated with transmission. In terms of clinical characteristics, pre-existing chronic respiratory disease and immunosuppressive disease were more likely to be associated with increased risk of transmission. There were also indications that index patients with more severe illness were more likely to transmit the disease; while 4/ 22 (18%) of the index patients who transmitted the disease had been admitted to ICU or HD by the time of isolation, this was observed in none of the index patients who did not transmit the disease (p = 0.002, Fisher's exact test). Moreover, the need for oxygen, shortness of breath, vomiting, and higher lactate dehydrogenase levels and higher neutrophil counts, were all more frequent in index patients who transmitted the disease. The results of the multivariate logistic regression are presented in Table 3 . On multivariate analysis, the strongest predictor of transmission was admission to a non-isolation ward, followed by delay in isolation, and having a lactate dehydrogenase value >650IU/L. No significant interactions were identified between these three covariates. We used the coefficients given in Table 3 to construct a tool for predicting if an index patient would have caused transmission by the time he/she is identified and isolated. The predicted probabilities (p i ) of transmission in Table 4 are calculated for the i th individual (i = 1 to 98), using the equation below, where x1, x2, and x3 are indicator variables taking on the values of 1 for those with delayed isolation, admission to a non-isolation ward, and lactate dehydrogenase > 650 IU/L respectively; the term takes the value of 0 in the absence of each respective indicator. The results of the predictive model (Table 4 ) illustrate the wide variability in transmission observed between index patients. While index patients with two or more risk factors have a predicted probability of transmission exceeding 0.5, they comprise only 20% of our study population. If we use the model in a predictive fashion, we see that, under the best-case scenario, when no risk factors are present at the point of isolation, the predicted probability that transmission has occurred is less than 5%. In contrast, in a situation where all the three significant risk factors are present, the model predicts that there is a 97% chance of secondary transmission having occurred. An ROC for the model was plotted (not shown). The area under the curve was 0.903 (95%CI 0.832-0.974), indicating that the model had a very good ability to discriminate index patients with secondary transmission from those with no secondary transmission. Our study confirms previous observations that the majority of SARS transmission is caused by a minority of cases [7] . Moreover, we have identified several key factors associated with an increased probability of transmission, and show that the main determinants were delayed isolation (Day 7 or later), admission to a non-isolation ward, and higher lactate dehydrogenase values at the time of isolation. These findings are important in several ways. lation has been previously noted by Lipsitch and colleagues [14] , we note that the strongest predictor for transmission having occurred is admission to a non-isolation ward. This has previously been suspected based on outbreak descriptions from healthcare settings. For example, in an outbreak of SARS among hospital workers in a community hospital in Hong Kong, it was observed that most staff was infected from exposure to patients with unsuspected SARS in non-isolation wards [15] . Similar observations of nosocomial transmission arising from unsuspected, non-isolated cases of SARS have been made across different countries and different hospital settings. These include other community hospitals [16, 17] , acute care wards [5, 18, 19] , intensive care units [20, 21] , and emergency rooms [22, 23] . While such descriptive accounts may be faulted on the basis of reporting biases, we were able to confirm through this analytic study that index patients admitted without isolation precautions were much more likely to result in secondary transmission of SARS. In addition, we are able to show the association of patient factors with transmission probability. While multiple symptoms and clinical manifestations are significantly associated with transmission in the univariate analysis, variables such as oxygen dependency, prior history of comorbid illness, higher neutrophil counts and lactate dehydrogenase levels are correlated to each other and to disease severity [24, 25] . It was not surprising therefore that, in the multivariate analysis, only the strongest predictor of disease severity is retained. Higher levels of lactate dehydrogenase had previously been found to be one of the best correlates of severe disease [26] . As higher viral loads have also been found to be associated with more severe disease [27] , it would hence be reasonable to hypothesize that higher viral loads may be the underlying mechanism for the increased risk of transmission in patients who are more severely ill. Although no one has directly linked viral load to infectiousness, others have previously observed an association between disease severity in the index patient and disease transmission. Shen and colleagues found an association between disease severity, defined as eventual progression to death, and the extent of transmission, although their findings were based only on a univariate analysis of four super-spreading events against 73 other case-patients [5] . Numerous outbreak reports, including incidents of transmission in intensive care units [19, 21] , the key superspreading event in hotel M [28] , the outbreak in Vietnam [29] , as well as the flight from Hong Kong to Beijing on which 13 passengers were infected [30] , all involved index patients who succumbed to their illness shortly after. It would appear therefore that, not only is disease severity associated with the probability of transmission, but it is also a factor in the intensity of transmission. The link may lie in the number of susceptible contacts exposed to such severely ill patients, and follow-up studies on the Singapore data are in progress to verify this hypothesis. Our findings have several key implications. Firstly, because admission to non-isolation wards was the most important predictor of secondary transmission, good case-detection and infection control at the point of entry into the inpatient healthcare setting is a priority area in the management of SARS outbreaks. Secondly, we have shown through our predictive model that there is a vast and real difference in the risk of transmission between different index patients. In the majority of cases, with no risk factors, the predicted probability of secondary transmission is small, whereas in those with a combination of the various factors, the risk of transmission is many times greater. Moreover, as Lloyd-Smith and colleagues pointed out, this variability in transmission risk amongst cases is not restricted to SARS, but may be common to other infectious diseases [8] . This raises the possibility that management of future outbreaks of emerging and re-emerging infectious diseases should include real-time analysis of clinical and epidemiological correlates similar to the one we have performed, so as to facilitate targeted outbreak interventions. We acknowledge several limitations and weaknesses in our study. Firstly, the assessment of epidemiological linkages was partially subjective, and dependant on the information volunteered by index patients and family members. However, close to 7 out of every 10 infected contacts were healthcare workers and household members (data not shown), for which epidemiological linkages were obvious, and we are hence confident that the 22 index patients analysed as "cases" did transmit SARS to at least one other individual. The key weakness of our study therefore, is whether some of the index patients analysed as "controls" actually caused undetected transmission. With regards to this, we note that, within TTSH itself, 18 of 105 individuals infected in TTSH could not be definitely linked to a single index patient by the criteria we used. Of these, 3 were suspected to be linked to 3 of the index patients which were already classified as "cases" on the basis of having infected other individuals, 10 were exposed to multiple index cases within one single ward where a super-spreading event had occurred, and in 5 individuals we could not trace the infection to a specific source case or even a definite location within TTSH. In addition, any of our 76 controls could also have caused asymptomatic secondary infections, and we hence acknowledge that some bias from misclassification of "cases" as "controls" would be inherent in our study. However, we believe that comparing the index patients which caused secondary transmission against a set of imperfect controls is a superior approach to simply drawing qualitative conclusions on possible factors associated with transmission based on descriptions of index patients with transmission, and we have done so while attempting to minimise biases by the use of fairly stringent criteria in linking any index patients and secondary infections. We also acknowledge that recall and interviewer bias could have affected ascertainment of symptomatology and the timing onset. However, the other key findings of admission to a non-isolation ward and having higher lactate dehydrogenase levels are fairly objective criteria. Thirdly, the number of patients who transmitted was small. Consequently, the study may have lacked power to identify some important risk factors. The small number of events also contributed to the wide confidence intervals for some of the odds ratios. Also, unmeasured confounders could have affected the results of the study. In particular, other than including a variable for admission to non-isolation wards, we did not measure the characteristics of exposed susceptible individuals and the environmental circumstances surrounding the index patients. These would have included the size and configuration of households exposed to these index patients, and for patients admitted to non-isolation wards, the type of wards and any procedures the index patients were subjected to, as these may have resulted in differences in opportunities for transmission. Household factors have been explored in other work [10, 11] but hospital-related factors remain a subject for future research, and would require a larger dataset of index patients admitted to non-isolation wards, or methodologies treating entire wards rather than individual index patients as the unit of analysis. In the absence of more detailed data and a larger dataset on household and hospital-related circumstances surrounding the index patients, we have opted simply to group the combined effect of transmission in these settings, and hence cannot rule out the possibility that unmeasured factors in the household and hospital environment may have confounded our findings. However, while we could not adjust for these potential confounders, it must be noted that those factors found to be significant in the multivariate analysis had reasonably high odds ratios, indicating that any residual confounding are less likely to affect our conclusions. Finally, we acknowledge that the predictive model derived from the study needs to be validated externally against retrospective data from other countries, or through prospective data, in the unfortunate event that SARS re-emerges. This is particularly since the infectiousness of the virus is likely to vary between strains. Interstrain variations in infectiousness was not likely to be an important confounder in our study, since all the index cases here were within a few generations of the original imported case which started the outbreak in Singapore. However, any re-emergence of the virus would possibly involve a strain with fairly different features, and hence extreme caution should be exercised in generalising our results to any future outbreaks. However, the final model presented in this study does explain to a large extent the variability in transmission that was observed during the outbreak, and the prominent role of healthcare institutions and severely ill patients in the propagation of the outbreak. In the event of a future outbreak of SARS, our analysis provides a starting point for risk assessment and risk communication. Our study also makes the case for the timely collation and correlation of clinical with epidemiologic data. In any future outbreak of emerging and re-emerging infectious diseases, such analyses may well allow index patients with higher transmission probability to be identified, and consequently facilitate better targeting of outbreak control measures. In this observational study amongst probable SARS patients, we have found that delayed isolation, admission to a non-isolation ward, and higher lactate dehydrogenase levels were independently and significantly associated with increased transmission risk. Our paper is the first analytic study to demonstrate the above associations while adjusting for possible confounders. Our final regression model based on these three factors, while not perfect, does explain some of the variability in transmission between cases. It also hints that a similar approach may be possible for other emerging and re-emerging infectious diseases, if data on epidemiological data on transmission can be linked to clinical and laboratory data in real-time. 
paper_id= ffe2156eae2e795ce8546944bc07cd8e1df073f4	title= Revisiting hydroxychloroquine and chloroquine for patients with chronic immunity-mediated inflammatory rheumatic diseases	authors= Edgard  Torres;Reis  Neto;Adriana Maria Kakehasi;Marcelo  De Medeiros Pinheiro;Gilda Aparecida Ferreira;Cláudia  Diniz;Lopes  Marques;Maria  Licia;  Henrique Da Mota;Eduardo  Dos;Santos  Paiva;Gecilmara  Cristina;Salviato  Pileggi;Emília Inoue Sato;Ana Paula Monteiro;Gomides  Reis;Ricardo Machado Xavier;José Roberto Provenza;	abstract= Hydroxychloroquine and chloroquine, also known as antimalarial drugs, are widely used in the treatment of rheumatic diseases and have recently become the focus of attention because of the ongoing COVID-19 pandemic. Rheumatologists have been using antimalarials to manage patients with chronic immune-mediated inflammatory rheumatic diseases for decades. It is an appropriate time to review their immunomodulatory and anti-inflammatory mechanisms impact on disease activity and survival of systemic lupus erythematosus patient, including antiplatelet effect, metabolic and lipid benefits. We also discuss possible adverse effects, adding a practical and comprehensive approach to monitoring rheumatic patients during treatment with these drugs. 	body_text= Hydroxychloroquine (HCQ) and chloroquine (CQ), known as antimalarial (AM) drugs, are widely used in the treatment of rheumatic disorders, especially in immune-mediated such as systemic lupus erythematosus (SLE), cutaneous lupus [1] [2] [3] [4] and rheumatoid arthritis (RA) [5, 6] . Besides that, both the Brazilian Society of Rheumatology (SBR) and the European League Against Rheumatism (EULAR) recommend, in specific circumstances, the use of HCQ for primary Sjögren syndrome (pSS) [7, 8] and antiphospholipid syndrome (APS) [9] . HCQ is currently preferred over chloroquine as it has a better safety profile [10] , especially regarding the risk of retinopathy [11] . In this narrative review, the mechanism of action of these medications, as well as their main clinical, biological and safety effects in patients with chronic immune-mediated inflammatory rheumatic diseases (CIMID) will be discussed. Therefore, studies of these drugs related to COVID-19 will not be addressed in this review. The new scenario of COVID-19 pandemic brought many medical challenges to physicians and health care systems. In view of this situation, The Brazilian Society of Rheumatology established a team of specialists from its commissions to respond to the demands related to the topic, especially those come from the Brazilian Ministry of Health. The discussion about the possible use of AM in SARS-cov2 infection showed the opportunity to revisit the topic by rheumatologists. A writing committee started gathering published research and analyzed it carefully. After discussion and debate, the committee members agreed on what would be the most useful knowledge to be highlighted about AM for rheumatologists, and prepared this manuscript. In a time of rapid response to a public health emergency, this type of document needed to be produced quickly and was evidence-informed, but not supported by complete evidence reviews. CQ is a 4-aminoquinoline known since 1934, discovered in the first half of the twentieth century as an effective substitute for quinine. Currently, CQ is the drug of choice for the treatment of malaria [12] . Hydroxychloroquine is a hydroxylated analogue of CQ that has both antimalarial and antiinflammatory activities (Fig. 1 ). These two molecules enter cells as non-protonated forms and become protonated, inversely proportional to pH, according to Henderson-Hasselbach's law. Therefore, these drugs are concentrated in acidic organelles, including endosomes, lysosomes and Golgi vesicles, increasing the pH [13] . Both drugs are weak bases and have a large volume of distribution with a half-life of about 50 days. These drugs interfere with lysosomal activity and autophagy, interact with membrane stability and may alter signaling pathways and transcriptional activity, resulting in inhibition of cytokine production and modulation of certain costimulatory molecules. At the cellular level, they inhibit the Toll-like receptors signaling and reduce the CD154 molecule expression in T cells. Effects on plasmacytoid dendritic cells (pDCs), B cells and other antigen presenting cells have also been described [13] . HCQ is administered as a sulfate while chloroquine is administered as a phosphate salt. The differences between the pharmacokinetic properties of CQ and HCQ are presented in Table 1 . The exact mechanism of action of HCQ and CQ in the treatment of CIMID is not yet fully understood, but there is strong evidence that they have an immunomodulatory and antithrombotic effect [13, 14] . The proposed mechanisms to explain these effects are (Fig. 2) : Alkalinization of lysosomes and other intracellular acid compartments with interference in phagocytosis. The increase of intracellular pH causes a selective change in the presentation of proper antigens; Blockage of T-cell response and reduction of proinflammatory cytokine production, including INF-γ, TNF, IL-1 and IL-6; Blockage of Toll-like receptors 7 and 9, especially in plasmacytoid dendritic cell with inhibition of INF-α, which plays an important role in the pathophysiology of SLE; cGAS-STING signaling inhibition; Inhibition of phospholipase A2 activity; Stimulation of nitric oxide production by endothelial cells with antiproliferative effect; Antithrombotic effect through the inhibition of platelet aggregation in a dose-dependent manner, decreased production of arachidonic acid by activated platelets and action on antiphospholipid antibodies. Action on glucose metabolism and lipid profile as a non-immunomodulatory mechanism This class of medication has been used in the treatment of SLE for more than 50 years. It is a chronic autoimmune inflammatory disease that can affect several organs and systems and has a variable incidence, with 8.7 cases/100,000 inhabitants in Brazil [15] . It mainly affects young women aged from 15 to 45 years-old with heterogeneous and pleomorphic clinical manifestations [16, 17] . In 1976, Urowitz et al. described a bimodal mortality pattern in lupus patients, with premature deaths related to disease activity or infection, while late rate was more associated with atherosclerotic disease [18] . Considering the improvements in diagnosis and treatment, as well as the reduction of complications related to disease itself or its own treatment, the survival rate has increased in the two last decades [19] . The treatment of SLE patients may be individualized and targeted, according to the disease activity and severity. Additionally, patient education about the disease, sun exposure protection, regular physical exercise, diet, treatment of comorbidities (hypertension, diabetes, dyslipidemia, osteoporosis), avoiding smoking and performing adequate contraception and vaccines are important approaches and should be stimulated for all patients. The main goals of treatment in SLE are to increase long-term survival, to induce and maintain remission, to prevent damage and to improve quality of life [10] . CQ or preferably HCQ should be always recommended for lupus patients, regardless other immunosuppressive medications and severity or type of clinical manifestations, except if some contraindication or previous toxicity [10, 11] . Both of them promote multiple benefits, including direct or indirect effects [20] , such as reducing disease activity and new flares [21] ; improvement of skin lesions and joint symptoms [22, 23] ; prevention of accrual damage [24, 25] ; possible mortality risk reduction [26, 27] ; as well as some benefits on glucose and lipid metabolism and reduction of thrombotic phenomena [14] . AMs are widely used and recommended for the treatment of SLE, since they promote an immunomodulatory effect of the immune response and better control of disease activity [2] . Tsakonas et al. demonstrated 57%-risk reduction of severe activity in quiescent lupus patients after HCQ, suggesting some prevention benefit on disease activity (RR = 0.43; 95% CI 0.17-1.12) [21] . The Canadian Hydroxychloroquine Study Group randomized 47 lupus patients, who were with stable dose of HCQ, to maintain on (n = 25) or to switch to placebo (n = 22). After 6-month HCQ withdrawal, there was significant 2.5-increase of SLE activity (95% CI 1.08-5.58; p = 0.02). Interestingly, there was a non-significant higher risk for severe activity, including vasculitis, transverse myelitis and nephritis, in those that had stopped the medication (RR = 6.1; 95% CI 0.72-52.54) [28] . Additionaly, some other studies have shown clinical worsening after drug discontinuation [29] . Regarding lupus nephritis, Petri et al. have demonstrated higher remission rate in 450 patients using HCQ when compared to non-users after 1-year follow-up (64% vs. 22%; p = 0.036) [30] . Also, the HCQ was a stronger predictor of complete renal remission in lupus patients when combined to mycophenolate than mycophenolate in monotherapy [31] . Thus, the SBR, the ACR, and the EULAR consensus and recommendations for treating lupus patients have recommended the HCQ as an adjunctive treatment [1] [2] [3] [4] . Several studies have found relevant damage accrual in SLE patients [24, [32] [33] [34] [35] . Accordingly to the LUMINA study, HCQ users had lower risk of developing new damage in patients with less than 5 years of disease (HR = 0.73; CI 95% 0.52-1.00; p = 0.05), especially in patients with no damage at baseline (HR = 0.55; CI 95% 0.34-0.87; p = 0.011) [24] . Another LUMINA analysis in 203 patients with lupus nephritis without renal damage found that HCQ delayed the onset of kidney failure (HR = 0.12; CI 95% 0.02-0.97; p = 0.046). The accumulated kidney damage was higher in HCQ non-users in class IV lupus nephritis [35] . Petri et al. evaluated 2054 patients and found that age, hypertension and use of corticosteroids were main predictors of damage, while HCQ had a protective effect (p = 0.06) [34] . HCQ reduces platelet aggregation and its antithrombotic effect can be explained by the reduction of the formation of antiphospholipid-β2-glycoprotein complexes on monocytes surfaces [36] with protective effect in patients with SLE [26, [37] [38] [39] [40] (OR = 0.17; CI 95% 0.07-0.44; p < 0.0001) [38] and HR = 0.28; CI95% 0.08-0.90) [26] . Jung et al. compared 54 patients with SLE and prior thrombosis with 108 lupus patients with no thrombosis and demonstrated that AM were associated with lower risk of thrombotic events, both arterial and venous (OR = 0.31; CI 95% 0.13-0.71) [41] . In vitro and experimental models demonstrated that HCQ improves insulin secretion and peripheral insulin sensitivity [14] . Penn et al. found HCQ was associated with lower fasting glycemia and Homeostatic Model Assessment (HOMA) index in patients with SLE [42] . Moreover, AM in monotherapy or associated with glucocorticoids (GC) have also improved the lipid profile in lupus patients because they provide hepatic cholesterol synthesis reduction with inhibition of lysosomal function, as well as lysosomal cholesterol transport and metabolism blockage. Other explanations are related to lower LDL receptor activity and bile steroid precursors and HMG-CoA reductase function gain [14] . Besides that, chloroquine diphosphate increases low-density lipoprotein removal from plasma in SLE patients [43] . Petri et al. found that HCQ was associated with lower total cholesterol serum levels, regardless dosage (200 mg or 400 mg/day), and it was able to mitigate the deleterious prednisone effect (10 mg/day) on total cholesterol [44] . Rahman et al. reported 4.1%-reduction of total cholesterol serum levels after starting AM in 3 months (p = 0.02). In 181 patients using GC and AM, the mean total cholesterol was 11% lower than in 201 patients receiving comparable dosage of GC (p = 0.002) [45] . Cairoli et al. demonstrated a significant decrease in total cholesterol (198 ± 33.7 vs. 183 ± 30.3 mg/dL; p = 0.023) and LDL levels (117 ± 31.3 vs. 101 ± 26.2 mg/dL; p = 0.023) after the 3 months of HCQ therapy in SLE patients which determined a significant decrease in the frequency of dyslipidemia (26% vs. 12.5%; p = 0.013) [46] . A recent systematic review and meta-analysis involving data from nine studies and 823 participants has stated that HCQ significantly reduced mean total cholesterol plasmatic levels (26.8 mg/dL; 95% CI 8.3-45.3), as well as mean LDL serum levels (24.3 mg/dL; CI95% 8.9-39.8). However, it is important to note that other studies had an extensive heterogeneity among them, including lack of information about statin use [47] . Similarly, there are controversial data regarding HDL status [14] . Ruiz-Irastorza et al. evaluated a cohort with 232 lupus patients (64% on AMs). Among 23 patients who died, 19 (83%) had never received AMs. The cumulative 15-year survival rate was higher in those using AM drugs (0.98 vs. 0.15; p < 0.001) [26] . Shinjo et al., analyzing 1480 patients from the GLADEL (Grupo Latino Americano para Estudo do Lupus) found lower mortality rate in AMs users for at least six consecutive months (4.4% vs. 11.5%; p < 0.001). In addition, the protective effect on mortality rate increased according to longer exposition time to AMs [6 to 11 months: 3.85 (95%CI 1.41 to 8.37); 12 to 24 months: 2.7 (95%CI 1.41 to 4.76); and more than 24 months: 0.54 (95%CI 0.37 to 0.77)]. After adjustment to potential confounders, AMs were associated with a 38% reduction in mortality (HR = 0.62; 95%CI 0.39-0.99) [27] . The use of CQ and HCQ is not only allowed but is recommended during pregnancy and lactation in SLE patients [14] . A HCQ placebo-controlled study suggested beneficial effect on disease activity [48] and the interruption of HCQ was related to higher risk of flares during pregnancy. In other words, AMs are recommended during the preconception period, pregnancy and lactation [49] . The presence of anti-Ro/SSA and anti-La/SSB antibodies are associated with 1 to 2% risk of congenital total atrioventricular block. When there is a maternal history of an affected fetus or child, the recurrence rate can increase 13 to 18%. HCQ is associated with lower occurrence of neonatal cardiac lupus, especially if recurrent [50, 51] . More recently, a systematic review and metaanalysis involving 6 studies and 870 pregnancies have found no difference concerning prematurity and restricted intrauterine growth in lupus patients exposed (n = 308) or not exposed (n = 562) to HCQ. It is important to emphasize that these results should be addressed with caution due to huge heterogeneity among the studies [52] . AMs are important as adjunctive therapy to treatment with disease-modifying drugs (DMARDs) in RA, including recommendations for treatment of SBR [5] and ACR [6] . HCQ has been shown to improve clinical and laboratory findings in RA, particularly in early and mild disease, although there was no protective effect on radiographic progression [53] . Because of its good safety profile, it currently being studied for the prevention of future onset of rheumatoid arthritis (RA) in individuals who have elevations of anti-cyclic citrullinated peptide (anti-CCP3) antibodies [54] . Similarly to data from lupus patients, most of effects are also seen in RA, including improvement in lipid profile and insulin resistance [55, 56] . In a multicenter study with 4905 RA patients, Wasko et al. demonstrated that HCQ was associated with lower risk of diabetes mellitus (HR = 0.62; 95% CI 0.42-0.92) [56] and could be used for controlling traditional cardiovascular risk factors [57] . Antimalarial drugs may be used to treat sarcoidosis, including cutaneous sarcoidosis, pulmonary sarcoidosis, neurosarcoidosis, and arthritis [58] . Although less effective than in patients with SLE, AM can be useful in for cutaneous manifestations in dermatomyositis [59] . AMs are usually effective, safe, and well tolerated. According to the SBR, the Brazilian Society of Dermatology and The Study Group on Inflammatory Bowel Diseases, patients with CIMID on AMs are considered as nonimmunosuppressant medications [60] . There is no increased risk of infections or even neoplasms in the short-and long-term [61] . More frequently the adverse events are related to gastrointestinal complaints, such as abdominal pain, nausea, vomiting and diarrhea. To decrease these adverse effects, the HCQ can be taken once or twice daily with a meal [62] . Patients with psoriasis, porphyria and alcoholism may be more susceptible to adverse skin events, usually without severity. In rare cases, hemolysis may occur in patients with glucose-6-phosphate dehydrogenase deficiency [63] . Besides G6PD deficiency, the concomitant use of HCQ with dapsone may enhance the risk of hemolytic reactions [64] . There is no current recommendation to reduce the dose of HCQ in patients with chronic kidney disease [13] . Some experts recommend reducing the dose of chloroquine phosphate by 50% if the glomerular filtration rate is less than 10 mL/minute, and in hemodialysis or peritoneal dialysis patients [65] . Neither safety nor efficacy of HCQ has been established for chronic use in children for juvenile idiopathic arthritis or for juvenile SLE. The main adverse events related to the use of AM are summarized in Fig. 3 . Both CQ and HCQ can cause ocular deposition, an effect more associated with CQ. Retinal changes are related to lysosomal degradation of the external photoreceptor with lipofuscin accumulation in retinal pigment epithelium [66] . Once symptomatic, the retinopathy associated with AM is characterized by abnormalities of the retinal pigment epithelium, which are detectable clinically, and may later develop into the classic appearance of 'bull's eye maculopathy' with retinal pigment epithelial loss. At this stage the visual loss is severe and irreversible and may be complicated by secondary cystoid macular oedema, epiretinal membrane and other sequalae [67] . Although rare, the retinopathy is one of main adverse events related to AMs [14] . Considering the recommended dosages, the 5-year, 10-year and 20-year toxicity risk is lesser than 1%, below 2 and 20%, respectively. After 20 years of use, the risk increases 4% each year for those no previous toxicity [11] . More recently, the hydroxychloroquinemia has been reported as a risk factor for retinopathy in 537 lupus patients (total prevalence = 4.3%) [68] . Other risk factors associated with retinopathy were age, duration of use and high body mass index (BMI). In 2016, the American Society of Ophthalmology updated its recommendations for retinopathy screening in CQ or HCQ users. According to them, the maximum daily dosage of HCQ should be ≤5 mg/kg. The main risk factors for ocular toxicity are daily dose above the recommended, duration of use, renal failure, previous maculopathy or retinopathy and concomitant use of tamoxifen. Other risk factors include advanced age, liver failure and genetic factors related to abnormalities of the ABCA4 gene or cytochrome P450. It is recommended that patients initiating the drug undergo eye examination within the first year of treatment. Although visual field examination and spectral-domain optical coherence tomography (SD OCT) are very useful, they are not mandatory at the beginning of treatment, unless the Fig. 3 Main adverse events related to the use of antimalarials patient has risk factors or other diseases that may affect the initial screening tests. In the absence of major risk factors, screening tests may be performed annually after 5 years of baseline assessment. If risk factors are present, screening tests should be performed annually or at shorter intervals soon after beginning AMs, and automated visual field assessment and OCT-SD are recommended. Additional tests in some situations may be indicated, such as the multifocal electroretinogram (mfERG), which provides objective information of visual field, especially in Asian patients [11] . The use of antimalarials may provoke adverse dermatologic effects of varying severity, being drug eruptions or rashes the most common [69] . Both CQ and HCQ bound to melanin and can deposit on the skin, with the possibility of cutaneous hyperpigmentation (grayish color) in long-term, especially with CQ [66] . A study that compared acitretin with HCQ for the treatment of cutaneous lupus found around 27% of patients with dry skin complaints; itching and burning sensation on the skin in 17%; dermatitis in 3% and desquamation in 3% of those using HCQ [70] . Also, grayish pigmentation of the skin and oral mucosa has been associated with longer use, higher levels of hydroxychloroquinemia, as well as the use of acetyl salicylic acid and oral anticoagulants, sometimes with reports of microtrauma and local ecchymosis preceding hyperpigmentation [71, 72] . Cases of worsening psoriasis are also described with the use of medication [73] . Acute generalized exanthematous pustulosis is rare and described in 1/5,000,000 inhabitants [74] . A recent systematic review including ninety-four articles, comprising a total of 689 adverse dermatologic side effects, has shown that drug eruption or rash (358 cases) were the most frequent, followed by cutaneous hyperpigmentation (116 cases), pruritis (62 cases), acute generalized exanthematous pustulosis (27cases), Stevens-Johnson syndrome or toxic epidermal necrolysis (26 cases), hair loss (12 cases), and stomatitis (11 cases) [69] . Although rare, it can be a serious adverse event [75] . Both cardiomyopathy and conduction disorders (for example, QT prolongation) are described. A possible mechanism involves a lysosomal pathway dysfunction with metabolite products (glycogen and phospholipids) intracellular accumulation [76] . A systematic review about CQ and HCQ cardiotoxicity found 86 articles, comprising only 127 patients in case reports or small case series, most of them were SLE (n = 49) or RA patients (n = 28). Most patients (58.3%) were treated with CQ with a median time of use of 7 years (3 days to 35 years) and median cumulative dose of 803 g (1235 g for HCQ). Heart rhythm problems were the main reported side effects, affecting 85% of patients. Other non-specific cardiac events included ventricular hypertrophy, hypokinesia, valve dysfunction and pulmonary arterial hypertension. It is worth mentioning that 38 cases were classified as probably related to adverse drug events, 69 as possibly associated and in 20 cases it was not possible to indicate this association. It was not possible to classify this association as definitive for any case, using the Naranjo Scale. The authors could not definitively exclude the possibility that some cardiac complications were due to the disease itself or to differential diagnoses (Fabry disease, for example). Determination of the risk for cardiac complications attributed to the medications was not possible because of the lack of randomized controlled trials [75] . Other studies suggest that older age, duration of medication use, dosage above that recommended by weight, use of CQ instead of HCQ, pre-existing heart disease and renal failure may be risk factors for medication cardiotoxicity. In addition, the risk may be greater in those who use other medications that also lead to prolongation of the QT interval or that increase the serum level of QC [77] [78] [79] . A study suggested that SLE patients using AM drugs with persistently elevated creatine phosphokinase (CPK) should be monitored periodically and specific biomarkers, such as troponin and brain natriuretic peptide (BNP), may be useful as a screening tool for cardiotoxicity diagnosis by AMs. The electrocardiogram, echocardiogram and magnetic resonance imaging can provide more information in suspicious cases, as well as endomyocardial biopsy, if necessary [80] . At the moment, there are no consensus and guidelines which are the best methods and interval to monitor cardiotoxicity with chronic use of AM. On the other hand, it is important to highlight HCQ and CQ have a protective effect on cardiovascular risk, anti-thrombotic mechanisms and on survival rate in lupus patients. It has been described in a few cases, especially associated to CQ. Patients with myopathy have proximal muscle weakness without myalgia or muscle enzymes changes (or slightly elevated more rarely). Patients can improve with medication discontinuation [63] . Central nervous system toxicity includes headache, dizziness, vertigo and tinnitus. There are rare case reports of seizures related to reduction of seizure threshold and psychosis, especially when combined to GC. Neuromyopathy and peripheral polyneuropathy are also rare, occurring in patients with worsening renal function and using CQ [62, 63] . HCQ and CQ are substrates for cytochrome P450 enzymes, responsible for the metabolism of multiple drugs. Cytochrome P450 enzymes dealkylate AMs to their active metabolites. Thus, the concomitant use of AMs can lead to increased levels of digoxin, cyclosporine and metoprolol [62] . HCQ can reduce gastrointestinal absorption of methotrexate, since it alters the local pH and it can explain lower toxicity of methotrexate when combined. Antacids may decrease oral bioavailability of CQ [13] . Special attention should be given to other concomitant drugs, such as macrolides, quinolones, some antivirals and antipsychotics, because they can also lead to QT interval enlargement (Fig. 4) [13, 81] . Since this is a narrative review, it is not possible to make formal recommendations, but suggestions for monitoring and proposal of key messages are valuable, and provide information about AM use for health-care providers, especially rheumatologists. These key messages are depicted in Table 2 . Given its multiple benefits, the use of AMs, preferably HCQ, should be encouraged to SLE patients, unless there is any contraindication. In other diseases like RA, pSS, APS, dermatomyositis and sarcoidosis some studies also show positive data, especially under specific circumstances. The majority of the side effects occur after a wide range of cumulative dosages. It is a low-cost and widely available medication, whose safety profile is well known and acceptable. In addition, considering its pharmacokinetic properties (long half-life), it is possible to measure its serum concentration as a marker of treatment adherence and potential long-term toxicity, when necessary and available. Table 2 Key messages regarding safety of treatment with antimalarial drugs -Daily dose not greater than 5 mg/Kg -Regular screening for retinal toxicity according to risk factors -Monitoring of complete blood count at the beginning and during prolonged therapy -Physical examination with attention to muscle strength and reflexes -Monitoring of QT interval prolongation in at-risk patients -Caution in hepatic and renal impairment, use of other medications that lead to prolongation of the QT interval or that increase the serum level of antimalarials, alcoholism, concurrent antidiabetic agents, porphyria, psoriasis. 
paper_id= ffe3811f22f7d5fb42e77092ba79cc5f6aac8a27	title= Corticosteroids treatment in severe patients with COVID-19: a propensity score matching study	authors= Qian  Chen;Yang  Song;Lu  Wang;Yipeng  Zhang;Lu  Han;Jingru  Liu;Mengyu  Yang;Jingdong  Ma;Tao  Wang;	abstract= To explore the efficacy of corticosteroid treatment in patients with severe COVID-19 pneumonia and the association between corticosteroid use and patient mortality. Methods: A retrospective investigation was made on the medical records of the patients with severe and critical patients with COVID-19 pneumonia from January to February 2020. First, the patients who received corticosteroid treatment were compared with patients without given corticosteroid treatment. Then, a propensity score matching method was used to control confounding factors. Cox survival regression analysis was used to evaluate the effect of corticosteroid therapy on the mortality of severe and critical patients with COVID-19. Results: A total of 371 severe and critical patients were included in our analyses. Two hundred and enine patients were treated with corticosteroid therapy. Most of them were treated with methylprednisolone (197[94.3%]). The median corticosteroid therapy was applied 3 (IQR 2-6) days after admission, 13 (IQR 10-17) days after symptoms appeared. Temperature on admission (OR = 1.255, [95%CI 1.021--1.547], p = 0.032), ventilation (OR = 1.926, [95%CI 1.148-3.269], p = 0.014) and ICU admission (OR = 3.713, [95%CI 1.776-8.277], p < 0.001) were significantly associated with corticosteroids use. After PS matching, the cox regression survival analysis showed that corticosteroid use was significantly associated with a lower mortality rate (HR = 0.592, [95%CI 0.406-0.862], p = 0.006). Conclusion: Corticosteroid therapy use in severe and critical patients with COVID-19 pneumonia leads to lower mortality but may cause other side effects. Corticosteroid therapy should be used carefully. ARTICLE HISTORY 	body_text= The coronavirus disease 2019 (COVID-19) has been considered as an urgent public health crisis worldwide with the rapidly increasing number of confirmed cases and death tolls, since the outbreak in December 2019. It is reported that more than 46 million have contracted the disease, around 1.2 million died, in 190 countries or areas up to 2 November 2020 [1] . The outbreaks lead to a huge demand for hospital beds and impose great challenges for physicians as well. However, the clinical courses and predictors for the outcome of the patients remain to be fully investigated. Currently, while no specific antiviral or immunomodulatory treatment for COVID-19 has proven effective, therapies recommended for patients with COVID-19 are largely aligned with that of other viral pneumonia, mostly consisting of a set of supportive care strategies [2] . Data from several clinical observational investigations show a significant fraction of the hospitalization patients with COVID-19 received corticosteroid treatment as supportive care. The proportion of patients received corticosteroid treatment varied from 18.6% to 51.0% [3] [4] [5] [6] [7] depending on the settings and severity of illness. However, the role of corticosteroids in COVID-19 patients is controversial. While the guidance for critical care management from the World Health Organization advocates against their use, there are expert consensus and guidelines incorporate corticosteroids in the clinical management of COVID-19 in severe conditions [8] . For instance, A recent meta-analysis shows that corticosteroid therapy leads to lower 28-day all-cause mortality [9] . Chinese experts consensus recommend short-term therapy with low-to-moderate dose corticosteroids in COVID-19 patients with ARDS [10, 11] . Waleed Alhazzani et al. suggest using systemic corticosteroids in mechanically ventilated adults with COVID-19 and ARDS [12] . The debate on the use of corticosteroids in patients with COVID-19 indicates the knowledge gap in understanding the benefits and associated adverse effects of these clinical interventions [13] . The current knowledge base on corticosteroid treatment in viral pneumonia is largely built upon previous experience with severe patients infected by SARS, MERS, and H1N1, and the treatment effect on clinical outcomes is inconclusive. A retrospective study revealed that proper use of corticosteroids in critical SARS patients was associated with a lowered mortality and shorter length of hospital stay without significant secondary lower respiratory infection and other complications [14] . In an observational study in patients with MERS, corticosteroid therapy did not result in a difference in mortality after adjustment for time-varying confounders but led to delayed MERS coronavirus RNA clearance [15] . Inconsistent results also exist in the studies of influenza viral pneumonia [16, 17] . To date, few studies have investigated the impact of corticosteroid treatment in patients with COVID-19. Experience from Korea suggests that low-dose steroid oral tablets/inhalers at the earlier stage of COVID-19 and highdose steroid treatment according to the severity of the disease can play important roles in decreasing fatality and pulmonary fibrosis [18] . Zheng et al. analyzed 55 medical records of COVID-19 patients and concluded that early and shortterm use of low-dose methylprednisolone was beneficial and did not delay SARS-CoV-2 RNA clearance and influence IgG antibody production [19] . An observational study in 31 patients reported there were no associations between corticosteroid therapy and outcomes in patients without acute respiratory distress syndrome [6] . Recently a systematic review reports that mortality from corticosteroid use in COVID-19 patients with ARDS seems lower than who did not use [20] , and ARDS is an important factor leading to serious consequences and death. However, these findings may not be reliable due to the very small sample size and weakness in study design. Coping with the pandemic of COVID-19 is extremely challenging for clinicians. Those who considering corticosteroids for severe patients with COVID-19 must balance the potential reduction in mortality with the potential downsides. Understanding the evidence for harm or benefit from corticosteroids in COVID-19 is of immediate clinical importance. However, data from studies on corticosteroid treatment in COVID 19 of sufficient sample size and carefully controlling potential confounding factors and biases are lacking. Therefore, the objective of this study is to identify the factors associated with corticosteroid use and its impact on outcomes in severe patients with COVID-19 accounting for potential selection bias by propensity score matching analysis. This is a retrospective observational study. We collected all the 718 cases confirmed with COVID 19 admitted to Zhongfa Xincheng Branch of Tongji Hospital affiliated to Tongji Medical College, Huazhong University of Science and Technology, from Jan 28 to 29 February 2020. Zhongfa Xincheng Branch of Tongji Hospital was temporarily designated to offer 800 beds in refitted isolation wards for severe and critical patients with COVID 19 by the local government during the outbreak. All tests, procedures, therapies were ordered by the attending physician. The study was approved by the Ethics Committee of Tongji Hospital (IRB: TJ-IRB20200353). Patient identities were protected via anonymization, and the requirement for informed consent was waived due to the observational nature of the study. The inclusion criteria were patients with a definite diagnosis of Covid-19 based on the World Health Organization interim guidance; patients with a definite outcome (dead or discharged). The exclusion criteria were patients taking chronic corticosteroid therapy for preexisting health conditions; patients receiving corticosteroid therapy as rescue therapy (i.e. due to shock). Data were collected from the electronic medical records using the standardized International Severe Acute Respiratory and emerging Infection Consortium (ISARIC) case report forms [21] . The extracted variables included patients' demographic characteristics, co-morbidities existing prior to the admission, time of illness onset and hospital admission, signs and symptoms on admission, radiology findings, laboratory test results, medications and supportive care, complications during the hospital stay, and clinical outcomes. Data on the type, maximum daily dose, and duration of corticosteroids were collected. Four internists (Yipeng Zhang, Lu Han, Jingru Liu, Mengyu Yang) performed the data extraction and data entry independently. The differences between them were detected in the database via an R code and finally adjudicated by Tao Wang. Coronavirus disease 2019 (COVID-19) was defined in accordance with the World Health Organization interim guidance. Nasal and pharyngeal swab specimens were collected for detecting SARS-COV-2, which were tested via rt-PCR according to the World Health Organization interim guidance on laboratory testing for COVID-19 in suspected human cases. Severe and critical patients with COVID-19 were defined following Chinese interim guidelines for diagnosis and treatment for COVID-19 patients (version 7.0). Severe patients were those who had at least one of the clinical features including respiratory rate exceeding 30 breaths/minute at rest, oxygen saturation ≤93% without oxygen support, arterial oxygen partial pressure/fractional inspired oxygen (PaO2/FiO2) ≤300 mmHg at rest, the total lesions on chest CT rapid progress ≥50% within 24-48 hours. Critical patients were those who had at least one of the clinical features including respiratory failure requiring mechanical ventilation, presence of shock, complicating with other organ failures. Corticosteroid treatment was defined as the use of systemic corticosteroids ordered by the attending physician. All the dosages were converted into methylprednisolone equivalent doses (hydrocortisone 5:1, dexamethasone 1:5, prednisolone 5:4). High-dose corticosteroid therapy was defined as the highest daily dose of ≥80 mg of methylprednisolone equivalent. Low to moderate dose corticosteroid therapy was defined as the highest daily dose of <80 mg of methylprednisolone equivalent. Early use of corticosteroid was defined as corticosteroid therapy within 7 days after hospital admission. The primary objective of this study is to determine whether corticosteroid use was associated with hospital mortality. Additionally, the primary outcome was examined in the following subgroups according to clinical features on hospital admission: (1) severity of illness (severe group vs critical group based on the previous definition); (2) requiring mechanical ventilation on hospital admission (yes vs no); (3) Asthma/ chronic pulmonary disease (yes or not); and (4) inflammatory response to C-reactive protein (CRP < 25 vs. ≥25 mg/dL). The secondary objective of this study is to investigate risk factors associated with corticosteroid use in severe patients with COVID-19. Hospital length of stay, ICU length of stay, mechanical ventilation days, SARS-COV-2 RNA clearance, and presence of complications were also examined in survivors between groups receiving and not receiving corticosteroid treatment. Continuous variables were presented as means with standard deviation (SD) or medians and interquartile range 25-75% (IQR) and categorical variables as counts (percentage). For demographic variables, baseline clinical characteristics, cointerventions, and outcome variables, differences between patients received corticosteroid treatment during hospitalization and those who did not receive any corticosteroid treatment were compared via Student t-test or Mann-Whitney U test for continuous variables and the chi-square test or Fisher exact test for categorical variables as appropriate. Multiple logistic regression model was used to examine the association between baseline. Variables on hospital admission and corticosteroid use. A set of priority-decided variables of clinical interest [7, 15, 22] and all significant variables at the univariable level (P < 0.2) were included in the multivariate model. The probabilities for the entry and removal of variables in a stepwise forward manner were 0.05 and 0.10, respectively. Model integrity was tested by standard diagnostic statistics and plots; the goodness of fit for each model was assessed with the Hosmer-Lemeshow test. Cox proportional hazards models were used to examine outcomes as a time to event (i.e., death, hospital discharge, SARS-COV-2 RNA clearance, presence of complications), with corticosteroid therapy as a time-varying covariate and adjusting for the same above-mentioned baseline covariates. To minimize the effect of a corticosteroid treatment selection bias and to control for potential confounding factors, a propensity score matching procedure was performed. The treatment group (patients receive corticosteroid treatment) and control group (those who did not receive corticosteroid treatment) were fully matched with weights based on a full-matching propensity score analysis. The propensity scores were determined using multivariate logistic regression models, which included a set of prespecified covariates without considering the outcomes. C-statistics were used to assess the model's discrimination. The treatment and control pairs were matched via the nearest neighbor matching approach. To ensure balanced matches, a caliper, maximum acceptable difference between two groups, was set as 0.2 (i.e., 0.2 X standard deviation of the logit of the propensity scores) resulting in a relatively narrow difference between matched patients. In addition, the paired t-test or Wilcoxon signed-rank test for continuous as appropriate, and the McNemar test for categorical variables was conducted, respectively, to assess the covariate balance between the treatment and control groups. After the matching, a Kaplan-Meier survival plot was generated to track hospital mortality over time for the treatment and control group. And the Cox regression model mentioned above was performed again to examine the impact of corticosteroid treatment on patient outcomes after balancing the potential confounding factors and controlling for selection bias. Finally, a series of subgroup analyses were conducted. Because a corticosteroid therapy effect may be dosedependent, may vary according to the time of initiation, and may vary according to the length of use. We performed all previous models on the following stratified groups: patients who received different doses of corticosteroid therapy, patients who had different corticosteroid initiation time, and patients who had different lengths of corticosteroid therapy. We also compared days for virus clearance between different subgroups and survival time of the death group. All tests were two-sided, and p-values <0.05 were considered statistically significant. Results from logistic or Cox regression analysis were reported as odds ratios (OR) or hazard ratios (HR) with 95% confidence intervals (CIs) as appropriate. Data analysis was performed with the R software package (cran. r-project.org). During the observation period, a total of 762 patients were admitted for COVID-19 infection. After data cleaning, 371 samples met our inclusion criteria of severe or critical patients and were included in this study. Data cleaning path is shown in Figure 1.  Clinical characteristics between the corticosteroid group and the non-corticosteroid group are given in Table 1 . According to the initial condition, laboratory tests, and clinical records, patients with corticosteroid treatment are more severe than patients without corticosteroid treatment. Compared with the noncorticosteroid group, the corticosteroid group had higher LDH (P < 0.001), CRP (P < 0.001), and D-dimer (P < 0.001), and lower platelets (P = 0.025). Antiviral (P = 0.029), interferon (P < 0.001), antibiotics (P < 0.001), and inotropes/vasopressors use (P = 0.001) are more frequent than the non-corticosteroid group. Renal replacement (P = 0.002) and noninvasive ventilation therapy use (P = 0.007) more than non-corticosteroid group. Complications (P < 0.001), ICU admission (P < 0.001), length of hospitalization (P = 0.001), and overall death rate (P < 0.001) are larger than non-corticosteroid group. All of this suggests that patients with corticosteroid therapy are more critical. Two hundred and nine patients received corticosteroid therapy during hospitalization. Of these, 197 (94.3%) patients were treated with Methylprednisolone, 39 (18.7%) prednisone, 6 (2.8%) methylprednisolone tablets, and 1(0.5%) dexamethasone. 1 After admission, the median (IQR) corticosteroid therapy was applied 3 (2-6) days later, 13 (10-17) days after symptoms appeared. Among all patients with corticosteroid treatment, the median duration of corticosteroid therapy is 7(4-12) days and 5(3-8.75) for the survivor group and 10 (7-14) for the death group. Details are shown in Table 2 . To clarify which factors influencing corticosteroid treatment use, we selected indicators for patients before corticosteroid therapy with discretion to build a stepwise logistic regression model. Age, critical, the temperature on admission, ventilation, and ICU admission were included finally. Table 3 .  One hundred and two of 209 (48.8%) patients with corticosteroid treatment died, which mortality rate is higher than the non-corticosteroid group ( To minimization selection bias, a PS-full matching model was applied to our dataset and every sample in the non-corticosteroid group and corticosteroid group was given a PS full matching weights. Age, gender, the temperature on admission, days from symptom onset to admission, critical on admission, LDH≥225 U/L, CRP≥1 mg/L, D-Dimer ≥ 0.5ug/mL, any complications, chronic heart disease, chronic liver disease, chronic renal disease, chronic neurological disease, hypertension, hyperlipidemia, renal replacement therapy, ICU on admission were the independent variables included in the logistic regression analysis of the PS-full matching model. Summaries of balance for matched data and unmatched data are shown in Table 4 . After PS-full matching, we performed a logistic regression model and a Cox regression model to determine the impact of corticosteroid use in severe patients with COVID-19. With PSfull matching, the logistic regression model showed that corticosteroid use is linked to a lower mortality rate (OR = 0.308, [95%CI 0.112-0.771], p = 0.016). Our cox regression model (Table 5 ) and adjusted cox regression survival plot (Figure 3 ) confirm that corticosteroid use was significantly associated with a lower mortality rate (HR = 0.592, [95%CI 0.406-0.862], p = 0.006). To make the results more robust, we performed several subgroup analyses. First, we separated the corticosteroid group by a maximum dosage of corticosteroid use, days from admission to corticosteroid use, and total days of corticosteroid use. Table 4 . Comparison of baseline characteristics between treated and untreated subjects in the original sample and in the propensity score-matched sample. Original Figure 2 part2. Our results confirm that corticosteroid therapy in severe and critical patients with COVID-19 is associated with a lower mortality rate, which is consistent with several recent researches [9, 23] . Although it appears that the mortality rate of the corticosteroid group is higher than the control group, we found that corticosteroid therapy reduces the mortality rate after balancing the condition of patients by PS full matching. Corticosteroids therapy in pneumonia and COVID-19 pneumonia remains a controversy. Corticosteroid has functions of anti-inflammatory, anti-allergic and hypothermic. Cytokine release syndrome (CRS) in COVID-19 often leads to multiorgan dysfunctions and links to the severity of the syndrome and the outcome [24] [25] [26] . Therefore, it is suggested that using corticosteroid as adjunctive therapy in COVID-19 treatment. On the other hand, due to its immunosuppression property, potential risks of corticosteroid therapy including secondary infections, long-term complications, and delayed virus clearance prevent doctors from using Corticosteroid therapy [25] . Scientists have ambiguous opinions on corticosteroid therapy that affect COVID-19 pneumonia. A variety of articles claim it benefits or stays neutrality [27] [28] [29] [30] [31] [32] [33] ; meanwhile, there is also evidence that corticosteroid treatment is associated with high mortality and it is recommended not to use corticosteroid [34] [35] [36] [37] . Given that the above studies did not exclude endogenous problems well and COVID-19 is still in the epidemic and based on conflicting perspectives, we aimed at severe and critical patients with COVID-19 pneumonia and balance patient conditions before corticosteroid treatment by PS full matching to perform a more robust result whether corticosteroid works on severe and critical patients with COVID-19. In our results, corticosteroid treatment was significant benefits to the survival of severe and critical patients with COVID-19 pneumonia after balancing the selection bias, although it seems that the corticosteroid group has a higher mortality rate at first glance (unbalanced). Most retrospective research has ignored the selection bias and did not provide robust results. However, physicians consider whether to use corticosteroids often refers to the severity of illness, which leads to serious endogenous problems. To the best of our knowledge, no study using a similar analysis to examine corticosteroid treatment in severe and critical patients with COVID-19 pneumonia. Only one study uses 1:1 PS matching on not severe patients with COVID-19 infected. By a group of 70 matched patients, Yuan et al. [38] concluded that corticosteroid might harm lung injury recovery in nonsevere COVID-19 pneumonia patients. As an effective candidate treatment therapy, the corticosteroid is being concerned by researchers recently [39] [40] [41] . However, more attention should be paid to severe and critical patients. In our subgroup analysis, long-term use of corticosteroids is associated with a higher survival rate. The effect of large-dose use and late use on survival rate seems not significant. But in a group of discharged alive with a clear timeline for virus clearance, long-term use (weak significant p = 0.522) and late use extend the duration of viral shedding, which is consistent with most existing studies and reflects the harm of corticosteroid therapy [25, 31, 32] . Even so, in the death group, corticosteroid also prolongs the survival time of the patients, which gives the doctor more buffer to perform clinical interventions. This study collects a relatively large dataset to balance selection bias and endogenous by PS full matching method, which makes our research more robust. But there still exist limitations. First, data are collected from single-center and our hospital mainly receives patients with severe illness. Our results and conclusions are only applicable to severe and critical patients with COVID-19 pneumonia. Second, this is a retrospective study, some interesting clinical evidence is not collected such as CT source image, SOFA score, etc. Third, in the early stage, due to the lack of nucleic acid detection reagents, some patients did not have a complete virus clearance trace; thus, we lost part of the data in our subgroup analysis. Finally, the common defect of all retrospective studies is that we can't completely control the existence of bias. However, as PS matching analysis can balance the selection bias and endogenous problems, it is the best evidence available for physicians. In a group of severe and critical patients with COVID-19 pneumonia, after balancing selection bias and endogeneity, corticosteroid therapy was significantly associated with decreased mortality. Physicians should use it carefully because it also prolongs the duration of virus clearance.  This work was supported by HUST COVID-19 Rapid Response Call 2020kfyXGYJ015 to Tao Wang. Note 1. Some patients changed corticosteroid type during the treatment process. For rigors' sake, here are the statistics of patients who have used this type of corticosteroid drug. Which means that these percentages add up to more than 1 due to some patients received more than 1 type of corticosteroids.  The study was approved by the Ethics Committee of Tongji Hospital ((IRB: TJ-IRB20200353)). Patient identities were protected via anonymization, and the requirement for informed consent was waived due to the observational nature of the study. Written informed consent for publication was obtained from all participants. The dataset used and analyzed during the current study are available from the corresponding author on reasonable request. 
paper_id= ffe5049d4a9928890638f93bfc0b2f1113df2416	title= Structural interactions between pandemic SARS-CoV-2 spike glycoprotein and human 1 Furin protease 2 3	authors= Naveen  Vankadari;	abstract= 17 The SARS-CoV-2 pandemic is an urgent global public health emergency and 18 warrants investigating molecular and structural studies addressing the dynamics of viral 19 proteins involved in host cell adhesion. The recent comparative genomic studies highlight 20 the insertion of Furin protease site in the SARS-CoV-2 spike glycoprotein alerting possible 21 modification in the viral spike protein and its eventual entry to host cell and presence of Furin 22 site implicated to virulence. Here we structurally show how Furin interacts with the SARS-23 CoV-2 spike glycoprotein homotrimer at S1/S2 region, which underlined the mechanism and 24 mode of action, which is a key for host cell entry. Unravelling the structural features of 25 biding site opens the arena in rising bonafide antibodies targeting to block the Furin cleavage 26 and have great implications in the development of Furin inhibitors or therapeutics. 27 28 	body_text= The pandemic Corona Virus Disease 2019 (COVID-19) caused by Severe Acute 30 Respiratory Syndrome Coronavirus 2 (SARS-CoV-2), is an urgent public health emergency 31 and made a serious impact on global health and economy (1). To date, more than 86,000 32 deaths and 1.5 million confirmed positive cases were reported globally, making the most 33 contagious pandemic in the last decade (www.coronavirus.gov). Since the initial reports on 34 2 this pneumonia-causing novel coronavirus (SARS-CoV-2) in Wuhan, China , mortality and 35 morbidity are increasing exponentially around the globe despite several antiviral and 36 antibody treatments (2). Most available neutralising antibodies in use are targeting the SARS-37 CoV-2 spike glycoprotein, which is essential for host cell adhesion via ACE2 and CD26 38 receptors (3, 4), but infection control is still insignificant. Meanwhile, several antiviral drugs 39 (Ritonavir, Lopinavir, Chloroquine, Remdesivir and others) targeting different host and viral 40 proteins are been clinically evaluating and repurposing to combat SARS-CoV-2 infection (2, 41 5). With the drastic increasing number of the positive cases around the world (www.cdc.gov), 42 moderate response to antivirals under clinical trials and poor response to antibodies targeting 43 spike SARS-CoV-2 spike glycoprotein is a serious concern and warrants detail understanding 44 of the molecular and structural features of SARS-CoV-2 structural proteins in native 45 condition and post-viral infection. This will abet in understanding the dynamics and 46 mechanism of viral action on the human cell. The overall complex structure shows three Furin proteases binding to the mid or 89 equatorial region (mid region of S1 and S2 domain (S1/S2)) of SARS-CoV-2 spike 90 glycoprotein homo-trimer at the off-centric and adjacent side of spike trimer ( Fig. 1 R298, W328 and Q346 (10, 11) ( Fig. 2 and S2) . Interestingly, these residues are also well-100 positioned to interact with the viral spike protein cleavage site in our complex structure and 101 the entire substrate-binding pocket of Furin protease appears like a canyon-like crevice, 4 which can accommodate a large portion of target protein/peptide. The results show that the 103 SARS-CoV-2 spike glycoprotein amino acid residues N657 to Q690 are the prime interacting 104 residues with the Furin protease. The position and orientation of these unique residues 105 involved in Furin recognition are well exposed and organise in a flexible loop. The spike 106 protein residues N657, N658, E661, Y660, T678, N679, S680, R682, R683, R685, S689, 107 Q690 makes the strong interaction with the Furin protease ( Fig. 2A) . The interaction between 108 the viral spike glycoprotein and Furin protease is mediated via several van der Waals or by 109 hydrogen bonding. Furthermore, the entire cleavage loop of viral spike protein fits into the 110 canyon-like substrate-binding pocket of Furin protease. It is quite interesting to notice that 111 none of the previously known coronaviruses had this novel Furin protease cleavage site in the 112 spike glycoprotein, which accentuates the novelty and uniqueness of SARS-CoV-2. In 113 addition, previous reports on the glycosylation of spike glycoprotein show that Furin 114 cleavage site in the SARS-CoV-2 spike glycoprotein is not targeted by the glycosylation, 115 hence this cleavage loop is completely solvent-exposed (4). This further corroborates the 116 potential attack of Furin protease over the S1/S2 cleavage site in the SARS-CoV-2 spike 117 glycoprotein. Based on the Furin binding mode and structural interaction, we propose the 118 following supposition. The binding and cleaving (priming) the spike glycoprotein at S1/S2 119 region by Furin protease might cut the spike glycoprotein into N-terminal S1 domain 120 spike glycoproteins also could make the ACE2 and CD26 inhibitors of least effective, as 129 upon cleavage the N-terminal S1 domains are not required for the cell penetration. This also 130 raises a caution that while making neutralizing antibodies targeting SARS-CoV-2 spike 131 glycoprotein, these cleavage activities need to be considered. Hence, we speculate that 132 antibodies against S2 domain and drugs targeting S1 trimerization could be more promising.  
paper_id= ffe66018d8d249d451bc2993582242533d59fcf8	title= 	authors= 	abstract= 	body_text= In late 2019, a new coronavirus was isolated from a group of patients admitting with viral pneumonia in Wuhan, China. With the increasing global people movement, the major concern over the possibility of this novel virus spreading and reaching pandemic levels has unfortunately now come true. In the first group of patients, the virus was believed to be transmitted from the wet "seafood market" in Wuhan. Since then, numerous reports which are related with human-to-human transmission have been published. It has rapidly spread to all countries world-wide. New countries affected by the virus are reported every day. The novel virus is one member of the coronavirus family and Coronavirus Disease 2019 (COVID-19) spreads from human-to-human like flu (1) . Due to the increasing number of patients in our country since Mid-March 2020, more attention has been given to infection control measures especially in hospitals that have the potential to spread the virus not only among patients but also healthcare personnel. These measures should be implemented at the national level, not just at the hospital level and regional. It is important to comply with the general hygiene rules for public persons and health professionals. After the World Health Organization (WHO) declared that the COVID-19 global pandemic has developed on March 11, 2020, general hygiene rules have been published by Republic of Turkey Ministry of Health and international related institutions to prevent the spread of disease and to avoid new infected cases (Annex 1) (2). However, more strict and branch-specific protection rules are necessary, since healthcare professionals are more likely to come in contact with the infected and/ or probable infected cases. Because the lung findings in the computerized tomography (CT) images are useful in the diagnosis of COVID-19, although there are articles about how they can be careful to reduce to the risk of outbreaks in the radiology departments, there are very few application recommendations for nuclear medicine (NM) clinics (1, 3, 4) . Despite the similarities, there are pertinent differences between radiology and NM regarding the urgency of diagnostic and therapeutic applications, the length of patient contact, the ability for portable imaging instruments and the duration of scans, which necessitate a separate set of advice. Therefore, just like infection protection algoritms prepared by other medical specialities, preventive applications guide has become mandatory in the NM clinics, too. NM departments are the clinics that necessarily apply the principles of infection protection published by WHO in their daily practices due to the ionizing radiation protection measures (distance, time, shielding). Similar to the radiology departments, NM specialists, nurses and technicians are generally at the highest risk of exposure to the COVID-19. COVID-19 is believed to be mainly transmitted through respiratory droplets and close contact with infected patients or with the asymptomatic carriers in the incubation period. Also, it may be transmitted indirectly through touching in contaminated surfaces or objects. There have been no reports of airborne transmission of the COVID-19 virus to date. However, it can not be denied that some aerosol-forming process such as supportive oxygen therapy or intubation, would be able to cause transmission in the healthcare units. There are some experiences providing important recommendations and measures, which have been vested from prior coronavirus epidemics of SARS and MERS outbreaks which in the same class with COVID-19, to combat the current outbreak (1, 3) . Nuclear medicine department is partially lucky compared to other branches because the majority of imaging methods and radionuclide treatment (RNT) performed are elective and tend to be outpatient. In addition, NM workers will have the chance to take the necessary precautions, since the new studies will usually be performed in the patients who have already been hospitalized and secreened for COVID-19. Nevertheless, the absence of portable single photon emission computerized tomography (SPECT) or positron emission tomography (PET)/CT instruments is the most important risk. Setting appointments over the phone will help to identify infected and probable patients, it partially will prevent COVID-19 contamination to the patients and staff, and also the development of outbreak in NM clinics. However, the risk of encountering the virus will not be zero by patients who have not been diagnosed yet, with no suspicious history in terms of disease, but any form of asymptomatic infection at the time of admission. Therefore, this guideline including some suggestions has been prepared with a goal for providing the protection of patients and healthcare workers in the NM clinics that continue to perform the diagnostic and therapeutic procedures even under the risk of contamination by COVID-19 virus which is defined as pandemic by WHO. The recommendations set out in this guide should be adapted to the clinics' own working conditions, fields of activity and the number of patients. 1. In the patients whose new appointments will be arranged, the clinician or assisstant physician responsible for the patient referring to the NM clinic should be asked to give detailed information on COVID-19 and the patients's previous illnesses. 2. During recourse and enrollment to the department, the patients travelling to the country with known COVID-19 outbreak should be encouraged to self-report, especially when they are symptomatic (Annex 2) (2). a. The first registration, if possible, should first be accepted by phone and an inquiry should be made in this regard. b. If it is not possible to make an appoinment by phone, the necessity to fill the notification form prepared for patients to declare their own information in the NM secretariat will be useful in the identifying infected and suspicious cases (Annex 2). 3. During admission, the fever measurement of the patients who registered for outpatient-treatment, and who will undergo a diagnostic procedure may be useful in distinguishing asymptomatic patients. In the large departments dealing with outpatient or referral service to the healthcare provider, thermal screening by thermal scanning and mass screening systems that measure the skin temperature at high-speed using temperature measurement equipment as used in airports must be considered. 4. Healthcare staff should well know the symptoms of COVID-19 infection such as fever, dry cough, fatique and shortness of breath. Additionally, they should be aware that patients are asymptomatic carriers of the virus and that history of contact with a suspicious case is important for probable infection. 5. If there is a suspicious case for COVID-19 when setting an appoinment over the phone, the responsible NM specialist should be informed by the interviewing staff. After that, the competent authority must be notified immediately and the patient should be transferred to the competent hospital. 6. Advisory staff who are arranging the patient procedures in nuclear medicine, making an appoinment and providing the necessary documents exchange should be properly trained for the risk of COVID-19 transmission. It is strongly recommended to use anti-virus protective equipments such as disposable head caps, disposable masks, disposable gloves and hospital-only work clothes. 7. When such patients are identified, they should be placed in a separate waiting area and must be consulted to the infectious diseases specialist or local line accross, our country 184, should be called in the case of strong suspect. Taking into consideration of current information, the patients should use a surgical mask in order to minimize the potential risk of transmission while waiting for results of assessments. 9. If possible, it is recommended to postpone all patient procedures (for diagnostic or therapeutic purposes) until the results of the COVID-19 tests are obtained. 10. When the COVID-19 endemia develops, giving disposable masks to all patients to put on while they are in the department should be considered. This practice must be applied at the expense of the patient's objection. 11. If there is more than one receptionist, it is recommended that the work desks are placed at a distance of 1 m, and similarly, a barrier may be created in front of the counseling desk with a distance of 1 m between the desk and the patients or accompanying persons. 1. Waiting areas must be close to hand washing facilities, masks should be in easy reach. Patients can be encouraged/provided to follow basic hygiene practices. 2. Since the risk of coronavirus transmission increases within one (1) m, the waiting area should have enough space in order to provide enough distance while waiting. The concept of sufficient distance should be designed according to the daily patient load of each clinic. In NM clinics that cannot provide 1 m distance measure between the patients, keeping the admitted patient and/or accompanying persons outside the clinic for providing the social isolation distance; or that to take in the patients individually and in a way that will not cause a confluence in the waiting area, or limiting the number of new appointments based on the size of the waiting area is recommended. 3. The reception hall and waiting area should be good ventilated by equipment with a high-efficiency particulate air filter. If the Patient has been Called for Procedure 1. NM staff welcoming patients such as NM technicians or nurses are individuals who will be in the most close contact with infected patients. 2. Because physical contact is unavoidable during catheter insertion intravenously and it may take a long time, it is very important to identify probably infected patients before this step. 3. Therefore the informations in terms of COVID-19 infection should be added into the cases' anamnesis (Annex 2). 4. If COVID-19 outbreak is defined in the country, it is mandatory that workers use proper personal protective equipment (PPE). PPE (level 1 protective equipment) is generally sufficient in the NM clinics. Level 1 protective equipment includes a disposable surgical mask, special work clothes, disposable latex gloves and/or disposable insulating clothing. Additionally, staff wearing contact lenses must switch to glasses to protect health. Wearing eyeglasses can shield their eyes from probable transmission by contaminated hands or infected respiratory droplets (5). However, level 2 PPE should be used during the examination and imaging of the confirmed or probable patient, and during cleaning of the equipment used to the patient with a suspicious or confirmed disease. Unlike level 1, level 2 PPE requires the use of N95 or equivalent mask, disposable protective suit, googles or face shield (3). Especially the beard and/or mustache interfere with the N95 masks or respirators fitting to entire face and thereby virus transmission risk increases. It is important to be clean shaved for complete protection against infection (6). 5. However, the procedures performed with full PPE are difficult and may affect the ability of personnel to handle the same patient load as before. 6. These practices may change depending on the number of positive cases in the hospital and the departmental success to catch infected or suspicious patients who have been admitted to the department for any procedure or the number of confirmed or probable COVID-19 cases referred. Most of NM imaging and procedures require a processing time from several minutes to several hours following radiopharmaceutical administration. During this time, 1. 1 m distance between patients placed in the separate radioactive patient areas (PET patients) or in the common waiting rooms (for gamma camera patients) should be maintained to prevent novel coronavirus transmission among patients. 2. Waiting rooms of radioactive patients should be good ventilated by an equipment with a high-efficiency particulate air filter. 3. The crowded rooms may increase possible transmission risk due to cases who are asymptomatic and have an inappropriate history. It is adviced to limit the number of patients according to the size of radioactive patient room. 4. Similar measures as those defined for the nonradioactive patients and accompanying persons in the registration hall should be applied to all other stages after radiopharmaceutical administration. First of all, all of them should be supported to wear a protective mask. When the Patient is Scanned 1. Once patients' scan is complete, scanners and room surfaces should be disinfected to avoid potential spread. 2. As a general rule of Public Health Units; a. In the absence of visible pollution after scanning, disinfectants produced for especially the hospitals or the 1/1000 (1000 parts per million) parts of chlorine solutions containing are recommended to use in the disinfection of scanning instruments and clinical rooms used (7). b. Terminal cleaning procedures must be applied in case of the presence of patient secretions, urine or stool contamination (7). c. Adequate and appropriate training is recommended for cleaning staff to work properly.  1. Taking into consideration asymptomatic patients, patient in a good clinical condition without any strict contraindication criterion, performing pharmacological stress test (especially with vasodilatator agents) shortens the procedure time instead of treadmill exercise test. This recommendation can keep safe the staff and also the patient from possible transmission. 2. Since the pharmacological stress test will be performed at a less distance than 1 m, particular attention should be paid to the use of PPE. If possible, it is recommended to wear a disposable clothing over the lead apron, and to properly disinfect the materials and the clothes used during the procedure. Additionally, patient must wear a protective mask during the entire procedure. cases, yet. Another important issue here is that hydroxychloroquine preparations (hydroxychloroquine is a drug known with strong antimalarial effect. However, today it is used in the treatment of autoimmune disease rather than malaria treatment) has been reported to be effective in COVID-19 treatment and even for prophylaxis, may cause similar cardiac findings. The side effects of hydroxychloroquine drugs are not dose dependent in each case and may also develop idiosyncratic side effects in individuals who are sensitive to 4-aminoquinoline compounds. Therefore, it should be taken into account that side effects of the drugs may be thought as the cardiac manifestations of the disease. Additionally, it may cause serious retinopathy and skin lesions (9, 10, 11) . It is recommended to be used prophylactically with the decision of the expert and under the supervision of specialist in the healthcare workers at high risk of disease. The most common symptoms in COVID-19 patients are high fever, cough, myalgia and fatigue. Pneumonia involving multiple lobes, bilaterally has been reported in the published studies. The SARS-Cov-2 pneumonia resulting extensive inflammation and infiltration is localized particularly in the posterior and peripheral zones of the lungs (12,13) (Table 1) . However, patients especially who are at risk for pulmonary embolism (PE) such as deep vein thrombosis, malignancy, and presence of immobilization may be considered as PE because of that typical complaints are not detected and no severe infiltration findings in lungs in the early stages. PE can lead to non-specific findings such as arterial hypoxemia and hypocapnia in the patients. These findings are also seen in chronic obstructive pulmonary disease, lung cancer or pulmonary fibrosis besides PE. The pulmonary X-rays will be helpful in the differential diagnosis of non-PE diseases. But its sensitivity is very low (40%). Although the sensitivity (82%) and specificity (96%) of perfusion-only study is inferior to combined ventilation/ perfusion study, these values are still high in the diagnosis of pulmonary embolism. 1. In the clinics where there is no SPECT/CT equipment, patients suspicious for PE should be referred to lung CT scanning in terms of the possible COVID-19 pneumonia, even if their body temperature is not high. After that if necessary, pulmonary perfusion scintigraphy should be performed following CT. Planar or SPECT perfusion study should be evaluated together with clinical findings and simultaneous CT images (14, 15, 16) . shaped perfusion defects, resembling lung parenchymal disease. a. There is a risk of transmission of all viral and bacterial infections, including COVID-19, by means of ventilation devices. b. Additionally, the lung ventilation procedure can cause aerosolization and the formation of microdroplets. Therefore, lung ventilation study should not be performed in patients with probable PE during the pandemic (6). c. If necessary, perfusion-only study should be settled in these cases as defined above. Diagnostic lung CT imaging should be performed in both conditions. 5. The flow-chart recommended in probable cases should be followed. The Tc-99m macroagregated albumin (MAA) particles used for lung perfusion scintigraphy are 15-100 μm in size and the particle distribution accurately shows regional lung perfusion. Since MAA particles block pulmonary capillaries and precapillary arterioles, the number of particles injected is important. Normally 100.000-500.000 (ideally 400.000) particles are injected during perfusion study. However, 60.000 particles are sufficient for obtaining uniform distribution of activity reflecting regional perfusion. Taking into account that the number of pulmonary capillaries and precapillary arterioles present in the lungs, the administration of up to 400.000 MAA particles will result in obstruction in a very small fraction of pulmonary vessels. But it is recommended to inject the minimum number of MAA particles (60.000 particles) which sufficient for good quality images (14, 15) . In this way, developing widespread microemboli will be prevented and kept pulmonary functions in the patients with the possibility of COVID-19 pneumonia which can cause severe damage in the lungs. 1. There is increasing number of reports which describe CT findings of COVID-19 associated pneumonia. Recently a manuscript containing 4 (four) cases about random CT findings suggesting COVID-19 in the 18 F-fluorodeoxyglucose (FDG) PET/CT scans and a case report including a patient who underwent FDG PET/ CT scan due to a lesion in the lung were published. COVID-19 was detected in one case in each publication (17, 18, 19) . 2. If interlobular septal thickening and ground-glass density areas with high metabolic activity are detected in the cases who underwent to 18 F-FDG PET/CT imaging for other reasons, these cases should be referred to the relevant institutions or departments. 3. Similar lung CT findings for COVID-19 pneumonia observed in the SPECT/CT slices of the thorax should be reported to the Infection Control Committee or to the call centers in the hospital by NM specialists. 4. Based on these cases, a more careful evaluation of PET/ CT or SPECT/CT images of each patient, especially the lungs in the CT components, and reporting the lung findings observed by NM specialists will offer very useful informations to the clinicians in the early diagnosis of COVID-19 lung involvement (12, 13) . Because such patients may be asymptomatic, and being unaware, can spread the virus to the surrounding people and to the people who they are in contact with. Imaging findings are similar to previous coronaviruses such as MERS or SARS. These nonspecific findings become meaningful in terms of COVID-19 infection if they comply with suspicious case definition. 5. Information about the patient's clinical history will be useful in the diagnosis for evaluation of radiology and NM images, as in routine practice. Risk factors such as chronic illness, the history of contact with confirmed 6. Staff performing injection and imaging of these patients must be questioned in detail for a history of contact with patient (such as duration, proximity, whether using PPE) . Table 1 shows the CT findings and their distributions regarding to lung lobes that can be observed during COVID-19 infection. Although cancer patients have a higher risk than healthy people, there is still no published chronic disease and algorithm for COVID-19 infection in these patients. It is accepted that the presence of other being older than 65 years will rise the infection risk. Although there is a small number of reports on surgical treatments in cancer cases suspicious for COVID-19 infection or infected (20, 21) , any published report regarding RNT is not present, yet. If necessary, the guide for surgery of these cases may be applied. According to these rules;  A flow-chart similar to cancer cases should be followed in patients whose I-131 treatment is planned due to hyperthyroidism. Antithyroid treatment should continue until COVID-19 is identified as negative. 6. During treatment, COVID-19 positive and negative cases, and also personnel applying treatment should be followed the measures suggested for people at high transmission risk but no with cancer diagnosis, and hygiene rules (5,6,12). test positive soon after RNT should be recorded by the Radiation Protection Officer at the time of hospital discharge and the epicrisis related to treatment process should be given (22, 23) . 8 . In case of deterioration in the general condition of these patients (use of a ventilator, intubation, hemodialysis) or death, relevant national and international guidelines should be consulted for measures and practices to be taken in terms of radiation safety and infection transmission (22,23,24,25,26.27,28) . 1. Visual materials such as brochures and in-hospital broadcasts may be used to promote hand washing and good respiratory hygiene measures in the department. 2. All gamma camera gantries and image monitoring station mouses and keyboards, sphygmomanometer cuffs, all surfaces (tables, seats, chairs and beds), should be wiped with disinfectant regularly and after each contact with suspicious patients. 3. Make sure disinfectant bottles in the work area are easy accesible and refilled regularly. 4. The risk encountered at the time of cleaning is not the same as contacting an infected patient who coughs or sneezes. However, cleaning personnel who makes clean all areas in the department during work and out of working hours should be specifically trained for professional cleaning of potentially contaminated surfaces after high-risk patient contact (3, 7) . During cleaning time, cleaning personnel should; a. Use proper PPE. Informations should be given on how to put on and take off this equipment and practical training should be done. b. Informed about not touching their faces, especially mouth, nose and eyes. c. Wear water-proof disposable gloves and use a surgical mask, eye protective or face shield, shoes cover. d. Wear glasses instead of lenses (5). e. Use alcohol-based hand disinfectants before and after wearing their gloves. f. Use alcohol-based disinfectants with a virucidal effect before and after wearing surgical mask. g. If there is a possibility of visible contamination with respiratory secretion or other body fluids; in addition to surgical masks and eye protection equipment, disposable protective clothing that covers the entire body should be worn prior to cleaning work. • For outpatient cases, the imaging process should be carried out in accordance with hospital policy. If imaging is really necessary, special patient waiting rooms equipped with high-efficiency particulate air filters are recommended. • An algorithm must be developed to ensure that probable cases are identified in a timely manner. • All staff must receive appropriate training to ensure maximum compliance measures. • Staff should be contacted and given the message that they should stay at home even in the presence of mild COVID-19 symptoms. • In case of signs of illness suggesting COVID-19 or confirmed infection in your personnel, emergency and business continuous workflow plans should be adapted in order to prevent the disruption of function. Although there are many issues raised for NM applications concerning the current COVID-19 outbreak, NM departments can make a significant contribution to reducing the effect of COVID-19 infection on patients and staff, if adequately prepared for PPE and disinfection procedures. The lessons learned from the current experience and the data obtained from the case groups will help to improve preparedness and address possible deficiencies in case of new outbreaks in the future. Peer-review: Internally peer-reviewed. Concept: A.A., F.S.K., Design: F.S.K., A.A., Data Collection or Processing: A.A., F.S.K., Analysis or Interpretation: F.S.K., A.A., Literature Search: A.A., F.S.K., Writing: F.S.K., A.A. 
paper_id= ffe7169e14eea9af001ae388632123d43eeb9697	title= Nasally administered Lactobacillus rhamnosus strains differentially modulate respiratory antiviral immune responses and induce protection against respiratory syncytial virus infection	authors= Yohsuke  Tomosada;†  ;Eriko  Chiba;Hortensia  Zelaya;Takuya  Takahashi;Kohichiro  Tsukida;Haruki  Kitazawa;Susana  Alvarez;Julio  Villena;	abstract= Background: Some studies have shown that nasally administered immunobiotics had the potential to improve the outcome of influenza virus infection. However, the capacity of immunobiotics to improve protection against respiratory syncytial virus (RSV) infection was not investigated before. Objective: The aims of this study were: a) to evaluate whether the nasal administration of Lactobacillus rhamnosus CRL1505 (Lr05) and L. rhamnosus CRL1506 (Lr06) are able to improve respiratory antiviral defenses and beneficially modulate the immune response triggered by TLR3/RIG-I activation; b) to investigate whether viability of Lr05 or Lr06 is indispensable to modulate respiratory immunity and; c) to evaluate the capacity of Lr05 and Lr06 to improve the resistance of infant mice against RSV infection. Results: Nasally administered Lr05 and Lr06 differentially modulated the TLR3/RIG-I-triggered antiviral respiratory immune response. Lr06 administration significantly modulated the production of IFN-α, IFN-β and IL-6 in the response to poly(I:C) challenge, while nasal priming with Lr05 was more effective to improve levels of IFN-γ and IL-10. Both viable Lr05 and Lr06 strains increased the resistance of infant mice to RSV infection while only heat-killed Lr05 showed a protective effect similar to those observed with viable strains. Conclusions: The present work demonstrated that nasal administration of immunobiotics is able to beneficially modulate the immune response triggered by TLR3/RIG-I activation in the respiratory tract and to increase the resistance of mice to the challenge with RSV. Comparative studies using two Lactobacillus rhamnosus strains of the same origin and with similar technological properties showed that each strain has an specific immunoregulatory effect in the respiratory tract and that they differentially modulate the immune response after poly(I:C) or RSV challenges, conferring different degree of protection and using distinct immune mechanisms. We also demonstrated in this work that it is possible to beneficially modulate the respiratory defenses against RSV by using heat-killed immunobiotics. 	body_text= Acute lower respiratory tract infections are a persistent public health problem. Despite the remarkable advances in antibiotic therapies, diagnostic tools, prevention campaigns and intensive care, respiratory infections are still among the primary causes of death worldwide, and there have been no significant changes in mortality in the last decades [1] . Childhood acute community-acquired pneumonia is one of the leading causes of morbidity and mortality in developing countries. In children who have not received prior antibiotic therapy, the main bacterial causes of clinical pneumonia in developing countries are Streptococcus pneumoniae and Haemophilus influenzae type b, and the main viral cause is the respiratory syncytial virus [2] . Respiratory syncytial virus (RSV), a pneumovirus in the family Paramyxoviridae, infects nearly all children within the first 3 years of life. Primary RSV infections can cause severe bronchiolitis and pneumonia, which are associated with significantly increased risk of developing wheeze during childhood that lasts until teenage years. Symptomatic reinfections occur in every age group, but the frequency and severity of symptoms are highest in children below 5 years of age [3] . As described for several respiratory viruses, such as pandemic influenza virus strains and the human coronavirus that causes SARS, it is believed that the immune response plays a critical role in the outcome of RSV-induced bronchiolitis and pneumonia. Acute RSV infection is able to induce an exacerbated disease due to immune-mediated pulmonary injury resulting in severe morbidity and mortality [4] . Therefore, identifying novel approaches to modulate virus-induced immunopathology would be beneficial in treating acute RSV infections. Several studies have demonstrated that certain lactic acid bacteria (LAB) strains can exert their beneficial effect on the host through their immunomudulatory activity. In this regard, some studies have centered on whether immunoregulatory probiotic LAB (immunobiotics) might sufficiently stimulate the common mucosal immune system to provide protection in other mucosal sites distant from the gut [5] . The studies of our laboratory demonstrated that some orally administered LAB are able to increase S. pneumoniae clearance rates in lung and blood, improve survival of infected mice and reduce lung injuries [5] [6] [7] [8] . Moreover, we found that the effects of LAB treatments were related to an up-regulation of both respiratory innate and adaptive immune responses. In addition, considering that the nasal route can induce systemic and respiratory immune responses superior to those obtained using oral stimulation [9] , we also focused on the ability of nasal stimulation with immunobiotics to improve respiratory immune responses. Our studies showed that nasally administered LAB are capable of modulating lung immunity and increase resistance against S. pneumoniae in both immunocompetent and immunocompromised mice and that in many cases, nasal priming is more effective than oral administration to beneficially modulate the respiratory immunity [10, 11] . Recently, our laboratory studied the capacity of immunobiotics to improve respiratory antiviral immune response. To mimic the pro-inflammatory and physiopathological consequences of RNA viral infections in the lung such as those induced by RSV infection, we used an experimental model of lung inflammation based on the administration of the artificial toll-like receptor 3 (TLR3) and retinoic acid-inducible gene I (RIG-I) ligand and dsRNA analog poly(I:C) [12] . In vivo studies using mice demonstrated that nasally administered poly(I:C) results in TLR3-and CXCR2-dependent neutrophilic pulmonary inflammation, bronchiolar epithelial hypertrophy, interstitial edema and altered lung function [13, 14] . These changes are accompanied by elevated levels of interleukin (IL)-8, RANTES, monocyte chemotactic protein (MIP)-1, and type I interferons (IFNs) in broncho-alveolar lavages (BAL) [13] . When we evaluated the effect of two Lactobacillus strains, Lactobacillus rhamnosus CRL1505 (Lr05) and L. rhamnosus CRL1506 (Lr06) in this mice model, we found that orally administered Lr05 beneficially regulate the balance between pro-inflammatory mediators and IL-10 in lung of poly(I:C)-challenged mice, allowing an effective control of the inflammatory response and avoiding tissue damage [12] . Moreover, our studies demonstrated that Lr05 is able to increase the number of CD3 + CD4 + IFN-γ + T cells in the gut, induce the mobilization of these cells into the respiratory mucosa and improve the local production of IFN-γ and the activity of lung antigen presenting cells (APCs) [12] . Our results suggested that Lr05 is a potent inducer of antiviral cytokines and may be useful as a prophylactic agent to control respiratory virus infections. However, whether the nasal priming with Lr05 is more effective than oral administration to beneficially modulate the respiratory immune response triggered by poly(I:C) challenge has not been evaluated before. Moreover, further studies using real challenges with respiratory viruses such as RSV are needed in order to conclusively demonstrate the protective effect of Lr05. Considering this background, the aims of this study were: a) to investigate whether the nasal administration of Lr05 or Lr06 are able to improve respiratory antiviral defenses and beneficially modulate the immune response triggered by TLR3/RIG-I activation; b) to evaluate whether viability of Lr05 or Lr05 is indispensable to modulate respiratory immunity considering that it was reported that heat-killed lactobacilli strains are able to improve lung defenses [11, 15, 16] and; c) to conclusively demonstrate the protective effect of Lr05 and Lr06 by evaluating their capacity to improve the resistance of infant mice against RSV challenge. Lactobacillus rhamnosus CRL1505 (Lr05) and CRL1506 (Lr06) were obtained from the CERELA culture collection (Chacabuco 145, San Miguel de Tucumán, Argentina). Both strains were selected because their previously reported immunomodulatory capacities [6, 12] . The culture was kept freeze-dried and then rehydrated using the following medium: peptone 15.0 g, tryptone 10.0 g, meat extract 5.0 g, distilled water 1 l, pH 7. It was cultured for 12 h at 37°C (final log phase) in Man-Rogosa-Sharpe broth (MRS, Oxoid). The bacteria were harvested by centrifugation at 3000 g for 10 min, washed three times with sterile 0.01 mol/l phosphate buffer saline (PBS), pH 7.2, and resuspended in sterile 10% non-fat milk. Non-viable Lr05 and Lr06, designated as HkLr05 and HkLr06 respectively, were obtained as follows: bacteria were killed by tyndallization in a water bath at 80°C for 30 min and the lack of bacterial growth was confirmed using MRS agar plates [11] . Female 3-week-old BALB/c mice were obtained from the closed colony kept at Tohoku University. They were housed in plastic cages at room temperature. Mice were housed individually during the experiments and the assays for each parameter studied were performed in 5-6 mice per group for each time point. Lr05, Lr06, HkLr05 or HkLr06 were nasally administered to different groups of mice for 2 consecutive days at a dose of 10 8 cells/mouse/ day in 50 μl of PBS. The treated groups and the untreated control group were fed a conventional balanced diet ad libitum. This study was carried out in strict accordance with the recommendations in the Guide for the Care and Use of Laboratory Animals of the Guidelines for Animal Experimentation of Tohoku University, Sendai, Japan. The present study was approved by the Institution Animal Care and Use Committee of Tohoku University and all efforts were made to minimize suffering [5] [6] [7] [8] . Administration of the viral pathogen molecular pattern poly(I:C) was performed on day 3, after the two days treatments with lactobacilli, as described previously [12] . Mice were lightly anesthetized and 100 μl of PBS, containing 250 μg poly(I:C) (equivalent to 10 mg/kg body weight), was administered dropwise, via the nares. Control animals received 100 μl of PBS. Mice received three doses of poly(I:C) or PBS with 24 hs rest period between each administration. Blood samples were obtained through cardiac puncture at the end of each treatment and collected in heparinized tubes. BAL samples were obtained as described previously [8] . Briefly, the trachea was exposed and intubated with a catheter, and 2 sequential bronchoalveolar lavages were performed in each mouse by injecting sterile PBS; the recovered fluid was centrifuged for 10 min at 900 × g; the pellet was used to make smears that were stained for cell counts; and the fluid was frozen at −70°C for subsequent cytokines analyses. Tumour necrosis factor (TNF)-α, IFN-α, IFN-β, IFN-γ, IL-6 and IL-10 concentrations in serum and BAL were measured with commercially available enzyme-linked immunosorbent assay (ELISA) technique kits following the manufacturer's recommendations (R&D Systems, MN, USA) [6] . Protein and albumin content, a measure to quantitate increased permeability of the bronchoalveolar-capillarity barrier, and lactate dehydrogenase (LDH) activity, an indicator of general cytotoxicity, were determined in the acellular BAL fluid [8] . Protein content was measured by the bicinchoninic acid assay (BCA) protein assay (Pierce Biotechnology Inc., Rockford, IL). Albumin content was determined colorimetrically based on albumin binding to bromcresol green using an albumin diagnostic kit (Wiener Lab, Buenos Aires, Argentina). LDH activity, expressed as units per liter of BAL fluid, was determined by measuring the formation of the reduced form of nicotinamide adenine dinucleotide (NAD) using the Wiener reagents and procedures (Wiener Lab). Lung wet:dry weight ratio was measured as previously described [12] . Wet:dry weight ratio was calculated as an index of intrapulmonary fluid accumulation, without correction for blood content. Single lung cells from mice were prepared using the previously described method [12] . Mice were anaesthetized with diethyl ether and killed the next day by exsanguination. Lungs were removed, finely minced and incubated for 90 min with 300 U of collagenase (Yakult Honsha Co., Tokyo, Japan) in 15 ml of RPMI 1640 medium (Sigma, Tokyo, Japan). To dissociate the tissue into single cells, collagenase-treated minced lungs were gently tapped into a plastic dish. After removal of debris, erythrocytes were depleted by hypotonic lysis. The cells were washed with RPMI medium supplemented with 100 U/ml of penicillin and 100 mg/ml of streptomycin and then resuspended in a medium supplemented with 10% heatinactivated foetal calf serum (FCS). Cells were counted using Trypan Blue exclusion and then resuspended at an appropriate concentration of 5 × 10 6 cells/ml. Lung cell suspensions were pre-incubated with antimouse CD32/CD16 monoclonal antibody (Fc block) for 15 min at 4°C. Cells were incubated in the antibody mixes for 30 min at 4°C and washed with FACS buffer. The following antibodies from BD PharMingen were used: anti-mouse CD3-FITC, anti-mouse CD4-PE, antimouse CD8-PE, anti-mouse IFN-γ-APC, anti-mouse CD11b-FITC, anti-mouse CD11c-PE, anti-mouse IFN-γ -PE, anti-mouse MHC-II-PE, anti-mouse IL-10-PE and anti-mouse CD103-biotin. Following incubation with biotinylated primary antibodies, the labeling was revealed using streptavidin-PercP. In all cases, cells were then acquired on a BD FACSCalibur™ flow cytometer (BD Biosciences) and data were analyzed with FlowJo software (TreeStar). The total number of cells in each population was determined by multiplying the percentages of subsets within a series of marker negative or positive gates by the total cell number determined for each tissue [12, 17] . Human RSV strain A2 was grown in Vero cells as described by Murawski et al. [18] . Briefly, Vero cells were infected with RSV at a multiplicity of infection (MOI) of 1 in 5 ml of Dulbecco's modified Eagle's medium (DMEM). Cells were infected for 2.5 h at 37°C and 5% CO 2 . After infection, 7 ml of DMEM with 10% fetal bovine serum (Sigma, Tokyo, Japan), 0.1% penicillinstreptomycin (Pen/Strep) (Sigma, Tokyo, Japan), and 0.001% ciprofloxacin (Bayer) was added to the flask. Flasks were incubated until extensive syncytium formation was observed. Then, cells were scraped from the flask and sonicated three times, 5 s per time, at 25 W on ice. Cell debris was removed by centrifugation at 700 g for 10 min at 4°C. Virus supernatant was sucrose density gradient purified and stored in 30% sucrose at −80°C. Uninfected flasks were treated identically to generate Vero cell lysate control. For in vivo infection, mice were lightly anesthetized with isoflurane and intranasally challenged with 2.4 × 10 6 PFU of RSV strain A2. Lung tissue was removed without BAL harvest and stored in 30% sucrose for plaque assay. Lungs were homogenized using a pellet pestle and centrifuged at 2,600 × g for 10 min at 4°C to clarify supernatant. Twenty-four-well tissue culture plates were seeded with 1.5 × 10 5 Vero cells/well in DMEM containing 10% FBS, 0.1% Pen/Strep, and 0.001% ciprofloxacin. Cells were incubated overnight at 37°C and 5% CO 2 . Medium was removed from confluent monolayers, and serial dilutions of lung tissue-clarified supernatants were absorbed to monolayers. All samples were run in triplicate wells. Plates were incubated at 37°C and 5% CO 2 for 2.5 h for optimum infection. After incubation, supernatant was removed, and 1 ml of fresh DMEM medium containing containing 10% FBS, 0.1% Pen/Strep, and 0.001% ciprofloxacin was overlaid on monolayers. When extensive syncytia developed, the overlay was removed and monolayers were fixed with 1 ml of ice-cold acetone:methanol (60:40). Primary RSV anti-F (clones 131-2A; Chemicon) and anti-G (Mouse monoclonal [8C5 (9B6)] to RSV glycoprotein, Abcam) antibodies were added to wells for 2 h, followed by secondary horseradish peroxidase anti-mouse immunoglobulin antibody (Antimouse IgG, HRP-linked Antibody #7076, Cell signaling Tehcnology) for 1 h. Plates washed twice with PBS containing 0.5% Tween 20 (Sigma) after each antibody incubation step. Individual plaques were developed using a DAB substrate kit (ab64238, Abcam) following manufacture's specifications. Results for immunoplaque assay were expressed as log10 PFU/g of lung. Experiments were performed in triplicate and results were expressed as mean ± standard deviation (SD). After verification of the normal distribution of data, 2-way ANOVA was used. Tukey's test (for pairwise comparisons of the means) was used to test for differences between the groups. Differences were considered significant at p<0.05. Nasally administered L. rhamnosus CRL1505 and L. rhamnosus CRL1506 differentially modulate respiratory immunity In order to evaluate the changes induced by nasally administered LAB in the respiratory immune system we determined the levels of different cytokines in BAL ( Figure 1A ). The four nasal treatments used in this study Lr05, HkLr05, Lr06 and HkLr06 increased the levels of IL-6, IFN-α and IFN-β in BAL, however concentrations of these cytokines were significantly higher in Lr06-and HkLr06-treated mice than in mice receiving Lr05 or HkLr05 ( Figure 1A ). All the treatments were also able to increase BAL IFN-γ, being both viable and heat-killed L. rhamnosus CRL1505 more efficient to improve the levels of this cytokine than L. rhamnosus CRL1506 ( Figure 1A ). BAL TNF-α and IL-10 concentrations were increased by the immunobiotic nasal treatments. We observed that viable cells were more efficient to upregulate TNF-α than heat-killed strains while no differences were found in IL-10 levels when comparing viable and heat-killed lactobacilli ( Figure 1A ). When we evaluated the levels of IL-6, IFN-α, IFN-β, IFN-γ, TNF-α and IL-10 in serum we found that lactobacilli treatments induced similar changes than those observed in the respiratory tract ( Figure 1B ). IL-6, IFN-α and IFN-β were more efficiently increased with L. rhamnosus CRL1506 treatments while the highest levels of serum IFN-γ were observed in Lr05 and HkLr05 groups ( Figure 1B ). Serum TNF-α and IL-10 concentrations were also enhanced by the immunobiotic nasal treatments, being viable and heat-killed lactobacilli equally effective to improve both cytokines whit the exception of HkLr06 that induced significantly lower levels of IL-10 when compared with the other treatments ( Figure 1B) . We also evaluated the changes induced by nasally administered lactobacilli in lung immune cells using flow cytometry. Slightly increases of lung CD3 + CD4 + T cells were observed in lactobacilli-treated mice, however when we studied the cells able to produce IFN-γ within this population, mice receiving Lr05, HkLr05 and Lr06 showed significant differences when compared to control mice ( Figure 2 ). Nasally administered Lr05 and HkLr05 A B Figure 1 Effect of lactobacilli on systemic and respiratory immunity. Effect of viable (Lr05) or heat-killed (HkLr05) Lactobacillus rhamnosus CRL1505 and viable (Lr06) or heat-killed (HkLr06) L. rhamnosus CRL1506 nasal administration on the tumor necrosis factor (TNF)-α, interferon (IFN)-α, IFN-β, IFN-γ, interleukin (IL)-6, and IL-10 concentrations in broncho-alveolar lavages (A) and serum (B). Lr05, Lr06, HkLr05 or HkLr06 were nasally administered to different groups of mice for 2 consecutive days at a dose of 10 8 cells/mouse/day and the levels of BAL and serum cytokines were studied on day 3. The results represent data from three independent experiments. Different letters indicate significant differences (P < 0.05). increased the number of CD3 + CD4 + IFN-γ + T cells in lungs being Lr05 more efficient than HkLr05 to increase this cell population ( Figure 2 ). Only Lr05 enhanced the number of CD3 + CD4 + IL-10 + T cells in lungs ( Figure 2 ). No modifications were observed in the number of CD3 + CD8 + and CD3 + CD8 + IFN-γ + T cells in lactobacilli-treated mice ( Figure 2 ). Two populations of myeloid DCs can be defined in lungs using CD11c, CD11b, CD103 and MHC-II antibodies as described previously [12, 19] : MHC-II + CD11c + CD11b low CD103 + and MHC-II + CD11c + CD11b high CD103cells. Therefore, we next aimed to evaluate the effect of nasally administered lactobacilli on these populations of DCs from lungs. Lr05 and HkLr05 significantly increased the number of both lung CD11c + CD11b low CD103 + and CD11c + CD11b high CD103 -DCs. In addition, Lr06 and HkLr06 enhanced the number of lung CD11c + CD11b low CD103 + DCs while no quantitative changes were detected in CD11c + CD11b high CD103 -DCs populations in lungs of Lr06-and HkLr06-treated mice ( Figure 2 ). In addition, the expression of MHC-II in both DCs population was significantly improved with all the treatments, however L. rhamnosus CRL1505 was more efficient than L. rhamnosus CRL1506 to upregulate the expression of MHC-II in lung DCs ( Figure 2 ). We next aimed to evaluate the effect of nasally administered lactobacilli on the immune response triggered by nasal administration of the viral pathogen-associated molecular pattern poly(I:C). Our previous work demonstrated that the nasal challenge of mice with poly(I:C) significantly alters lungs function and induce lung injuries [12] . In this experimental model an altered wet:dry weight ratio can be observed after poly(I:C) challenge ( Figure 3 ). Moreover, significantly increased levels of LDH activity as well as protein and albumin concentrations can be found Figure 2 Effect of lactobacilli on respiratory immune cells populations. Effect of viable (Lr05) or heat-killed (HkLr05) Lactobacillus rhamnosus CRL1505 and viable (Lr06) or heat-killed (HkLr06) L. rhamnosus CRL1506 nasal administration on CD3+CD8+IFN-γ+, CD3+CD4+IFN-γ+ and CD3+CD4+IL-10+ T cells and CD11c+CD11b low CD103+ and CD11c+CD11b high CD103-dendritic cells from lung. Lr05, Lr06, HkLr05 or HkLr06 were nasally administered to different groups of mice for 2 consecutive days at a dose of 10 8 cells/mouse/day and lung immune cells were studied on day 3. The results represent data from three independent experiments. Different letters indicate significant differences (P < 0.05). in BAL samples of challenged mice indicating that poly(I:C) produces local cellular damage and impairment of the alveolar-capillary barrier ( Figure 3 ). We observed that nasally administered lactobacilli prior to poly(I:C) challenge significantly reduced wet:dry weight ratio and BAL LDH ( Figure 3 ). In addition, all the treatments were able to significantly reduce BAL protein and albumin concentrations however L. rhamnosus CRL1505 was more efficient than the CRL1506 strain to reduce the impairment of the alveolar-capillary barrier ( Figure 3 ). The pulmonary immune response induced by the nasal challenge with poly(I:C) and the effect of nasally administered lactobacilli in that response were next evaluated. We have previously study the levels and kinetics of IFNα, IFN-β, IFN-γ, IL-6, IL-4, TNF-α, IL-1β, IL-8, MCP-1, IL-10 and TGF-β in BAL after poly(I:C) challenge and the effect of orally administered immunobiotics in that response [12] . The most significant changes induced by oral treatment with immunobiotics were found in the levels of IFN-α, IFN-β, IFN-γ, IL-6, TNF-α and IL-10, therefore these cytokines were studied in this work. Nasal administration of poly(I:C) significantly increased respiratory levels of the pro-inflammatory mediators IFN-α, IFN-β, IL-6 and TNF-α ( Figure 4 ) as previously reported [12] . All lactobacilli treatments significantly increased the levels of BAL IFN-α, IFN-β and TNF-α however, Lr06 and HkLr06 were more efficient to enhance the concentration of these cytokines than Lr05 or HkLr05 ( Figure 4 ). In addition, IL-6 was not modified in Lr06 and HkLr06 groups while the levels of this cytokine were lower in L. rhamnosus CRL1505-treated mice when compared to controls ( Figure 4 ). Poly(I:C) challenge also induced an increase in the respiratory levels of IL-10 and IFN-γ and the levels of both cytokines were significantly higher in lactobacilli-treated mice, being Lr05 and HkLr05 more efficient than Lr06 and HkLr06 to achieve that effects (Figure 4) . The nasal challenge with poly(I:C) also increased cytokines levels in serum as previously reported [12] . Moreover, the effect of Lr05, HkLr05, Lr06 and HkLr06 treatments on the production of serum IFN-α, IFN-β, IFN-γ, IL-6, TNF-α and IL-10 was similar to that found in BAL (data not shown). We also studied the changes in lung immune cells induced by nasally administered lactobacilli in poly(I:C)challenged mice ( Figure 5 ). Poly(I:C) administration increased CD3 + CD8 + IFN-γ + and CD3 + CD4 + IFN-γ + T cells as we described previously [12] . In addition, in this work we also observed an increase in CD3 + CD4 + IL-10 + T cells after the challenge with poly(I:C) ( Figure 5 ). Our results showed that nasally administered lactobacilli were able to increase both CD3 + CD4 + IFN-γ + and CD3 + CD4 + IL-10 + T cells in lungs, however the levels of these cell populations in Lr05-and HkLr05-treated mice were significantly higher than those observed in Lr06 and HkLr06 groups ( Figure 5 ). No differences were observed between lactobacilli-treated mice and controls when evaluating CD3 + CD8 + IFN-γ + T cells ( Figure 5 ). Poly(I:C) challenge also increased the number of pulmonary CD11b high CD103 -MHC-II + and CD11b low CD103 + MHC-II + DCs when compared to basal levels in all the experimental groups ( Figure 5 ). Nasal administration of both L. rhamnosus CRL1505 and L. rhamnosus CRL1506 significantly increased the numbers of CD11b low CD103 + MHC-II + DCs cells in lungs when compared to controls while only Lr05and HkLr05-treated mice showed improved levels of pulmonary CD11b high CD103 -MHC-II + DCs ( Figure 5 ). We next addressed the question of whether changes observed in respiratory immune system caused by the intervention with immunobiotics affected the outcome of RSV infection in mice. Therefore, infant mice were nasally treated with Lr05, HkLr05, Lr06 or HkLr06 and then challenged with 10 6 PFU of RSV. Viral loads in lungs of infected mice were followed for five days after the challenge ( Figure 6 ). RSV was detected in lungs of all the experimental groups during the five days postinfection and all groups showed a peak of virus counts on day 4 after the challenge. However, lactobacillitreated mice showed significantly lower lung viral loads when compared to control mice. Lr05 and HkLr05 treatments were equally effective to reduce RSV replication in lungs while in the case of L. rhamnonus CRL1506 viable bacteria was more effective than heat-killed cells to improve protection against the respiratory viral infection ( Figure 6 ). In addition, we observed significantly differences in the body weight of infected mice when comparing lactobacilli-treated mice and controls ( Figure 6 ). Lr05, HkLr05 and Lr06 significantly improved the body weight during RSV infection while HkLr06-tretated mice showed no differences when compared to controls ( Figure 6 ). We also evaluated the markers of lung tissue damage in RSV-infected mice. As showed in Figure 7 , challenge with RSV significantly increase lung wet:dry weight, BAL protein concentrations and LDH activity. All these markers of lung tissue damage were significantly higher in RSV-challenged control mice than in those previously treated with Lr05, HkLr05 or Lr06 (Figure 7) . On the contrary, lung wet:dry weight, BAL protein concentrations and BAL LDH activity in HkLr06-treated mice were not different from controls ( Figure 7) . Finally, we addressed whether the improvement in the resistance against RSV induced by lactobacilli treatments was related to a differential modulation of cytokines production during infection ( Figure 8 ). Therefore, we evaluate the levels of BAL and serum IL-6, IFN-β and TNF-α on day 2 post-infection and the concentrations of IFN-γ and IL-10 on day 5 after challenge. We observed that the challenge with RSV significantly increased the levels of the cytokines studied in all experimental groups. We also detected that lactobacilli-treated mice showed significantly higher levels of BAL IL-6, IFN-β and TNF-α than controls, being Lr06 and HkLr06 treatments more efficient than Lr05 or HkLr05 to improve the production of these factors (Figure 8 ). In addition, levels of IFN-γ and IL-10 were significantly improved by Lr05, HkLr05 and Lr06 treatments while nasal administration of HkLr06 increased IFN-γ but not IL-10 production in the respiratory tract. Moreover, Lr05 and HkLr05 were more effective than Lr06 to increase BAL IFN-γ and IL-10 concentrations in response to RSV challenge ( Figure 8 ). The effect of Lr05, HkLr05, Lr06 and HkLr06 treatments on the production of serum TNF-α, IFN-β, IL-6, IFN-γ and IL-10 was similar to that found in BAL (data not shown). In the present work we studied the effect of nasally administered immunobiotic lactobacilli on the respiratory antiviral immune response and evaluated their capacity to improve protection of infant mice against RSV infection. Three important conclusions can be inferred from the results presented in this study: a) nasally administered Lr05 and Lr06 differentially modulate the TLR3/ RIG-I-triggered antiviral respiratory immune response; b) nasal priming with Lr05 or Lr06 strains increase the resistance of infant mice to RSV infection and; c) the viability of the immunobiotic strains is not a necessary condition to achieve the immunoregulatory protective effect. a) Nasally administered Lr05 and Lr06 differentially modulate the TLR3/RIG-I-triggered antiviral respiratory immune response. We have previously demonstrated that nasal administration of three once-daily doses of poly(I:C) resulted in a marked impairment of lung function that was accompanied by inflammatory cell recruitment into the airways and the production of pro-inflammatory mediators [12] . Accordingly, in the present work we observed increased LDH activity and albumin concentration in BAL as well as increased levels of type I IFNs, TNF-α and IL-6, in the respiratory tract of poly(I:C)-challenged mice. We also showed that this immune response can be modulated with the preventive nasal administration of Lr05 or Lr06, demonstrating in addition that each strain has a different immunoregulatory effect. While Lr06 administration had a significant effect on the production of IFN-α, IFN-β and IL-6 in the response to poly(I:C) challenge, nasal priming with Lr05 was more effective to improve levels of IFN-γ and IL-10. It was demonstrated that poly(I:C) elicited the secretion of type I IFNs, TNF-α and IL-6 and other cytokines in respiratory epithelial cells [20] , therefore a likely source of these cytokines following poly(I:C) administration may be the airway epithelium. Then, Lr06 administration would have a significant effect on respiratory epithelial cells. Taking into consideration that IFN-α and IFN-β up-regulate several genes involved in viral defense but also genes of major importance for the development of a strong Th1 response, it can be speculated that Lr06 may play an important role in the improvement of innate and specific immune responses against respiratory virus through the stimulation of antiviral defenses in epithelial cells (Figure 9 ). In addition, we have previously demonstrated that nasal administration of poly(I:C) activates respiratory MHC-II + CD11c + CD11b low CD103 + (CD103 + DCs) and MHC-II + CD11c + CD11b high CD103 -(CD11b high DCs) cells and increases CD3 + CD4 + IFN-γ + and CD3 + CD4 + IFN-γ + T cells in lungs, indicating the generation of a Th1 response [12] . The result presented here shows that nasal administration of Lr05 has the capacity to improve Th1 response since higher levels of BAL IFN-γ and lung CD3 + CD4 + IFN-γ + T cells were found in Lr05-treated mice. Moreover, we showed that Lr05 administration significantly activated CD103 + DCs, an affect that was not achieved by Lr06. Considering that recent studies suggested that lung CD103 + DCs are more potent at eliciting Th1 and Th17 responses than CD11b high DCs [21] , we can speculate that Lr05 is more efficient than Lr06 to stimulate CD103 + DCs and improve Th1 response in the respiratory tract ( Figure 9 ). Lr05, Lr06, HkLr05 or HkLr06 were nasally administered to different groups of mice for 2 consecutive days at a dose of 10 8 cells/mouse/day. After lactobacilli treatment, mice were nasally challenged with RSV and BAL cytokines were studied 12 (TNF-α, IFN-β and IL-6) or 48 (IFN-γ and IL-10) hours after the challenge. The results represent data from three independent experiments. Different letters indicate significant differences (P < 0.05). Nasal treatment with lactobacilli significantly reduced lung injuries caused by poly(I:C) administration. We previously suggested that IL-10 would be valuable for attenuating inflammatory damage and pathophysiological alterations in lungs challenged with the viral pathogenassociated molecular pattern poly(:IC) [12] . We demonstrated here that both Lr05 and Lr06 significantly improved the production of IL-10 in response to poly(:IC), however Lr05 was more efficient than Lr06 to upregulate the levels of this cytokine in the respiratory tract. Moreover, the lung tissue injury markers used in this study were significantly lower in Lr05-treated mice than in those receiving the Lr06 strain. Therefore, we confirmed that there is a direct connection between the improvement of IL-10 levels induced by immunobiotics and the protection against lung injury after respiratory poly(I:C) challenge. Moreover, in this work we demonstrated that CD3 + CD4 + IL-10 + T cells would be the source of the IL-10 produced after poly(I:C) challenge and that this immune cell population would be quantitatively and functionally modulated by Lr05 since increased levels of CD3 + CD4 + IL-10 + T cells were found in lungs of Lr05-treated mice when compared with controls and those receiving Lr06 (Figure 9 ). Recently, Figure 9 Proposed mechanism for the improvement of antiviral immunity and resistance against respiratory syncytial virus (RSV) infection induced by nasally administered lactobacilli. Weiss et al. [4] demonstrated that CD4 + T cells produce the majority of IL-10 in vivo during an acute RSV infection and that this cell population is involved in the protective effect against lung tissue damage. Therefore, the increase in the numbers of CD4 + IL-10 + T cells induced by nasal treatment with lactobacilli could have an important role in the protection against RSV infection. Similar to the changes observed in intestinal immunity after oral administration of Lr05 or Lr06 [12] , we demonstrated in this work that nasal priming with both lactobacilli has the ability to improve antiviral immunity but using different mechanisms ( Figure 9 ). Moreover, considering that activating host immune responses during RSV infection is dependent on complex signaling events initiated in part by PRRs such as TLR3 or RIG-I and that these coordinated signaling events promote the production of cytokines, chemokines, IFN-α, IFN-β and IFN-γ in the lung that are crucial for the virus clearance, we speculated that Lr05 or Lr06 nasal treatments would beneficially modulate the immune response against RSV and improve resistance of mice against this viral respiratory infection. b) Nasal priming with Lr05 or Lr06 strains increase resistance of infant mice to RSV challenge. In the last years, some lines of evidence showed that nasal administration of immunobiotics is able to increase resistance against respiratory viral infections [22] . It was reported that intranasal priming with L. rhamnosus GG to BALB⁄ c mice significantly reduced the frequency of accumulated symptoms and induced a higher survival rate than control mice after challenge with influenza virus H1N1 [23] . Authors demonstrated that L. rhamnosus GG significantly increased lung NK cell activation and expression of TNF-α, IL-1β and MCP-1 enhancing respiratory cell-mediated immune responses. In addition, it was reported recently that nasal priming with lactobacilli is highly effective at suppressing virus-induced inflammation in a pneumonia virus mouse model [16] . Nasal priming with lactobacilli resulted in marked suppression of IFN-inducible protein-10 (CXCL10), MCP-1 (CCL2), neutrophil-activating protein-3 (CXCL1), MIP-1γ (CCL9), TNF, and eotaxin-2 (CCL24) in response to pneumovirus infection and significantly increased the resistance against the lethal disease. In this work we extend these findings by demonstrating that nasally administered immunobiotics are able to increase protection against RSV infection in infant mice. We observed that challenge of three weeks old BALB/c mice with RSV significantly altered lung function, induced tissue injury and triggered inflammatory response. This is in line with previous work reporting that RSV intranasal infection led to significant clinical characteristics in female BALB/c mice [24] . Authors described ruffled fur and ataxia that occurred early after hour 12 post-RSV infection and continued for 3 days [24] . Natural human RSV infection in children and experimental RSV inoculation in mice result in prominent local secretion of proinflammatory cytokines, such as TNF-α, IL-6, IL-8, MIP-1, RANTES, and MCP-1 as well as type I IFNs [25] . It has been shown that type I IFNs, IL-6 and TNFα contribute to clearance of the virus during the early stages of RSV infection however, continued production of these pro-inflammatory mediators exacerbates illness and tissue injuries during the late stages of RSV infection [26] . Interestingly, it was found that IL-10 deficiency during RSV challenge did not affect viral load, but led to markedly increased disease severity with enhanced weight loss, delayed recovery and a greater influx of inflammatory cells into the lung and airways and enhanced release of inflammatory mediators [27] . Therefore, during acute RSV infection, it is imperative that the host's inflammatory response is tightly regulated, enabling virus elimination but limiting the detrimental effects of inflammation on the lung tissue. Then, an adequate balance of pro-inflammatory and antiinflammatory factors is essential for a safe and effective antiviral immune response [28] . Moreover, considering that it was reported that no changes in peak viral titers or viral clearance were observed between IL-10 KO or anti-IL-10R mAb-treated mice compared with their controls [4] , preventive or therapeutic approaches aimed at increasing IL-10 production may offer a means to decrease RSV-induced immunopathology without affecting viral clearance. We demonstrated in this work that nasal administration of Lr05 or Lr06 improved the production of proinflammatory mediators in response to RSV challenge and also the production of IL-10, which would allow an effective immunological clearance of the virus without affecting lung tissue. Similarly to our results using poly (I:C) challenge, we observed that Lr05 was the strain with the highest capacity to improve levels of IL-10 and was more effective than Lr06 to enhance virus clearance and to protect lungs against the inflammatory damage. Then, our results also support the idea that modulation of respiratory IL-10 during RSV infection is an effective way to improve the outcome of viral disease. Moreover, we demonstrated here that nasally administered immunobiotics are an interesting alternative to achieve that immunoprotective effect. Following RSV infection, there is an initial influx of NK cells to the site of infection that produce IFN-γ and are cytotoxic to virus-infected cells. This is followed by recruitment of helper CD4 + and cytotoxic CD8 + lymphocytes to the site of infection. IFN-γ enhances the differentiation of CD8 + lymphocytes and influences the differentiation of CD4 + lymphocytes that contribute to the generation and amplification of the humoral and cellular immune responses. While the RSV-specific T cell response plays a major role in viral clearance and the clinical outcome of infection, both Th2-biased CD4 + and CD8 + T cells have been implicated in immunopathogenesis [29] . In general, a Th2 immune response is favored during RSV infection, especially in younger hosts. RSV-induced pulmonary inflammation in mice was previously found to cause a shift from Th1 to Th2 cell inflammation. RSV uses multiple mechanisms to induce a Th2 cell response in the host, including RSV G protein-mediated effects [30] , increasing IL-4 production from basophils [31] and induction of alternatively activated macrophages [32] . Moreover, the excessive mucus production, airway plugging, wheezing, and long-lasting effects on lung function that are common manifestations of RSV disease have some similarity with asthma, which involves a Th2-bias [29] . Therefore, strategies aimed to improve Th1 during RSV infection would beneficially modulate the outcome of the infections especially in younger hosts. In this work, we showed that nasally administered immunobiotics were able to improve respiratory Th1 response since significantly higher levels of IFN-γ were found in the respiratory tract of lactobacilli-treated mice. Then, modulation of respiratory immunity potentiated by the immunobiotic strains might contribute to an improved Th1 response and thereby favor protective immunity against viral infections such as RSV. c) Viability of the immunobiotics strains is not a necessary condition to achieve the immunoregulatory protective effect. Few studies have demonstrated that the nasal administration of heat-killed immunobiotics is able to improve resistance against respiratory pathogens [11, 15, 16] . In this regard, earlier studies by Hori et al. [15] showed that the nasal administration of heat-killed L. casei Shirota stimulated cellular immunity in the respiratory tract and significantly increased the resistance of adult BALB/c mice to influenza virus infection. The authors investigated the production of various cytokines by mediastinal lymphoid node cells in mice receiving L. casei Shirota intranasally and found that the Shirota strain strongly induced production of IL-12 in these cells, which is an important cytokine for cytotoxic T cells and NK cells stimulation and enhancement of Th1 cytokines. In addition, both IFN-γ and TNF-α levels were improved in mediastinal lymphoid node cell cultures from mice administered L. casei Shirota intranasally, after influenza virus challenge [15] . Later it was reported that intranasal administration of heat-killed L. pentosus S-PT84 strongly enhanced Th1 immunity, IFN-α production and NK activity in the respiratory immune system and protected against influenza virus infection [33] . In addition, as mentioned above, it was demonstrated that priming of the respiratory mucosa with lactobacilli results in full protection from the otherwise lethal severe pneumovirus infection and that protection is observed in response to both live and heat-killed L. plantarum and L. reuteri [16] . That work demonstrated that nasal treatment with heat-killed immunobiotics resulted in diminished virus recovery at multiple time points and prominent suppression in the production of virus-induced proinflammatory mediators. The results of our work are in line with these previous observations since administration of both Lr05 and HkLr05 were equally effective to improve resistance of infant mice to RSV infection and reduce lung injuries. Interestingly, although both Lr06 and HkLr06 showed a similar capacity to reduce lung RSV titers, Lr06 was more effective than HkLr06 to reduce lung injuries during RSV infection. These differential effects achieved by the four treatments would be related to their specific capacities to modulate the production of IFN-γ and IL-10 after RSV challenge. All of them were able to improve respiratory IFN-γ levels and reduced viral loads, while Lr05, Lr06 and HKLr05 but not HkLr06 increased IL-10 and reduced lung injuries. Then our results suggest that not all the heat-killed bacteria derived from immunobiotic strains maintain the immunoregulatory effect after heat treatment. This would be an important point to consider when selecting immunoactive nonviable strains. Previously, we demonstrated that the nasal treatment of malnourished mice with heat-killed L. casei CRL431 was able to increase their resistance to the infection with the respiratory pathogen S. pneumoniae [11] . The results from that study suggested that heat-killed lactobacilli are also effective in the immunomodulation of the respiratory immune system in immunocompromised hosts. Therefore, immunobiotic bacteria in the form of live cells may not be required for enhancing respiratory defenses against bacterial and viral pathogens. Our previous and present results show that non-viable immunobiotics or their cellular fractions could be an interesting alternative as mucosal adjuvants, especially in immunocompromised hosts in which the use of live bacteria might be dangerous. In addition, heat-killed immunobiotic have the advantages of allowing a longer product shelf-life, easier storage, and transportation. Therefore, to study the capacity of non-viable Lr05 or its cellular fractions to beneficially modulate the immune response against respiratory virus infections in immunocompetent and immunocompromised hosts is an interesting topic for future research. In the present work we demonstrated that nasal administration of immunobiotics is able to beneficially modulate the immune response triggered by TLR3/RIG-I activation in the respiratory tract and to increase the resistance of mice to the challenge with RSV. As it has been reported for orally administered probiotic bacteria, our results demonstrated that the immunoregulatory effect of nasally administered lactobacilli is a strain dependent effect. Comparative studies using two Lactobacillus rhamnosus strains of the same origin and with similar technological properties [6, 7] showed that each strain has an specific immunoregulatory effect in the respiratory tract and that they differentially modulate the immune response after poly(I:C) or RSV challenges, conferring different degree of protection and using distinct immune mechanisms. We also demonstrated in this work that is possible to beneficially modulate the respiratory defenses against RSV by using heat-killed immunobiotics. Moreover, our results showed that not all heat-killed bacteria derived from viable strains with immunomodulatory capacity, are also able to functionally modulate the respiratory immune system. Therefore, detailed studies of the immunoregulatory capacities of heat-killed immunobiotics or their cellular fractions are necessary in order to find those with the highest potential to be used for improving defenses against respiratory viruses.  
paper_id= ffe833361699d2f16badccffd240b831813f38c2	title= 	authors= 	abstract= 	body_text= of COVID-19 (<4%). Another zoonotic novel coronavirus, MERS-CoV, was responsible for the Middle East respiratory syndrome, which had a case-fatality rate of 34%. Our experiences in coping with the previous coronavirus outbreaks have better equipped us to face the challenges posed by COVID-19, especially in the health care setting. Among the insights gained from the past outbreaks were: outbreaks caused by viruses are hazardous to healthcare workers; the impact of the disease extends beyond the infection; general principles of prevention and control are effective in containing the disease; the disease poses both a public health as well as an occupational health threat; and emerging infectious diseases pose a continuing threat to the world. Given the perspectives gained and lessons learnt from these past events, we should be better prepared to face the current COVID-19 outbreak. coronavirus, COVID-19, health care, occupational health, outbreaks, public health, SARS-CoV-2 confirmed cases and over 62 000 deaths spread over 200 countries and territories. 3 Initial reports suggested that the overall case-fatality of COVID-19 infection appeared to be approximately 2%. 4 The case-fatality was much higher in the city of Wuhan (currently around 4.9%) as compared to other parts of China and the rest of the world. The disease has an estimated mean incubation period of 5.2 days (95% CI 4.1-7.0) and a basic reproductive number (Ro) of 2.2 (95% CI 1.4-3.9). 5 It is possible that people with COVID-19 may be infectious even before showing significant symptoms. 6 However, it is believed that those who have symptoms are the ones who are primarily causing the spread of the infection. These figures have been updated recently, with an estimated Ro of between 2.0 and 2.5 for SARS-CoV-2, incubation period of 1-14 days (mean 5-6) days and a mortality rate of about 3.8%, based on a larger sample of 55 924 cases. 7 This event evoked a strong sense of déjà vu, as it has many parallels to the outbreak of SARS during 2002 and 2003. Similar to COVID-19, SARS originated during winter in China, spread rapidly all over the world affecting 37 countries, and was caused by a zoonotic novel coronavirus. However, in comparison, SARS affected far fewer people (8098 reported cases) but had a higher case-fatality rate of 9.6% (774 deaths). Another zoonotic novel coronavirus, MERS-CoV, originated in the Middle East in 2012. In all, there have been 2494 reported MERS-CoV cases resulting in 858 deaths (case-fatality rate, 34%) in 27 countries. Outbreaks of MERS have been reported in hospitals in countries such as Saudi Arabia, Jordan, and South Korea. 8 Our experiences in coping with the previous SARS and MERS outbreaks have better prepared us to face the new challenge posed by COVID-19. In particular, we learnt many valuable lessons from dealing with the SARS outbreak, which was an unprecedented event. The perspectives and experiences gained from managing SARS, 9 and comparisons to our responses to the COVID-19 outbreak include the following: Healthcare workers (HCWs) in health care establishments include doctors, nurses, laboratory and paramedical staff, health attendants and cleaners. Other than health care workers, anyone who are physically present or associated with health care institutions were at high risk of infection by SARS-CoV. Worldwide, HCWs comprised a significant 21% of all SARS patients, but in countries such as Canada and Singapore, more than 40% of the patients were HCWs. 10 Performance of certain procedures, such as intubation and nebulization of SARS patients was recognized as having a significant risk of infection. However, even low-exposure situations and transient exposures to infected cases posed infection risks. There were also reports of "super-spreaders," who often were initially undiagnosed, and who spread the disease to clusters of HCWs. Healthcare workers are also a recognized high-risk exposure group to SARS-CoV-2. As of 2 March 2020, more than 3400 HCWs have been infected in China, with 13 deaths reported. 11 A COVID-19 "super-spreader" was reported in a Wuhan hospital. The patient presented with abdominal symptoms and was initially admitted to a surgery department, resulting in over 10 HCWs being infected. 12 In many other countries, currently thousands of HCWs have been infected, and hundreds have died, though not all have occurred because of occupational exposure. This is because household and community transmission have also played a role in the infection of HCWs. In general, HCWs are now better equipped and better trained and prepared with infection control techniques as compared to the time of SARS. The number of HCWs as a proportion of all cases of COVID-19 appears to be smaller and HCWs comprise less than 20% of cases. Several factors may have contributed to this. For example, in Singapore, there are established occupational medicine departments to protect HCWs in major Singapore government hospitals in 2020. In comparison, no such departments existed in 2003. To reduce the risk of occupational exposure to infection, personal protective equipment (PPE) have been stockpiled, HCWs are mask fitted, repeated training in infection control techniques have been given, and strict adherence to infection control protocols is mandated. There is also extensive daily monitoring of health among staff. 13 World Health Organization has developed several technical guidance documents regarding COVID-19 for HCWs, including rights, roles and responsibilities of HCWs 14 which comprises key considerations for safety and health. As part of a WHO preparedness and response initiative, they have also established a risk assessment tool that is to be used by health care facilities to determine the risk of SARS-CoV-2 infection of all HCWs who have been exposed to a COVID-19 patient. This tool also provides recommendations for appropriate management of these HCWs, according to their infection risk. 15 However, the magnitude of the pandemic, with its explosive increase in number of infected patients requiring treatment in health care facilities, has resulted in health care establishments being overwhelmed in many countries. There are shortages of PPE such as N95 masks and surgical masks for health care workers, and ventilators for patients even in the more developed countries with robust health care systems. 16 There has also been a profound loss of trust in authority with perceptions that policy is guided more by scarcity | 3 of 6 OPINION than science. 17 In the developing countries, the situation is even more dire. During the SARS outbreak, HCWs in affected countries worked under great stress and in constant fear. Besides being exposed to the virus, they experienced fatigue, burnout, stigma and were at risk for physical and psychological violence. The need to protect themselves by having to wear uncomfortable PPE at all times, the necessity to monitor body temperature several times a day, enforced restriction of movements within and between health care establishments and having to work long hours in physically separate teams were common features of health care work. About a third (29%-35%) of hospital workers in Toronto 18 experienced a high degree of distress, as measured on the Impact of Event Scale. In Singapore, over ten thousand HCWs in nine health care settings were surveyed during the SARS outbreak. 19 Many reported feeling more stressed at work, experiencing an increase in workload and having to work overtime. Most respondents agreed that "people close to me are worried for my health," and that "people close to me are worried they might get infected through me." In addition, there was also fear and stigmatization of HCWs and their family members from the public because of their occupation. (Table 1) . During this COVID-19 outbreak, HCWs are similarly working under extreme conditions over long hours. Many HCWs have fears for their personal health and many have their family members worried for them. The need for management of stress and fatigue among HCWs is important and should be recognized and provided for. 20 At the same time, stigmatization and ostracization of HCWs have been witnessed. Due to their occupation, HCWs are shunned and harassed by some members of a fearful public. These reactions arise largely from ignorance and anxiety. Such adverse reactions have also been directed towards other groups of people, such as those under quarantine, or persons of specific races and nationalities. The CDC has identified persons of Asian descent, people who have travelled and emergency responders or healthcare professionals as groups who may be at risk of being stigmatized. 21 On the other hand, there have are also been positive reactions from the public and strong expressions of gratitude and support for HCWs. Many members of the public do appreciate the HCWs' dedication to work and the sacrifices they make and there has been a general outpouring of support from the public for HCWs in many countries. Besides the obvious stressors of working long hours under conditions of risk of infections, and separation from their families and loved ones another mental health challenge faced by health care workers is the dilemma they face when deciding how to ration scarce health care resources to patients. 22 In countries such as Italy, where the number of patients who need ventilators outnumbered the available equipment, health care workers were forced to make uncomfortable life or death decisions. 23 The lack of resources contributed in part, to the high COVID-19 death rate in Italy. This is unfortunately another challenge seen in COVID-19 outbreak that was not encountered during the SARS outbreak. Mental health support for HCWs can be provided via multidisciplinary mental health teams, which include psychiatrists, psychiatric nurses, clinical psychologists, and other mental health workers. Regular, accurate and clear communication updates should be provided in order to allay prevailing uncertainty and fear that the HCWs are experiencing. HCWs caring for COVID-19 patients may also require regular clinical screening for depression and anxiety. 20 However, specific mental health issues may need to be managed by different approaches. For example, one possible solution to help clinicians directly involved in managing critically ill patients from making ethically difficult choices such as deciding who receives ventilator care, is to form a triage committee. 24 Such a committee can comprise volunteers, including respected clinicians and leaders, among their peers. The committee can be tasked to make these difficult choices, in order to spare the frontline clinicians from the dilemma of rationing scarce medical resources. Severe acute respiratory syndrome was spread to HCWs mainly by direct mucous membrane contact with infectious respiratory droplets and exposure to contaminated surfaces. Prevention and control measures were early detection and isolation of cases and quarantine of exposed members of the public. These measures were effective. For example, secondary cases of SARS were minimal when the source cases were isolated within 2 days of onset of symptoms. However, if isolation was delayed, the number of secondary cases increased rapidly. For HCWs, effective preventive measures involve wearing of gloves, gowns, eye protection, N95 masks, practising good personal hygiene, and self-monitoring for early disease symptoms and early treatment. Although the implementation of such measures on a massive scale was initially challenging, most health systems and HCWs eventually coped. However, sustaining such extensive preventive measures over prolonged periods was difficult. For the COVID-19 outbreak, we have witnessed extensive contact tracing and quarantine measures implemented in major Chinese cities, affecting millions of people. Implementing such measures requires advance planning of suitable locations for quarantine, giving support to patients who are in quarantine and being strict to those who break the laws by enforcing penalties. Health care resources have also been marshalled and mobilized on an unprecedented scale to respond to those who require treatment. However, a current challenge is the worldwide shortage of medical supplies such as PPE and medical equipment such as ventilators, and even test kits for diagnosis. Advice given for the general public for COVID-19 has seen a greater emphasis placed on hand washing, personal hygiene (such as not touching the face with contaminated hands), respiratory hygiene (eg, practising cough etiquette by coughing or sneezing away from others or into the sleeves and wearing masks if feeling unwell), social distancing, avoidance of crowds, travel advice and advice that persons who are well need not wear masks. 25 Due to the limited supply of masks, the initial general advice has been for masks to be worn by ill patients and people who have close contact or who are looking after ill patients. For the general public, information on types of masks to wear (surgical masks would suffice, rather than N95 masks), the proper way of wearing and disposing of masks in a safe and socially responsible manner has been given. 26 However, it is important to note that such information continues to evolve. The latest guidelines from Centers for Disease Control and Prevention (CDC) in April 2020 27 recommend that masks or face coverings should be worn by the public when they go out. One reason for the change in advice is the recognition that there may be asymptomatic cases who might spread infected respiratory droplets to other members of the public if they are not wearing face coverings or masks. Social distancing has been implemented as part of the efforts to "flatten the curve". The "curve" refers to the projected number of people who will contract COVID-19 over a period of time. By implementing community isolation measures, the daily number of disease cases can be kept at a manageable level for medical providers, hence it may help lessen the healthcare burden. 28 As the situation evolves, some countries are employing more restrictive measures such as travel bans and lockdowns. It was reported that by the end of March 2020, more than 100 countries had instituted either a full or partial lockdown, impacting billions of residents. 29  Severe acute respiratory syndrome was widely viewed as a public health threat but was less appreciated as an occupational disease. Among the occupational groups at risk were HCWs, animal and food preparation handlers, transport workers (ranging from flight attendants to taxi drivers), and laboratory researchers working with the SARS-CoV. For example, more than a third of the early cases of SARS (pre-February 2003) occurred in persons who handled, killed, or sold food animals, or in those who prepared or served food. Thus, in addition to public health measures, an appropriate occupational health response is also necessary. 10 In the current COVID-19 outbreak, diverse occupational groups are recognized to be at risk. For example, in Singapore, 68% of the first 25 locally transmitted cases were probably related to occupational exposure. 30 The workers who are in the hospitality, retail, food and beverage industry who served infected tourists, transport workers, multinational company workers who attended an international meeting, a domestic worker and even a security officer who served quarantine orders were all at risk. These occupations were not covered by the occupational health legislation of Singapore in 2003, which was the Factories Act. However, in 2020, the Workplace Safety and Health Act in Singapore applies to many of these workers. Thus, if and when the disease is officially recognized as an occupational disease, these workers (and not only factory workers) will be covered by the health and safety law. A major concern of many workers is the fear of job losses or loss of income. This is apparent from the economic impact of COVID-19 where in many countries, non-essential services have been halted, and many people stay at home and avoid going out for shopping or entertainment. Self-employed workers, workers in a gig economy, and those working in entertainment, hospitality, tourism and travel sectors, to name a few, will be threatened with loss of income and job losses. In order to manage the economic fallout, many governments have provided stimulus packages to assist such groups. 
paper_id= ffe956818d45ae4bd8957e0dc86adf3ccae67baf	title= •NEWS FOCUS• Spatiotemporal changes of epidemics and their relationship with human living environments in China over the past 2200 years	authors= Shengsheng  Gong;Haichao  Xie;Fahu  Chen;	abstract= 	body_text= The Chinese character "Yi", meaning "epidemic", was inscribed on oracle bones as long ago as the Yin-Shang period (1300B.C.E.-1046B.C.E.), and the earliest extant description of the relationship between epidemic diseases and animals in China is in the oldest physical geographical monograph of China, entitled Shan Hai Jing. Subsequently, historical documents such as official histories, chronicles, memoirs, archives, anthologies, medical records, newspapers, and periodicals, also provide valuable historical materials for the study of epidemics in China. During the past 20 years, the historical medical geography research team of Central China Normal University in Wuhan has focused on the study of epidemic history in China (Gong, 2003 (Gong, , 2019 , and it recently produced a book entitled Annals of Epidemics in China Over the Past 3000 Years (Gong, 2019) . More than 8931 historical documents and 6622 references were used and it is the most comprehensive compilation of documentary evidence of epidemics in China. More details about this book please see Appendix 1 (https://link.springer. com). Historical records of epidemics predating the Qin Dynasty are rare and therefore in the present study, based on the data of Gong (2019), we discuss the spatiotemporal pattern and possible mechanisms of epidemics since the Qin Dynasty (220B.C.E.-AD1949). During the past 2200 years, the frequency of epidemics in China has gradually increased ( Figure 1a ). During the Han Dynasty (206B.C.E.-AD220), epidemics occurred roughly once every two decades; during the Wei, Jin, Southern and Northern dynasties (AD220-AD581), they occurred everỹ 5 years; during the Northern Song Dynasty (AD960-AD1127), every~3 years; and during the period of the Republic of China (R.O.C.), roughly every year. In terms of epidemics the entire historical period can be divided into three stages. In the first stage (220B.C.E.-AD1000), the epidemic index (the epidemic index is the number of years in which epidemics occurred per decade) was less than 5, with an average of 1.46. During the Han and Tang dynasties, the epidemic index was the lowest, and during several decadal intervals there were no epidemics. During the late Eastern Han Dynasty and the Wei, Jin, and Northern and Southern dynasties, the epidemic index was relatively high, resulting in the first epidemic peak during the past 2200 years. During the second stage (AD1000-AD1450), epidemics occurred almost once every decade, and the epidemic index increased substantially, averaging 4.64. Only during the Southern Song Dynasty (AD1240-AD1280) and the early Ming Dynasty (AD1430-AD1440) were epidemics infrequent. During the third stage (AD1450-AD1949) epidemics were the most frequent during the past 2200 years, with an average epidemic index of 9.44, and within the territory of modern China, epidemics occurred almost every year (Figure 1a ). During the first two stages there is a roughly centennial-scale cyclicity in the number of epidemics, each cycle characterized by a gradual increase followed by a rapid decrease, which is more clearly evident in the smoothed record ( Figure 1a ). In addition to the long-term increasing trend and high-frequency fluctuations, there are also temporal changes in the seasonality of epidemics. More details please see Appendix 2. During the last 2200 years, there were pronounced spatial differences in the number of epidemic years per province in China, with the largest number in the southeast, followed by central China, and with the smallest number in the northwest ( Figure 2a ). During the past 300 years, since the spatial resolution of epidemics can be increased to county level, the spatial differences in epidemics can be evaluated on the basis of the modern county administrative divisions of China in 2011 ( Figure 2b ). Among all 2435 counties in China today, 2211 of them have experienced epidemics, which means that as many as 90.8% of the counties experienced epidemics during the past 300 years. During this interval there were a cumulative total of 27511 counties which experienced epidemics; the proportion for densely populated southeastern China is 88.03%, and that for sparsely populated northwestern China is 11.97%. Of the 1926 counties in southeastern China, 1860 of them have experienced epidemics, with a coverage of 96.6%. In contrast, only 351 out of 509 counties in northwestern China experienced epidemics, with a coverage of 69.0%. Thus the spatial distribution of epidemics was essentially the same as that of the human population. In contrast to other natural disasters, epidemics are population density dependent; therefore, coastal areas with a high population density and fast economic development (such as the Yangtze River Delta), the transport hub cities with high population mobility (such as Lanzhou, Wuhan, and Shanghai), and major traffic routes (major road routes such as the Hexi Corridor in Gansu, and waterways such as the Grand Canal) had a high risk of epidemics ( Figure 2) . The frequency of epidemics in China has also changed spatially, with the epidemic center moving from north to south over the past 2200 years. During the Pre-Qin and Han dynasties, the major epidemic-affected areas were located in the middle and lower reaches of the Yellow River and then gradually moved southwards. During the Han and Jin dynasties, the epidemic center reached the Yangtze River and its major tributary areas. After the Tang and Song dynasties, the center expanded to the entire Yangtze River drainage and the region of Fujian and Zhejiang provinces. After the Yuan Dynasty, the center further expanded to Guangdong, Guangxi and Yunnan provinces. In the Ming Dynasty, Hainan began to experience epidemics, and in the Qing Dynasty, Taiwan, northeast China, Xinjiang, and other frontier areas were also affected. Outbreaks of disease caused by Falciparum malaria were the oldest epidemics in China and were called "Zhang Yi". Over the past 2000 years, the northern boundary of areas in which "Zhang Yi" was prevalent has gradually moved southward. During the interval from the Warring States period to the West Han Dynasty, its northern boundary extended along the Qinling Mountain to the Huaihe River; from the Sui through to the Tang to the Five dynasties, it was along the Dabashan Mountains; and during the Ming and Qing dynasties it was along the Nanling Mountains (Gong, 1993) . Epidemics are biological disasters resulting from the wide spread of infectious diseases caused by bacteria, viruses and parasites. The complex natural environment and living environment (Chen et al., 2019) have a major impact on the spatial and temporal distribution of epidemics. Important environmental factors such as precipitation (humidity) and dust storms can affect the frequency of epidemics via their effects on pathogens, hosts, and vectors. Study of historical epidemics in China shows that within a given year epidemics mainly occur in the relatively warm and wet summer and autumn seasons; and in terms of spatial distribution, relatively warm and humid southeastern China is more prone to epidemics. Thus the cumulative number of epidemic years exhibits the spatial pattern of a gradual decrease from southeast to northwest (Figure 2 ), which is highly consistent with the spatial pattern of the major attributes of China's natural environment. In terms of spatial distribution, the climate of China has not significantly changed during the past 2200 years, and therefore we calculated the correlation between average annual temperature and precipitation of counties in modern China with their respective cumulative number of epidemic years. The results showed that irrespective of historical period or region, there is a significant positive correlation. On the millennial scale, the frequency of epidemics is significantly negatively correlated with temperature (r= −0.334, p=0.01) during the past 2200 years, which indicates that epidemics were relatively frequent in cold periods. This relationship is also evident on the centennial scale; for example, epidemics were relatively rare during warm periods, such as in the Qin-Han dynasties and the Sui-Tang dynasties, and relatively frequent during cold periods, such as in the Wei-Jin, Southern and Northern dynasties, and the Ming-Qing dynasties (Figure 1a) . Changes in the temperature anomaly record of the Northern Hemisphere and eastern China, and annual precipitation in northern China, are negatively correlated with changes in the epidemic index (Figure 1a, 1e-1g) . During the past 2200 years, the importance of human activities has exceeded that of natural factors, becoming the dominant factor in determining the incidence of dust storms in northern China (Chen et al., 2020) . During warm and humid periods, such as the Han and Tang dynasties and the medieval warm period, large-scale land reclamation and clearance resulted in substantial damage to the natural vegetation, leading to frequent dust storm outbreaks (Figure 1d ). This contrasts with the relatively few epidemics during this period (Figure 1a) . Notably, however, climate and other environmental changes are not the dominant factors determining the frequency of epidemics. For example, the relationship between epidemics in Europe and temperature changes is not obvious (Yue and Lee, 2018) . In fact, the trend of epidemic frequency in China during the past 2200 years is more consistent with population change (Figure 1c ). Since the Five dynasties and Ten Kingdoms periods the trend of gradually increasing population in China is consistent with that of the epidemic index (Figure 1 ). This is because large and dense populations are more susceptible to the outbreak and spreading of epidemics. However, population size alone is not the dominant factor determining the occurrence of epidemic outbreaks. For example, the population of the Han and Tang dynasties was relatively large but the epidemic frequency was low ( Figure  1a ), which indicates that epidemic outbreaks were also related to social stability. During times of civil unrest and warfare (Figure 1b) , such as the Wei, Jin, Southern and Northern dynasties, epidemics were more frequent. This is evidenced by the fact that on short timescales, changes in the frequency of epidemics are largely consistent with the occurrence of wars ( Figure 1a) . From the perspective of human history, "epidemics always follow wars" (Gong et al., 2019) . In northern China, the climate was relatively warm and wet when the East Asian summer monsoon was strong. These conditions promoted the development of rainfed agriculture and resulted in periods of cultural development, social stability, and national unity, when epidemics were always relatively rare. In contrast, the climate was relatively cold and dry when the East Asian summer monsoon was weak, which resulted in frequent droughts and poor harvests, and these periods were prone to social unrest and national secession and epidemics were always relatively frequent. Human society has always faced threats from the natural environment, amongst which epidemics caused by pathogenic microorganisms are the most important. Pathogens have evolved along with humans, with the result that epidemics have occurred throughout human history. With the development of agricultural civilization from the beginning of the Neolithic age, the human living environment has altered and generally improved. Agricultural civilization in China originated in the Yellow River drainage from which it gradually expanded outwards. With the migration of populations and shifts in civilization centers, the areas subjected to epidemics has also changed. However, in southern China, the warm and humid climatic conditions, hence the flourishing vegetation and favorable natural environment, as well as the rapidly improving living and working environment in cities and towns with large populations, were conducive to the occurrence and spread of epidemics. As a consequence, the southern region of China has been prone to major epidemics in the past and this is likely to remain in the future. With the development of civilization, the intensification of human modification of the natural environment has increased the risk of epidemics. For example, because of a misunderstanding of Chinese traditional dietary culture ("Medicines and foods are homology"), the consumption of wild animals has been and remains relatively common . This increases the risk of the spreading to humans of pathogenic microorganisms originally only found in wild animals, resulting in the outbreak and spreading of new epidemics. For example, several plagues were caused by the consumption of marmots in China. Thus a prohibition on the consumption of wild animals is likely to be an important means of promoting human development within the context of a stable natural environment, which in turn promotes human welfare and safety and hence national security. Notably, legislation was recently introduced in China banning the trade in and consumption of wild animals, and that is learning from the past can inform and shape modern social and economic policy. Further systematic study of spatiotemporal changes in past epidemics in China is likely to assist in the prevention, control, and emergency management of future epidemics. 
paper_id= ffea289feac032cd0ba2c8890a9a725c763af652	title= What if the worst consequences of COVID-19 concerned non-COVID patients?	authors= Radjiv  Goulabchand;Pierre-Géraud  Claret;Benoit  Lattuca;	abstract= We highlight in this short article the side-effects of COVID-19 pandemic on the management of non-COVID patients, with potential detrimental and irreversible complications. We thus propose adjusted strategies to deal with both COVID and non-COVID patients. 	body_text= All health systems worldwide are currently facing a unique and fearsome sanitary situation and, consequently, are entirely distracted by the management of potential COVID-19 patients. In France, the first case of COVID-19 was confirmed on the 24 th of January, with an increasing number of cases reaching over than 190,000 cases, 8,000 patients requiring intensive care and more than 29,000 deaths related to the disease or its complication [1] . As described in the large majority of countries affected by the COVID-19 epidemic, the French health system was quickly and severely strained in cluster regions, before a nationwide dissemination. Use of a mobile intensive care unit (MICU) with a unique national emergency number (#15) and on-board emergency physicians, delivering prehospital therapies and providing rapid diagnosis, evaluation of severity, direct transfer to appropriate centers and use of mobile extracorporeal life support facilities for the sickest patients, is unique to the French health care system. However, the existing possibilities for patients to warn the health system were quickly overwhelmed, both for emergencies and subacute diseases. Hence, the access to the national emergency number rapidly became unavailable because of the high number of calls. As a consequence, the ability of the emergency departments to receive, evaluate and address COVID and non-COVID patients and maintain isolation procedures has been challenged. In the first cluster regions affected by the disease, the capacity to admit patients with severe respiratory syndrome was undermined because of the lack of beds in intensive care units (ICU). These findings and those from our European neighbors, especially from Italy, led the French government to enforce strict containment measures as of March the 17 th with the aim of influencing the epidemiological curve of the disease by drastically decreasing the reproduction risk (so-called "R0"). The influx of COVID-19 patients was slightly delayed in comparison to neighboring countries. This precious time enabled us to set up a wide range of measures and procedures: J o u r n a l P r e -p r o o f (i) a huge increase of available phone lines dedicated to emergency calls through our national emergency number; (ii) training and education of caregivers and doctors, and the dispatching of staff to emergency departments or COVID-19 dedicated wards; (iii) a dramatic increase in ICU bed availabilities, including the transformation of recovery rooms into ICU units; (iv) the cancelation or postponement of all non-urgent activities (surgeries, radiology acts, consultations); (v) the separation of medical beds into "COVID" and "non-COVID" beds in order to split both activities while in parallel globally increasing the number of these medical beds. The emergency department appears as the cornerstone for the application of these improved procedures. The assigned nurses and physicians conduct pre-examination and triage to divide visits into high-suspected and low-suspected Covid-19 patients using dedicated questionnaires. Patients are then conducted through specified pathways into high-risk and low-risk areas. After complete evaluation (clinical examination, SARS-CoV-2 polymerasechain reaction testing in nasopharyngeal sample, and thoracic CT-scan), patients are either hospitalized in dedicated wards or ICU, or may go back home (with instructions and scheduled phone visits). Nevertheless, this crisis and the subsequent reorganization of both hospital and town medicine would be associated with substantial side-effects that we would like to highlight. Due to the overload of the emergency lines, some patients were obliged to come to the hospital by their own transport even with severe conditions, such as myocardial infarction or stroke. As a consequence, they were exposed to delayed management, and a higher risk of complications and subsequent mortality. Even after reaching the emergency department, the diagnostic procedures and the access to technical facilities may have been delayed because of the dedicated measures to COVID's patient management. and with a high intensity of the COVID wave), decrease of 26% of all emergency department visits was observed, including a decrease of 34% of strokes, 32% of transitory ischemic attacks, 64% of unstable angina, 42% of appendicitis and 36% of seizures. We assume that patients may have been reluctant to call emergencies and come to hospital in this pandemic context and rather postponed healthcare even though their life could be at stake. This phenomenon has already been described for strokes [3, 4] . Emergency physicians should definitely be worried about this collateral damage that the COVID-19 outbreak is generating [5] . Such behavior could be very detrimental for patients with nonspecific initial symptoms associated with autoimmune diseases, for patients with undiagnosed or untreated cancers [6] , or for those with mental diseases [7] , and could potentially lead to irreversible complications. In a second time, COVID-19 outbreak is also having a huge impact on long-term diseases, especially people experiencing disability. Outpatient activities stopped for 87%, involving 318,000 patients per day in Italy, Belgium and UK, leading to an estimate range of 1,3-2,2 million in Europe [8] . This may lead to future cumulative effects due to reduced functional outcomes and consequent increased burden of care. Apart from the unavaibility of health care resources for COVID and non-COVID patients, this pandemic has also led to a media uproar, potentially detrimental to people's health [9] . Since the beginning of the disease in France, every doctorand every media outlet has speculated about predictive factors associated with the appearance of respiratory distress. The use of anti-inflammatory drugs was initially suspected (and still debated). As a consequence, many patients decided of their own accord to stop their long-term steroid treatment for their autoimmune diseases for instance. Even more seriously, some patients with heart diseases stopped taking their anti-antiplatelet therapy as they considered aspirin to be part of these anti-inflammatory drugs. Unfortunately, the exact proportion of patients experiencing this brutal treatment disruption is not easily reportable. Consequences of such disruption may appear in the following weeks. Finally, the great controversy around the potential benefits of hydroxychloroquine led to the reduction of pharmacies' entire stock of hydroxychloroquine [10] . This had two main consequences: (i) reported cases of chloroquine intoxication or adverse effects [11] ; (ii) difficulties for patients with systemic lupus erythematosus to obtain their usual treatment, risking a relapse of their disease, especially in low-income countries [12] . For all these reasons and despite the required reorganization of the health care system, we believe that adjustments to our management of such a crisis should be considered and could be extrapolated to other countries. Firstly, the emergency call number could be split into two lines: the first one dedicated to the "COVID-19" infection with trained people able to J o u r n a l P r e -p r o o f explain containment measures as well as disease surveillance at home, and the level of symptom severity at which an examination at hospital is required; and the second one dedicated to "non-COVID" patients. This second line could manage calls reporting heart problems, strokes, accidents, cancers, and other infectious or dysimmune diseases involving immunocompromised patients, and lead quickly and directly to consultations or tests with the appropriate specialists, completely bypassing the emergency department. This line, with a dedicated access to general practitioners, could reduce the delay in diagnosing and treating "non-COVID" patients. Some adjustments have already been proposed concerning heart diseases [13] . These procedures should also involve the management of chronic diseases and disabilities, in order not to let severe complications happens within this population, altogether with preventing them from being contaminated by the COVID-19. In this period when all potential inpatients can be carriers of the disease, all medical and paramedical teams are aware of potential risk of respiratory failure associated to COVID-19 infection. It is thus most unlikely that a patient, initially considered as "highly non suspicious of COVID", would suffer a delay in the management of a COVID-19 respiratory failure happening in a second time. This strategy should be supported by a large information campaign, supported by the media and validated by scientists and the government [9] . This could avoid the potential lostof-chance for patients with undiagnosed diseases, and also probably reduces the secondary risk of limited access to scheduled medical activity, in relation with the expected rebound after the epidemic. Moreover, in case of very large number of cases, a further degree of the health care system's reorganization should be the designation of "COVID" and "non-COVID" health centers in each town to reduce the risk of contamination, improve the patients' pathways, and to consolidate health facilities and experienced caregivers on the one hand while enabling This dreadful crisis severely challenges our health care system. In the past 10 years, H1N1 flu, SARS, and MERS epidemics have threatened global population but the contagiousness and mortality rates were never as high as those of COVID-19. The current crisis should make us think about how to manage an ongoing epidemic while maintaining a sufficient level of healthcare for other patients. This could also be helpful in the case of a prolonged epidemic or recurrences. One of the answers could be to propose procedures that would facilitate more direct access to hospitals for subacute diseases, without overloading the emergency department and the overall health care system. The authors declare there is no conflict of interest regarding this article and this topic.  
paper_id= ffea4b1b52a7d126ceceb64571992a3357e49a6b	title= Journal Pre-proof Interconnectable solid-liquid protein extraction unit and chip-based dilution for multiplexed consumer immunodiagnostics Interconnectable solid-liquid protein extraction unit and chip-based dilution for multiplexed consumer immunodiagnostics 3	authors= Georgina M S Ross;Daniel  Filippini;Michel W F Nielen;Gert Ij Salentijn;	abstract= While consumer-focused food analysis is upcoming, the need for multiple sample preparation and 14 handling steps is limiting. On-site and consumer-friendly analysis paradoxically still requires 15 laboratory-based and skill-intensive sample preparation methods. Here, we present a compact, 16 inexpensive, and novel prototype immunosensor combining sample preparation and on-chip reagent 17 storage for multiplex allergen lateral flow immunosensing. Our comprehensive approach paves the 18 way for personalized consumer diagnostics. The prototype allows for handheld solid-liquid extraction, 19 pipette-free on-chip dilution, and adjustment of sample concentrations into the appropriate assay 20 dynamic working range. The disposable and interconnectable homogenizer unit allows for the 21 extraction and 3D-sieve based filtration of allergenic proteins from solid bakery products in 1 minute. The homogenizer interconnects with a 3D-printed unibody lab-on-a-chip (ULOC) microdevice, which 23 is used to deliver precise volumes of sample extract to a reagent reservoir. The reagent reservoir is 24 implemented for on-chip storage of carbon nanoparticle labeled antibodies and running buffer for 25 dilution. The handheld prototype allows for total homogenization of solid samples, solid-liquid protein 26 extraction, 3D-printed sieve based filtration, ULOC-enabled dilution, mixing, transport, and 27 J o u r n a l P r e -p r o o f 2 smartphone-based detection of hazelnut and peanut allergens in solid bakery products with limited 28 operational complexity. The multiplex lateral flow immunoassay (LFIA) detects allergens as low as 0.1 29 ppm in real bakery products, and the system is already consumer-operable, demonstrating its 30 potential for future citizen science approaches. The designed system is suitable for a wide range of 31 analytical applications outside of food safety, provided an LFIA is available. 32 Introduction On-site and personalized food safety tests are growing in popularity, with developments in rapid, 34 affordable, sensitive, and disposable handheld assays driving the move from the laboratory to a 35 consumer-based approach [1, 2]. Consumer detection of food allergens is particularly relevant [3, 4], 36 and more so now than ever, with the Food and Drug Administration (FDA) announcing temporary 37 changes to food labeling and allowing of ingredient alterations to prevent any disruption to the global 38 food supply chain during the SARS-CoV-2 pandemic [5]. Amendments that overlook hidden or novel 39 allergens put the allergic individual at risk, exemplifying the necessity for personalized, disposable, 40 and simplified analysis of allergens, from sample preparation to detection. To date, the lateral flow 41 immunoassay (LFIA) is the most successful application of consumer diagnostics [6]. Combining LFIAs 42 with smartphones as optical detectors allow for 'on-the-go' decentralized screening [7] and 43 smartphones can even provide semi-quantitative results by calibrating test and control line intensity 44 values toward a particular antigen concentration [8]. 45 Despite these advantages, LFIAs also have some disadvantages, including a limited dynamic range, 46 they work only with liquid samples and predominately target only a single analyte. Within a sandwich 47 LFIAs dynamic working range, the test line intensity increases alongside increasing analyte 48 concentration. However, at high analyte concentrations, the signal intensity can paradoxically 49 decrease as the excess of unlabeled analyte saturates the capture and detector antibodies (mAbs) 50 binding sites [9]. The reduction in test line intensity can mimic the signal at a much lower analyte 51 concentration. Dilution to within an assays appropriate concentration range is required to avoid false-52 negative results. False negatives are particularly problematic for consumers. 53 J o u r n a l P r e -p r o o f Moreover, when analyzing a complex solid matrix such as food, sample preparation, including 54 homogenization of the solid food and extraction of the relevant proteins, as well as reagent storage, 55 are pivotal bottlenecks. Even integrated systems often require pre-treatment [10] or heat-assisted 56 actuation to extract proteins into a testable liquid [3]. Finally, excluding a few multiplex LFIAs [11-13], 57 allergen LFIAs are restricted to singleplex detection, which is limiting for individuals with co-existing 58 allergies. Sample preparation is a major issue; indubitably, consumers do not have the laboratory 59 skills required for extracting, pipetting, and diluting samples, and fully integrated analytical systems 60 have so far mainly been developed for DNA-based analysis [14-16]. Systems with integrated solid-61 phase extraction for aqueous samples are reported [10, 17], but the extraction of solid samples is 62 more complex and still requires offline pre-treatment. 63 In a parallel advancement, the emergence of 3D-printing has revolutionized the rapid prototyping of 64 multifunctional lab-on-a-chip [18] and disposable [19] devices for analytical chemistry. Modification of 65 Computer-Aided Designs (CADs) takes little cost and time, and prototypes can be refined iteratively 66 multiple times in a single day, outside of a cleanroom environment. A unibody lab-on-a-chip 67 (ULOC)[18, 20] is a monolithic device with all the analytical functionalities in-built on one side, takes 68 less than an hour to manufacture, and is printed in a single step [21]. The ULOC's unibody 69 connectors, ending in unidirectional valves, can be connected to silicon tubing as manual finger 70 pumps [22]; or to detachable devices such as syringes or pumps, for pipette-free, active control of 71 sample actuation with volume metering [23]. Moreover, 3D-printed devices with on-chip reagent 72 storage [14, 24] can combine with and benefit from the capabilities of paper-based devices [25, 26]. Here we present a multifunctional and miniaturized sample preparation unit that integrates with a 74 consumer-operable prototype immunosensor for handheld solid-liquid multi-allergen extraction. The 75 interconnectable ULOC then enables on-chip sample handling for equipment-free dilution, transport, 76 and LFIA detection of hazelnut and peanut allergens in the low ppm range in spiked and commercial 77 bakery products. 78 2. Materials & Methods 79 2.1. Reagents and Consumables 80 J o u r n a l P r e -p r o o f 102 The Netherlands) protein analyzer. Different range of sample types was utilized to characterize each 103 module of the prototype immunosensor (Table 1). 104 <Table 1.> 105 106 2.3. Development and Fabrication 107 Computer-aided design (CAD) software Autodesk Fusion 360 (Autodesk Inc. San Rafael, CA; USA) 130 syringe pressure is then used for user-controlled actuation. 131 132 2.3.2. ULOC Dilutor 133 The ULOC dilutor (60 mm W x 40 mm L) has all functional features printed onto a single side. One 134 side is left open, so uncured resin can be removed from 1 mm deep fluidic channels (1 mm wide) by 135 sonicating (FinnSonic m15, FinnSonic Oy, Lahti, Finland) in ethanol (Sigma Aldrich, Steinheim, 157 158 2.4.2. ULOC Dilutor Before characterization, 5 or 10 µL dye was actively injected via a disposable syringe into the 160 manifold. Injections were repeated multiple times for distance verification, with the 5 and 10 µL 161 distances being marked on the ULOC for convenience with subsequent sample loading (see SI Figure   162 S5). The ULOC was characterized for its dilution ability by mixing dye with water (sample type A) at 163 various dilution factors (DFs). Adjustable water volumes were pipetted into the ULOC reservoir. Dilution factors of x10, x15, x20, and x40 were achieved by injecting 10 µL of aqueous dye solution to 165 the mark on the ULOC. For comparison with a manually pipetted sample, the same DF dye/water was 166 pipetted into the ULOC reference well. Smartphone images of the ULOC were acquired using 167 OpenCamera (v1.47.3) to keep exposure and focus constant on a Google Pixel 2 XL (Google, ppm (n=3). Despite using the prototype for analysis, the LOD here is not much higher than in our 313 previous work (0.5 ppm; n=20)[13], which was obtained using standardized laboratory conditions, 314 pipettes, and equipment. Sample B measurements are reproducible (RSD ± 2.9%), indicating the 315 ULOC mixes well and delivers persistent volumes, and that the LFIA still works when combined with 316 the ULOC. Solid sample type C (BC spiked with THP extract (w/v); Figure 6C ) was extracted and 317 analyzed to reflect an actual solid-liquid extraction, with a LOD of 1 ppm (RSD at 1 ppm ± 3.7%). The 318 slight increase in T/C deviation could be due to the crushing efficacy of the homogenizer. Small 319 differences in buffer incubation times between repeat measurements and non-uniform dispersion of 320 liquid THP extract could be consequential to the somewhat higher variation. 321 J o u r n a l P r e -p r o o f we included a reference well in the ULOC pre-containing RB and CNP-mAb for a blank control. The 344 consumer can then use this to directly compare the physical appearance of the test and control lines 345 in realtime. Of course, in a dedicated smartphone-app, any human error would be avoidable, 346 triggering an alert when the LFIA falls outside normality. 347 348 3.4.3. Incurred Multi-Allergen Screening 349 Sample type D2 (HC and PC in BC (w/w); Figure 7) demonstrates the prototype's effectiveness for 350 simultaneously co-extracting and detecting unrelated processed allergens. Both analytes were 351 detectable at 0.1 ppm (n=3, RSD ± 2.5% and 1.6% for hazelnut and peanut, respectively). There is a 352 J o u r n a l P r e -p r o o f 373 positive and negative results for the spiked and blank samples, signifying the early prototype is 374 already is operable by non-skilled individuals after only a short explanation. 375 376 Conclusions The reported handheld immunosensor allows for interconnectable sample preparation, solid-liquid 378 protein extraction, dilution, delivery, detection, and smartphone readout of multiple allergens in bakery 379 products. The detachable homogenizer efficiently co-extracts and filters two major but distinct 380 allergens from solid samples in record time. Active injection of the extracted liquid sample into the 381 ULOC mixes the extract with RB for arbitrary sample dilution and with labeled bioreagents before 382 delivery to the detection chamber. This pipette-free dilution limits the occurrence of false-negative 383 J o u r n a l P r e -p r o o f results in LFIA. Realtime results are automatically readable as they develop on the phone screen. While the results are readable on the phone screen within 5 minutes, they are optimum after 15 385 minutes. The interchangeable LFIA cartridge means the reported system with ULOC-enabled sample 386 dilution can easily be applied to test different LFIAs targeting various food, biomedical and forensic 387 applications, affirming the value of such a simplified, adjustable, and multifunctional system. The immunosensor is inexpensive, with current material costs of less than 1$/USD. The prototype is 390 already consumer-operable, and further advancements, such as image processing in a dedicated 391 smartphone app, will continually improve the usability of the system. The presented handheld system 392 is an encouraging development for affordable, simplified multiplex consumer immunodiagnostics. 393 394 395 396 	body_text= have previously been developed, characterized, and validated [13, 27] . Running buffer (RB)/extraction albumin (BSA; Sigma-Aldrich, Zwijndrecht, The Netherlands) and 0.05% tween-20 (v/v) (Merck, Darmstadt; Germany). The 10 mL and 1 mL disposable plastic syringes that were used for the 86 homogenizer and air displacement syringes were purchased from Becton-Dickinson (Utrecht, The 87 Netherlands), and low binding syringe filters used to filter total protein extracts (5 µm; 1.2 µm; 0.45 88 µm) were acquired from Pall Life Sciences (Pall Netherlands B.V., Medemblik; The Netherlands). Silicon tubing for ULOC connectors was purchased from Esska-Tech (Arvika; Sweden). ULOCs were  Standardized certified reference materials for food allergens are not currently available; therefore, 98 total hazelnut protein (THP), total peanut protein (TPP), and blank cookie (BC) extracts were 99 prepared in-house [13, 27] . See Supplementary Information (SI) Table S1 for ingredient lists and Figure S1 ), device holder (SI Figure S2) , and interchangeable LFIA cartridges (SI Figure S3 ).  The handheld and interconnectable homogenizer unit enables total homogenization and solid-liquid 122 protein extraction from solid food samples. 3D-printed sieves with approximate pore sizes of 0.5 mm 123 were cut by laser (HL40-5g, Full Spectrum Laser LLC, Las Vegas, NV; USA) into discs (18 mm 124 diameter; SI Figure S1 ). Two sieves ( Figure 1A ) insert into a 10 mL syringe at an offset to each other. As the plunger pushes solid material against the first 3D-printed sieve, it breaks into smaller pieces 126 which are subsequently blocked by the second sieve that is kept at an offset, preventing particles 127 from blocking microchannels in the ULOC. Silicon tubing (1.5 mm inner diameter, 40 mm length) 128 connects with the ULOC unibody connector. The tubing can be used as a finger pump or is connected 129 by a second larger piece of silicon tubing (2.5 inner diameter, 20 mm length) to the syringe tip. The remaining two unibody connectors were joined together by silicon tubing (see Figure 1B ). The ULOC could be inserted into an opening (50 mm W x 35 mm L) in the 3D-printed device holder, 147 which shielded the assay from ambient light (see SI; Figure S2 ). The LFIA cartridge, which fits 2 148 LFIAs (4 or 5 mm wide), ensured that the appropriate LFIAs were aligned with the test and reference  Images and videos were acquired by smartphone, attached to the holder frame, using OpenCamera 179 to ensure fixed acquisition conditions (fixed focus, locked exposure, controlled illumination, for videos: 180 30 frames per second (fps), 720 x 480 pixels). LFIA results appeared on the screen as they emerged. Subsequently, videos were split into images of 1 fps using Adapter (v2.1.6), and the resulting time 182 point images were analyzed offline in ImageJ by splitting the images into their color channels. A blue 183 channel pixel intensity (BCPI) reading was taken from below the test line at t=0 as a background 184 response; the BCPI measurements from the test and control lines were then subtracted from this to 185 give the corrected BCPI (cBCPI) value. In the assay dynamic working range, cBCPI increases as test  Experiments were performed in triplicate; see Figure 3 for the pictogram operation procedure and SI Video 1 for a short demo of the total system operation from extraction to LFIA readout. Before sealing 195 the ULOC with adhesive tape, the reagent reservoir was filled with 2 µL CNP-mAb and 190 µL RB, particles, and analyzed their size distribution using ImageJ (see SI Protocol S1 for full details). For sample types C and D (approximately 0.25 g), and then incubated with 1 mL RB for 1 minute, 204 before filtering through the 3D-sieves. Finally, to characterize the system for detecting real-life 205 incurred and processed allergens, sample type D was investigated. The volume of the pre-loaded 206 CNP-mAb was increased to 4 µL (2 µL for the anti-hazelnut mAb-CNP and 2 µL for the anti-peanut  A major restriction of allergen analysis is the lengthy extraction process, which typically includes 224 weighing, heating, grinding, and numerous filtering steps [2, 11] . As such, extended extractions delay 225 rapid screening tests such as LFIA. While a 2 minute magneto-assisted allergen antigen extraction 226 has been reported, this still required off-chip microwave pre-heating [3]. Previously we [13] described 227 a method for extracting total proteins from cookies and peanut flour at room temperature (RT) in 30 228 minutes. This method is promising because even with the shorter extraction time and at RT, the indicating a much shorter extraction time could still be appropriate for extracting relevant allergenic proteins without delaying the analysis. To test this, we evaluated different extraction times to attempt  Allergenic proteins exist in foods over a broad dynamic range and must be detected at trace levels for 255 protecting sensitive individuals. Still, it is vital to understand that highly concentrated samples can 256 yield paradoxically low signal intensities, which could easily be misinterpreted by a consumer. However, it is reported that sample dilution (DF x 10-100) can minimize the occurrence of false-258 negatives [11] [12] [13] . While sample dilution is a prerequisite for allergen analysis, we cannot expect the extracted sample is injected into the reservoir, it efficiently mixes with the pre-stored CNP-mAbs by air 262 displacement and is also diluted in RB by an adjustable DF. Figure 5A compares the BCPIs for on-chip (ULOC-enabled) versus off-chip (manually pipetted) 265 dilutions (dye in water, sample type A, n=3) using different DFs. Figure 5B indicates where to take the 266 pre-dilution (DF x 0), the mid-dilution (5C), and the on-chip and off-chip (5D) BCPIs measurements. For consistency, the measurements were always taken below the dye's meniscus. The ULOC DFs 268 invariably matched the manually pipetted DFs, suggesting that the ULOC delivers well-defined 269 sample volumes (see SI Figure S5 ). ULOC devices for other applications have already been  To investigate the influence of assay duration on the signal development, the LFIAs were readout  Allergenic proteins can be subject to conformational alterations during food processing [29] . Therefore, biosensors must demonstrate proficiency in detecting allergens in both raw and processed 300 products. Here, solid cookie samples were pre-weighed (0.25 g) for consistency. Still, in real life, the user can instead fill the homogenizer with the cookie to the 1 mL mark to approximately obtain the 302 same sample weight (see SI Figure S7 ). Though this method is less precise, it would suffice for 303 qualitative assessment of bakery products for the presence of allergens. Previously, we found that  For sample B (THP extract spiked into BC extract (v/v); Figure 6B ), the T/C ratio detection limit is 1 Thermal processing, such as baking, can affect allergen detectability [29] . See Figure 6 to compare 324 signal development for samples using total allergen protein extracts (6E) with samples containing 325 incurred allergens (6D&F). Testing commercial hazelnut cookies mixed with blank cookies (sample 326 type D1) exemplifies the effectiveness of extracting incurred proteins from a solid matrix into a 327 testable liquid and detecting the allergenic proteins in this liquid. Sample D1 has a LOD of 0.1 ppm 328 (n=3, RSD ± 3.03%; see Figure 6D and F) for processed hazelnut. Compellingly, the D1 LOD is lower 329 than the LOD for sample C. The LFIA is more sensitive towards processed hazelnut. This sensitivity 330 has also been indicated in our singleplex hazelnut LFIA where the same mAb reached the same LOD  In Figure 6 , high-concentration effects (1000 and 100 ppm) are evident. Even with the faster 340 extraction time and ULOC-dilution concentration-dependent effects still occur, affirming the necessity 341 to dilute allergen samples before analysis [11, 12] . For consumer testing, the loss of the control line 342 (at 1000 ppm) could be problematic, and some tests have additional target lines to limit this [1]. Here, slightly lower deviation in multiplex measurements owing to increased control line stability, from using 353 two different CNP-mAbs compared with singleplex analysis. The sensitivity is even better than when  [30] ECSA, Ten principles of citizen science European Citizen Science Association https://ecsa.citizen-science.net/, 2020.  
paper_id= ffea7ba3e851069566953f62c666856312ef5e10	title= Oral typhoid vaccine Ty21a elicits antigen-specific resident memory CD4 + T cells in the human terminal ileum lamina propria and epithelial compartments Open Access Journal of Translational Medicine	authors= Jayaum S Booth;Eric  Goldberg;Robin S Barnes;Bruce D Greenwald;Marcelo B Sztein;	abstract= Background: Salmonella enterica serovar Typhi (S. Typhi) is a highly invasive bacterium that infects the human intestinal mucosa and causes ~ 11.9-20.6 million infections and ~ 130,000-223,000 deaths annually worldwide. Oral typhoid vaccine Ty21a confers a moderate level of long-lived protection (5-7 years) in the field. New and improved vaccines against enteric pathogens are needed but their development is hindered by a lack of the immunological correlates of protection especially at the site of infection. Tissue resident memory T (T RM ) cells provide immediate adaptive effector immune responsiveness at the infection site. However, the mechanism(s) by which S. Typhi induces T RM in the intestinal mucosa are unknown. Here, we focus on the induction of S. Typhi-specific CD4+T RM subsets by Ty21a in the human terminal ileum lamina propria and epithelial compartments. Terminal ileum biopsies were obtained from consenting volunteers undergoing routine colonoscopy who were either immunized orally with 4 doses of Ty21a or not. Isolated lamina propria mononuclear cells (LPMC) and intraepithelial lymphocytes (IEL) CD4+T RM immune responses were determined using either S. Typhi-infected or noninfected autologous EBV-B cell lines as stimulator cells. T-CMI was assessed by the production of 4 cytokines [interferon (IFN)γ, interleukin (IL)-2, IL-17A and tumor necrosis factor (TNF)α] in 36 volunteers (18 vaccinees and 18 controls volunteers). Although the frequencies of LPMC CD103+ CD4+T RM were significant decreased, both CD103+ and CD103− CD4+T RM subsets spontaneously produced significantly higher levels of cytokines (IFNγ and IL-17A) following Ty21a-immunization. Importantly, we observed significant increases in S. Typhi-specific LPMC CD103+ CD4+T RM (IFNγ and IL-17A) and CD103− CD4+T RM (IL-2 and IL-17A) responses following Ty21a-immunization. Further, differences in S. Typhi-specific responses between these two CD4+T RM subsets were observed following multifunctional analysis. In addition, we determined the effect of Ty21a-immunization on IEL and observed significant changes in the frequencies of IEL CD103+ (decrease) and CD103− CD4+T RM (increase) following immunization. Finally, we observed that IEL CD103− CD4+T RM , but not CD103+ CD4+T RM , produced increased cytokines (IFNγ, TNFα and IL-17A) to S. Typhi-specific stimulation following Ty21a-immunization. © The Author(s) 2020. This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article' s Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article' Conclusions: Oral Ty21a-immunization elicits distinct compartment specific immune responses in CD4+T RM (CD103+ and CD103−) subsets. This study provides novel insights in the generation of local vaccine-specific responses. Trial registration This study was approved by the Institutional Review Board and registered on ClinicalTrials.gov (identifier NCT03970304, Registered 29 May 2019-Retrospectively registered, http://www.Clini calTr ials.gov/NCT03 97030 4) 	body_text= Background CD4+ T cells are crucial for the generation of vaccine-mediated immune responses providing effective immunity against pathogens. Recent studies have demonstrated that both circulating T memory (T M ) cells and tissue resident memory T cells (T RM ) abundant in peripheral tissues are central to elicit protective immunity [1, 2] . T RM represent a non-migratory population of T M that is phenotypically different from circulating T M subsets [e.g., T central memory (T CM ) and T effector memory (T EM )] and mediate rapid effector immune responses following antigen recall [2] . Human T RM are mainly characterized phenotypically by high expression of CD69, a key marker for distinguishing between circulating and resident T M [3] . Integrin αEβ7 (CD103), the ligand to E-cadherin, is also used to characterize T RM but CD103 expression is mostly confined to CD8+ T RM and a minor subset of CD4+T RM [3] [4] [5] [6] . Recent studies have characterized CD4+ T RM subsets in multiple organs including lungs, liver, skin, intestines, vagina and lymphoid sites where they provide protective responses and direct the recruitment of immune cells [7] [8] [9] [10] . In the human intestine, the majority of CD4+T RM are CD69+CD103− and a minority are CD69+CD103+ [3] but little information is available concerning their role in oral immunization or enteric infections. It is unclear whether live oral attenuated typhoid vaccine, Ty21a, could elicit and generate antigen-specific CD4+T RM responses in the human terminal ileum (TI) (site of infection for S. Typhi). Salmonella enterica serovar Typhi (S. Typhi), the etiological agent of typhoid fever, is a human restricted pathogen, that infects around 21 million people globally resulting in approximately to 223,000 deaths yearly [11] [12] [13] . After actively infecting primarily the TI [14] [15] [16] , S. Typhi disseminate to the systemic organs (e.g., liver, spleen and others) leading to systemic illness [17] . To our knowledge, no information is available on the induction of CD4+T RM responses to S. Typhi in the TI mucosa following wild-type S. Typhi infection or immunization with Ty21a. Since the gastrointestinal track is a major reservoir of CD4+T RM , it is important to understand their role in protective immunity against S. Typhi and other enteric pathogens at their preferred site of natural infection. This knowledge will provide novel insights for the development of tailored mucosal vaccines to enteric pathogens. Currently, there are two licensed typhoid vaccines namely, Ty21a and the parenteral Vi polysaccharide vaccine [18, 19] and both have their limitations. Following Ty21 immunization, a modest level of long-lived protection (60-80%, 5-7 years) is obtained [20] [21] [22] while the Vi vaccine is moderately immunogenic but well tolerated [17, 23] . Therefore, effective vaccines that offer durable, long-lasting protection are urgently needed for children and adults. Induction of humoral and CMI responses have been thoroughly examined in peripheral blood mononuclear cells (PBMC) obtained from healthy individuals vaccinated with Ty21a [24] [25] [26] [27] [28] [29] [30] . These studies demonstrated that both CD4+ and CD8+ T cell responses (e.g., IFNγ, cytotoxic T cells (CTL), proliferation) are induced following Ty21a-vaccination [22, 24, 25, 27, 31, 32] . Moreover, S. Typhi responsive CD4+ T cells are mediated mostly by T EM and RA+T EM (T EMRA ) and, to a lesser extent, by T CM [22, 32, 33] . Furthermore, CD4+T EM and T EMRA subsets responses displayed increases in S. Typhi-specific multifunctional (MF) cells post-vaccination mainly producing IFN-γ and/or TNF-α, while IL-2, MIP-1β, IL-17A and CD107a expression (a marker associated with cytotoxicity) were produced in a small proportion of MF cells [33] . Recently, we reported that oral Ty21a-immunization elicits significant TI lamina propria mononuclear cells (LPMC) S. Typhi-specific CD8+ T M [34] , CD8+T RM [16] , and CD4+T M [35] responses with distinct effector functions. However, it is unknown whether LPMC CD4+T RM are elicited following oral Ty21a immunization. Likewise, we have characterized intraepithelial lymphocytes (IEL) CD8+T RM S. Typhi specific responses in the epithelium compartment following Ty21a-immunization [16] . Interestingly, no information is available on the role of the IEL CD4+T RM following oral Ty21a-vaccination. Functional studies with human intestinal IEL are challenging due to their low yield during the isolation process. Nonetheless, IEL are among the first line of defense and it is therefore important to understand their role in oral Ty21a-vaccination and S. Typhi infection. In this study, we have characterized TI LPMC and IEL CD4+T RM subsets following Ty21a immunization. We then determined and compared CD4+T RM S. Typhi-specific responses between the two groups. Finally, we assessed single and multifunctional S. Typhi specific responses in CD4+ T RM subsets by single cell analysis. These comparisons provide unique insights into the generation of S. Typhi specific responses in the human TI mucosa. Individuals (aged 50-74 years) undergoing routine colonoscopy were enrolled from the Baltimore-Washington metropolitan area and University of Maryland, Baltimore campus. Volunteers who have no previous history of typhoid fever and were not vaccinated with the attenuated Ty21a typhoid vaccine were assigned to each of two groups. Four recommended doses of Ty21a (Vivotif enteric-coated capsules; Crucell, Bern, Switzerland) were administrated to the first group (n = 18) but not to the control group (n = 18) (Additional file 1: Fig. S1 ). Blood samples were collected at least 21 days before colonoscopy (pre-immunization) and on colonoscopy day (day 0) together with TI biopsies using large capacity forceps (Additional file 1: Fig. S1 ). PBMC were isolated immediately after blood draws by density gradient centrifugation and cryopreserved in liquid nitrogen following standard techniques [32] . Written informed consent was obtained from subjects and all procedures were approved by the University of Maryland, Baltimore Institutional Review Board (IRB) and registered on ClinicalTrials.gov (identifier NCT03970304). The study was conducted in accordance with the principles of the International Conference of Harmonization Good Clinical Practice guidelines. TI-LPMC and IEL were freshly isolated as previously described [16, [36] [37] [38] . Briefly, after collection of biopsies from volunteers undergoing routine colonoscopy, tissues were treated with HBSS (without CaCl 2 , MgCl 2 , MgSO 4 ; Gibco, Carlsbad, CA) and EDTA (10 mM; Ambion, Grand Island, NY) and were vigorously shaken for 45 min to isolate IEL. Next, the biopsies were digested enzymatically with collagenase D (100 μg/mL; Roche, Indianapolis, IN) and DNase I (10 μg/mL; Affymetrix, Cleveland, OH) for 45 min followed by homogenization using the Bullet Blender homogenizer (Next Advance Inc, Averill, NY) to extract LPMC. LPMC and IEL were either stained immediately for immunophenotyping by flow cytometry or stimulated overnight. Autologous Epstein-Barr virus (EBV)-transformed lymphoblastoid cell line (EBV-B cells) were generated from each participant's pre-immunized PBMC (Additional file 1: Fig. S1 ) as previously described [25, 32] . Target cells were then infected with wt-S. Typhi strain ISP1820 at a MOI of 7:1 as previously described [35] . Infected target cells were then gamma-irradiated (6000 rad) before used for ex vivo LPMC and IEL stimulation. To confirm S. Typhi infection, target cells were stained with anti-Salmonella common structural Ag (CSA-1, Kierkegaard and Perry, Gaithersburg, MD) and analyzed by flow cytometry as previously described [25, 32] . Freshly isolated TI-LPMC and IEL were used as effector cells as previously described [16, 34, 37] . Briefly, LPMC and IEL were co-cultured, respectively, with either uninfected or S. Typhi-infected EBV-B (MOI of 7:1). LPMC and IEL cultured with media only or in the presence of α-CD3/CD28 (Life technologies, Grand Island, NY) were used as negative and positive controls, respectively. After 2 h, 0.5 μL of Golgi Stop (Monensin, BD) and 0.5 μL Golgi Plug (Brefeldin A, BD) were added and cultures continued overnight at 37 °C in 5% CO 2 . Following overnight stimulation, TI LPMC and IEL were stained for flow cytometry analysis as previously described [16] . Briefly, LPMC and IEL were stained for live/dead discrimination (YEVID) (Invitrogen, Carlsbad, CA) and then the Fc receptors were blocked using human immunoglobulin (3 μg/mL; Sigma). This was followed by surface staining. Briefly, cells were stained at 4 °C for 30 min with fluorescently labeled monoclonal antibodies directed to CD13-Pacific Orange (conjugated in-house), CD19-BV570 (HIB19, Biolegend, San Diego, CA), CD3-BV650 (OKT3, Biolegend), CD4-PE-Cy5 (RPA-T4, BD), CD8-PerCP-Cy5.5 (SK1, BD), CD45RA-biotin (HI100, BD), CD62L-APC-A780 (DREG-56, eBioscience, San Diego, CA), and CD103-FITC (Ber-ACT8, BD). After a wash, cells were stained with streptavidin (SAV)-Qdot800 (Invitrogen) at 4 °C for 30 min. Cells were then fixed and permeabilized using IC fixation and permeabilization buffers (8222/8333, eBioscience). This was followed by staining with monoclonal antibodies directed to interleukin (IL)-17A-BV421 (BL168, Biolegend), interferon (IFN)-γ-PE-Cy7 (B27, BD), tumor necrosis factor (TNF)α-Alexa 700 (MAb11, BD), and CD69-ECD (TP1.55.3, Beckman Coulter, Danvers, MA), and IL2-BV605 (MQ1-17H12, Biolegend). After staining, cells were stored in 1% paraformaldehyde at 4 °C until data collection. Data were collected using a customized LSRII flow cytometer (BD) and then analyzed using the WinList version 7 (Verity Software House, Topsham, ME) software package. S. Typhi-specific responses were expressed as net percentage of positive cells (background after stimulation with uninfected cells were subtracted from values obtained with S. Typhi-infected targets). The FCOM function of WinList was used to determine S. Typhi-specific MF responses in TI LPMC and IEL. Flow cytometry experiments were performed at the Flow Cytometry and Mass Cytometry Core Facility of the University of Maryland School of Medicine Center for Innovative Biomedical Resources (CIBR), Baltimore, Maryland. Data were analyzed using the statistical software Graph-Pad Prism ™ version 5.03 (Graphpad, San Diego, CA, USA). Statistical differences in median values between two groups were determined using Mann-Whitney tests. Wilcoxon matched pair tests were used to assess statistical differences between LPMC and IEL paired responses. Correlations (LPMC versus IEL S. Typhi-specific responses) were evaluated using Spearman correlation tests. The datasets supporting the findings of this study are available within the article and its additional information files. Recent evidence suggest that CD4+T RM may influence local immune responses at the site of infection [7, 9] . However, the effect of oral Ty21a-immunization on TI CD4+T RM is unknown. To explore this phenomenon, we characterized CD4+T RM subsets (CD103+ and CD103−) from freshly isolated TI-LPMC obtained from biopsies of Ty21a-vaccinated and unvaccinated volunteers (Fig. 1a ). As expected, using CD69 and CD103 markers, TI-LPMC CD4+T RM were comprised of two populations, namely CD69+CD103− (~ 70%) and CD69+CD103+ (~ 20%) (Fig. 1a) . As observed in the representative volunteers ( Fig. 1a) , cumulative data demonstrated that following Ty21a-immunization, CD103+CD4+T RM frequencies were significantly decreased (p < 0.05) whereas CD103− CD4+T RM frequencies showed a trend to exhibit increased frequencies (p = 0.1; Fig. 1b ), indicating that Ty21a-immunization affects the frequencies of TI-LPMC CD4+T RM . As expected, virtually no CD4+T RM were observed in PBMC isolated from the same volunteers (Additional file 2: Fig. S2 ). CD4+T cells is the principal population in the TI lamina propria (LP) comprising most of the local CD4+T RM (Fig. 1a) . The observation that oral Ty21a-immunization influences CD4+T RM frequencies ( Fig. 1b) suggests that TI-LPMC CD4+T RM subsets might respond differently in magnitude and characteristics following stimulation. To test this hypothesis, we determined the cytokine pro- Therefore, we observed that both CD4+T RM subsets are responsive to S. Typhi-stimulation following oral Ty21a immunization. Interestingly, we noted that both CD4+T RM subsets exhibited differences in the background (unstimulated) and α-CD3/CD28 stimulation responses between Ty21avaccinated and unvaccinated volunteers. Therefore, we hypothesized that Ty21a-immunization might influence the induction of spontaneous cytokines (background responses) and the capacity of CD4+T RM to respond to stimulation. To address this hypothesis, we evaluated CD4+T RM (CD103+ and CD103−) cytokines producing cells when LPMC cells were cultured overnight either alone (unstimulated) or following stimulation with α-CD3/CD28 beads. Cumulative data from unstimulated LPMC show that there were significant (p < 0.05) increase in IFNγ-S (but not IFNγ-MF) producing cells in both CD4+T RM subsets (Fig. 2a) . Interestingly, we observed that the baseline level of IFNγ producing cells by CD103+ CD4+T RM cells were significantly higher than those from CD103− CD4+T RM as shown by green lines (Fig. 2a) . Next, we determined spontaneous IL-17A responses and observed that CD103+ CD4+T RM exhibited significantly higher levels of IL-17A-S following Ty21a-immunization (Fig. 2b) . Furthermore, we noted that the level of IL-17A (S and MF) were significantly higher in CD103+CD4+T RM than in CD103− CD4+T RM (Fig. 2b) . Similar assessments were made for IL-2 and TNFα for background responses (Fig. 2c, d) . Interestingly, no differences in either CD4+T RM subset were noted for IL-2 and TNFα, except for CD103− CD4+T RM that showed a decreased trend in IL-2-S ( Fig. 2c ) and an increased trend in TNFα-S (Fig. 2d ) responses following Ty21a-immunization. Interestingly, we also observed significantly higher levels of IL-2-MF and TNFα-MF CD4+CD103+T RM than CD4+CD103− T RM in Ty21a-vaccinees (Fig. 2d) . Likewise, CD4+T RM subsets responses were examined in both groups of volunteers following stimulation with α-CD3/CD28 beads. Remarkably, while α-CD3/CD28 stimulated equally both CD4+T RM subsets to produce higher IFNγ-MF in the two groups, oral Ty21a-immunization significantly induced IFNγ-S responses (Additional file 4: Fig. S4A ). When IL-17A responses were examined following stimulation with α-CD3/CD28 beads, no significant differences were observed following Ty21a-immunization (Additional file 4: Fig. S4B ). However, CD103+ CD4+T RM produced significantly higher levels of IL-17A-MF than CD103− CD4+T RM (Additional file 4: Fig. S4B ). We also determined the IL-2 and TNFα responses of the CD4+T RM subsets following α-CD3/CD28 stimulation (Additional file 4: Fig. S4C, D) . Interestingly, while both CD4+T RM subsets produced higher levels of IL-2-MF and TNFα-MF equally in both volunteer groups to α-CD3/CD28 stimulation, IL-2-S was significantly decreased in both subsets following Ty21a-immunization (Additional file 4: Fig.  S4C, D) . Additionally, CD103+ CD4+T RM produced significantly (p < 0.05) higher levels of TNFα-S and MF than CD103− CD4+T RM following Ty21a-immunization (Additional file 4: Fig. S4D ). These granular data suggest that oral Ty21a-immunization influence intrinsic differences of LPMC CD4+T RM resulting in distinct background and stimulatory characteristics. CD4+T RM respond rapidly following re-exposure to pathogens. However, no information is available on CD4+T RM vaccine-induced responses, particularly the characteristics of CD103+ and CD103− CD4+T RM following oral Ty21a-immunization in the human TI. Thus, we hypothesized that TI CD103+ and CD103− CD4+T RM would contribute differently in the CMI responses following Ty21a immunization. To test this hypothesis, we evaluated the ability of CD4+T RM subsets to elicit responses following stimulation with autologous S. Typhi-infected EBV-B (targets) in Ty21a vaccinated (n = 18) and unvaccinated (n = 18) volunteers. Interestingly, the cumulative data demonstrated that LPMC CD103+ CD4+T RM exhibited significantly (p < 0.05) higher levels of IFNγ and IL-17A in Ty21a-vaccinated than unvaccinated volunteers following Ty21a-immunization (Fig. 3a) . No difference in IL-2 and TNFα were noted (Fig. 3a) . In contrast, the predominant LPMC CD103− CD4+T RM subset produced significantly (p < 0.05) higher levels of IL-17A and IL-2 following Ty21a-immunization (Fig. 3b) . No differences were observed in CD103− CD4+T RM IFNγ and TNFα levels CD103− CD4+T RM (Fig. 3b) . In various diseases including typhoid fever, induction of antigen-specific multifunctional T cells have been shown to be associated with favorable disease outcome, higher effector function and higher protective efficacy after immunization [39] [40] [41] [42] [43] compared to monofunctional T cells. In addition, recent evidences show that there are molecular differences between multifunctional and monofunctional CD4 + T cells [44] . Thus, we deemed that examination of multifunctional CD4+T RM in the terminal ileum mucosa is important for assessing the quality of the responses following Ty21a immunization. To further investigate the differences in S. Typhi-specific responses between LPMC CD103+ and CD103− CD4+T RM , we used the Winlist FCOM function to analyze multiple cytokines (i.e., IFNγ, IL-17A, IL-2, and TNFα) in individual S. Typhi-specific responding cells and classified them as either single cytokine producer (S) or multifunctional (Sum of double, triple, and quadruple cytokine producers) (MF). First, we analyzed the net S. Typhi-specific IFN-γ responses of LPMC CD4+T RM (CD103+ and CD103−) subsets stimulated with autologous targets for multifunctionality in Ty21a-vaccinees and controls (Fig. 4) . Interestingly, no significant differences were noted in LPMC CD103− CD4+T RM (S or MF) IFN-γ responses following Ty21a-immunization (Fig. 4a) . In contrast, CD103+ CD4+T RM produced significantly higher level of IFNγ-S but not MF following Ty21a-immunization (Fig. 4a ). Both LPMC CD4+T RM subsets exhibited higher levels of IL-17A following Ty21a-immunization (Fig. 4b) . However, CD103+ CD4+T RM produced higher levels of IL-17A-S (p = 0.1) whereas CD103− CD4+T RM exhibited increases in IL-17A-MF (p = 0.1) (Fig. 4b) . Furthermore, CD103+CD4+T RM showed a trend for higher levels of IL-17A-S production than CD103− CD4+T RM (green line) (Fig. 4b) . Similarly, we determined MF IL-2 and TNFα responses of CD103+ and CD103− CD4+T RM . No significant differences were observed in IL-2-S and TNFα-S or MF responses from CD4+T RM subsets following Ty21a-immunization (Fig. 4c, d) . However, CD103− CD4+T RM exhibited a trend (p = 0.1) to higher responses of IL-2-MF following Ty21a-immunization (Fig. 4c) . CD4+T M cells have been shown to respond to both autologous S. Typhi-infected and soluble antigens (e.g., Ty21a homogenate) [35] . Thus, we stimulated both CD4+T RM subsets with Ty21a homogenate (10 μg/ mL) and determined their net responses (% of Ty21a homogenate responses-% of unstimulated (cell alone) responses) in a subgroup of Ty21a vaccinated (n = 8) and unvaccinated (n = 8) volunteers. Remarkably, following stimulation with Ty21a homogenates, no significant differences were noted in cytokines (IFNγ, IL-17A, IL-2 and TNFα) producing CD103+ CD4+T RM between the volunteer groups (Additional file 5: Fig. S5A ). In contrast, CD103− CD4+T RM elicited significantly higher levels of IFNγ and TNFα in Ty21a vaccinated than unvaccinated volunteers following stimulation with Ty21a homogenate (Additional file 5: Fig. S5B ). Taken together, these data suggest that CD4+T RM subsets exhibit unique characteristics following Ty21a immunization. Next, we studied S. Typhi-specific IFN-γ multifunctional responses following Ty21a homogenate stimulation in CD103+ and CD103− CD4+T RM obtained from Ty21a-vaccinated and unvaccinated volunteers (Fig. 5) . CD103+ CD4+T RM produced significantly higher levels of IFNγ-MF but not S in Ty21a-vaccinees than controls (Fig. 5a) . However, CD103− CD4+T RM only showed trends to exhibit increases of IFNγ-S and MF in Ty21a-vaccinated compared to unvaccinated volunteers following stimulation with the Ty21a homogenate (Fig. 5a) . Likewise, we determined multifunctional IL-17A responses and observed no significant differences in IL-17A-S or MF production following Ty21a-immunization (Fig. 5b ) except for CD103− CD4+T RM that exhibited a trend to show increased responses by IL-17A-S in Ty21a-vaccinees (Fig. 5b) . Similarly, we determined IL-2 and TNFα multifunctional responses from CD4+T RM subsets. No significant differences in IL-2-S or MF and TNFα-S or MF CD103− CD4+T RM responses were found following Ty21a-immunization (Fig. 5c, d) . However, CD103+ CD4+T RM exhibited significantly higher levels of TNFα-MF and a trend to show increased IL-2-MF following Ty21a-immunization (Fig. 5c, d) . The intestinal mucosa is composed of a single layer of intestinal epithelial cells that is superimposed on the LP. Because LPMC CD103 +CD4+T RM expresses CD103, a ligand for E-cadherin on epithelial cells, it is reasonable to speculate that CD103+ CD4+T RM are poised to migrate and contribute to S. Typhi responses in the epithelial microenvironment. Thus, we hypothesized that oral Ty21a-immunization might influence the frequencies and baseline responses of intraepithelial lymphocytes (IEL) CD4+T RM in the human TI epithelial compartment. To address this hypothesis, we freshly isolated IEL and phenotypically characterized CD4+, CD8+ T cells and CD4+T RM subsets in Ty21a-vaccinated (n = 17) and unvaccinated (n = 17) volunteers. Representative examples of the gating strategy for a Ty21a vaccinated and an unvaccinated individual are shown in Fig. 6a . Interestingly, we observed that the frequencies of IEL-T cells were significantly altered following oral Ty21a immunization. Specifically, the frequencies of CD3+CD8+ IEL T cells were significantly (p < 0.005) decreased while CD3+CD4+ IEL T cell frequencies were significantly (p < 0.0005) increased following Ty21aimmunization (Fig. 6b) . To confirm that there were no discrepancies in cell yields, the number of viable IEL per mg of TI tissues obtained from biopsies of Ty21avaccinated and unvaccinated volunteers were compared. No significant differences in cell yields were observed between the two groups (Additional file 6: Fig. S6 ). Similarly, almost identical cell yields were observed in LPMC between the two groups. However, significantly higher cell numbers were observed in LPMC as compared to IEL (Additional file 6: Fig. S6 ). Next, we observed that TI CD103 + and CD103− CD4+T RM were present in similar proportions in IEL as in LPMC (Fig. 6a) . Interestingly, the frequencies of IEL CD103+ CD4+T RM were significant (p < 0.05) decreased following oral Ty21a-immunization (Fig. 6C) . In contrast, IEL CD103− CD4+T RM frequencies were significantly (p < 0.05) increased in Ty21a vaccinated as compared to unvaccinated volunteers (Fig. 6c) . Thus, these data suggest that the increase in IEL CD4+T cells result mainly from increases in the CD103− CD4+T RM subset. We next measured and compared the frequency of baseline S. Typhi single (S) and multifunctional (MF) producing cytokines (IFNγ, IL-17A, IL-2 and TNFα) in unstimulated IEL-CD4+T RM subsets obtained from Ty21a vaccinated (n = 17) and unvaccinated (n = 17) volunteers following an overnight incubation (Fig. 7) . Exvivo unstimulated IEL CD4+T RM obtained from Ty21a immunized volunteers exhibited significantly higher level of IFNγ-S but not MF than controls (Fig. 7a) . In contrast, no significant differences in the production of spontaneous IL-17A-S or MF, TNFα-S or MF and IL-2-S from IEL CD4+T RM subsets following Ty21a-immunization ( Fig. 7b-d) . Of note, IEL CD103− CD4+T RM produced significantly (p < 0.05) less IL-2-MF in Ty21a vaccinated than in unvaccinated volunteers (Fig. 7c) . We then studied multifunctional responses of IEL CD103+ and CD103− CD4+T RM in Ty21a vaccinated (n = 10) and unvaccinated (n = 8) volunteers following stimulation with α-CD3/CD28 beads. Remarkably, while α-CD3/ CD28 stimulated equally CD4+T RM subsets from both groups (Additional file 7: Fig. S7A-D) , we observed trends to show significant increases in TNFα-S CD103+ CD4+T RM and in IFNγ-S CD103−CD4+T RM , as well as significant (p < 0.05) increases in TNFα-S responses by CD103− CD4+T RM in Ty21a vaccinees (Additional file 7: Fig. S7A-D) . Above we showed that LPMC S. Typhi-specific CD4+T RM were elicited by oral Ty21a immunization. Thus, we hypothesized that oral Ty21a-immunization would induce distinct S. Typhi specific IEL CD4+T RM responses. To test this hypothesis, net S. Typhi responsive IEL CD4+T RM were evaluated following stimulation with autologous S. Typhi-infected targets in Ty21a vaccinated (n = 7) and unvaccinated (n = 6) volunteers. No significant differences were observed in cytokine production (IFNγ, IL-17A, IL-2 and TNFα) from IEL CD103+ CD4+T RM following Ty21a-immunization (Additional file 8: Fig. S8A ). In contrast, the predominant IEL CD103− CD4+T RM exhibited a trend to show increased IFNγ and TNFα (p = 0.06) responses in Ty21a-vaccinated than in unvaccinated volunteers (Additional file 8: Fig.  S8B ). Next, we studied multifunctional S. Typhi-specific IEL-CD4+ T RM responses in Ty21a-vaccinated (n = 7) and unvaccinated (n = 6) volunteers. First, we evaluated IFN-γ responses in IEL CD103+ and CD103− CD4+T RM and observed no significant differences in IEL-CD103+ CD4+T RM (S or MF) responses following Ty21a-immunization (Fig. 8a) . In contrast, CD103− CD4+T RM showed significant (p < 0.05) increases in IFNγ-S but not MF in Ty21a-vaccinated than unvaccinated volunteers (Fig. 8a) . Next, we determined multifunctional IL-17A responses from IEL CD4+T RM subsets. No significant differences were detected in IEL CD103+ CD4+T RM (S or MF) responses following Ty21a-immunization (Fig. 8b) . However, IEL-CD103− CD4+T RM produced significantly (p < 0.05) higher level of IL-17A following Ty21a-immunization (Fig. 8b) . Similarly, we determined multifunctional IL-2 responses from IEL CD4+T RM subsets. No significant differences were observed in IEL CD103− CD4+T RM (S or MF) responses following Ty21a-immunization (Fig. 8c) . However, CD103+ CD4+T RM exhibited significant increases in IL2-S but not MF in Ty21a-vaccinated compared to unvaccinated volunteers (Fig. 8c) . Finally, we assessed multifunctional TNFα responses and found no significant differences in IEL CD103+ CD4+T RM (S or MF) responses following Ty21a-immunization (Fig. 8d) . However, IEL CD103-CD4+T RM produced significantly (p < 0.05) higher levels of TNFα-S but not MF in Ty21a-vaccinated compared to unvaccinated volunteers (Fig. 8d) . We conclude that oral Ty21a-immunization elicits mainly cytokine-producing IEL CD103− CD4+T RM in the TI epithelium. Intestinal T RM (CD4+ and CD8+) are located in both the lamina propria and epithelial compartments, which provides unique immunological niches. Thus, we hypothesized that the CD69 and CD103 expression profile of CD4+T RM subsets may differ between LP and epithelial compartments following Ty21a immunization. To address this hypothesis, we measured the mean fluorescence intensity (MFI) of CD69 and CD103 expressed on LPMC and IEL CD4+T RM subsets obtained simultaneously from Ty21a vaccinated (n = 8) and unvaccinated (n = 8) volunteers (Additional file 9: Fig. S9 ). As shown in cytograms from a representative unvaccinated volunteer, CD103 is highly expressed on both LPMC and IEL CD103+ CD4+T RM with similar intensities in IEL and LPMC (Additional file 9: Fig. S9A ). Next, we compared the level of CD103 MFI between IEL and LPMC CD4+T RM subsets obtained from either Ty21a-vaccinated (n = 8) or unvaccinated (n = 8). Cumulative data showed no significant differences in the levels of CD103 expression between LPMC and IEL CD4+T RM subsets (Additional file 9: Fig. S9B ). We also compared the levels of CD69 expression (MFI) of CD4+T RM subsets between LPMC and IEL following Ty21a-immunization as shown in an unvaccinated representative volunteer (Additional file 9: Fig. S9C ). Interestingly, significantly (p < 0.005) higher expression of CD69 was detected in LPMC CD103+ CD4+T RM than in LPMC CD103− CD4+T RM in unvaccinated volunteers (Additional file 9: Fig. S9D) . Similarly, we observed a trend (p = 0.07) to show higher levels of CD69 expression in LPMC CD103+ CD4+T RM than in LPMC CD103− CD4+T RM following Ty21a-immunization (Additional file 9: Fig. S9D ). However, there were no significant differences in CD69 MFI levels in Ty21a-vaccinated compared to unvaccinated volunteers in either LPMC CD4+T RM subsets (Additional file 9: Fig. S9D) . In contrast in IEL we observed significantly (p < 0.005) higher expression of CD69 in IEL-CD103+ CD4+T RM than IEL CD103− CD4+T RM in Ty21-vaccinated volunteers (Additional file 9: Fig. S9D ). No differences in CD69 expression were observed between CD4+T RM subsets in unvaccinated volunteers (Additional file 8: Fig. S8D ). However, significant (p < 0.05) decreases in CD69 expression were observed in IEL CD103− CD4+T RM following Ty21a-immunization (Fig. S9D) . Comparisons between the two compartments showed trends for CD69 expression on CD103+ CD4+T RM to display higher levels of CD69 expression (p = 0.10) in IEL than LPMC following Ty21a-immunization (Additional file 8: Fig. S8D) . In contrast, a trend (p = 0.10) to show increases in CD69 expression was noted in CD103− CD4+T RM in IEL compared to LPMC from unvaccinated volunteers (Additional file 9: Fig. S9D) . Furthermore, we explored the relationship between the generation of S. Typhi specific immune responses between LPMC and IEL individually by performing Spearman correlation tests between LPMC and IEL CD4+T RM S and MF responses in both Ty21a vaccinated and unvaccinated volunteers. Interestingly, in unvaccinated volunteers, the frequencies of LPMC CD103+CD4+ T RM IL-2-S responses only were significantly positively correlated to their IEL counterparts while CD103−CD4+T RM subset has significant positive correlation only in IL-17A-S (Additional file 10: Table S1 ). However, following Ty21a-vaccination, the frequencies of LPMC CD103+CD4+ T RM S (IFNγ) were significantly positive correlated while their MF (IL-17A and IL-2) counterparts were significantly negatively correlated to their IEL counterparts (Additional file 10: Table S1 ). In contrast, the frequencies of LPMC CD103−CD4+T RM S (IFNγ and IL-17A-negative; TNFα-positive) but not MF were significantly correlated to their IEL counterparts (Additional file 10: Table S1 ). Vaccine-mediated protective immunity is generally enabled by the induction of both, antibodies and appropriate cellular immune responses, including those mediated by CD4+T cells. There is growing evidence that T RM (e.g., CD4+T RM ) cells play a crucial role in protective immunity following natural infection and their subsequent secondary exposure. However, there are no previous reports on the role of CD4+T RM following oral Ty21a-immunization in the human TI. Here, we examined the responses elicited by Ty21a-immunization on TI LPMC and IEL CD4+T RM subsets. We observed that Ty21a-immunization activated LPMC CD103+ and CD103− CD4+T RM resulting in increased spontaneous cytokines (IFNγ, TNFα and IL-17A) production. Importantly, LPMC CD4+T RM subsets contributed significantly to S. Typhi-specific IFNγ, IL-17A and IL-2 responses following stimulation with S. Typhi-infected targets. Moreover, these responses differed in magnitude and characteristics between CD103+ and CD103− CD4+T RM , suggesting a dichotomy in their contributions and possibly different roles in S. Typhi immunity. Finally, we demonstrated that Ty21a-immunization induces IEL CD4+T RM subsets in the epithelial compartment both spontaneously and following antigenspecific stimulation. Interestingly, IEL CD103− CD4+ T RM contributed significantly to S. Typhi specific IFNγ, IL-17A and TNFα as single producing cells while IEL CD103+CD4+T RM contributed to IL-2 production. Taken together, these results contribute major novel information of the effects of oral Ty21a-vaccination and the role of CD4+T RM in human TI mucosal responses. The intestinal mucosa is a major site of antigenic exposure from various sources including the microbiome and enteric pathogens. Multiple specialized populations of immune cells (adaptive and innate) including CD4+T RM reside in the intestine and contribute to intestinal "homeostasis" and protective immunity against enteric pathogens. S. Typhi or the attenuated S. Typhi vaccine Ty21a strain can effectively invade the TI mucosa, preferred site of S. Typhi infection [45, 46] . Then, it is reasonable to hypothesize that exposure to S. Typhi elicits both innate and adaptive responses including CD4+T RM in the lamina propria and the epithelial compartments of the TI which might play a significant role in protection against S. Typhi. The complexities involved in obtaining cells from the human terminal ileum biopsies (LPMC and IEL), are likely to have contributed to the lack of information available on CD4+T RM immune responses following oral vaccination. Using this unique model of oral live attenuated Ty21a vaccine in humans, we showed that oral Ty21a-immunization resulted in the spontaneous production of cytokine cells in the LP from CD103+ (IFNγ-S and IL-17A-S) and CD103− (IFNγ-S and TNFα-S) CD4+T RM and in the epithelium where IFNγ-S was produced spontaneously from both CD4+T RM subsets. Similar effects (e.g., increase in IFNγ-S and TNFα-S in both LP and IEL) elicited by Ty21a-immunization were observed following stimulation with α-CD3/CD28 stimulation. We hypothesize that there may be multiple cell subsets within each CD4+T RM subpopulation that differ in their requirements for activation and/or their recognition of specific cognate antigens [47] [48] [49] . Future studies should focus on fully understanding the heterogeneity and contribution of the resident cells in spontaneous cytokine production following Ty21a-immunization. These cells might play a role in protection, albeit likely to be of lesser importance than those mediated by antigenspecific responses. Overall, these unique data indicate that immunization with bacterial oral vaccines may have immunomodulatory effects beyond those that are specific for the vaccine being administered leading to a generalized level of activation. While the cellular requirements and transcriptional basis for the induction of CD8+T RM has been studied in other systems, our understanding of CD4+T RM generation and maintenance is poor. CD4+T RM unlike CD8+T RM are less uniform in their expression of CD103 and consequently two distinct subsets are present in tissues. Here, TI CD103+ and CD103− CD4+T RM elicited immune responses were evaluated following Ty21a immunization. Our data provided the first evidence of distinct responses between these two subsets in terms of spontaneous secretion of cytokines, stimulation with anti-CD3/CD28, and S. Typhi-specific responses resulting from stimulation with S. Typhi-infected targets and Ty21a homogenate antigens. Specifically, our results showed striking differences in terms of the characteristics (IFNγ and S for CD103+; IL-2 and MF for CD103−) of these responses. Of note, a recent study reported differences in IFNγ responses between lung CD103+ and CD103− CD4+T RM following stimulation with α-CD3/ CD28 [7] . Taken together, these results suggest that CD103+CD4+T RM subset (Th1 and Th17) are distinct from CD103− CD4+T RM subset (Th1) and might play a different role in the mucosa following oral vaccination and/or infection. We have previously characterized S. Typhi-specific LPMC CD4+T EM cells and observed increasing trends in IFNγ and IL-17A production following Ty21a immunization [50] . Here, we showed that the IFNγ and IL-17A are primarily produced by CD4+T RM (a subset of CD4+ T EM ) in significantly higher levels in the terminal ileum mucosa following Ty21a immunization. In addition, we observed that CD103+ CD4+T RM produced cytokines as monofunctional cells whereas CD103− CD4+T RM cells produced mostly multifunctional cytokines, similar to our observations in LPMC CD4+T EM [50] . These results suggest that examination of heterogeneous CD4+ T populations may mask the 'true' impact of oral Ty21a immunization due to the 'averaging effect' inherent to the analyses of whole populations/subsets which are composed of responding and non-responding cells. Thus, it is essential to focus on the fine granularity (e.g., S. Typhispecific CD103− or CD103+ T RM subsets, S vs MF) to better characterize the responses and properly study differences between cells in various immune compartments. Here, we demonstrated that oral Ty21a immunization elicited CD4+ T RM subsets in both, the LP and epithelial compartments, involving effector mechanisms (e.g., IL-17A) that might be well suited for protection against intracellular pathogens. The type of antigen used for in vitro stimulation influence the responses of CD4+T M . Thus, we addressed this issue by using S. Typhi-infected targets (moderately efficient CD4+ T stimulation) and Ty21a homogenate antigens (an efficient CD4+ T stimulation) [51] to stimulate CD4+T RM subsets. Remarkably, our results indicate that following stimulation with Ty21a homogenate antigens, LPMC CD103+T RM largely produced significantly higher levels of S. Typhi-specific IFNγ-MF, IL-2-MF and TNFα-MF in Ty21a vaccinated participants than those in the unvaccinated group. These results were different from those observed following stimulation with S. Typhi-infected targets (IFNγ-S and IL-17A-S), which largely display S. Typhi antigens in the context of both MHC-I and II molecules. Taken together, these data suggest that LPMC CD4+T RM subsets are versatile in responding to S. Typhi antigens. Of note, we recently described that oral Ty21a-immunization elicits the induction of S. Typhi-specific CD8+T RM [16] and were able to respond following in vitro stimulation with soluble antigens and S. Typhi-infected targets, indicating that multiple effector T cell responses are likely to be concomitantly induced. Future studies focusing on the activation requirements of CD4 and CD8 populations in human tissues would be vital to better understand these phenomena to accelerate the development of mucosal vaccines targeting the induction of CMI. Intestinal T cells are located diffusely throughout the epithelial compartment as intraepithelial lymphocytes (IEL). The predominant population of IEL-T cells are CD8+T with a minor population of CD4+T cells. IEL are part of the first line of defense and are deemed to be an important cell subset involved in immune responses at mucosal surfaces. Thus, in this study, we hypothesized that CD103+CD4+T RM (CD103 binds to E-cadherin on IEC) located in the LP are poised to migrate to the epithelium and play a consequential role in S. Typhi immunity. We describe, for the first time, that oral Ty21a-immunization influenced the frequencies of IEL-T cells with a significant shift in the proportion of CD4+T to CD8+T cells (significantly lower CD8+ and higher CD4+T cells). Surprisingly, we observed that this increase in frequency of IEL CD4+T cells was chiefly in the IEL CD103− CD4+T RM subset but not in the CD103+CD4+T RM subset. These data suggest that there may be other receptor-ligand pair(s) (apart from CD103/E-cadherin) involved in the migration of CD103− CD4+T RM from LP to the epithelial compartment. Alternatively, CD103− CD4+T RM cells may be more likely to move freely around the epithelium since it does not depend on their interaction with the E-cadherin on epithelial cells. Regarding S. Typhi-specific cytokine production, we found that oral Ty21a-immunization elicits significant IEL CD103+ CD4+ T RM mainly producing IL-2-S. In contrast, IEL CD103− CD4+T RM exhibited significant increases in IFNγS, IL-17A-S and TNFα-S following stimulation with S. Typhi-infected targets in Ty21a-vaccinated than unvaccinated volunteers. These antigen-specific responses showed diverse characteristics in the epithelium, suggesting that migrating and/or locally differentiated CD103− CD4+T RM are likely Th1 and Th17 which might play a significant role in S. Typhi protective immunity. In contrast, CD103+ CD4+T RM producing IL-2 might be important in supporting T cells proliferation and likely supporting T regulatory cells in this compartment. Taken together, these data suggest that following oral Ty21a immunization, both CD4+T RM effector subsets are recruited from the LP and activated/ differentiated uniquely in the epithelium, contributing distinct effector responses involved in effective S. Typhi immunity. This compartmentalization of T RM responses in the TI LPMC and IEL may provide important clues for the optimal design of future vaccines. In summary, we provide the first evidence of the induction of increased spontaneous and S. Typhi-specific cytokine production by CD4 + T RM in the human TI LP and epithelial compartments following oral Ty21a-immunization. These results contribute novel insights in our understanding of the generation of gut local immunity in humans following immunization with oral attenuated bacteria and suggest that CD4 + T RM play a key role in protection following immunization and/or infection with S. Typhi. 
paper_id= ffeaad1ccfefb0825e0b8a7b8041d5c37609c079	title= A tale of two cities: a comparison of Hong Kong and Singapore's early strategies for the Coronavirus Disease 2019 (COVID-19)	authors= Michelle  Lee;Zhi  Qing;De  Deyn;Qin Xiang Ng;Wayren  Loke;Wee Song Yeo;Michelle Lee Zhi;Qing  De Deyn;	abstract= 	body_text= In HKSAR, it is known that community-wide masking was practiced by the general population at an early stage of the pandemic 1 . Based on news reports and official press releases, it is evident that many Asian countries, which have successfully contained the first wave of infections, are now experiencing a second wave of imported cases from abroad and worsening local transmission 2 . As the authors rightly mentioned, Singapore and HKSAR share several similarities in that they are both sprawling city states with developed economies, a high trade-to-gross domestic product (GDP) ratio, advanced infrastructure and healthcare systems, and they both had varying successes thus far in the management of the ongoing pandemic, albeit adopting different approaches 3, 4 . Singapore saw her first imported case of the novel coronavirus infection on 23 January 2020. Prior to this, the authorities had a working multi-ministry taskforce, issued travel advisories for China to the public, precautionary advisory to preschools (good hygiene, travel declarations and temperature screening for staff) and implemented temperature screening at land and sea checkpoints. Given the evolving situation in China at that time and the high volume of international travel to Singapore, the Singapore government had accurately pre-empted the possibility of more suspect cases and imported cases 5 . After the 2003 severe acute respiratory syndrome-related coronavirus (SARS-CoV) outbreak, Singapore authorities had put in place a multi-ministry taskforce and a Disease Outbreak Response System Condition (DORSCON) framework that enables the whole-of-government to respond immediately to any disease outbreak and guide interventions 5 . While Hong Kong saw her first imported case on 22 January 2020 and had taken similar pre-emptive measures prior to this 6 . In terms of the alert level, Singapore raised its risk assessment (in accordance with the DORSCON framework) from DORSCON yellow to DORSCON orange on 6 February 2020 after the country saw an increased number of cases of local transmission. In Hong Kong, the serious response level was activated in public hospitals on 4 January 2020 and the hospital authority activated the emergency response level on 25 January 2020. Both countries contained the first wave of imported cases well and boasted robust testing and contact-tracing capabilities. However, a key difference was that the Hong Kong government implemented social-distancing measures and partial closures earlier, extending the Lunar New Year holidays and keeping all schools and government offices closed after the holidays, while the Singapore government had all schools (including preschools) running up till the implementation of a sweeping 'circuit breaker' measure on 7 April 2020, only allowing residents to leave their homes to exercise alone or purchase essential items. All private or public gatherings, non-essential services and sports and recreation facilities have ceased. Although both countries did not advise compulsory mask-wearing hitherto, most members of public in Hong Kong chose to wear a mask when going outside anyway. The initial advice from the Hong Kong government was for people who have respiratory symptoms, visiting hospitals and those travelling to wear a mask (and continue to do so until 14 days after returning). The generally high mask usage amongst the public could be partly due to their previous experience with SARS-CoV or their distrust in the HKSAR central government 7 . Although there are no official statistics, it was evident from media and news reports that most Singaporeans still went about their daily activities without the use of a mask, unless they were unwell 8 . The Singapore government has now dramatically reversed their recommendations on the use of masks because it is thought that they may confer additional protection against the COVID-19, due to the variable incubation period for clinical disease and possibility of asymptomatic spreaders 9 . It is now mandatory to wear a mask when going out. Tough laws have been put in place to enforce this; first-time offenders would be fined while egregious cases may be prosecuted in court 10 . The COVID (Temporary Measures) Act was passed by Singapore parliament on 7 April 2020, and under the act, the government has the powers to make regulations to further prevent or control the incidence or transmission of COVID-19. Either way, it appears that Hong Kong has managed to flatten the coronavirus curve, while Singapore is seeing a rapid upward trend (Figure 1) , mostly due to an outbreak of cases amongst foreign workers living in the dormitories 11 . There are more than 250,000 foreign workers on 'construction work permits' in Singapore, and the growing outbreak amongst workers living in the dormitories could be due to the limited space, their culture of communal cooking and sharing food, and communal toilets. Given the above context and factors, it is difficult to precisely apportion the contribution of masking versus social distancing, rigorous contact tracing and other control measures to COVID-19 containment. Properly-worn face masks probably help stem the spread of the coronavirus, albeit other environmental and ambient factors such as temperature, wind velocity and humidity would also affect how the respiratory droplets travel. Although the efficacy of wearing masks in public cannot be stated definitively, we believe the takeaway lessons are clear. Caution and pre-emptive measures yield significant preventive benefits in this crisis. The public needs to act responsibly and pay heed to these advice and physical distancing measures. Given the escalating medical and socioeconomic costs associated with this pandemic, the adage that 'prevention is better than cure' is especially relevant today.  
paper_id= ffead62e83d4331759f9251918a818f1129e7284	title= 	authors= 	abstract= 	body_text= Urinary tract infections are one of the non-access-related infections in chronic dialysis patients. Patients with residual urine production usually present with the same clinical picture as patients without kidney disease. However, anuric patients may present with only bladder discomfort. Asymptomatic pyuria without bacterial infection is common in chronic dialysis patients, however, its clinical significance for bacterial infection is controversial. Most of the studies have shown no correlation between the presence of white blood cells and urinary tract infection in asymptomatic dialysis patients [1] [2] . Moreover, urinary tract infection is the most common nosocomial infection in dialysis patients who have undergone urinary catheterization [3] . Patients with end-stage renal disease (ESRD) due to polycystic kidney disease have an increased risk of pyelonephritis. We present a case of a young female with ESRD who presented with highgrade fever and was found to have left renal pyelonephritis. A 24-year-old female with a past medical history of systemic lupus erythematosus (SLE), ESRD on hemodialysis, chronic anemia, secondary hyperparathyroidism, hypertension, systolic heart failure, anxiety disorder, and panic disorder presented to the emergency department complaining of resting, non-positional chest pain, nausea, non-bloody/non-bilious vomiting, productive cough, diarrhea, subjective fever and chills of one-day duration. Of note, she was discharged from the hospital one day prior for hypertensive emergency with flash pulmonary edema secondary to non compliance with hemodialysis sessions. A triple lumen central venous catheter was introduced into the left internal jugular vein during that admission which was later removed prior to her discharge from the hospital. On presentation, her rectal temperature was 104.6 Fahrenheit, heart rate was 102 beats/min, respiratory rate was 29 breaths/min, blood pressure was 148/89 mm Hg, and oxygen saturation was 96% on room air. Physical exam was significant for rales in bilateral upper lung fields and epigastric tenderness, but with no guarding or rigidity. Open pus draining cutaneous abscess was noted on the left neck at the site where the catheter was introduced during the previous admission. Code sepsis was called in the emergency department and the patient was started on intravenous hydration. Serum lactic acid, blood cultures, stool cultures, stool Clostridium difficile antigen tests, and cultures from the neck abscess were sent. Electrocardiogram (ECG) showed sinus tachycardia with left ventricular hypertrophy but no acute ST or T wave abnormalities as seen in Figure 1 . Brain natriuretic peptide: 1354 pg/ml (<100 pg/ml) from 5000 ng/ml six days prior to presentation. COVID 19 polymerase chain reaction (PCR) test was negative. Urine drug screen was positive for marijuana. Chest X-ray showed clear lung fields with no signs of consolidation as seen in Figure 2 . CT of the abdomen was suspicious for left kidney pyelonephritis with focal hypo-density at the mid and lower pole associated with minimal peri-nephric fat stranding as seen in Figure 3 . The patient was started on broad-spectrum antibiotics in the emergency department accounting for her history of penicillin allergy. Soon after admission, she became hypotensive and was started on norepinephrine drip along with stress dose of corticosteroids (40 mg IV push followed by 100 mg every eight hours). Infectious disease and surgery were consulted. The neck abscess was drained, however, the surgical team had reported that it was superficial and an unlikely cause of systemic symptoms. Repeat complete cell count showed a platelet count of 31,000/UL (dropped from 56,000/UL) when tested in sodium citrate (blue topped) bottle. Hematology-Oncology was consulted who attributed the thrombocytopenia to sepsis at this point. Anticoagulation was held in the light of worsening thrombocytopenia. Lower extremity venous duplex was negative for deep vein thrombosis (DVT) in bilateral lower extremities. CT chest with contrast was unremarkable for pulmonary embolism as seen in Figure 4 . Troponin levels continued to trend upward during hospital stay (0.18 ng/ml > 1.78 ng/ml > 5.39 ng/ml > 6.66 ng/ml) and cardiology was consulted. The patient was not complaining of any chest pain at that time. Findings were thought to be secondary to demand ischemia vs myocarditis from lupus flare. An echocardiogram (ECHO) showed left ventricular ejection fraction of 35%-40% with moderately dilated left atrium and moderate tricuspid regurgitation, unchanged from prior echocardiograms. Carvedilol and angiotensin-converting enzyme inhibitors were held in the light of hypotension. The patient was not a candidate for anti-coagulation as outlined above. Complement levels were C4: 8 (15-57), C3: 48 (83-193) and CH50: <10 (31-60). The following day, the patient became afebrile, blood pressure stabilized and no pressure support was required, however, her platelet counts continued to drop to a nadir of 36,000/UL. Blood cultures and neck abscess cultures grew Staphylococcus aureus sensitive to vancomycin and resistant to penicillin G. Next day in the afternoon, the patient had a brief episode of mild chest tightness during which ECG showed ST depression in lateral leads (V5, V6, I and aVL) and lead II. Left atrial enlargement and left axis deviation were also noted. In the evening, she had sudden bradycardia and asystole. Telemetry showed broad QRS complexes, followed by sinusoidal waves and asystole. Code blue was called. Advanced cardiac life support (ACLS) protocol was followed. Resuscitation efforts were done for 1 hour and 15 minutes. She was given multiple doses of calcium chloride, magnesium sulphate, and sodium bicarbonate all of which failed to be of any benefit. Bedside ECHO showed very poorly contracting right ventricle but no evidence of right ventricular dilation or strain. The patient was then pronounced dead. ESRD or chronic kidney disease (CKD) is an irreversible and debilitating condition that requires prompt treatment with frequent dialysis, hemofiltration, hemodiafiltration, renal replacement therapy, or kidney transplant. More commonly, ESRD is a complication of other chronic disease such as diabetes, SLE, hypertension, polycystic kidney disease, analgesic misuse, or amyloidosis [4] . In addition, CKD commonly results in frequent urinary tract infections secondary to increased inflammation, defective barrier function, or dialysis treatment in general [5] [6] [7] [8] . There are more than 7 million urinary tract infections per year reported in the United States, 250,000 of these cases consist of episodes of pyelonephritis [8] . However, it still remains unclear how many of these are attributed to CKD [8] . In spite of the limited research, it still remains unclear how prevalent acute pyelonephritis superimposed by ESRD is documented in patients with CKD, and which antibiotic regimen is the most efficacious. Our case presents a 24-year-old female with ESRD secondary to lupus nephritis who presented to the emergency department for intractable nausea, vomiting, diarrhea, and coughing; after workup, the patient was diagnosed with a sepsis likely secondary to pyelonephritis as abscess was superficial and unlike source as per surgical evaluation. The patient was started on broad-spectrum antibiotics: vancomycin, and aztreonam after blood/abscess cultures were drawn. Given the high sensitivity of perinephric stranding finding on CT scan for pyelonephritis and septic shock state the patient presented with initially, pyelonephritis was diagnosed based on imaging. We attributed the origin of acute pyelonephritis to a transcending infection of the residual urine in the bladder. It is illustrated in this report that our anuric patient was diagnosed with pyelonephritis in the setting of ESRD and was successfully treated with antibiotics. Research attributed to urinary tract infection in patients with CKD is limited, which makes the choice of antibiotic treatment more challenging. Early research from Bennett et al. had concluded that ampicillin and trimethoprim-sulfamethoxazole are sufficient in treating urinary tract infections in anuric patients with renal failure [9] . This finding was later fine-tuned by Gilbert et al. who reported that fluoroquinolones (ciprofloxacin, levofloxacin) and trimethoprim are sufficient in treating urethritis/cystitis in patients with CKD. Furthermore, their research concluded that nitrofurantoin had limited efficacy and should not be used due to low urine drug concentrations [8] . Due to the scarcity of cases, there is limited evidence to support or reject the use of other broad-spectrum antibiotics such as vancomycin or aztreonam in the treatment of pyelonephritis in anuric patients with ESRD. Urinary tract infection should be in the differential diagnosis in chronic dialysis patients, especially with residual urine production, presenting with fever. Given the scarcity of cases in the literature, no guidelines exist for antibiotic stewardship of these patients. Early diagnosis and proper antibiotic selection are vital to decrease the mortality in such patients. More research is needed to prove if a superior or inferior treatment regimen for anuric patients with pyelonephritis in the setting of ESRD exists. Human subjects: Consent was obtained or waived by all participants in this study. Conflicts of interest: In compliance with the ICMJE uniform disclosure form, all authors declare the following: Payment/services info: All authors have declared that no financial support was received from any organization for the submitted work. Financial relationships: All authors have declared that they have no financial 
paper_id= ffeb5183a5edb2e7477d21b217cd72ccb07c51b5	title= Transcranial Electrical Stimulation targeting limbic cortex increases the duration of human deep sleep HHS Public Access	authors= Evan  Hathaway;Kyle  Morgan;Megan  Carson;Roma  Shusterman;Mariano  Fernandez-Corazza;Phan  Luu;Don M Tucker;	abstract= Researchers have proposed that impaired sleep may be a causal link in the progression from Mild Cognitive Impairment (MCI) to Alzheimer's Disease (AD). Several recent findings suggest that enhancing deep sleep (N3) may improve neurological health in persons with MCI, and buffer the risk for AD. Specifically, Transcranial Electrical Stimulation (TES) of frontal brain areas, the inferred source of the Slow Oscillations (SOs) of N3 sleep, can extend N3 sleep duration and improve declarative memory for recently learned information. Recent work in our laboratory using dense array Electroencephalography (dEEG) localized the sources of SOs to anterior limbic sites -suggesting that targeting these sites with TES may be more effective for enhancing N3. Methods: For the present study, we recruited 13 healthy adults (M = 42 years) to participate in three all-night sleep EEG recordings where they received low level (0.5 mA) TES designed to target anterior limbic areas and a sham stimulation (placebo). We used a convolutional neural network, trained and tested on professionally scored EEG sleep staging, to predict sleep stages for each recording. Results: When compared to the sham session, limbic-targeted TES significantly increased the duration of N3 sleep. TES also significantly increased spectral power in the 0.5-1 Hz frequency This is an open access article under the CC BY license band (relative to pre-TES epochs) in left temporoparietal and left occipital scalp regions compared to sham. These results suggest that even low-level TES, when specifically targeting anterior limbic sites, can increase deep (N3) sleep and thereby contribute to healthy sleep quality. 	body_text= Impaired sleep may be a causal factor in the progression from healthy aging to Mild Cognitive Impairment (MCI) and eventually to Alzheimer's Disease (AD) [1] . Not only does sleep restriction lead to accumulation of beta amyloid proteins, but the onset of sleep impairment is typically observed well before the clinical presentation of AD [2] . A physiological mechanism for the neurotoxic effects of sleep impairment is provided by evidence that the brain's glymphatic (glial-lymph) removal of metabolic toxins may be limited to the period of sleep [3] . Recent evidence that slow wave sleep (Non-REM Stage 3 or N3) is specifically required for glymphatic clearance of neurotoxins in humans further supports the causal role of sleep fragmentation in the pathophysiology of mental decline in AD [4] . The important implication of these findings is that improving deep sleep could prevent the silent, chronic neurotoxicity that causes dementia. The defining EEG features of deep sleep are the large (>75 μV) Slow Oscillations (SOs). Massimini et al. (2004) reported that SOs reflect traveling waves that originate in frontal regions and propagate over widespread areas of neocortex [5] . Although additional studies using dense array EEG to measure SOs have been reported more recently [6, 7] , none have directly challenged the conclusion that SOs reflect global traveling waves with frontal onset. In examining dense array EEG recordings of human N3 sleep in our own lab, we observed that the typical SO pattern of a frontal-negative potential field was invariably accompanied by a simultaneous positive potential field over posterior regions. The dipolar inversion of the field suggests the neural source of the typical SO is then located in the anterior temporal lobe (see Fig. 1 b) . In twelve normal young adults, we conducted detailed electrical source analysis of dense array sleep EEG, finding that the great majority of SOs in young adults are generated by anterior ventral limbic cortex, including the parahippocampal gyrus of medial temporal lobes and the caudal regions of orbitofrontal cortex (Morgan et al., unpublished work). Although previous studies have demonstrated successful synchronization of SOs with slow TES pulses [5, 8, 9] , the stimulation in those studies used electrodes in dorsolateral frontal areas (F3, F4 or F7, F8 versus contralateral mastoids), consistent with the assumption that human SOs emanate from frontal neocortex. In contrast, once we knew to target the anterior limbic sources of SOs, straightforward computational simulation showed that electrodes at frontopolar and inferior frontal head sites (versus posterior return electrodes) would be most effective in targeting the limbic sources of SOs. In the present study, we hypothesized that Transcranial Electrical Stimulation (TES) could be applied to frontopolar and inferior frontal head sites in order to synchronize the limbic sources of SOs specifically and thereby enhance the duration of N3 sleep. Furthermore, based on our computational modeling with this more optimal targeting of the limbic sites, we hypothesized that we could use lower TES current levels (0.5 mA versus 1 or 2 mA in previous studies) that would be unlikely to disrupt sleep and that may still be successful in synchronizing SOs to enhance the adaptive neurophysiology of deep sleep. Thirteen healthy adults (6 women, 7 men) ranging between 20 and 67 years old (M = 42) participated in the study. Subjects were screened to exclude those with a history of seizures, epilepsy, brain trauma or injury, insomnia, and sleep apnea, or those using medications that may affect the EEG. Two subjects were omitted from analysis for poor data quality and one subject was omitted for failing to initially disclose the consumption of an excluded medication, resulting in 10 subjects being retained for analysis. Informed consent was obtained from all subjects prior to their participation in the experiment. The plan was for a representative sample (N = 60) including younger and older adults, but the study was halted in March 2020 after collecting 13 participants because of COVID-19 restrictions. All experimental protocols were approved by the Oregon Research Institute (ORI) Institutional Review Board. All study sessions were conducted in the Brain Electrophysiology Laboratory (BEL) located in Eugene, OR, USA. The experiment consisted of three overnight sessions lasting approximately 8 hr; each beginning at 11:00 PM and ending around 7:00 AM the next morning. The first session was an adaptation night, consisting of a full-night EEG recording. During the second and third sessions, subjects received either TES or a sham (placebo) protocol in addition to EEG monitoring. The second and third sessions were spaced one week apart to allow any effects of TES or sham to diminish. The order of sham and TES were counterbalanced across subjects to control for order effects. Subjects were blinded to the condition being delivered during nights 2 and 3. Experimenters were also blinded to the experimental condition until they opened an email that specified the treatment condition for that night after the subject was asleep. EEG was recorded and TES delivered with a 256-channel Geodesic Sensor Net (Elefix conductive paste) and a Net Amps 400 GTEN Amplifier (EGI/Philips Neuro, Eugene, OR). EEG was sampled at a rate of 1000 Hz and referenced to Cz with reference-independent voltage mapping (average reference or PARE correction) [10] used for field topography analyses. EMG electrodes were placed on the mandible. For the TES protocol, a 0.5 Hz sine wave with 520 μA total current was delivered through two frontopolar and two inferofrontal source channels to four sink channels, located at the mastoid and back of neck (Fig. 2, right) . Source/sink current alternated with the phase of the AC sine wave. To minimize sensation from the current injection, the Elefix conductive paste for the stimulating electrodes was mixed with a lidocaine solution for both stimulation and sham nights. TES was administered in five blocks of 5 min each with 1-min rest periods between blocks. The first block was started after subjects showed stable N2 sleep for 4 min. If subjects showed any signs of awakening during the TES protocol, the protocol was paused, and the remainder of the protocol was administered after the next four consecutive minutes of N2 or the start of N3. Signs of wake were determined by characterizing the streamed EEG and by video monitoring of subjects. The TES protocol was paused for two subjects. In both cases the protocol was paused and restarted twice, and all pauses occurred during a stimulation block. No TES was administered during the sham night. The selection of frontopolar and inferofrontal electrode injection sites with back of head/ neck return sites was made on the basis of computational modeling of optimal stimulation sites for the limbic sources of SOs localized in our previous study (Morgan et al., unpublished work). This modeling and source localization were computed with a head conductivity model constructed for each subject. The model contained a tessellated cortical surface with 1200 dipole patches per hemisphere, each containing a dipole that had the net vector orientation of that patch's surface. The model also contained electrode positions, determined by 3D photogrammetry [11] and registered with the MRI surface. A finite element model of the head was constructed to compute the current flow from each electrode to all cortical surface patch dipoles [12] . Source estimation of the generators of SOs from several subjects, conducted with the Bayesian Multiple Sparse Priors constraint [13] in the Sourcerer 1.0 software (BEL, Eugene, OR, USA), indicated the primary sources for the most typical SOs lie in the medial anterior temporal area (parahippocampal gyrus) and caudal orbitofrontal limbic cortex. An example of a limbic source of a typical SO is shown in Fig.  1 . To develop TES targeting of the typical ventral anterior limbic sources (sinks), we conducted forward projections from sources in these sites in Sourcerer and confirmed that the optimal electrodes for TES would create source/sink inversions between inferior frontal sites (frontopolar and inferolateral frontal) and the back of the head (occipital and neck) sites, similar to the head surface voltage field pattern of the SO shown in Fig. 1b . To contrast the current delivery montage for the present study with that used by Marshall et al. (2006) (dorsal frontal F3 & F4 sites versus mastoids), we used Sourcerer to estimate the current density distributed at the cortical surface for each montage (Fig. 2) [8] . Consistent with the greater (1.0 mA) current levels used by Marshall et al. (2006) , this simulation suggested higher current density than with the 0.5 mA impressed current used in the present study [8] . Nonetheless, the present montage was able to deliver source/sink inversions (redblue transitions in the palette of Fig. 2 ) that were well-localized at the target anterior ventral limbic sites. In order to classify sleep stages automatically, we trained a convolutional neural network (CNN) on human scoring of 730 overnight polysomnographies (PSGs) from the Cleveland Family Study (CFS) [14] [15] [16] . Each recording was scored by a trained technician according to Rechtschaffen and Kales (R&K) criteria. We divided the PSGs into training (511), validation (110), and test (109) sets with similar age distributions. The CNN was given 30 s blocks of stacked signal data from five channels (C3-M2, C4-M1, Left EOG-M2, Right EOG-M1 and EMG) downsampled to 64 Hz. The input signals went through 10 convolutional layers (384 filters per layer), a dense layer and a softmax classification layer with five outputs for the probability of each sleep stage (Schwabedal, Hathaway, Luu, & Tucker, unpublished work). The most probable stage was taken to be the stage scored for each block. After each training epoch, the validation set was used to estimate the accuracy of the classifier and the most accurate model was stored. The CNN predicted the human scoring of the test set with good accuracy (F1 = 73, acc = 84%, kappa = 0.77) for young and old subjects (Fig. 3) . These accuracy scores fell within the range of typical interrater reliability between professional sleep scorers. For example, an analysis of two American Society for Sleep Medicine (AASM)-trained scorers' annotations of 72 sleep recordings revealed an accuracy of 82% and kappa of 0.76 between them [17] . Another analysis of three scorers' annotations of 21 recordings, using AASM-2007 standards, reported kappa scores of 0.80, 0.46, and 0.49 between pairs of raters [18] . The CFS data were recorded with R&K PSG positions, which place both electrooculogram (EOG) electrodes below the eyes; however, this convention has been replaced by the AASM standard, with one EOG channel above the eyes and one below the eyes. In order to demonstrate the generalizability of the ML sleep staging to the current PSG standard, we selected channels in the 256 dEEG montage that matched AASM electrode positions for our sleep recordings. We used an auxiliary electrode for EMG. Signals from these channels were fed into the CNN to make sleep stage predictions. We used the ML sleep staging to calculate total time spent in each sleep stage and time spent in each stage as a percentage of total sleep time. Large artifacts introduced by TES prevented us from scoring epochs that occurred during stimulation, so these stimulation epochs were excluded from the analyses. In order to make accurate comparisons between stimulation and sham, placeholders were inserted in the sham recordings to signify when blocks of TES would have been delivered, and these segments were excluded from analysis. The 1-min rest blocks separating stimulation blocks were mostly artifact-free and the channels required for sleep staging could be used to score these epochs to examine the acute effects of stimulation. In addition to including these rest blocks in our analysis of time spent in each sleep stage throughout the whole night, we analyzed time spent in each sleep stage during the rest blocks and during the 5 min following the final stimulation block (or stimulation block placeholder for sham) in order to see if N3 increased directly following stimulation. For this particular analysis we excluded epochs following a pause in the TES protocol. With treatment (stimulation versus sham) as a within-subjects factor, a repeated-measures ANOVA was used to assess the manipulation contrast (stimulus versus sham) for each of the sleep stage duration outcome measures. A Wilcoxon signed-rank test provided an alternative nonparametric contrast for these comparisons. As an alternative measure of slow wave (SW) activity, we analyzed spectral power in the SW (0.5-1 Hz) frequency band. For this analysis, we defined a pre-stimulation block (the 5 min preceding the first stimulation block) and a post-stimulation block (TES rest blocks, plus 5 min following the final stimulation block). For each block of interest, segments of clean data lasting at least 5 s were marked by hand. The 20 s of data following each stimulation block (and equivalent segments for sham) were automatically excluded because of TES-induced artifact. A Wilcoxon signed-rank test revealed that the total duration of clean data did not differ between stimulation (M = 522.59 s) and sham (M = 551.55 s) conditions (T = 17.0, p = 0.944). Clean data segments were tapered with a Tukey window (α = 0.2) and then concatenated separately for pre-stimulation and post-stimulation. Periodograms were computed for the concatenated signals using Welch's method with a Hanning window (2.56-sec segments and 1.28-sec overlap between segments). Absolute PSD in the SW frequency band was calculated by averaging the PSD values for all frequency bins contained in the 0.5-1 Hz band for post-stimulation blocks. Relative PSD in the SW frequency band was defined as the percent increase in SW band PSD between prestimulation and post-stimulation. We analyzed these measures of absolute and relative PSD in 10 different scalp regions by averaging PSD in clusters of channels comprising each region. We excluded one outlier who was awake during the majority of the TES protocol and we excluded another subject because of a lack of clean segments post-stimulation. This left us with eight subjects for the spectral analysis. Because the data were not normally distributed, a Wilcoxon signed-rank test was used to compare absolute and relative SW band PSD between stimulation and sham for each scalp region. The sham (placebo) control was effective; subjects were unable to guess beyond chance on which night the stimulation was administered. As hypothesized, the repeated measures ANOVA revealed that subjects spent more time in Fig. 4, top) . We found considerable individual differences in N3 duration, and these were related to the age of the subjects, with older subjects having less N3. The Pearson's correlation between age and N3 duration for the stimulation condition was (r [9] = −0.56, p = 0.09). Similar negative correlations with age were observed for N3 duration in the sham condition and for the percent N3 measures in stimulation and sham conditions (r = −0.59, −0.56, and −0.56, respectively). Although N3 thus clearly declines with age even in this healthy sample, the difference in N3 duration between stimulation and sham did not correlate significantly with age (r [9] = −0.29, p = 0.41) (see Table 1 ). Examining the immediate effects of stimulation on neural activity of sleep, we conducted analyses on the TES rest blocks and 5 min following the final stimulation block. In this window subjects typically remained in stages N2 and N3 (Fig. 4, bottom) . Excluding one outlier who was awake during most of these epochs on the stimulation night, there was no significant difference in time spent in N2 (F [1, 8] Table 2 . As demonstrated in Table 2 , one subject severely broke the trend, presenting N2 for a large majority of epochs during the stimulation night and N3 for a large majority of epochs during the sham night. This drove up the variance of the difference scores such that these large mean differences in N2 and N3 duration following stimulation blocks were not significant. For each treatment condition and scalp region, we report mean absolute (Table 3) and relative (Table 4 ) PSD in the SW frequency band. As demonstrated in Table 3 , there were no significant differences in absolute post-stimulation SW band power between stim and sham. However, the relative PSD results are more telling. Table 4 shows that the largest increase in SW band power across treatment conditions was in the medial prefrontal region (M = 543.7%). The increase in SW band power depended on the laterality of the scalp region for the stimulation condition, but not for sham. A two-tailed Wilcoxon signed-rank test revealed that for the stimulation condition, the increase in SW band power was greater in the left temporoparietal region (M = 626.6%) than in the right temporoparietal region (M = 538.0%; T = 1.0, p = 0.021), but for sham, there was no difference between these regions (T = 14.0, p = 0.624). The one-tailed Wilcoxon signed-rank tests comparing stimulation and sham indicated there was a greater percent increase in SW band power for stimulation than for sham in the left temporoparietal (T = 3.0, p = 0.021) and left occipital (T = 5.0, p = 0.040) regions, but no significant difference between stimulation and sham for other regions. This left-lateralization of the increase due to TES synchronization of SOs is consistent with the left-lateralization of SOs observed in the source localization results of Morgan et al. (unpublished) . A closer look at the means in Table 3 reveals that the percent increase in SW band power in medial prefrontal and medial frontal regions is comparable between stimulation and sham, whereas for the lateral frontal regions, the difference between stimulation and sham is more pronounced. This phenomenon can also be observed in Fig. 5 , where we plot percent change in PSD by frequency for left frontal, medial frontal, and right frontal regions. Here we see that although the largest increases in SW band power are in the medial frontal region, the largest differences between stimulation and sham are observed on the left and right. In light of the recent controversy over whether TES, as commonly implemented, can influence neural activity with small currents applied at the head surface [19] , the present results show that even small currents (~0.5 mA) are able to synchronize brain oscillations when applied with appropriate anatomical targeting and neurophysiological timing. The spatial targeting of TES in the present study included computational modeling (Fig. 2) to place electrodes to optimally target the limbic sources of slow oscillations (SOs). The timing of TES application was delayed until after a period of N2 sleep, when the brain normally transitions to N3. As expected, the slow oscillatory TES was effective in synchronizing N3 SOs immediately, as shown by the enhanced N3 duration during the rest blocks and immediately after stimulation. This finding is strengthened by the significantly increased 0.5-1 Hz band power in left temporoparietal and left occipital regions following TES. Furthermore, the fact that the total N3 duration for the night was significantly enhanced by TES, leading to 13% longer N3 sleep compared to the sham control, suggests that the synchronization by externally applied currents was sufficient to entrain the SOs of deep sleep that were then maintained as endogenous neurophysiological activity beyond the TES windows. Unexpectedly, the TES synchronization of SOs in the first N3 period led to greater REM sleep later in the night. Whereas a decrease in other sleep stages would be expected as a result of increasing the duration of N3 (if total sleep time remained constant), this increase of REM (although not significant in this small sample) may suggest a global change in sleep architecture and should be examined in future studies. Further research on external electrical synchronization of SOs of deep sleep may have both practical and theoretical significance. Practically, applying TES for an immediate enhancement of N3 sleep has been shown to improve daytime memory performance in both young and older adults [8, 9] . Older persons with Mild Cognitive Impairment (MCI) have considerably poorer sleep than healthy seniors [20] . Given the significant decline in memory experienced by many older adults, a simple procedure for maintaining strong deep sleep beyond age 30 may be a useful approach to successful aging. Furthermore, several findings suggest that the decline of deep sleep may be a causative factor in the progression to dementia. Measures of amyloid plaque accumulation, thought to lead to excitotoxic damage of neural tissue in Alzheimer's Disease, do not predict memory decline in older subjects unless those subjects also exhibit a decline in N3 sleep [1] . A mechanism for this apparent protective role of deep sleep in avoiding dementia has been suggested by the recent observation that the physiologic activity of SOs changes brain volume with each SO event. During this volumetric change, sufficient hydrodynamic force moves CSF out of the cranial volume thereby facilitating the lymphatic excretion of amyloid and tau neurotoxins [4] . If a simple procedure for TES synchronization of SOs can be made practical for routine use, it may enhance the health benefits of deep sleep and decrease the risk for dementia that accrues with increasing age. Theoretical insight into the mechanism through which SOs organize brain activity may be gained through manipulation of TES synchronization in relation to specific memory consolidation tasks. An interesting question is how the large (75 μV) limbic field fluctuations associated with SOs may regulate limbic-neocortical consolidation dynamics [21] in response to the synchronizing effects of claustrum projections [22, 23] . In preliminary observations, we have noted that some small sources in neocortex, coupled in time with the large limbic sources, invert in phase with the large limbic sources (in the phase transition from the frontal-negative SO peak to the small frontal-positive following wave, as shown in Fig. 1a & d) . Source modeling shows that the EEG recordings of most SOs are fully explained by the limbic sources; the associated neocortical sources are invariably small and contribute negligibly to the SO features observed in the EEG (Morgan et al., unpublished work). Another question remaining is whether our use of 4 source electrodes (whereas other studies used 2) led to wider current spread, despite the weaker current level used in the present study (0.5 mA in ours vs 1 or 2 mA in previous studies). Future research should include a more accurate model of current flow using physical models of each individual's brain and head geometry. Nonetheless, the small phase-aligned neocortical sources are measurable in many SO samples, and they should be separable in theory from the limbic electric fields in source analysis with dense array EEG. Intracranial recordings in human epileptic patients have shown that the down states associated with SOs are often observed in local neocortical sites [24] . Furthermore, a typical pattern in the intracranial recordings are down states in medial frontal lobe that propagate to medial temporal sites [24] , perhaps consistent with the noninvasive source localization results of Morgan et al. In future noninvasive studies of sleep and memory, analysis of the source waveforms from both limbic and neocortical sites, with interactions inferred with appropriate nerve conduction times for cortico-cortical connections, would be instructive to combine with experimental manipulation of anatomically targeted memory consolidation tasks. The left-lateralization of the SO sources in the Morgan et al. (unpublished) study was observed for posterior non-limbic neocortical sites, including the fusiform gyrus. In the present results, left-lateralization was observed for the slow wave power increases as a result of TES in both temporal and occipital areas. The implication may be that the response to treatment is greatest in areas with the largest endogenous generation of SOs. Why these SO generators are stronger in the human left hemisphere is an interesting question for future research. Although functional interpretation must await more analytic experiments, the noninvasive EEG source localization results indicate that focal regions of limbic cortex, and not global waves over lateral neocortex, create the large potentials of SOs observed in the EEG during human deep sleep. For such focal limbic events to create the impressively large fields of SOs in the EEG implies that these events must involve highly synchronous local generators. By understanding the ventral limbic location of these synchronous SO generators, the present results suggest that targeting them specifically with artificial synchronizing currents can enhance the SO activity to extend the duration, and perhaps the health benefits, of deep sleep. Targeting of SO sources with TES begins with electrical source localization of SOs. a. SO in the frontal EEG channels 1-43. Positive is up and vertical blue lines are 1-sec marks. b. Head surface topography at the negative peak of the SO marked by the brown vertical synch line in a. The SO has a large negative deflection in frontal channels coincident with a posterior positive field. c. & e. Source localization of the negative peak of the SO to the individual's cortical surface. The main source of the SO is a tight source-sink inversion over the right anterior parahippocampal gyrus. d. Butterfly plot (all channels) of the SO. conditions. An asterisk indicates a significant difference (*p < 0.05). Post-stimulation PSD relative to pre-stimulation PSD for left frontal (top), medial frontal (middle), and right frontal (bottom) scalp regions. Data are averaged across subjects. The shaded area shows the 0.5-1 Hz frequency band of interest. Table 2 Time (seconds) spent in each sleep stage during 1-min TES rest blocks and 5 min following the final stimulation block.  
paper_id= ffeb5d85e2e10fdab4484a704a6162b379f27e43	title= Determinants of SARS-CoV-2 infection in Italian healthcare workers: a multicenter study	authors= Paolo  Boffetta;Francesco  Violante;Paolo  Durando;Giuseppe  De Palma;Enrico  Pira;Luigi  Vimercati;Alfonso  Cristaudo;Giancarlo  Icardi;Emma  Sala;Maurizio  Coggiola;Silvio  Tafuri;Vittorio  Gattini;Pietro  Apostoli;Giovanna  Spatari;	abstract= Background. Healthcare workers (HCW) are at increased risk of being infected with SARS-CoV-2, yet limited information is available on risk factors of infection. Methods. We pooled data on occupational surveillance of 10,654 HCW who were tested for SARS-CoV-2 infection in six Italian centers. Information was available on demographics, job title, department of employment, source of exposure, use of personal protective equipment (PPE), and COVID-19-related symptoms. We fitted multivariable logistic regression models to calculate odds ratios (OR) and 95% confidence intervals (CI). Findings. The prevalence of infection varied across centers and ranged from 3.0% to 22.0%, being strongly correlated with that of the respective areas. Women were at lower risk of infection compared to men. Fever, cough, dyspnea and malaise were the symptoms most strongly associated with infection, together with anosmia and ageusia. No differences in the risk of infection were detected between job titles, or working in a COVID-19 designated department. Reported contact with a patient inside or outside the workplace was a risk factor. 	body_text= Healthcare workers (HCW) are a group at high risk of infection in general 1 and specifically SARS-CoV-2 infection. 2, 3 However, few studies have been reported in the literature on prevalence of COVID-19, and on related risk factors in this group of workers: 4 a study of 1654 HCW from England, 5 one on 72 infected HCW from China, 6 and two small studies of HCW from Switzerland and Singapore who had a contact with a case. 7, 8 Given the lack of information on determinants of infection in this important occupational group, and the relevance of such data for other groups of the population, we undertook an analysis of clinical and occupational data collected among more than 10,000 Italian HCW who were tested for presence of SARS-CoV-2 during March and April 2020. The project was conducted in seven academic centers under the auspices of the Scientific Committee of the Italian Society of Occupational Medicine. All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint this version posted July 30, 2020. . https://doi.org/10.1101/2020.07.29.20158717 doi: medRxiv preprint A total of 10,654 HCW were included in the analysis. Period of testing, incidence and mortality rates of COVID-19 infection in the general population of the study areas are shown in Table 1 . Key characteristics of the study population are included in Table 2 . Women represented two thirds of HCW included in the analysis (which reflects the HCW demographics in Italy); average age ranged from 34 to 47 years, with overall mean of 45.4 years (sd 0.53). In all centers, nurses and doctors represented two thirds or more of HCW tested for SARS-CoV-2. Overall, 843 (10.3%) HCW tested positive among 8,203 subjects included in surveillance systems. With the exclusion of the center in Genoa, in which only faculty members and residents who presented a clinical picture suggestive for COVID-19 (suspected cases) were tested (34% positive), the prevalence of HCW positive for SARS-CoV-2 ranged from 3% in Bari and Pisa to 22% in Brescia, while this prevalence in the other centers was between 5% and 7% (Table 3) : There was a strong correlation between prevalence of infection in HCW and COVID-19 incidence (r = 0.93, p = 0.002) and mortality (r = 0.99, p = 0.0001) across the study centers. The ORs of SARS-CoV-2 infection for sex, age, and self-reported symptoms are reported in Table 4 . Female seemed less at risk of infection than male HCWs; no differences were detected according to age. Self-reported (or measured) fever was strongly associated with infection; additional symptoms associated with SARS-CoV-2 infection include cough, dyspnea, malaise, and ageusia or anosmia; information on the latter two symptoms was available only for a subset of HCW, but they showed a very strong association with infection. Conversely, self-reported sore throat and diarrhea were not associated with infection in this population. Results on the association between SARS-CoV-2 infection and job-related circumstances of exposure are reported in Table 5 . No differences in the risk of infection were detected for job titles, although there was some heterogeneity in results among centers (not shown in detail). Working in a COVID-19 designated department was not a risk factor for infection The analysis on the potential source of infection indicated that contact with a patient was associated with a higher risk of SARS-CoV-2 infection compared to contact with a colleague, which represented the majority of contacts at the workplace. All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint this version posted July 30, 2020. . https://doi.org/10.1101/2020.07.29.20158717 doi: medRxiv preprint The use of surgical mask was associated with a reduced risk of infection, while the use of a filtering facepiece 2 or 3 (FFP2/FFP3) mask did not appear to confer additional protection. Use of gloves was also associated with a reduced risk of infection, while no difference was detected for use of face shield or gown. After adjusting for personal use of mask, the fact that the contact (patient or colleague) wore a mask was associated with a strong reduction in risk of infection (OR 0.52; 95% CI 0.32-0.85). If both the HCW and the contact wore a mask, the risk of infection was strongly reduced (OR 0.31; 95% CI 0.17-0.57) compared to the situation in which neither did (reference case). The results of the two-stage meta-analyses replicated those of the pooled analyses; for example, the meta-OR for employment as health care attendant was 1.08 (95% CI 0.80-1.36; p-value of test for heterogeneity = 0.73), that for using a medical mask was 0.61 (95% CI 0.41-0.81, p heterogeneity = 0.92). All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint this version posted July 30, 2020. . https://doi.org/10.1101/2020.07.29.20158717 doi: medRxiv preprint This analysis revealed that the prevalence of infection in HCW varied across centers, with results collected in centers with comparable protocols ranging from 3.0% to 22.0%, and was strongly correlated with that of the respective geographic areas. These figures fitted the prevalence of infection and mortality from COVID-19 in the general population of the study areas. Despite the limitations in the available data, in particular those from the general population, 9 these results confirm and quantify the variability in the risk of infection experienced by HCW according to the distribution of SARS-CoV-2 infection in the patient population of the hospitals where they worked. A survey of 1,097 HCW from six hospitals in the Netherlands also reported ample variability between regions. 10 Middle-aged women in Italy experienced a slightly higher incidence of SARS-CoV-2 infection compared to men, although incidence at older age and the proportion of cases of severe COVID-19 and death across in the whole age range were lower. For example, as of June 3rd, 2020, the female/male ratio in the number of cases in the age range 30-69 was 1.07. 11 compared to a ratio of 1.02 in the Italian population. In this population of HCW, on the other hand, the risk of infection was lower in women compared to men; however, in the subset of HCW with information on source of contact, no difference in risk was shown according to gender (OR 0.97; 95% CI 0.74-1.27 after adjustment for contact with colleague and patient). This suggests that any difference may be due to circumstance of exposure rather than gender differences. In a study of 72 HCW from Wuhan, China, men were at higher risk of infection compared to women. 6 Age was not associated with risk of exposure in this population, similar to a small study from Wuhan, China. 6 These results suggest that differences by age observed in the general population 11 might be due either to higher opportunity of exposure, or to higher number of tests performed. Information on symptoms was available for most HCW undergoing testing for SARS-CoV-2 infection. Fever and, to a lesser extent, cough, dyspnea and malaise were the symptoms most strongly associated with infection, while sore throat and diarrhea did not predict a positive results. Information on ageusia and anosmia was collected in a few centers, and only after several weeks of testing: even if they were available on a relatively small number of HCW, these symptoms were strongly predictive of infection. 12 The analysis by job title and department of employment represents one of the most important contributions of this study. The lack of a clear pattern in risk according to job categories All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint this version posted July 30, 2020. . https://doi.org/10.1101/2020.07.29.20158717 doi: medRxiv preprint indicates that all HCW, at least those in the centers included in the analysis, were at comparable risk of becoming infected. The lack of information on the denominators, that include also HCW who were not tested because they did not fulfill the criteria initially established by the Italian Ministry of Health 13 limits our ability to draw conclusions on the absolute risk in the different occupational groups. To address this question, we analyzed a group of HCW from one center (Bari) who were tested outside the protocol based on contact and symptoms. Among 2373 such HCW, 11 cases of SARS-CoV-2 infection were detected, of whom three among 831 nurses, seven among 798 physicians (OR 4.1; 95% CI 1.0-16.4) and one among 381nurses or other health care professionals (OR 0.7; 95% CI 0.1-6.5). The lack of difference in infection prevalence by job is consistent with the results of a study of 1654 HCW from England 5 . The lack of difference in risk between HCW who worked in designated COVID-19 departments and those who worked in other departments is reassuring as it indicates that working in high-risk environment did not entail a higher risk of infection, probably because of increased awareness and proper use of PPE by the employees. For example, the proportion of HCW employed in COVID-19 departments who reported using surgical mask or FFP2/3 was 87%, compared to 78% of other HCW. Our results are also comparable to those reported in a study of 183 SARS-CoV-2 positive US HCW, among whom the prevalence of infection was comparable between frontline and non-frontline HCW. 14 The analysis of circumstances of exposure with a COVID-19 case suggested a role for contact with a patient compared to contact with a colleague or a contact outside the workplace. The effect of PPE use on the risk of becoming infected with SARS-CoV-2 varied according to the device. The use of a surgical mask or a FFP2/3 mask appeared to be the single most effective approach to reduce risk. We found no difference in the effect of using a FFP2/3 instead of using a surgical mask (results not shown in detail). Reported use of gloves was also associated with a reduced risk of infection, although the result did not reach the formal level of statistical significance, while use of face shield or disposable gown was not associated with reduced risk of infection. It is important to note that this information was self-reported by the HCW, and might be subject to some degree of misclassification, which is likely to be non-differential with respect to infection, since it was collected before the HCW knew about their status. Such misclassification would likely reduced the measured protective effect of PPE. An original and important finding of our analysis is the strong protection offered by the All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint this version posted July 30, 2020. . https://doi.org/10.1101/2020.07.29.20158717 doi: medRxiv preprint use of a surgical or FFP2/3 mask by both the HCW and the patient: the effects of the two devices appear to be additive, with a measured risk of SARS-CoV-2 infection that was less than one third when both were used compared to when neither was. The World Health Organization recommends that HCW caring for inpatient COVID-19 patients wear a medical mask, gown, gloves and eye protection. 15 Wearing a medical-grade mask is consistent with our findings, there is strong evidence in the literature that use of medical-grade masks protects against viral infection in both outpatient and inpatient settings, 16, 17 including infection with Coronaviruses. 18 When we searched for randomized trials comparing the protective effects of surgical vs. FFP2/3 masks against Coronaviruses, we did not identified any for novel SARS-CoV-2 causing COVID-19. The debate concerning the comparison of the protection exerted by FFP2/3 masks compared to surgical masks is still ongoing and needs further research. 19, 20 No data were available in the literature on the effect of mask wearing by the source of exposure (patient or colleague). The evidence concerning which characteristics of other PPE affect their effectiveness in preventing SARS-CoV-2 infection is still limited. 17, 21 Our study suffers from some limitations. Data were collected independently in the different centers; the harmonization process might have generated some misclassification, which however was likely non-differential with respect to infection status since information on risk factors was collected before tests were conducted. Although standard protocols were established at the national level to test HCW, it is possible that some individuals were tested outside such protocols. In addition, rhinopharyngeal swabs detect the presence of SARS-CoV-2 at the time of the test, and it is possible that some HCW who tested negative had been previously infected and developed an asymptomatic disease. In addition, the test itself has estimated sensitivity of 80% or less, while the specificity is as high as 99%. 22 Finally, no information was available on the viral load of positive tests. Advantages of our study include, in addition to the large number of HCW and the heterogeneity of their jobs and exposure circumstances, the availability of data on multiple potential determinants of SARS-CoV-2 infection, and the prospective collection of information, that reduces the likelihood of reporting bias. In conclusion, our study showed the importance of studying SARS-CoV-2 infection in HCW with potential high exposure to the virus, and identified several determinants of infection that are also relevant to other occupational groups and the population at large. More large-scale All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint this version posted July 30, 2020. . https://doi.org/10.1101/2020.07.29.20158717 doi: medRxiv preprint studies are warranted on HCW from other countries which experienced a range of severity of the COVID-19 epidemic and implemented different approaches for the prevention of infection in HCW. All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint this version posted July 30, 2020. . https://doi.org/10.1101/2020.07.29.20158717 doi: medRxiv preprint We pooled the data on SARS-CoV-2 infection collected by the occupational medicine centers involved in the surveillance of HCW in large urban Italian hospitals in Bari, Bologna, Brescia, Genoa, Pisa and Turin. Starting in March 2020, surveillance systems were established in each center to monitor HCW for possible infection with SARS-CoV-2, including testing them with swabs to detect SARS-CoV-2 RNA by RT-PCR, in Regional Reference Laboratories, and databases were established to monitor and follow them up. No information on viral load was available. In all centers, HCW were tested for SARS-COV-2 infection using either a rhinopharyngeal or an-oro-pharyngeal swab. 23 Samples were analyzed according to the guidelines proposed by the World Health Organization. 13 In general, HCW were tested if matched one of the following case definitions, although some details differed across different centers, and definitions changed slightly over time: (i) patients with severe acute respiratory illness (fever and at least one sign or symptom of respiratory disease, i.e., cough, shortness of breath, and requiring hospitalization) in the absence of an alternative diagnosis that fully explains the clinical presentation; (ii) patients with acute respiratory illness (fever and at least one sign/symptom of respiratory disease, e.g., cough, shortness of breath), and a history of travel to or residence in a location reporting community transmission of COVID-19 disease prior to symptom onset; (iii) subjects with or without acute respiratory illness or symptoms, who have been in contact with a confirmed or probable COVID-19 case in the absence of adequate protection. Tests were repeated for most subjects with a positive result to monitor the infection; a proportion of subjects with a negative result were re-tested because of repeated contact with a COVID-19 case. In one center (Bari) additional HCW, including subcontract workers, who did not fulfill the inclusion criteria described above, were also tested. In another center (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint this version posted July 30, 2020. . https://doi.org/10.1101/2020.07.29.20158717 doi: medRxiv preprint HCW in centers located in heavily affected areas (Bologna, Brescia, Genoa, Turin) started to be tested at the beginning of March 2020, while testing started in subsequent weeks in centers located in less affected areas (Bari, Pisa). The present analysis included results of tests performed up to April or early May 2020, depending on center. Different formats were used to collect data in each center, we established a minimum data record whit this set of variables: basic demographic data, job title, hospital department or department of employment, including working in a designated COVID-19 department, selfreported circumstances of contact with a case, self-reported use of personal protection equipment (PPE), including, in one center, use of PPE by the contact person, and selected self-reported (or measured) symptoms. The list of variables is specified in Supplementary   Table 1 . We retained in the analysis HCW with at least one valid test result; the outcome of the analysis was the presence of at least one positive result. Multivariable logistic regression models were fitted to the data to estimate odds ratios (OR) of positive result in at least one test, together with 95% confidence intervals (CI). All models included sex, age group and center as potential confounders. Models including additional potential confounders were also run, but in general there was little evidence of reciprocal confounding between exposure variables. In a secondary analysis, center-specific OR for selected risk factors were combined using a random-effects meta-analysis 24 to assess the validity of the data pooling approach. The study was reviewed and considered exempt by the Ethics Committee of the University of Bologna. All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint this version posted July 30, 2020. All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint this version posted July 30, 2020. . https://doi.org/10.1101/2020.07.29.20158717 doi: medRxiv preprint The authors declare no conflicts of interest. The study was funded with internal resources of the participating institutions. All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint this version posted July 30, 2020. . https://doi.org/10.1101/2020.07.29.20158717 doi: medRxiv preprint (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint this version posted July 30, 2020. . https://doi.org/10.1101/2020.07.29.20158717 doi: medRxiv preprint † OR adjusted for sex and center ‡ Self-reported symptom at least one contact; OR adjusted for sex, age and center All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint this version posted July 30, 2020. . https://doi.org/10.1101/2020.07.29.20158717 doi: medRxiv preprint Use of mask by contact Any mask (contact) § 32/742 0·52 0·32-0·85 Any mask (HCW and contact) 22/659 0·30 0·16-0·55 OR, odds ratio; CI, confidence interval; Ref, reference category * OR adjusted for sex, age, and center ** OR adjusted for sex, age, center, and job title † Self-reported source of contact at least one contact; OR adjusted for sex, age, center and other sources of contacts ‡ Self-reported PPE at least one contact; OR adjusted for sex, age, center, job title and other PPE § Self-reported at least one contact; OR adjusted for sex, age, center and use of mask by HCW All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint this version posted July 30, 2020. . https://doi.org/10.1101/2020.07.29.20158717 doi: medRxiv preprint 
paper_id= ffeb79b4d1f203723682a8479f5455b61edf7b91	title= Altitude does not protect against SARS-CoV-2 infections and mortality due to COVID-19	authors= 	abstract= 	body_text= To the Editor, We read with interest the paper from Millet et al. (2021) , which discuss reports suggesting that high-altitude residence may be beneficial in the novel coronavirus disease . They conclude that no evidence-based knowledge is presently available on whether and how altitude/hypoxia may prevent, treat, or aggravate COVID-19. They suggest that the reported lower incidence and mortality of COVID-19 in high-altitude places observed by others (Stephens et al., 2021) remain to be confirmed. Stephens et al. (2021) study two groups: one living in counties located at altitudes less than 914 m and the other at altitudes higher than 2133 m. Castagnetto et al. (2020) assessing data at district level have demonstrated that at different altitude ranges from 0 to <1000 m, 1000 to 2500 m, and 2500 to 4700 m in Peru, the negative association with altitude was observed only in the group ranging from 0 to <1000 m. If analysis is done using all altitudes from 0 to 4700 m a negative association is observed between altitude and cases with SARS-CoV-2. Fernandes et al. (2021) assessing 154 cities in Brazil located between 5 and 1135 m. They observe a negative association between altitude and cases of SARS-CoV-2. Altitudes higher 1135 m were not assessed. Cano-Pérez et al. (2020) in Colombia analyzed data from 70 selected municipalities with altitudes between 1 and 3180 m. They suggest that living at high altitude can reduce the impact of COVID-19, especially the case fatality rate. However, when the total of 1122 municipalities in the same country were analyzed no altitude gradient that is protective against SARS-CoV-2 infection or COVID-19 mortality could be demonstrated (Valverde-Bruffau et al., 2020) . In this letter we relate cases, deaths, and CFR by COVID-19 to altitude using the district approach in Peru and Colombia. At all, 2881 districts in Peru and Colombia have been analyzed, varying in altitudes from 2 to 4705 m. Data from 1,745,498 cases and 65,797 COVID-19 deaths were included in the analysis. Data is publicly available and as it is not possible to identify individuals in this data, then anonymity is guaranteed. We have assessed COVID-19 cases, deaths, and case fatality rates (CFR) in three gradients of different altitudes: from 2 to <1500 m, 1500 to <2500 m, and 2500 m to 4700 m. We have also controlled in the analysis by geographical area. When the entire altitude range was assessed, it was observed that the increase in altitude correlated with a decrease in positive cases of SARS-CoV-2 and COVID-19 deaths (p < 0.001), but positively with CFR/1000 (p = 0.0004). When evaluated at the three gradients of different altitude ranges, in the multivariate analysis is appreciated that the differences between altitudes are observed in the range from 2 to <1500 m (n = 1273 districts). In this range (from 2 to <1500 m), the altitude increase was inversely associated with the positive cases of SARS-CoV-2 (R 2 = 0.03; −0.69 ± 0.09; Coefficient beta ± standard error; p < 0.001), deaths (R 2 = 0.09; −0.04 ± 0.003; p < 0.001) and the CFR (R 2 = 0.01; −0.0013 ± 0.00420; p = 0.001) per COVID-19. Between 1500 and <2500 m (n = 500 districts), cases were positively associated with altitude (R 2 = 0.01; 0.38 ± 0.16; coefficient beta ± standard error; p = 0.018), deaths (R 2 = 0.01; 0.013 ± 0.006; p = 0.015), and CFR (R 2 = 0.002; 0.008 ± 0.017; p = 0.631). Between 2500 and 4700 m (n = 1108 districts), no association was observed between the increase in altitude and cases (R 2 = 0.003; −0.08 ± 0.06; Coefficient beta ± standard error; p = 0.17), between altitude and deaths (R 2 = 0.001; −0.005 ± 0.003; p = 0.09), and between altitude and CFR (R 2 = 0.003; 0.0027 ± 0.01; p = 0.785). According to this study, when the entire range of altitudes is used to correlate with positive cases of SARS-CoV-2, it is possible to obtain an inverse relationship, but this association is limited to the range between 2 and <1500 m. We confirmed the same pattern when deaths due to COVID-19 and CFR were evaluated. From this, it is suggested that altitude itself does not constitute protection against infection or CFR, and that the relationship between cases, deaths, and CFR at altitudes of 2 to <1500 m is due to other causes not related to environmental This is an open access article under the terms of the Creative Commons Attribution License, which permits use, distribution and reproduction in any medium, provided the original work is properly cited. © 2021 The Authors. Physiological Reports published by Wiley Periodicals LLC on behalf of The Physiological Society and the American Physiological Society factors clearly associated with increased altitude such as hypoxia, increased UV radiation, and aridity, among others (Millet et al., 2021) . In this altitude range (2 to <1500 m), we have observed that at a lower geographical area there are a greater number of deaths/100,000 inhabitants and CFR and this can partially explain this association between altitude and COVID-19 between 2 and <1500 m. At lower altitudes and particularly in capital cities, there is greater flow of people, higher vehicular traffic, greater agglomeration that facilitate contagion with SARS-CoV-2. The highest number of cases at lower altitudes in the range of 0-1500 m could also be due to high air pollution in large cities (Vasquez-Apestegui et al., 2020) . When all altitudes (2-4700 m) are assessed in a single analysis, an inversely proportional correlation with altitude is observed as reported in other studies Segovia-Juarez et al., 2020) . This relationship is apparently an artifact as only this negative correlation is evident at altitudes between 0 and <1500 m. From 1500 to 4700 m, no association of positive cases with altitude is observed even though variables such as partial oxygen pressure, humidity, solar and cosmic radiation and humidity, and production of vitamin D change proportionally as altitude increase (Pun et al., 2020) ; further production of the angiotensin 2-converting enzyme, a carrier molecule for the entry of SARS-CoV-2 virus into the host cell, has also been suggested (Millet et al., 2021; Srivastava et al., 2020) . Our data suggest that none of these factors protect people living in high altitudes to reduce SARS-CoV-2 infections or the severity of COVID-19 disease, and people must not be confident and engage in behaviors that could reduce the effectiveness of protective measures such as vaccination, physical distancing, washing hands, mask wearing, and avoiding large gatherings of people. It is likely that confounding factors such as availability of services to treat, diagnose, or test for COVID-19 as well as for determining the cause of death, among others-all of which are less available at higher altitudes sites perhaps gave rise to the earlier, now-apparently erroneous report of a "protective" effect of high-altitude residence. It is also important to highlight that we have presented retrospective and data-base based observations. This is just association, like all other related publications. We cannot establish causal association although other literatures have somewhat led that kind of narrative. In conclusion, altitude, especially above 1500 m, is not a protective factor for SARS-CoV-2 virulence or lethality. Understanding the epidemiology of COVID-19 is increasingly important in guiding control measures. Overall, this study should be followed by larger, better designed efforts, to increase our understanding of the impact of altitude in situations of comorbidities as obesity, diabetes mellitus, hypertension and excessive erythrocytosis. 
paper_id= ffec12aa4a9fd44c1cef69a933194c5f19e75bf0	title= Humanized Mice for Live-Attenuated Vaccine Research: From Unmet Potential to New Promises	authors= Aoife K O&apos;connell;Florian  Douam;	abstract= Live-attenuated vaccines (LAV) represent one of the most important medical innovations in human history. In the past three centuries, LAV have saved hundreds of millions of lives, and will continue to do so for many decades to come. Interestingly, the most successful LAVs, such as the smallpox vaccine, the measles vaccine, and the yellow fever vaccine, have been isolated and/or developed in a purely empirical manner without any understanding of the immunological mechanisms they trigger. Today, the mechanisms governing potent LAV immunogenicity and long-term induced protective immunity continue to be elusive, and therefore hamper the rational design of innovative vaccine strategies. A serious roadblock to understanding LAV-induced immunity has been the lack of suitable and cost-effective animal models that can accurately mimic human immune responses. In the last two decades, human-immune system mice (HIS mice), i.e., mice engrafted with components of the human immune system, have been instrumental in investigating the life-cycle and immune responses to multiple human-tropic pathogens. However, their use in LAV research has remained limited. Here, we discuss the strong potential of LAVs as tools to enhance our understanding of human immunity and review the past, current and future contributions of HIS mice to this endeavor. 	body_text= Live-attenuated vaccines (LAVs) have saved millions of lives and continue to prove themselves as one of the most important inventions in modern medical history. Smallpox was successfully eradicated in 1980 using the closely-related vaccinia virus [1] , and live measles vaccines averted an estimated 21.1 million deaths between 2010 and 2017 [2] . The immunogenicity of LAVs is superior to other vaccine types: inactivated, subunit, and toxoid vaccines may require additional doses and adjuvants to achieve s-ufficient immunity (Table 1 ) [3] . In contrast, a single dose of yellow fever vaccine provides protection against the disease for at least ten years [4] . While some concerns remain about the ability of LAVs to revert toward a wild-type genotype in humans, the adverse effects and risks associated with these vaccines remain very low and are clearly outweighed by the health benefits they provide at a global scale. Over recent years, alarming episodes of pathogen (re-) emergence for which there are no licensed vaccines, such as Ebola, Lassa, or Zika virus, have been witnessed [5] [6] [7] . Additionally, the emergence potential of currently neglected pathogens such as Powassan virus and Eastern equine Encephalitis Virus [8] strongly highlight the global need for better preparedness against future epidemics. A primary limitation in developing innovative vaccines against today and tomorrow's challenging infectious diseases is our inability to understand the molecular basis of current LAV attenuation and immunogenicity. This is mainly because of the empiric nature of their development [9] . Traditionally, LAV attenuation has been achieved either through the serial passage of a virulent pathogen or the isolation of a closely related virus or bacterium that produces mild disease in humans, such as cowpox for smallpox vaccines [9] . This means that the existence of a LAV directly correlates with the capacity of its parental or common ancestor strain to evolve and adapt to a novel host or environment. Therefore, LAV attenuation is tied to the stochasticity of evolution. The story of the generation of the yellow fever vaccine strain 17D strongly supports such reasoning. 17D was isolated by serially passaging a virulent yellow fever virus (YFV) strain in mouse and chicken embryo tissues [10] . Despite several attempts to repeat the process, 17D could not be isolated more than once [10] . Qualities including the immunogenicity, reactogenicity, and safety and stability of vaccine types are compared. The immunogenicity of live-attenuated vaccines (LAVs) is superior to that of other vaccine types, although they are more reactogenic. Although the risk is extremely low, LAVs have the potential to revert to a virulent phenotype, therefore their stability is lower than inactivated, subunit, and toxoid vaccines. Reactions to other vaccine types may also occur, but the risk is lower than for LAVs. +++: high; ++: moderate; +: low. A major roadblock in our quest to understand mechanisms of LAV attenuation and immunogenicity is the absolute need to study human-specific immune responses in vivo and in controlled experimental settings. When studying LAV responses in a cohort of voluntary human vaccinees, peripheral blood can be readily assessed, yet tissues from the site of infection or from secondary lymphoid organs where the immune response is primed cannot. Additionally, to precisely pinpoint the attenuation mechanisms of a LAV, it is important to perform a direct comparison between the immune responses induced by LAVs and by their virulent counterpart. However, such a comparison, as well as the genetic manipulation of the immune system, remains ethically impossible in human patient cohorts. Therefore, our understanding of LAV attenuation and immunogenicity fully relies upon the identification of a cost-effective and easily accessible animal model that can accurately recapitulate human immune responses upon vaccination. Humanized mice, or immunodeficient mice harboring human tissues and/or genes, have emerged as viable preclinical models for modeling human biological process and disease [11] . Mice engrafted with human immune system (HIS mice) components have notably been instrumental in studying the infectious cycle of human-tropic bacteria and viruses in vivo and understanding how these pathogens interact with the human immune system [12, 13] . However, the contributions of HIS mice to LAV research has remained limited in comparison to other model systems, such as the human model or non-human primate (NHPs) models, mainly because conventional HIS mice harbor important limitations for accurately modeling LAV-induced immunity, such as limited human hematopoiesis, improper immune priming and hampered adaptive response. Over the past five years, novel HIS mice have been developed and now demonstrate a superior ability to mount potent innate and adaptive immune responses against immunogens and pathogens [14] [15] [16] [17] [18] . These emerging models, and future ones to come, offer unprecedented opportunity to better understand how LAVs interact with the human immune system over time and space, at the site of infection and in secondary lymphoid tissues. This review will encompass the past, current and future applications of HIS mice for elucidating the molecular mechanisms governing LAV attenuation and immunogenicity. First, we will discuss the empiric development of several historically important LAVs and how they have positively impacted global human health, and review some of the molecular mechanisms suspected to underly their attenuation. Next, we will provide an overview of the past and current HIS mouse models and explore the contributions of HIS mice to our understanding of LAV attenuation and immunogenicity. Finally, we will identify the shortcomings associated with these models and describe how current and future efforts in refining HIS mice models will open avenues for a better understanding of LAV-induced immunity. For over 200 years, LAVs have served as invaluable tools for controlling and/or eradicating infectious diseases. The idea that infection by live pathogens can confer protection against deadly infectious diseases has been employed throughout history through procedures such as variolation [19] [20] [21] [22] [23] [24] . However, by the end of the 18th century, such an idea was popularized by Edward Jenner who demonstrated that inoculation with the cowpox virus, which only produced mild illness in humans, provided immunity against smallpox, a deadly disease induced by a closely related virus, the variola virus [25] . With this discovery, Jenner not only promoted the concept of live-attenuated vaccines, but the general concept of vaccination as well, an idea that would save hundreds of millions of lives in the following centuries. In the 1800s, research by Louis Pasteur with chicken cholera [26] , anthrax [27] , and rabies [28] introduced the concept that LAVs could be generated by attenuating deadly pathogens through exposure to heat or oxygen. During the early 20th century, the serial passage of pathogens in specific media was also found to be an effective way to attenuate pathogens and generate LAVs. The tuberculosis Bacille Calmette-Guérin (BCG) vaccine was the first vaccine to be attenuated through serial passages. Formulated by the repeated subculture of Mycobacterium bovis, the bacterium responsible for bovine tuberculosis, the generation of the BCG vaccine therefore successfully integrated the vaccination concepts described by Jenner and Pasteur [29] . By being made through the attenuation of pathogens related to the bacterium or virus responsible for a disease, the BCG vaccine hence represents what could be considered a second category of LAVs. The pentavalent rotavirus vaccine is another LAV of this category, which contains a combination of attenuated human and bovine rotavirus strains [30] . The advent of pathogen attenuation through serial passage brought forth the most predominant category of LAVs, which are made directly through attenuation of the causative pathogen of a disease. Serial passage was performed in animal hosts, embryonated eggs, and eventually in tissue culture [31] . Some of the most notable, established LAVs generated this way are the yellow fever [32] , poliovirus [33] , measles [34] , mumps [35] , rubella [36] , typhoid [37] , varicella [38] , and zoster [39] vaccines. LAVs undeniably represent the most effective class of vaccines ever developed. Based on the success of past LAVs, several LAV candidates targeting challenging pathogens such as Dengue virus [40] , Japanese Encephalitis virus [41] , West Nile virus [42] , Zika virus [43] , SARS [44] , Malaria [45] or Ebola virus [46, 47] , have been designed and evaluated. For instance, in response to the Ebola virus disease outbreaks in Africa between 2013 and 2016, a recombinant vesicular stomatitis virus (VSV) expressing the envelope glycoprotein of Ebola virus was generated (VSV-EBOV). VSV induces asymptomatic infection in humans and causes only mild, transient illness, and the vaccine has demonstrated safety and immunogenicity in multiple phase I and II/III clinical trials [46, 47] . Unlike other types of vaccines, LAVs carry both potent immunogenic epitopes and the ability to replicate, at a highly tuned rate, into their host. Therefore, their effectiveness relies primarily upon a very fine control of the virus's ability to replicate: LAVs have to replicate enough for immunogenic epitopes to spread and activate a potent immune response, but at a limited and stable enough rate to prevent adverse effects or genetic reversion toward a wild-type phenotype, as well as to allow subsequent immune control of infection (Table 1) . The methods by which established LAVs have been generated (use of related pathogens, attenuation of related pathogens, and attenuation of the causative agent of a disease), are all empiric and were conducted without any knowledge of the immunological mechanisms that are at play during LAV infection. Specific mutations in viral or bacterial genomes, which likely translate into differential host-pathogen interactions, have been clearly associated with pathogen attenuation. However, the nature of these specific host-LAV interactions, and how they trigger a potent immune response, remain mostly unknown. The emergence of new infectious diseases, minimization of adverse reactions to LAV and reversion to wild-type strains, short-tracking of vaccine production methods, and provision of immunization options for immunocompromised individuals, are only a few of the reasons why researching the mechanisms governing LAV attenuation still remain of critical importance today. Despite the development of potentially promising LAV candidates over the past two decades to fight many challenging infectious diseases (see examples above), a very limited number of them have actually been licensed so far, suggesting that novel rational approaches are needed to design more potent and/or safer LAVs in the future. To do so, the currently established LAVs represent a formidable source of information. In this first section, we will review the historical development and global impact of established LAVs, and discuss some of the attenuation mechanisms that have been identified for these vaccines. Smallpox was a deadly disease that plagued humanity for thousands of years until its eventual eradication in 1980 [1, 48] . The disease was caused by the virus variola from the Orthopovirus genus and poxvirus family, and had a case fatality rate of 30% [49] . Variolation, or infection with material from smallpox pustules or scabs, was practiced in China and India for thousands of years before its introduction into Europe in the 1790s [19] [20] [21] [22] [23] [24] 50] . Although variolation was safer than the contraction of smallpox itself, it had a 1% chance of mortality even if executed correctly [9] . While it was commonly known that those who contracted cowpox (a zoonotic poxvirus closely related to variola that only induced mild disease in humans) did not contract smallpox, deliberate inoculation with the virus was not popularized until the experiments of Edward Jenner were published in 1798. Jenner's findings summoned a wave of enthusiasm for what would eventually be termed 'vaccination' but issues with using cowpox as a LAV, which included bacterial contamination and unmeasured potency, caused vaccination support to wane. Early vaccines were made of lymph from infected calves or were propagated between vaccinees, but arm-to-arm transfer was considered unsafe because of the spread of bloodborne diseases [49] . A virus closely related to cowpox and variola known as vaccinia took the place of cowpox as the source of smallpox LAVs by 1900 through what may have also been contamination or the result of varied methods of virus passage [51] . Despite initial setbacks in smallpox vaccination, vital scientific advancements were made in the 20th century, including limitations on vaccine bacterial count (i.e., non-pathogenic bacterial contamination of vaccine preparation), determination of the required potency to induce smallpox immunity [51] , and the introduction of a seed lot system by the World Health Organization in 1959, which created a vaccine supply of greater consistency [52] . Temperature stabilization through freeze drying [53] and the invention of a bifurcated needle [54] allowed the vaccine to be transported over greater distances and administered more efficiently. LAV enhancements, along with the lack of an animal reservoir for the disease, made smallpox a prime candidate for global eradication. For these reasons, the World Health Organization (WHO) initiated the Smallpox Eradication Program in 1959 [51] . Many vaccine strains were used in this program, but some of the most commonly used LAVs were Dryvax and EM-63, derived from the NYCBH strain, Elstree of the Lister strain, and Tiantan of the Temple of Heaven Strain [51] . Through global effort, and with the aid of effective LAVs, smallpox was eradicated by 1980 [1] . Vaccinia LAVs are potent but reactogenic, and complications are very uncommon but can be serious [55] . They are also unsuitable for immunocompromised individuals who would be most at risk in the case of an epidemic [56] . In light of recent outbreaks of known and novel zoonotic poxviruses [57] [58] [59] [60] and the potential use of smallpox in bioterrorism [51] , second-generation vaccines such as ACAM2000 [61] and CJ-50300 [62] have been derived from original vaccinia strains in tissue culture, and other LAVs have been made from the highly attenuated, modified vaccinia Ankara strain [63, 64] . Further understanding of the interaction of vaccinia LAV with the human immune system will be a major part of developing new LAVs to combat Orthpoxviruses in the future. Following Jenner's success with smallpox vaccines, Louis Pasteur made strides that initiated a novel generation of LAV. This advancement was the attenuation, or weakening, of the pathogens responsible for disease. After finding that cultures of Pasteurella multocida, the bacterium of chicken cholera, were attenuated when left out and exposed to air, Pasteur procured a vaccine from this [26] . He proceeded to produce an anthrax vaccine in animals, whose etiological agent had been discovered by Robert Koch [27] . Pasteur's contribution to human LAV came to fruition through his work on rabies, which he attenuated though drying the spinal cord of infected rabbits [28] . Through a regimen of inoculation with progressively less dried (and more virulent) samples, rabies could be prevented and treated. In 1885, Pasteur's attenuated rabies vaccine was successfully administered to a human patient, establishing itself as the first truly attenuated human LAV [28] . This LAV incorporated wisdom from both Jenner and Pasteur's research in its production and offered protection from one of the deadliest pathogens in existence. Tuberculosis (TB) is a bacterial infection caused by Mycobacterium Tuberculosis. This bacteria has caused more deaths than any other infectious pathogen, although treatment and prevention methods exist [65] . Out of an estimated 1.7 billion individuals who are currently infected with M. tuberculosis, 5-10% will develop active TB and will transmit the bacteria to 10 to 15 individuals each year [66] . Therefore, a vaccine against TB has been, and still remains, essential to contain this infectious disease. Today, the only licensed TB vaccine available is the Bacille Calmette-Guérin (BCG) LAV [67] . M. tuberculosis was first identified by Robert Koch in 1882 [68] , although the bacterium used in the BCG LAV is Mycobacterium bovis, a pathogen responsible for bovine tuberculosis, and one that is equally virulent in humans. The attenuation of M. bovis was achieved by Albert Calmette and Camille Guérin of the Institut Pasteur, who subcultured an isolation of M. bovis in media containing potato, glycerol, and beef bile for 13 years and a total of 231 serial passages [69] . The attenuated BCG vaccine was successfully used on an infant in 1921 [70] . Apart from an M. tuberculosis contamination that resulted in the Lübeck disaster in 1930, which killed 72 infants and infected an additional 135 of 250 infants vaccinated [71] , BCG administration became widespread without ill effect, and multiple strains were thus developed [70] . Apprehension regarding TB following World War II led multiple health organizations to advocate for the BCG vaccine [70] . While its efficacy had been proven by earlier studies, it became evident that it varied in effectiveness in different areas of the world [70] . Consequently, the WHO introduced a seed lot system for LAV uniformity in 1956 and, in the countries reporting to the WHO, more than ten BCG strains are currently in use [67] . In recent years, the immunogenicity of the BCG LAV has been studied to a greater extent and appears to vary based on age at the time of vaccination, location, and previous exposure to mycobacteria or infection [72] [73] [74] [75] . Statistical analyses of efficacy studies show that the BCG vaccine is most effective in providing protection against severe extrapulmonary (non-lung) forms of pediatric TB, but is unreliable in its protection of adults against pulmonary TB [72] . However, despite its variable efficacy, the BCG vaccine has been incredibly beneficial, and prevented an estimated 45 million deaths in HIV-negative individuals and 9 million HIV-positive individuals between 2010 and 2017 alone [65] . TB remains one of the top ten causes of death in the world, and the source of major epidemics in developing countries [65] . Additionally, HIV-positivity is a contraindication for BCG vaccination, although these individuals are exponentially more likely to develop active TB [56, 65] . Another threat to the eradication of TB is the increasing prevalence of multidrug-resistant tuberculosis (MDR-TB) and extensively drug-resistant TB (XDR-TB) [65] . Efforts have continued to reach milestones that were established by the WHO's End TB strategy, but the current trajectory of TB decline will not meet previous deadlines [65] . Although BCG LAV plays an essential role in eradicating TB, new and effective vaccine strategies are now urgently needed. Calmette and Guérin's use of serial passage to develop an LAV opened new doors in vaccine production. The majority of LAVs have since been made using this method, including vaccines against yellow fever [32] , poliomyelitis [33] , measles [34] , mumps [35] , rubella [36] , typhoid [37] , varicella [38] , zoster [39] , and influenza [76] . Rotateq, a pentavalent reassortant rotavirus vaccine, was made with a strategy like that of BCG vaccines, and is comprised of a combination of attenuated human and bovine strains of the virus [30] . Techniques used in serial passage have evolved over time, occurring in a variety of media and host organisms. The effect these LAVs have had on disease prevention has been profound. In 1951, Max Theiler was awarded a Nobel Prize for his creation of the yellow fever LAV [77] , and remains the only recipient of such an award for the development of a vaccine [78] . We will summarize the history of some of the most well-known LAVs of this category, including yellow fever, measles, polio, and influenza vaccines. Yellow fever (YF) is a serious hemorrhagic fever endemic to tropical and subtropical regions of Africa and South America [79] . It is recognized as one of the most devastating infectious diseases of the 17th and 18th century, capable of killing thousands at a time [80, 81] . YF is caused by yellow fever virus (YFV), a positive-sense RNA flavivirus, and is transmitted by Aedes, Haemogogus, and Sabethes mosquito species [82] . The first isolations of YFV were obtained through serial passage of the serum of infected individuals, Francois Mayali and Asibi, in rhesus macaques [83, 84] . The isolation from Mayali was used by the Pasteur Institute to develop the French Neurotropic Vaccine strain (FNV), but was discontinued in 1982 due to a number of encephalitic reactions to the vaccine [10] . The YFV-Asibi strain was isolated and passaged by Max Theiler of the Rockefeller Institute, and was successfully attenuated in 1927 after 235-240 passages in mouse embryonic tissue, minced whole-chick embryo, and minced chick embryo with brain and spinal cord removed [32] . This LAV caused mild disease in macaques and human cohorts and conferred protective immunity against YFV [85] . This strain is known as YFV-17D and is the basis of all YFV vaccines used today [86] . Mass vaccination campaigns in Africa and South America have succeeded in controlling yellow fever in endemic areas over the past 80 years. However, recurrent outbreak episodes have taken place in areas with limited vaccination coverage since the introduction of YFV-17D, with dramatic consequences at times such as in Ethiopia in 1961, where over 100,000 cases and 30,000 fatalities were reported [82] . YFV-17D and derived LAVs are renowned for their safety, potency, and immunogenic properties. YFV-17D vaccines provide effective immunity in 80-100% of vaccinees 10 days after vaccination, and more than 99% of vaccinees within 30 days of vaccination [87, 88] . The incidence of serious adverse events, such as vaccine-associated neurotropic disease (YEL-AND) and vaccine-associated viscerotropic disease (YEL-AVD), remains extremely low [9] . YFV-17D appears to truly be a product of stochastic generation, as attempts to recreate it have been futile [10] . The diminishing supply of YFV-17D LAV in the face of unusual YF outbreaks is currently a cause for concern. Since 2016, cases of YF in nonendemic areas or areas with less prevalence of YF have increased [82] . The reason for the spread of YFV to these areas is unknown, but could be the result of increased human activity in ecosystems where sylvatic cycles of YFV transmission occur, or due to a widening range of favorable habitats for YFV-carrying mosquitoes to travel due to climate change [82] . With no antiviral treatment in existence for YF, and an existing reservoir for the disease in nonhuman primates, it is critical that vaccination in areas experiencing YF outbreaks remains consistent [79] . However, the shortage of YFV-17D vaccines has led to the use of factional doses of vaccine. These doses (ranging between 1:2 to 1:170 of the original dose) can induce neutralizing antibodies that remain detectable from one to up to eight years post vaccination [89] . It is unclear, however, whether the long-term immunity lesser doses provide will be comparable to full LAV doses at a global scale. As YF reemerges in nonendemic areas, effective surveillance protocols and adequate vaccine stockpiles will be key for preventing future outbreaks. Measles is a highly contagious illness produced by the negative-sense RNA measles virus (MV) of the family Paramyxoviridae, and is transmitted through respiration or aerosol [90] . Prior to the licensure of a vaccine in 1963, measles epidemics occurred every two to three years and caused 2.6 million deaths annually [91] . MV was first isolated in tissue culture by Enders and Peebles in 1954 using samples from an infected child named David Edmonston [34] . The virus was propagated on primary human kidney and amnion cells and attenuated through passages in chick embryos and chick embryo fibroblasts. This process gave rise to Edmonston A and B strains [34, 90] . While Edmonston A was too reactogenic for vaccination use, Edmonston B was licensed as an LAV in 1963, with further attenuation through five additional passages on chick embryo fibroblasts at 36-37 • C [9] . The reactogenicity of measles LAV (MV-LAV) led to the creation of further attenuated strains, such as AIK-C from the original Edmonston strain [92] , Schwarz from Edmonston A [93] , and Edmonston Zagreb [94] and Moraten [95] from the Edmonston B strain. Approaches to developing these strains involved further passage in chick cell culture [93] , passage in chick cell culture at elevated [95] or lowered temperatures [92] , and passage in human diploid fibroblasts [94] . Independent measles strains such as CAM 70 [96] , Leningrad 4 [97] , Changchun 47 and Shanghai 191 [98] were also produced. All MV-LAV cause fevers and a rash to some extent, but most side effects are mild and short-lived [9] . Safe, effective, and affordable MV-LAVs have had a profound effect on the disease: between 2000 and 2017, the incidence of measles decreased by 83%, and deaths due to measles fell by 80% [99] . Although a potent, cost effective, and reliable MV-LAV is available, gaps in vaccination coverage have led to its resurgence in many areas of the world. In 2017, 110,000 deaths from measles were recorded [99] , and the incidence of measles has continued to rise. The high communicability of MV necessitates a high level of global vaccination coverage, and the recent pattern of outbreaks due to vaccine hesitancy have the potential to undo decades of work invested into controlling this infectious disease. The Global Vaccine Action Plan aimed to eradicate measles (and rubella) in five WHO regions by 2020 [100] , but MV-LAVs must be extensively utilized to accomplish any eradication in a timely manner. Poliovirus LAVs have brought the once-feared paralytic disease, poliomyelitis, to the brink of eradication. Poliomyelitis became prevalent in the turn of the 20th century, instigating epidemiological studies that helped to characterize the virus and its pathogenesis [101] . The pathogenic theory of poliovirus was disrupted by a purely neurological view (due to the motor neuron infection that results in the disease of poliomyelitis) [9] , but this was disproved by Sabin and other scientists in the 1950s [102] [103] [104] . Complete immunity to poliovirus requires vaccination with LAV derived from its three serotypes [101] . The LAVs used against poliovirus are derived from the Sabin type 1, 2, and 3 vaccine strains developed in 1954, and originate from parental strains Mahoney, P712, and Leon, respectively [33] . The attenuation of these strains was achieved through tissue culture passage in a variety of cell types as well as intracerebral passage in the case of P712 [9, 105] . Following serial passage of each serotype, Sabin selected the most successful derivatives of Mahoney, P712, and Leon through neurovirulence testing in monkeys [33] . The live oral polio vaccine (OPV) was licensed in 1963 [9] . The introduction of OPVs decreased the incidence of polio by 99%, improving upon the 95% reduction achieved with inactivated polio vaccines (IPV) developed by Salk in 1955 and that were immediately used in mass vaccination campaigns [9, 106, 107] . The OPV has played a critical role in the Global Polio Eradication Initiative that began in 1988, decreasing the global burden of wild-type polio from more than 350,000 annual cases to just 33 cases in 2018 [108, 109] . Wild-type 2 poliovirus was last seen in 1999 and declared eradicated in 2015 [110] , and wild-type 3 poliovirus was declared eradicated in October of 2019 [111] . Despite these successes, any cases of polio pose a global health risk. On rare occasions, OPV can revert to a neurovirulent phenotype and induce vaccine-associated paralytic poliomyelitis, and virus excretion following vaccination can lead to circulating vaccine-derived polioviruses (cVDPVs) [9] . While cVDPVs can be controlled by consistent vaccination, long-term virus excretion by hypogammaglobulinemic vaccinees, known as immunodeficient vaccine-derived polioviruses (iVDPVs) is more difficult to manage [9] . The WHO and partners have introduced the Polio Endgame Strategy of 2019 to 2023, with the aim of wild-type and vaccine-derived polio eradication with the use of bivalent OPVs and IPVs [109] . Influenza (or, more commonly, flu) is a highly contagious acute respiratory infection caused by single-stranded RNA segmented influenza viruses of the Orthomyxovirus family, which are classified into four genera known as influenza A, B, C, and D viruses [76] , as well as into subtypes based on hemagglutinin (HA) and neuraminidase (NA) surface proteins [112] . Influenza affects 5-10% of adults and 20-30% of children annually across the globe [76] , and predisposes individuals to secondary infections such as pneumonia [113] . Vaccination has been shown to reduce influenza mortality and morbidity, but is complicated by antigenic drift (the introduction of mutations in gene segments) and antigenic shift (the emergence of a new influenza virus subtype by the reassortment of current subtypes) [112, 114] . Both inactivated vaccines and LAVs are currently used against the influenza virus. Trivalent inactivated vaccines (TIVs) are either whole-virus, split virus (where the virus is disrupted by a detergent), or subunit vaccines (which contain strain-specific HA and NA antigens). In contrast, live-attenuated influenza vaccines (LAIVs) are cold-adapted reassortant vaccines made with genes encoding for HA and NA proteins of the most recent seasonal or wild-type strain, and six additional gene segments from attenuated donor strains [76, 112] . The influenza virus was isolated from the nasal discharge of infected patients by Wilson Smith, Sir Christopher Andrews, and Sir Patrick Laidlaw in 1933 [115] . This isolation procedure was used to develop inactivated influenza vaccines, which are grown on the chorio-allantoid membrane of embryonated eggs [116] or in cell culture [76] , purified through centrifugation, and chemically inactivated [117] . Split virus and subunit inactivated vaccines followed the development of whole-virus inactivated influenza vaccines in the 1960s and 1970s [118] . These vaccines are used more than whole-virus influenza vaccines due to their reduced reactogenicity [76] . LAIVs have progressed from monovalent to trivalent and quadrivalent as new genera of influenza virus have been discovered [76, 118] . The most successful and stable LAIVs are reassortant cold-adapted vaccines [119] that are passaged at progressively lower temperatures until 25 • C has been reached on primary chicken kidney cells or embryonated eggs [120, 121] . The reduced growth of LAIV at higher temperatures restricts virus replication primarily to the upper respiratory tract, which mimics natural infection and prevents viral spread in the lower respiratory tract and throughout the body [76, 122] . LAIVs have been used in Russia for decades with very few adverse effects other than transient febrile reactions [123] , and the first LAIV to be used in the United States was a trivalent cold-adapted vaccine, which was licensed in 2003 [76] . Although TIVs are most frequently used for seasonal influenza vaccination [76] , immunity from subunit TIVs relies on HA and NA proteins of a specific influenza strain. In contrast, the inclusion of six viral proteins of cold-adapted donor influenza strains enables LAIV to exhibit heterosubtypic protection, or to improve the viral clearance of different sub-strains of influenza than the one vaccinated for [112] . However, contraindications due to age, asthma, immunosuppression, and pregnancy often preclude the use of LAIV in comparison to TIV [76] . An improved understanding of LAIV attenuation would assist with the development of new influenza vaccines that provide longer-lasting immunity and protect against multiple strains of influenza. The high potency of LAVs is well established. However, their attenuation and immunogenicity originated through a purely empiric process rather than from rational engineering and development. Whether a LAV was isolated from a related pathogen and propagated in various ways, or attenuated through serial passage, the environment in which a pathogen is grown influences its evolutionary path and therefore alters aspects of its infectious cycle. For this reason, the origin of vaccinia, the basis of the smallpox LAV, is lost to history. Prior to the eradication of smallpox, attempts to recreate a vaccinia-like LAV through the passage of the variola virus major or minor in embryonated eggs, tissue culture, and mice, were unsuccessful [124] . Additionally, even when methods of LAV passage are described in detail, as in the case of YFV-17D [32] , recreation of the vaccine is not always possible. The future of LAV generation, therefore, cannot be empiric; it must stem from a well-developed understanding of the differential host-pathogen interactions LAVs and virulent pathogens' display. Direct genetic comparisons between LAV strains with their virulent counterparts have been used to hypothesize correlates of attenuation and immunogenicity. For example, cases of vaccine-derived paralytic poliomyelitis (VAPP) and increased neurovirulence are associated with mutations in the 5' noncoding region of all three OPV strains after passage in the human gastrointestinal tract [125, 126] . Interestingly, besides vaccinia virus-derived smallpox LAV and deliberately reassortant LAIV, LAVs tend to show high genetic similarity to the pathogen they were derived from. YFV-17D and YFV-Asibi only diverge by 68 nucleotides (about 0.6% of the genome) and 32 amino acid changes that are scattered throughout the genome [127] . Additionally, Edmonston-derived MV-LAV have been shown to differ from the Edmonston strain at most by 0.3% [128] , and the RD1, a 9.5 kb deletion seen in all BCG LAVs [129] , accounts for only very little of the 4,345,492 bp genome of M. bovis [130] . Genetic variation between LAV strains can, however, complicate extrapolation from these data. In addition to the 9.5 kb RD1 deletion, further attenuated BCG LAV strains can have additional deletions, namely RD2 (10.7 kb) and/or RD3 (9.3 kb) [129] . Description of genetic differences alone can hardly explain the effects they have on host-pathogen interaction and immune regulations. Therefore, a large body of research on molecular mechanisms governing LAV attenuation has been conducted in vitro. LAVs have, for instance, been found to differ in cell entry mechanisms in comparison to their virulent counterpart. Enhanced CD46 receptor utilization by MV-LAV has been described and is influenced by mutations in the hemagglutinin (H) protein [131, 132] . YFV-17D and YFV-Asibi also exhibit distinctive usages of cell entry pathways and cell surface molecules, which were associated with differential viral spread and cell-intrinsic-immune-response induction [133, 134] . Differences in viral replication and cytokine induction have also been observed in cell culture. YFV-17D replicates more extensively than YFV-Asibi in hepatoma cell lines [135] [136] [137] . However, YFV-Asibi causes a stronger and more sustained pro-inflammatory response than YFV-17D [138] [139] [140] in primary human vascular endothelial cells, Kupffer cells, MDMs and monocyte-derived dendritic cells (MoDCs). The Sabin type 3 OPV displays genetic differences from poliovirus type 3 that decrease the binding of polypyrimidine tract-binding protein (PTB) at its viral internal ribosome entry site (IRES). Lower PTB binding to Sabin type 3 OPV IRES impaired protein translation in neurons, and was suggested to mediate viral attenuation in the central nervous system [141] . Finally, reduced YFV-17D replication in dendritic cells (DCs) in vitro has been suggested to promote antigen presentation in the lymph nodes (LN) by preventing DC apoptosis [142, 143] . Unlike YFV-Asibi, the replication of YFV-17D in MoDCs also stimulates IFNγ production in CD4+T cells [140] . Another interesting concept for explaining LAV attenuation relies upon the genetic stability of these pathogens. For instance, YFV-17D is poorly prone to diversify into multiple quasi-species in comparison to YFV-Asibi, and YFV-Asibi diversification is not associated with the emergence of attenuation mutations [144] . A more recent study reported that YFV-17D has a stronger resistance to mutations, and suggested that YFV attenuation could be related to a higher fidelity of the YFV-17D replication complex [145] . Although in vitro models have been useful in our understanding of LAVs, modelling host-LAV interactions in a spatiotemporal manner and in a relevant tissue context remains fundamental to accurately investigating the molecular mechanisms governing LAV attenuation and immunogenicity. To this end, animal models have been used to conduct research in vivo. For instance, RD1 deletion of BCG LAVs was reported to prevent the cytolysis produced by early secretory antigenic target (ESAT-6) and decreases tissue invasiveness in mouse models [146] . Mice defective for type I interferon signaling have been employed to explore the mechanisms of YFV-17D attenuation and immunogenicity. Humoral and CD4+ T cell responses, but not the CD8+ T cell responses, were found to be critical for YFV-induced protection [147] . IFN-γ responses were also reported to restrict the dissemination of YFV-17D, but not YFV-Asibi, in mice [148, 149] . Beyond mice, NHPs have also served to investigate the impact of cell-culture adapted mutations on viral virulence and inflammatory responses. The passage of a pathogenic wild-type MV (Davis87-wt) in Vero cells and chicken embryo fibroblasts led to the emergence of several adaptive mutations that resulted in attenuated clinical symptoms in rhesus macaques [150] . Interestingly, a back-to-back comparison of early transcriptomic responses during YFV-17D and -Asibi infection was conducted in rhesus macaques. The number of differentially expressed genes in peripheral mononuclear cells was higher upon YFV-Asibi than YFV-17D infection, and expression changes were mostly connected with the dysregulation of immune responses and apoptosis. In contrast, gene expression changes during YFV-17D infection were mainly linked to innate immunity [151] . Although agreement has been found between in vivo and in vitro studies, discrepancies have also arisen. Sabin type 1 OPV is attenuated in vitro upon exposure to high temperatures (>37 • C), unlike its parental strain, Mahoney. However, genetic determinants of temperature sensitivity do not contribute to viral attenuation in transgenic mouse models [152, 153] . Additionally, while most NYCBH-derived LAV strains examined by Lee et al. replicated to wild-type levels in vitro, their replication in mouse epithelium differed greatly from wild-type vaccinia virus, and their immunogenic effect was dose-dependent [154] . Other categories of discrepancies have also been reported. For instance, the route of administration seems to affect virus protein roles during vaccinia virus infection [155] , or BCG-induced immune responses in NHPs [156] . Moreover, multiple studies have found greater IFN production in MV-LAV strains [157] [158] [159] [160] [161] , but the potential presence of 5' copy-back defective interfering (DI) RNAs in laboratory virus stocks, which increase interferon (IFN) and interferon-stimulated gene (ISG) levels (among multiple other effects), may have confounded previous results [162] . Disparities between in vivo models have also been observed. Mice defective for type I interferon signaling have been suggested to be a relevant system to investigate YFV-17D-induced protective immunity [147] . However, an important number of genes upstream and downstream of the type I interferon signaling pathways are upregulated upon YFV-17D vaccination in humans [163] . Furthermore, although the authors reported that mouse CD8+ T cells do not contribute to protection in mice defective for type I interferon signaling, multiple studies have described the critical mobilization of human CD8+ T cells toward the robust effector and memory phenotypes during YFV-17D vaccination in human vaccinees [164] [165] [166] [167] . Finally, discrepancies have also been observed in the cytokine response of human and rhesus macaques MoDCs upon YFV-17D infection [140] . LAV attenuation is a multi-faceted and complex topic. The history of LAV passage is varied and mosaic, and the efficacious products we are left with are near impossible to trace to their roots. Our understanding of LAV attenuation appears to be mainly restricted to either genetic changes upstream of the vaccination process, or to immune correlates downstream of vaccination (such as antibody production). However, the immunological mechanisms and host-pathogen interactions that govern attenuation during vaccination remain uncharted. To explore these mechanisms, the choice of the right experimental system is of critical importance, as is exemplified by the instances of disagreement described above. An accurate understanding of LAV-induced immunity is therefore fully reliant on a suitable in vivo model, able to recapitulate human immunity to LAVs. A more comprehensive (although not exhaustive) summary of LAV attenuation mechanisms is provided in Table 2 , and is associated with the following additional references: . [191, 192] Mouse [194] ; NHP [195] ; Pig [196] Historical aspects of live-attenuated vaccine (LAV) creation, such as method of generation, as well as the date of the pathogen's isolation or attenuation are listed. Mechanisms of LAV attenuation are categorized into (i) genetic similarity to their parental strain or a virulent strain (with the exception of smallpox and influenza LAVs), (ii) studies on mechanisms governing LAV attenuation performed in vitro, and (iii) animal models used for in vivo investigation. NHP, non-human primates; n/a, non-applicable. References linking to the associated literature are indicated in brackets. Our limited understanding of the mechanisms of LAV attenuation and immunogenicity mostly originates from the lack of in vivo systems suitable for investigating the anti-LAV immunity. Although in vitro studies have shed light on the characteristics of some LAVs (see Section 2.3), LAV attenuation and immunogenicity is defined by specific interactions with different tissues and complex cell populations over time and space, and these interplays remain impossible to model in cell culture dishes. Mouse models have been truly transformative for our understanding of human immunology and will very likely continue to be [198] . Although they have served as a model organism to investigate anti-LAV immunity and evaluate LAV candidates in vivo [134, [146] [147] [148] [149] [153] [154] [155] 183, 184, 193, 194, [199] [200] [201] , important limitations and concerns have stalled their use for this purpose. Despite sharing many physiologic and metabolic processes, mouse and man diverged evolutionarily more than 96 million years ago [202] . This evolutionary divergence has differentially shaped the architecture of the immune system of both species, therefore leading to important differences in how each organism mobilizes its immune system to fight a given disease [203] . There are numerous immunological differences between mice and humans, such as differences in immune cell composition in the blood, differences in the spatiotemporal expression of many immune proteins and receptors (pattern recognition receptors, cluster of differentiation, immunoglobulin, Fc receptors, Major Histocompatibility Complex (MHC) etc.), differences in hematopoiesis, and differences in cytokine function [203] . As we aim to investigate anti-LAV immunity at the highest possible resolution, translating immunological mechanisms from mice to humans becomes perilous. Moreover, many viral and bacterial pathogens display a very narrow host tropism, often being restricted to humans and specific primate species. For example, the natural resistance of mice to viral pathogens such as MV, YFV, dengue virus, Ebola virus or Zika virus adds an additional layer of limitations to conventional mouse models. NHPs have been useful resources for studying anti-LAV immunity as well as evaluating novel LAV candidates. Especially, macaques have been intensively used to evaluate the immunogenicity of LAV strategies targeting human immunodeficiency virus 1 (HIV-1). However, HIV-1-like several other human-tropic viruses-is a virus that replicates poorly in NHP species. Therefore, many LAV strategies have been evaluated in NHPs using live-attenuated simian immunodeficiency virus (SIV) [204] [205] [206] [207] [208] [209] , the simian version of HIV-1 that only replicates in NHP species. Additional strategies employing HIV-1 related viruses [210, 211] or engineered viruses harboring HIV-1 or SIV antigens [212, 213] have also been tested in NHPs. NHPs have also been employed to investigate and/or evaluate the immunogenicity of many other LAVs, such as viral vectors harboring Ebola virus glycoproteins [214] [215] [216] [217] [218] , LAIV [195] , OPV [189, 190] , MV-LAV [150, 185, 197, [219] [220] [221] , BCG [156] , live-attenuated Venezuelan equine encephalitis virus [222] , YFV-17D [151] or live-attenuated Zika virus [43] . NHP models for LAV research, however, present significant limitations. First, a pathogen with a strictly restricted human tropism imposes the use of surrogate pathogens and/or a host environment that is not naturally permissive to the human pathogen that is targeted. Second, NHP studies are costly, require complicated logistics, and adequate reagents that are of limited availabilities. Finally, whether NHP cell-intrinsic innate immune responses differ from those of humans during antigen stimulation is unclear and may depend on the experimental context [140, 223, 224] . Therefore, although it is certain that NHPs represent an invaluable system for LAV investigation and development, they need to be complemented with more cost-effective and accessible models that allow for the monitoring of human-specific immune responses and human-pathogen-specific interactions. Major insights into anti-LAV immunity have been obtained from cohorts of human vaccinees. For instance, studies have identified specific cellular, transcriptomic and epigenetic signatures defining LAV immunogenicity in the blood of human vaccinees [164, 166, [225] [226] [227] [228] , providing unique insights into how the immune response is mobilized upon LAV vaccination in humans. However, these studies fall short of probing fundamental mechanisms that govern LAV attenuation and immunogenicity for several reasons. First, human cohort studies are often restricted to the analysis of the peripheral immune response. The upstream immunological events that occur at the site of vaccination and in draining lymph nodes, and with them the critical pathogens-host interactions that prime the immune system toward a specific direction, remain inaccessible in human vaccinees. Second, capturing key immunological mechanisms and regulations that define immunogenicity require the performance of a back-to-back comparison of the immune responses induced by a given LAV and the virulent strain it is derived from or related to. However, for obvious ethical reasons, this is not possible in a cohort of human patients. Finally, more and more studies have provided evidence that age, immunological history and microbiota status can influence immune responses to pathogens and vaccines. Therefore, identifying the immunological regulations that define a specific immune response to an LAV, independently of inter-individual variations that may impact such responses, remains a very complex task. Altogether, the limitations of the mouse, NHP and human models highlight the need for alternative in vivo systems that would successfully combine the advantages of each models while lessening the limitations. Mice engrafted with human tissues, and especially with components of the human immune system, nicely fit such a definition. Per definition, humanized mice can be referred to as mice engrafted with human tissues and/or expressing human genes. Here, we will focus on mice engrafted with components of a human immune system, or HIS mice, and describe the past and current models that are of relevance to studying LAV-induced immunity. The story of HIS mice began with the development of immunodeficient mouse strains. In 1983, a mouse harboring the severe combined immunodeficiency (SCID) mutation, a mutation in the Prkdc gene, was reported to lack functional B and T cells through defective V(D)J recombination [229] . In 1992, mice defective for expression of the two recombination activating genes, RAG1 and RAG2, were also reported to display no functional T and B cells through a similar mechanism [230, 231] . In the late 1980s, the transplantation of adult SCID mice with fetal human hematopoietic stem cells (HSC) and tissues [232] or peripheral blood leukocytes, led to the differentiation of human T and B cells in vivo [233] . Human HSC engraftment of bg/mu/xid mice was also reported at the same period. These animals were athymic and had lower levels of natural killer (NK) cells and lymphokine-activated killer cells (LAK). Intravenous inoculation of human bone-marrow-derived cells into adult irradiated bg/mu/xid mice led to the proliferation and differentiation of HSCs into macrophage progenitors in vivo [234] . Engraftment in these animals led to a higher number of progenitors than in SCID mice, likely due to the absence of NK and LAK activity in bg/mu/xid mice. A few years later, adult irradiated SCID mice (CB17-SCID) transplanted with human bone marrow cells and treated with multiple human cytokines (namely, erythropoietin, human mast cell growth factor, or with a fusion protein of human interleukin-3 and human granulocyte-macrophage colony stimulating factor) could be repopulated with differentiated human cells deriving from several myeloid and lymphoid lineages in the bone marrow [235] . However, engraftment levels still remained low. In 1995-1996, SCID mice built within the non-obese diabetic (NOD)-background were identified to support higher level of HSC engraftment in comparison to previously tested genetic backgrounds [236] [237] [238] . Around ten years later, the underlying mechanism of this superior engraftment was found to be related to a polymorphism in the Sirpa allele [239] . NOD SIRPα, a cell surface receptor expressed at the surface of macrophages and encoded by the Sirpa allele, displays significant similarities with the human one in comparison to those expressed by mice of other genetic backgrounds. This similarity allows NOD mouse macrophages to bind to human CD47 and, therefore, to recognize human hematopoietic cells as 'self'. Another important step in the development of the permissive mouse genetic background for human HSC engraftment went through the development of mouse defective or carrying a mutation into the common gamma chain of the Interleukin (IL)-2 receptor (IL2Rγ) [240] . IL2Rγ is a receptor for multiple cytokines such as IL-2, 4, 7, 9, 15 and 21. Disruption of IL-15-mediated signaling induces severe deficiencies in NK cell development and maturation, leading to a more severe immunodeficiency and more permissive environment for human HSC engraftment in adult RAG2 -/supplemented or unsupplemented by the exogenous administration of human cytokines [241, 242] , and in adult NOD-SCID mice [243] . Another important step that followed was the demonstration that the intrahepatic injection of human CD34+ cells into newborn Rag2 −/− IL2Rγ null and NOD-SCID IL2Rγ null mice led to the induction of low, albeit significant, functional adaptive responses in these mice [244, 245] , which opened the door to novel investigations of human immunity in vivo. An alternative model to the NOD-SCID IL2Rγ null mice, namely NOD-SCID JAK 3 null , has also been reported. In this model, JAK 3 , a gene encoding for a tyrosine kinase critical for IL2Rγ-mediated signaling is knocked-out instead of IL2Rγ, resulting in a complete lack of B, T and NK cells [246] . As of now, the vast majority of mouse strains used for human HSC engraftment still very much rely upon the mutations and polymorphism described above. The most commonly used strains are the NOD-SCID [236] [237] [238] , the NOD-SCID-IL2Rγ null (NSG or NOG, harboring a with NOD/LtSz or NOD/Shi background respectively) [243, 244, 247] , the BALB-c-RAG2 −/− IL2Rγ null (BRG) [241, 242, 245] or the NOD-RAG1 −/− IL2Rγ null (NRG) [248, 249] . Figure 1 summarizes the development timeline of these models. Despite significant improvements in HSC engraftment, developing HIS mice on the NSG, NOG, BRG or NRG background able to mount potent innate and antigen-specific human immune responses remained a considerable challenge by the mid-2000s. Indeed, these strains bear important limitations that restrict the development and functionality of a human immune system such as (i) lack of proper human cytokine environment; (ii) absence of large niche space; (iii) limited myelopoiesis; iv) lack of specialized human microenvironments and (v) lack of human MHC. Therefore, considerable efforts have been undertaken in the last decade to generate novel xenorecipient strains overcoming these multiple limitations. The development timeline of the models described below is shown in Figure 1 . Cytokine Environment and Niche Space One approach to enhance HIS mice has notably consisted of generating xenorecipient strains expressing human cytokines. A large numbers of mouse cytokines do not cross-react with human receptors, and the absence of cytokine-mediated signaling has been hypothesized to affect many arms of the immune system in HIS mice, such as myeloid and NK cell development, T cell education and B cell maturation. Consistently, many groups have reported the development of BRG (with transgenic expression of human Sirpa or not), NSG and NOG mice expressing one or several human cytokines through transgenic or knock-in approaches, such as human thrombopoietin (TPO), IL-3, IL-15, granulocyte-macrophage colony-stimulating factor (GM-CSF), stem cell factor (SCF) and/or macrophage colony stimulating factor (M-CSF) [14, [250] [251] [252] [253] [254] [255] [256] [257] [258] [259] [260] [261] . These models showed enhanced human hematopoiesis as well as the superior development and differentiation of multiple myeloid cell subsets. One of these models, named MISTRG (BRG-CSF1 h/h IL-3/CSF-2 h/h Sirpa tg TPO h/h RAG2 −/− IL2Rγ −/− ), combines the knock-in of human thrombopoietin, IL-3, GM-CSF and M-CSF, and displays enhanced innate immunity against viral and bacterial infection in comparison to NSG mice [14] . An alternative approach to promote enhanced human hematopoiesis and myeloid cell development has involved increasing the stem cell niche space for human hematopoietic lineages to develop. In NSG, NOG, NRG and BRG mice, murine myelopoiesis is not restricted, therefore closing-up the niche space required for proper human myelopoiesis. Additionally, the presence of a (semi)-functional mouse myeloid compartment can also interfere with the initiation of human-like innate responses in HIS mice hence, introducing bias into the human immune responses that are under analysis. An initial strategy to enhance niche space in humanized mice has consisted of irradiating newborn or adult immunodeficient mice prior to engraftment. However, this approach can have negative long-term effects on animal health. More recently, NSG and BRG mice harboring a mutation in the gene coding for c-KIT/CD117 (respectively, NSG-Kit w41 /NBSGW and BRG-Kit Wv/Wv /BRGWv) were generated [15, 262] . c-KIT is a receptor expressed at a high level at the cell surface of HSC and progenitor cells and c-KIT-mediated signaling has been shown to regulate hematopoiesis [263] . Interestingly, adult non-irradiated NSBGW and BRGWv mice demonstrated robust and sustained engraftment of human HSC at a similar level to irradiated NSG mice [15, 262] following the injection of cord blood-derived human CD34+ cells, underscoring that human the stem cell niche space can be significantly opened without the need for irradiation. Fms like tyrosine kinase 3 (FLT3), or fetal liver kinase-2 (Flk2), are cell surface receptors broadly expressed on early hematopoietic precursors in the bone marrow. Myelopoiesis is impaired in Flk2-deficient mice (Flk2 −/− ) [264, 265] and injection of the human version of the ligand of FLT3 (hFLT3LG) has been shown to promote dendritic cell development in NOD/SCID mice [266] . However, hFLT3LG cross-react to mouse FLT3, allowing for dual expansion of the mouse and human myeloid compartment in conventional immunodeficient strains. Recently, Flk2-deficient NRG, BRG and BRG-hSirpa mice (NRGF, BRGF and BRGSF, respectively) expressing-or injected with-hFLT3LG were reported to promote the selective expansion of human dendritic cells and natural killer cells following human HSC engraftment [17, 267, 268] . The strong myeloid development in these HIS mice was likely the result of an effective synergy between an increased myeloid niche space caused by the Flk2 deficiency and a robust hFLT3LG-mediated signaling in human cells. Finally, it is worth noting that expressing human cytokines through a knock-in approach into HIS mice can also represent a great strategy to opening niche spaces. Indeed, such an approach can concurrently promote the development and maturation of human hematopoietic lineages and deplete the mouse hematopoietic compartment of the essential cytokines required for its proper development and function. B-and T-cells are primarily educated in the bone marrow and thymus, respectively (the primary lymphoid tissues), where they undergo several rounds of positive and/or negative selection [269, 270] . This process, which is fundamental for proper lymphocyte maturation, antigen reactivity and specificity, is regulated by extensive cross-talk between immature lymphocytes and a large variety of cell types, molecules and members of the MHC residing in primary lymphoid tissues. A initial approach has been to generate HIS mice expressing transgenic MHC molecules to educate human T cells in the mouse thymus and to track human antigen-specific T cells upon microbial challenge [271] [272] [273] [274] . However, a more elaborate strategy has involved the construction of HIS mice engrafted with human primary lymphoid tissue in order to more physiologically model human hematopoiesis and lymphocyte education. In 1990, an introductory study reported that the co-engraftment of small fragments of the fetal liver and thymus into adult SCID mice (also referred as the SCID-hu model) led to the long-term maintenance of several human hematopoietic progenitors and to sustained human T lymphopoiesis [275] . About 16 years later, HIS mice co-engrafted with human HSCs and small pieces of fetal liver and thymus (namely bone marrow-liver-thymus mice or BLT mice) were generated using the NOD/SCID background [276, 277] , and later in the NSG background [278] . In these studies, BLT-HIS mice were shown to display enhanced lymphopoiesis and T-cell selection on human MHC molecules. BLT-HIS mice have notably constituted a model of choice to investigate the infectious cycle of HIV-1 in vivo, as mucosal infection, persistent viremia and cellular responses-albeit limited-could be recapitulated in this model [279] [280] [281] [282] [283] [284] [285] [286] [287] . BLT-HIS mice have also been a useful system to investigate the replication dynamics of the Dengue virus (DENV) in vivo, as well as anti-DENV adaptive immunity [288] [289] [290] [291] [292] . However, a major limitation of the BLT model is its absolute reliance on fetal tissues. Therefore, alternative approaches involving the engraftment of immunodeficient mouse strains with embryonic stem cell-derived, thymic epithelial progenitors [293] , or with neonatal thymus [294] , have been recently proposed. The bone marrow is critical for the survival and maintenance of HSC, as well as B-cell education and maturation. Several groups have reported the development of HIS mouse models (using mainly NSG and NSG-derived strains) engrafted with bone marrow-derived mesenchymal stromal cells (predifferentiated or not) leading to the formation of humanized ossicles in vivo [295] [296] [297] [298] [299] [300] [301] [302] [303] [304] . In these studies, the subsequent injection of HSCs into the ossicles or intravenously resulted into the development of a human microenvironment able to recapitulate human bone marrow morphology and function in vivo, as well as a more physiologically relevant human hematopoiesis. Another major caveat in immunodeficient mouse strains is their lack of functional draining lymph nodes (LN). IL2Rγ null immunodeficient mice are defective for IL-7-mediated signaling, which results into the absence of lymphoid tissue inducer cells (LTi) during development [305, 306] . Interestingly, the thymic stromal lymphopoietin (TSLP) shares a structural and functional homology with IL-7 but signals through a receptor that is IL2Rγ independent. It has been recently shown that TSLP overexpression compensates for the lack of IL-7 signaling and promotes robust LN development in BRG-hSirpa mice (BRGST) [307] . Even though such secondary lymphoid structures cannot be completely considered a human microenvironment per se, mouse LNs were effectively repopulated with human hematopoietic cells in BRG mice [307] . They provided a suitable niche for human T-and B-cell priming after immunization, leading to enhanced human antigen-specific responses. An alternative approach for restoring mouse LN has also been reported [308] . NOG mice harboring a transgene coding for murine IL2Rγ under the control of the endogenous promoter of RORγt were generated (NOG-pRORγt-γc), allowing the selective re-expression of IL2Rγ in a specific LTi lineage [308] . As with BRGST mice, this model was able to mount enhanced antigen-specific IgG responses. Beyond the genetic background and modifications of a given HIS mouse, multiple protocols of engraftment have been described and can significantly impact human immune reconstitution. Engraftment protocols are composed of several variables which mainly include: the source of HSC, engraftment of fetal tissues (or not), mouse age, transplantation route, sex, and conditioning [11] . Although some engraftment protocols are widely used across several strains of HIS mice and can interrogate many biological processes, other protocols (which, for instance, include the engraftment of additional human tissues or matrices) can be more specific to certain strains and biological questions. Therefore, in addition to the HIS mouse strain that is used, it is important to understand the characteristics of the engraftment protocol that is employed to make sure that it will be appropriate for a given investigation. Human HSCs can be isolated from the fetal liver or from an adult donor, the latter including various sources such as cord blood, adult-mobilized peripheral blood or adult bone marrow [11] . HSCs can be either injected into newborn mice (1-4 days old, through intravenous, intrafemoral, intrahepatic, or intracardiac routes) [14, 17, 245, 307] or into adult mice (4-12 weeks of age, through intravenous, intrafemoral, intrasplenic, or intraperitoneal routes) [15, 276, 277, 299] . Human peripheral blood lymphocytes can also be directly injected into adult HIS mice through the intrasplenic, intravenous or intraperitoneal route [11, [309] [310] [311] . Interestingly, sex has also been reported as an engraftment variable in some specific experimental settings [312] . The number of injected cells can vary from 10 3 to 10 6 cells per mice, depending on the model that is used [14] [15] [16] [17] [232] [233] [234] [235] [236] [237] [238] 267, 268, [307] [308] [309] [310] [311] [312] . Given the limited availability of HSCs, it is very common that a single (or very few) donors are used to generate several cohorts of HIS mice. Although this likely allows experiments to overcome interindividual variation and generate more robust datasets, the lack of donor diversity can also represent a limitation by overrating the significance of some of the biological aspects investigated in these models. To open-up niche space and decrease graft rejection, newborn or adult mice can be preconditioned prior engraftment through sublethal irradiation [14, 17, 245, 307] or antibody-pretreatment [313] [314] [315] . However, HIS mouse models, capable of reaching high levels of human immune reconstitution without the need of preconditioning, have been recently described [15, 262] . Human immune reconstitution takes, on average, between 8 and 12 weeks following engraftment, and the lifespan of HIS mice can range widely, from six months to up to 20 months [316] , depending on the protocol and genetic background of the model that is employed. Indeed, several HIS mouse models can be prone to graft-versus-host disease (due to the priming of allogeneic T cell responses by murine myeloid cells) or anemia (due, for instance, to human macrophages' activity against murine red blood cells). As mentioned in Section 'Human Microenvironment and Lymphocyte Education', engraftment protocols can also include the engraftment of additional human tissues or matrices, which includes: fetal or neonatal thymus, fetal liver or bone-marrow-derived mesenchymal stromal cells (among other cells/tissues). Thymus or liver tissues are commonly engrafted under the kidney capsule, prior to the injection of autologous HSC (through the intravenous route) [279] [280] [281] [282] [283] [284] [285] [286] [287] or not [275] . For the development of humanized ossicles, bone-marrow-derived mesenchymal stromal cells can be engrafted subcutaneously by either direct injection [299] or surgical implantation [298] . Autologous HSC can be then subsequently injected through an intravenous [296] or intraossicle route [299] . HIS mice have the potential to truly transform our understanding of LAVs. They can provide unique opportunities to investigate the molecular basis governing the high immunogenicity of currently established LAVs, and therefore shed new light onto our understanding of human immunity and vaccine design. Additionally, they can also present a cost-effective way to evaluate the replication fitness, immunogenicity, attenuation and safety of LAV candidates in a human immunological context. Here, we will review how conventional and emerging models of HIS mice have been harnessed to investigate LAV replication fitness, safety and immunogenicity. A summary of the contributions of HIS mice to LAV research can also be found in Figure 2 . HIS mice have been intensively used to investigate the HIV-1 infectious cycle in vivo and immune responses [317] . However, very limited studies have employed these systems to evaluate the efficacy and safety of attenuation strategies for HIV-1. One study reported a HIV-1 strain (HIV-rTA) for which the TAT-Tar mechanism required for virus replication and transcription was inactivated and the viral protein Nef was replaced by a doxycycline-dependent transcription system [318] . Authors showed that HIV-rTA could lead to production infection in doxycycline-fed BRG-HIS mice without CD4+ T cell depletion. Importantly, the virus was not found to escape doxycycline control over time and revert toward a wild type TAT-tar mechanism [318] . A few years later, a second version of the same virus expressing a ubiquitin-processed rTA-Ub-Nef cassette (HIV-rTA-Ub-Nef) was found to replicate more effectively in NSG-HIS mice than HIV-rTA without impacting the CD4+ T cell count [319] . Altogether, these studies demonstrate that HIS mice represent a useful system to evaluate the replication kinetics and safety of live-attenuated lymphotropic viruses that have a very narrow tropism. Humanized NOD-SCID JAK 3 null (NOJ) mice have been shown to be susceptible to MV-LAV infection [320] . This is especially evident in studies where MV-LAV could be significantly detected in the blood, spleen and bone marrow of the humanized NOJ, but not in non-humanized NOJ [320] . HIS mice's susceptibility to MV-LAV has been used to evaluate vaccine reversion and stability over time. Specifically, CB17-SCID-hu mice were employed to investigate whether long-term passages of an avirulent measles vaccine strain (Moraten strain) could cause enhanced virulence and thymic tissue adaptation over time [321] . Interestingly, the Moraten strain, collected 90 days post vaccination from the thymus of CB17-SCID-hu mice, showed high virulence in thymus implants in comparison to the original Moraten strain. These works demonstrate that HIS mice can represent a valuable resource to evaluate vaccine safety. We recently reported that NRG mice engrafted with human HSC (NRG-HIS) are susceptible to YFV-17D [249] . Indeed, although NRG mice engrafted with murine HSCs (NRG-MIS) were not permissive to infection, the reconstitution of NRG mice with human hematopoietic cells led to persistent infection in the peripheral blood and in the spleen, demonstrating a preferential tropism of YFV-17D for human cells. By tracking viral RNA into multiple subsets of the human immune system, we were able to probe the cellular tropism of YFV-17D over time and space in multiple human hematopoietic lineages. Using NRG-HIS mice and mice depleted for Stat1 signaling in the hematopoietic compartment, we also identified species-specific interactions between YFV-17D and this compartment that potentially regulate the outcome of infection. Altogether, this work provided important evidence that HIS mice can serve as a unique system to define the dynamics of interactions between LAVs and components of the human immune system over time and space. Live-attenuated influenza vaccines (LAIV) likely represent the LAVs that have been the most investigated using HIS mice. In 2008, Yu et al. reported the use of NSG-HIS mouse models engrafted with HLA-A*0201 HSC, then treated with hFlt3L and reinjected with autologous T cells prior to vaccination with a trivalent influenza LAV. Following vaccination, LAIV-CD8+ T cells targeting an HLA-A*0201-restricted epitope from the influenza matrix protein (FluM1) and nonstructural protein 1 were found to expand in the blood, spleen and lungs, underscoring the successful adaptive immune priming in this model [322] . Importantly, priming was reported to be myeloid-dependent, which highlights the critical need of a myeloid compartment in HIS mice for modeling anti-LAV immunity. In a follow-up study, the same authors identified that CD1c+ dendritic cells induce the differentiation and accumulation of mucosal effector CD103+ CD8+ T cells in a TGFβ-dependent manner in the lung following LAIV vaccination [323] . Finally, in a third publication, they identified CD141+ dendritic cells as the major driver of the establishment of the Th2 response in the lung upon LAIV infection, by promoting the differentiation of IL-3-and IL-4-producing CD4+ T cells through the OX40 ligand [324] . Overall, these studies demonstrate that HIS mice can be successfully employed to uncover immunological mechanisms regulating T cell priming and differentiation in relevant tissues during LAV infection. NSG-HIS mice have been reported to mimic clinical features of human tuberculosis infection [325] . Following aerosol infection, pulmonary lesions and necrotic granulomas displaying similarities with those observed in patients could be observed. Authors also demonstrated the relevance of such an infection model to evaluate drug regimens against M. tuberculosis in vivo. In another study, NRG-HIS mice were immunized subcutaneously with the BCG vaccine and then challenged four weeks later with M. tuberculosis via the respiratory route. BCG was able to partially protect NRG-HIS mice against M. tuberculosis infection, as demonstrated by a reduced bacterial load and granulomatous lesions in the lungs. The authors therefore concluded that HIS mice can represent a relevant platform to evaluate novel vaccinal strategies against M. tuberculosis, a method that they further implemented by testing a novel adenoviral-vectored (VV) vaccine expressing an immunodominant M. tuberculosis antigen [326] . The same year, an additional study from a different group reported the analysis of CD4+ and CD8+ T cell phenotypes (through their expression of IFNγ, TNFα and IL-2) in NOG-HIS mice upon M. tuberculosis infection, as well as BCG vaccination prior to M. tuberculosis infection [327] . These authors found evidence of potential T-cell activation upon M. tuberculosis challenge in the lungs, and further suggested that BCG vaccination affects the distribution of T-cell activation phenotypes in the same tissue upon subsequent M. tuberculosis infection. However, unlike the previous study, authors did not find that BCG could confer any level of protection against M. tuberculosis in NOG-HIS mice, in contrast to C57BL/6 mice and Hartley guinea pigs, where partial protection was observed [327] . Altogether, these studies highlight that HIS mice have potential in uncovering the molecular basis of BCG-induced immunity. However, they also underscore the need for more advanced HIS mouse models able to more accurately and reproducibly recapitulate human BCG-induced immunity. Previous studies using conventional HIS mice have demonstrated the importance of the myeloid compartment in HIS mice for accurately investigating anti-LAV immunity. More recently, we reported NRGF-HIS mice (see Section 'Cytokine Environment and Niche Space' for model description) and NRGF-HIS mice expressing transgenic HLA-A*0201 (NFA2-HIS) as a new HIS mouse model for investigating YFV-17D-induced immunity [17] . In this model, the adenovirus-mediated expressing of hFLT3L promotes the selective expansion of human dendritic cells, myeloid cells and granulocytes, unlike in conventional NRG, NSG or BRG mice, where hFLT3L expression/injection leads to the expansion of both the human and murine myeloid compartment. Although this might not be an issue for all LAVs, the mouse interferon response has been shown to strongly restrict YFV-17D infection in mouse models of infection [148, 328, 329] , in contrast to humans, where YFV-17D can overcome such a response. Therefore, hFLT3L-mediated expansion of the myeloid compartment in conventional HIS mouse strains can lead to important immune interference between the mouse and human hematopoietic compartment. Unlike conventional NRG-HIS mice (in which the human myeloid compartment is not expanded), we found that NFA2-HIS mice can mount a peripheral transcriptomic response upon YFV-17D that shared key similarities with the response observed in human vaccinees [17] . NFA2-HIS mice were consistently able to mount a YFV-17D-specific CD8+ T cell response as well as an YFV-17D-IgG response, which led to the clearance of viral RNA in the peripheral blood, unlike in conventional NRG-HIS mice where infection persist over time in periphery. This work demonstrates the strong potential of second-generation HIS mice and emerging models to uncover fundamental immunological processes regulating adaptive immune priming in vivo and in a human context, and therefore sheds light on the elusive mechanisms governing LAV-induced protective immunity. Although HIS mice have been instrumental to investigating the infectious cycles of important viral and bacterial pathogens [12, 13, 330, 331] , the use of these models to study established and candidate LAVs has remained scarce. One potential reason explaining this scarcity of this research is the current limitations of these models. Unlike virulent pathogens that often display low immunogenicity features, evade or impede the immune responses and lead to clinical symptoms, LAVs are strongly immunogenic pathogens that requires an in vivo system powerful enough to effectively 'read' them and harness them in order to develop a strong and long-lasting response. Therefore, although conventional HIS mice have been considered satisfying enough to study specific immunological processes during virulent pathogen infection, investigating LAV-induced immunity requires models able to mount polyfunctional, potent and long-lasting responses that involve multiple arms of the innate and adaptive immune system. However, conventional HIS mice, like NOD-SCID, NRG, NSG/NOG or BRG, display major caveats such as low myeloid engraftment, a lack of important lymphoid tissues and the absence of a proper lymphocyte education, therefore making these models poorly suitable to investigate LAV-induced immunity. While HIS mice are still a work in progress and will likely always be, models that have been emerging over the past five years represent a promising step forward in the development of better HIS mice for LAV research. For instance, models with enhanced myeloid and NK engraftment, such as the MISTRG [14] , NSG-SGM3-HIS [257, 332] or NFA2-HIS [17] , have been reported to display superior T and/or B cell activity upon microbial challenges than conventional models. Specifically, the NFA2-HIS mice provided a demonstration that enhancement of the human myeloid and NK compartment could promote a superior innate and adaptive immune response to an LAV in HIS mice [17] . Nevertheless, major challenges need to be overcome to establish models able to mount strong and sustained T and B cell responses that would resemble those observed in humans. Recently, we reported a new methodology to quantitatively evaluate the quality of the human immune response in HIS mice by defining a correlation index between the peripheral transcriptomic responses measured in a given HIS mouse and in humans following YFV-17D vaccination [17] . Although we were able to show that NRG-HIS had a significantly lower correlation index than NFA2-HIS mice, the correlation index of this latter model was found to reach no more than 0.18. This result highlights that major refinements are still urgently needed in second generation HIS mice to allow for an accurate modeling of the human immune responses to LAV. Back-to-back comparative studies of these responses in humans and HIS mice will be of critical help to rationally guide the design of novel models and evaluate the benefits of specific requirements. Given the importance of the myeloid and NK compartment in priming adaptive immune responses, one area of improvement relies upon enhancing the human cytokine environment of the most recent models through the combined knock-in of multiple cytokines such as IL-6, IL-15, Flt3LG, TPO, IL-3, GM-CSF and M-CSF [14, [250] [251] [252] [253] [254] [255] [256] [257] [258] [259] [260] [261] . The co-engraftment of novel cytokine-enhanced HIS mice with human HSC, human thymus and/or ossicles would provide an optimal environment for lymphocyte education, maturation and priming and would be highly relevant for LAV research. Models combining superior myeloid engraftment (via human cytokine knock-in), mouse MHC knock-out and the transgenic expression of a large panel of human MHC class I and II could also represent a suitable alternative (Figure 3) . A large number of LAVs (as well as other vaccines) are injected intra-muscularly and therefore prime the immune response in draining LN. However, current HIS mice used for LAV research lack functional secondary lymphoid structures because of defective IL7-mediated signaling (See section 'Human Microenvironment and Lymphocyte Education'). This suggests that immune priming in HIS mice upon LAV infection is either extremely limited when using an intra-muscular route, or occurs in irrelevant tissue context when using other routes of infection (priming will mostly occur in the spleen if using an intravenous route of infection). Several strategies have been suggested and/or reported to generate HIS mice with LN which include, among other cytokine-mediated restorations of mouse LN [307, 308] , engraftment of ex-vivo generated human artificial LN [333] , or direct engraftment of human LN [334] . HIS mice models combining functional LN structures, primary lymphoid human tissues or human MHC class I and II expression, as well as an enhanced myeloid-NK compartment could represent a fantastic resource for LAV research (Figure 3 ). Such models would open major avenues to a new ranges of studies where human immunological mechanisms governing LAV immunogenicity and attenuation could be decrypted in a relevant tissue and cellular context, and in a way that is not possible in human vaccinees. Other human tissues could enhance LAV-induced immunity in HIS mice. Conventional HIS mouse strains co-engrafted with human liver and human HSC have been previously reported and were shown to be a relevant system to study, for instance, hepatitis B virus infection replication and induced-immune responses [335, 336] . The liver is known to host several immune functions such as IL-6 production, secretion of acute phase proteins, and the production of complement or Kupffer-cell mediated immune regulation [337] . Therefore, it is likely that the immune function of the liver could synergize with those of the immune system when co-engrafted in refined HIS mouse models such as the one described above (Figure 3 ). More recently, BLT-HIS mice engrafted with human lungs have been reported [16] . Authors showed that human lung tissues could be repopulated with autologous human hematopoietic cells, and that mice could mount humoral and T-cell responses following microbial challenge. Human lung tissues could be of relevance when investigating immune responses to LAVs administrated through the respiratory route, such as LAIV, for instance ( Figure 3 ). Finally, more and more evidence points to the role of the microbiota in regulating immune responses to vaccines [338] . A recent study conducted in human vaccinees notably found that antibiotic treatment impairs IgA and IgG responses to H1N1 following influenza vaccination [339] , suggesting that particular bacterial species can modulate adaptive responses to vaccines. HIS mice present a significant advantage over humans for the study of the microbiota as their microbiome can be easily tracked and manipulated throughout the animal's life. Protocols have been recently developed to generate HIS mice engrafted with human microbiota [340] , and few preliminary studies have been conducted in conventional HIS mouse models [341] . As we approach the role of the microbiota in regulating human health, the development of innovative HIS mouse models able to mount superior adaptive responses and co-engrafted with human microbiota would open notable opportunities to investigate LAV-induced immunity in a more relevant physiological context and to better understand microbiota-mediated regulation of immune priming (Figure 3 ). LAVs undeniably represent one of the most important biomedical innovations in human history. Yet, much remain to be learned from these vaccines. The molecular basis of their attenuation and potent immunogenicity are still poorly understood, which consequently restricts our fundamental understanding of human immunity. LAVs represent formidable tools to better understand human immune responses and how they are regulated, and could not only provide us with effective new ways to rationally design innovative vaccines and immunotherapies against challenging infectious diseases, but also to treat other human pathologies, such as cancer. HIS mice have been instrumental in investigating the infectious cycle of many human pathogens. However, despite their strong potential, their applicability to LAV research has remained limited, at both the fundamental and pre-clinical (e.g., the testing of LAV candidates) level. This is mostly due to the limited ability of past and current models to mount a sufficiently strong and accurate immune response to these highly immunogenic pathogens, therefore pushing investigations toward less relevant (mouse) or more costly and logistically challenging (NHP) animal models. Although HIS mice will never be able to perfectly mimic human immune responses, recent model improvements have demonstrated that creating HIS mice able to mount more potent responses to LAV is possible. In the future, efforts will have to be directed toward the generation of HIS mice that can mount potent polyfunctional immune responses tightly regulated by different arms of the innate and adaptive immune system. The design and objective evaluation of models combining functional lymphoid structures with enhanced human cytokine environments, erythro-myeloid physiological engraftment and additional human-specific immunoregulators (such as the liver or microbiota) could be central to this endeavor. It is worth noting that recapitulating the establishment and persistence of memory B and T cell responses in HIS mice during vaccination will remain a considerable challenge. Nevertheless, the accurate modeling of the immunological events occurring from the time of vaccination to the activation of the adaptive response in secondary lymphoid tissues should be seen as a reachable objective. These set of events are likely critical in defining the potent immunogenicity of successful LAVs, and capturing them could definitively impact our understanding of LAV-induced immunity, and human immunity overall.  
paper_id= ffece58ead08c1a247ac3352aa4d73c5b10f205f	title= Survey of laboratory-acquired infections around the world in biosafety level 3 and 4 laboratories	authors= N  Wurtz;&amp; A Papa;&amp; M Hukic;&amp; A Di Caro;I  Leparc-Goffart;E  Leroy;&amp; M P Landini;&amp; Z Sekeyova;J S Dumler;&amp; D Bădescu;&amp; N Busquets;&amp; A Calistri;&amp; C Parolin;&amp; G Palù;I  Christova;&amp; M Maurin;&amp; B La Scola;&amp; D Raoult;	abstract= Laboratory-acquired infections due to a variety of bacteria, viruses, parasites, and fungi have been described over the last century, and laboratory workers are at risk of exposure to these infectious agents. However, reporting laboratory-associated infections has been largely voluntary, and there is no way to determine the real number of people involved or to know the precise risks for workers. In this study, an international survey based on volunteering was conducted in biosafety level 3 and 4 laboratories to determine the number of laboratory-acquired infections and the possible underlying causes of these contaminations. The analysis of the survey reveals that laboratory-acquired infections have been infrequent and even rare in recent years, and human errors represent a very high percentage of the cases. Today, most risks from biological hazards can be reduced through the use of appropriate procedures and techniques, containment devices and facilities, and the training of personnel. 	body_text= Laboratory-acquired infections (LAIs) are defined as all infections acquired through laboratories or laboratory-related activities, whether they are symptomatic or asymptomatic in nature. LAIs due to a wide variety of bacteria, viruses, fungi, and parasites are described in the literature. The largest survey of infections was reported in 1976 by Pike [1] , who found that 4079 LAIs were caused by 159 biological agents, although ten agents caused infections accounting for 50 % of cases (brucellosis, Q fever, hepatitis, typhoid fever, tularemia, tuberculosis, dermatomycoses, Venezuelan equine encephalitis, psittacosis, and coccidioidomycosis). There were no distinguishable accidents or exposure events identified in more than 80 % of the reported cases. During the 20 years following the Pike and Sulkin publications, a worldwide literature review by Harding and Byers revealed 1267 cases of infections, with 22 deaths [2] . Five deaths were fetus abortions as consequences of a maternal LAI. Mycobacterium tuberculosis, Coxiella burnetii, hantaviruses, arboviruses, hepatitis B virus, Brucella spp., Salmonella spp., Shigella spp., hepatitis C virus, and Cryptosporidium spp. accounted for 1074 of the 1267 infections. Like Pike and Sulkin, Harding and Byers reported that only a small number of the LAIs involved a specific incident. These studies reported cases of old infections and refer to periods with practices and types of exposure which have since considerably evolved, especially with the introduction of high-containment laboratories. More recently, Henkel et al. presented data reported to the Centers for Disease Control and Prevention (CDC) from 2004 to 2010 following the implementation of a nationwide program for monitoring the potential theft, loss, or release of biological select agents and toxins (BSATs) [3] . In total, 11 LAIs associated with BSAT releases were reported in an average annual population of approximately 10,000 individuals with approved access to BSATs. No cases of fatality or secondary human-to-human transmission was reported. These LAIs were associated with exposures to Brucella species (six cases), Francisella tularensis (four cases), and Coccidioides immitis/posadasii (one case) [4, 5] . They resulted from either unrecognized exposures or presumptive exposure to BSAT aerosols. These observations are consistent with Pike's and Harding's studies [1] . Two publications in science magazines have provided recent information about LAIs. The current Ebola crisis reveals that priority must be given to infectious diseases because of the potential consequences to individuals and society [6] . Some researchers argue for the need to increase research on Ebola virus to develop treatments, while others focus on recent incidents in biosafety facilities and the possible dissemination of these dangerous pathogens in the general population [6] . A recent example, in 2004, was the mishandling of the severe acute respiratory syndrome virus that resulted in tertiary infections and the death of an attending physician in China [7] . Lipsitch found that government data on US biosafety labs reveal accidents estimated to be between 100 and 275 potential releases of pathogens each year in labs that deal with select agents between the years 2008 and 2012; however, these reports include banal accidents like spills and record-keeping errors, and very few workers were infected [8] . However, until now, the true risk posed to laboratory workers after potential exposure to an infectious agent has been difficult to determine, in part because of the lack of systematic reporting of laboratory infections. It may vary greatly according to the pathogen and also the type of exposure considered. Currently available data are limited to retrospective and voluntary postal surveys, anecdotal case reports, and reports about selected outbreaks with specific microorganisms. The aim of this survey was to gather information on LAIs in biosafety level 3 and 4 laboratories around the world and to assess possible underlying causes of these infections, in order to identify real current risks and to propose preventative procedures. In this study, 119 private or public institutions with notified containment level 3 or 4 laboratories were contacted by email to complete a survey about LAIs. The mailing list was established by investigators in Marseille. In total, 15 questions were addressed to each respondent, consisting of single-answer questions and multi-answer questions, most of the questions being mandatory (see below). We also performed a literature analysis. To determine the worldwide number of LAIs, a systematic review of articles published during the period 1980-2015 was performed. The inclusion criterion was the presence of an accidental infection in workers or students in research laboratories working with French select agents. The following academic Internet search systems were used: PubMed, Google Scholar, and ISI Web of Knowledge. They were searched with the keywords Blaboratory acquired infection^, Blaboratory accident^, and with the list of the different French select agents. Please choose only one of the following: Please write your answer here: Please write your answer here (list all biological agents):  Please choose the appropriate response for each item:  The experience of two skilled researchers Prof. X, director of a laboratory dealing with rickettsial diseases in the United States, pointed out that he had not had any laboratory infections with biosafety level 3 rickettsial agents since 2000. He specified that he had personal experience with laboratory infections with rickettsial agents (particularly Rickettsia) in the USA, but these infections generally occurred before the implementation of biosafety level 3 laboratories and the exclusive handling of infectious agents in class II biosafety cabinets. He remembered one C. burnetii-related infection caused by a needle stick while the involved person was working with a previously infected mouse. However, at present, all work with C. burnetii is performed in a specific laboratory, and requires vaccination of the laboratory workers and the use of a powered air-purifying respirator in the biosafety level 3 laboratory. Prof. Didier Raoult established the Rickettsia Unit at Aix Marseille University in 1984. Since 2008, he has been the director of the BURMITE^, the research Unit in Infectious and Tropical Emergent Diseases, and employs 450 personnel (with 80 national and international students and PhD students). The laboratory coordinates European and international networks, and serves as a leader in research on several infectious diseases, including rickettsial diseases, Q fever, and arboviral diseases, and is directly involved in defense against bioterrorism and highly contagious diseases. In the 1980s, the laboratory had three LAI cases from skin wounds caused by the manipulation of glass tubes broken after centrifugation of an infectious suspension. The involved biological agents were Rickettsia species (including R. australis, R. conorii, and R. japonica). The persons exposed received appropriate antibiotic treatment and recovered without sequelae. These incidents occurred before the publication of regulatory procedures for biosafety level 3 and 4 laboratories and good laboratory practices. A total of 23 of the 119 contacted laboratories accepted to participate to this survey, of which five were biosafety level 4 laboratories. As shown in Fig. 1 , this survey was conducted on a global basis. Most of the questioned laboratories routinely use the following personal protective equipment: latex gloves (86 %), nitrile gloves (68 %), two pairs of gloves (77 %), FFP3 masks (64 %), goggle or face protection (81 %), waterproof coverall or gown (68 %), overboots or overshoes (90 %), and hygiene cap or hood (54 %) (Fig. 2) . Please write your answer here: Only four of the 23 surveyed laboratories reported 15 LAIs caused by four different pathogenic organisms. Bacterial infections predominated, particularly biosafety level 3 bacteria belonging to the following species: Mycobacterium tuberculosis (ten cases), Coxiella burnetii (two cases), and Brucella melitensis (two cases) ( Table 1 ). The remaining case was caused by a biosafety level 2 virus (foamy virus). The majority of the LAIs (73 %) occurred in a biosafety level 3 laboratory in the context of microbiology activities (42 %), followed by microscopy (22 %) and cell culture (22 %) (Fig. 3) . Laboratory technicians were most commonly infected (87 % of the cases), while in only 7 and 6 % of the cases, respectively, the infected person was an animal caretaker or a researcher. It should be noted that laboratory technicians are more numerous than researchers worldwide, and also probably more often exposed to biological agents. All 15 LAI cases recovered from their infection, without sequelae in 13 persons (87 %), but with sequelae in two patients. Fortunately, no deaths were reported. Notably, for 93 % of the cases, postexposure treatment was prescribed. A small percentage of exposed persons had only reinforced medical surveillance Fig. 1 Worldwide repartition of the laboratories that responded to the survey. Retrieved from http://www.jimmymack.org/worldmap.html (13 %) , and only 40 % were vaccinated against the involved pathogen prior to exposure. Regarding the potential transmission routes involved in LAIs, 87 % of the cases were airborne infections, while the others were percutaneous infections. In none of the LAIs was the infection transmitted to another person. Half of the cases were related to technical failures in equipment and infrastructure. However, these cases occurred in a single laboratory where the environment was not safe. Consequently, the laboratory was closed after this incident. For the remaining cases, three contaminations occurred because of not wearing personal protective equipment. Other incidents leading to LAIs were animal bites and scratches (two cases), splashes (one case), inadequate compliance with safety rules (one case), and spills (one case) (Fig. 4) . Not respecting certain biosafety practices (eight cases), lack of attention (six cases), lack of appropriate equipment and materials (four cases), and insufficient training (four cases) seem to be the principal causes of LAIs (Fig. 5) . Therefore, human error accounted for 78 % of the underlying causes of LAIs. LAIs represent an occupational hazard unique to laboratory workers, especially those working in microbiology laboratories. Before the introduction of regulations concerning biosafety levels in laboratories and good laboratory practices, laboratory manipulations, including the handling of cultures of human pathogens, took place on the bench, without any specific protection. For example, it was permissible to smoke, eat, or drink in such laboratories, to conduct an olfactory examination of the cultures, or to perform mouth pipetting of infectious suspensions, all practices that are now well known to be associated with a high risk of laboratory infections. Therefore, many LAIs occurred, as described by Pike [1] . The actual risk of LAIs is difficult to quantify because there is no systematic reporting system. Because of this lack of information, control measures are proposed and implemented by competent authorities, and regulations are increasing dramatically, which, in turn, profoundly affect research [9] . The additional, sometimes draconian, measures for laboratories working with highly pathogenic microorganisms have been implemented without solid evidence that they will provide additional public or laboratory safety. We performed a comparison of three different sources on the number of LAIs due to biological select agents: this survey, the reference [3] dealing with LAIs due to select agents in United States between 2004 and 2010, and a literature review of LAIs over the last 35 years (Table 2 ). It is evident that 23/ 119 (19 %) laboratories responding to our questionnaire present a limited proportion of laboratories and we understand that it is a limitation of this work. However, with the strengthening of regulations, we believe that some laboratories are reluctant to expose their accidents. However, due to the long period of survey for several investigators, we believe that the obtained results are a good reflection of the frequency and cause of accidents. Overall, this analysis allows us to conclude that LAIs have been infrequent with highly pathogenic microorganisms (two in our survey, ten in reference [3] , and 220 during the last 35 years), and even rare in recent years. This phenomenon is almost certainly due to the improvement of working conditions, particularly the biosecurity measures implemented during this time. This is especially true with biosafety level 4 laboratories, in which no accident was observed in the five laboratories participating in this work. Moreover, with the exception of a case of a technician accidentally inoculated with Ebola virus who did not develop hemorrhagic fever [10] , we could not find any LAI reports in biosafety level 4 laboratories ( Table 2) . As accidents in this kind of laboratories are likely to become publicized, we do not believe we missed any information. Recent incidents in the USA involving BSATs have focused attention on the need to improve and maintain a culture of biosafety and biosecurity in the life sciences. Notable incidents included the discovery of vials labeled Bvariola^, the virus responsible for smallpox, in a storage room in a Food and Drug Administration laboratory located on the Bethesda campus of the National Institutes of Health, the potential exposure of staff members at the CDC to Bacillus anthracis, and the inadvertent cross-contamination of a low pathogenic avian influenza A (H9N2) virus sample with a highly pathogenic avian influenza A (H5N1) virus and subsequent shipment of the contaminated culture to an external high-containment laboratory. None of these events resulted in human contamination, but suggested an inadequate compliance with existing regulations, policies, and procedures [11, 12] . After these events, the White House published a report dealing with national biosafety and biosecurity in order to protect the nation's health and in order to prevent, detect, and respond to infectious threats around the world, resulting in a set of recommendations in terms of biosafety and associated biosecurity [11] . Fortunately, none of these events resulted in a casualty. One important point that could improve safety in laboratories working with highly pathogenic agents is the training of personnel to reduce LAIs caused by human error, and, in particular, to avoid the involvement of people with psychological problems in research on highly pathogenic agents by means of a regular medical follow-up. An example of a failure in medical monitoring was provided during the anthrax attacks that occurred in the USA in 2001 [36, 37] . Dr. Ivins was working in a military laboratory at Fort Detrick, Maryland on the development of anthrax vaccines. He had mental health and professional problems because his work on anthrax vaccine was unsuccessful. An apparent lack of careful oversight allowed him to send letters containing anthrax spores to various offices and senators, resulting in the deaths of five people and infections in 17 others [38] . When examining actual biological hazards, human errors represent a very high percentage of LAIs. If in the past, as observed in laboratory number 3 of the present work, cases could be related to technical failures in equipment and infrastructure, today, most risks from biological hazards for The biological agents colored in red belong to annex 1 of the French regulation concerning select agents [35] (Highly pathogenic microorganisms presenting the highest risk to public health) The biological agents colored in green belong to annex 2 of the French regulation concerning select agents [35] ( humans can be reduced through the use of appropriate procedures and techniques, containment devices, and facilities. It should be noticed that humans are not the unique possible victims of biological hazards; cattle were affected by an outbreak of foot-and-mouth disease in the UK in 2007 that was later suspected to be due to a cracked pipe from two laboratories working on this virus in their vicinity [39] . The control measures used in the laboratories questioned are designed to protect employees and the public from exposure to infectious agents, and these measures seem to be sufficient. The consequence of not respecting such measures is that many of the laboratories will simply abandon the study of critically important biohazardous agents. The biosafety procedures adopted so far have greatly contributed to reducing the burden of LAIs in recent years, and data from this survey contribute to improving the balance between the need to facilitate research activities and assuring appropriate biosafety and biosecurity procedures. Tim Trevan recommended that scientists working with hazardous materials take lessons from the nuclear industry, hospitals, and other sectors that have established a safety culture [40] . In conclusion, there is still a need to implement a culture of biosafety in the life sciences, rather than strengthen regulations. 
paper_id= ffed3a7459ee46d57ecf21bc1aefb8a7d1494ebf	title= Development of a volumetric pancreas segmentation CT dataset for AI applications through trained technologists: a study during the COVID 19 containment phase	authors= Garima  Suman;Ananya  Panda;Panagiotis  Korfiatis;Marie E Edwards;Sushil  Garg;·  Daniel;J  Blezek;·  Suresh;T  Chari;·  Ajit;H  Goenka;Daniel J Blezek;Suresh T Chari;Stchari@mdanderson  Org;	abstract= Purpose To evaluate the performance of trained technologists vis-à-vis radiologists for volumetric pancreas segmentation and to assess the impact of supplementary training on their performance. Methods In this IRB-approved study, 22 technologists were trained in pancreas segmentation on portal venous phase CT through radiologist-led interactive videoconferencing sessions based on an image-rich curriculum. Technologists segmented pancreas in 188 CTs using freehand tools on custom image-viewing software. Subsequent supplementary training included multimedia videos focused on common errors, which were followed by second batch of 159 segmentations. Two radiologists reviewed all cases and corrected inaccurate segmentations. Technologists' segmentations were compared against radiologists' segmentations using Dice-Sorenson coefficient (DSC), Jaccard coefficient (JC), and Bland-Altman analysis. Corrections were made in 71 (38%) cases from first batch [26 (37%) oversegmentations and 45 (63%) undersegmentations] and in 77 (48%) cases from second batch [12 (16%) oversegmentations and 65 (84%) undersegmentations]. DSC, JC, false positive (FP), and false negative (FN) [mean (SD)] in first versus second batches were 0.63 (0.15) versus 0.63 (0.16), 0.48 (0.15) versus 0.48 (0.15), 0.29 (0.21) versus 0.21 (0.10), and 0.36 (0.20) versus 0.43 (0.19), respectively. Differences were not significant (p > 0.05). However, range of mean pancreatic volume difference reduced in the second batch [− 2.74 cc (min − 92.96 cc, max 87.47 cc) versus − 23.57 cc (min − 77.32, max 30.19)]. Conclusion Trained technologists could perform volumetric pancreas segmentation with reasonable accuracy despite its complexity. Supplementary training further reduced range of volume difference in segmentations. Investment into training technologists could augment and accelerate development of body imaging datasets for AI applications. 	body_text= The development of artificial intelligence (AI) models through supervised training of deep learning algorithms for medical imaging applications requires large training datasets with high-quality labels [1, 2] . Such datasets have been typically obtained through manual annotations of images by expert radiologists or trained image analysts. However, the process of manual segmentation and labeling on cross-sectional imaging is labor-intensive and not scalable. Recruitment of experts for segmentation and labeling also makes the generation of such datasets an expensive investment. Therefore, there is a need for alternate approaches to circumvent this bottleneck to accelerate the generation of labeled datasets for training and eventual clinical deployment of reliable AI models. Technologists are one of the key stakeholders in the medical imaging workflow [3] . They often gain working knowledge of cross-sectional anatomy and skillsets for image processing as part of their training and clinical assignments. In fact, some technologists also go on to become members of imaging core labs at many institutions. These attributes suggest that technologists can be a potential group to generate labeled body imaging datasets for AI applications. An advantage of training technologists in image annotation tasks is that the data do not have to leave institutional security firewalls. Secondly, these trained technologists could be integrated into the data annotation pipelines of multiple other body imaging AI projects. However, to the best of our knowledge, the feasibility of this approach has not been evaluated. Our group is developing AI-powered workflow modules to address the unmet needs in patients with pancreatic diseases. The pancreas is a solid retroperitoneal organ that can be hard to segment because of its small size, complex anatomy, and variability in location, morphology, and attenuation [4] . Furthermore, the variable degrees of peripancreatic fat, contrast enhancement, and subadjacent iso-attenuating structures such as collapsed bowel can further confound delineation of its exact boundaries [5] [6] [7] . These factors make manual segmentation of the pancreas a challenge and at least partly contribute to the underutilization of pancreas morphometrics and radiomics in both endocrine and exocrine diseases despite promising results [8] [9] [10] [11] . Therefore, there is a need for large volume segmented datasets to develop and test production-scale AI models for automated pancreas segmentation. During the coronavirus disease of 2019 (COVID-19) containment phase, similar to other institutions [12, 13] , we faced a situation of reduced clinical imaging volumes and redundancy of staff such as technologists due to voluntary deferral of all elective clinical care by our institution. We decided to leverage this opportunity to assess whether the skillsets of technologists could be augmented through focused training to create a CT dataset of segmented normal pancreas for AI applications in body imaging. The purpose of this project was to evaluate the performance of technologists vis-à-vis radiologists for volumetric pancreas segmentation after initial training and to assess the impact of focused supplementary training on their performance. The project was conducted as a part of an Institutional Review Board (IRB)-approved and Health Information Portability and Accountability Act-compliant study. The requirement for informed patient consent had been waived by the IRB due to the retrospective study design. We randomly selected 347 contrast-enhanced CT scans on the basis of a statement of a negative or unremarkable pancreas in the original radiologist's report. This was subsequently verified during manual pancreas segmentation by two radiologists (AP and GS with 7 and 3-years of post-residency experience, respectively). For each CT study, an axial portal venous phase series (≤ 3-mm slice thickness) was identified and confirmed with the use of information from the series name and DICOM header. All CT studies were de-identified by anonymization of Digital Imaging and Communication in Medicine (DICOM) tags utilizing Clinical Trial Processor [14] . These anonymized CT datasets were extracted and converted into the Neuroimaging Informatics Technology Initiative (NIfTI) format. These anonymized datasets were stored in an offline shared folder for radiologists' review on a free and open-source software package for image analysis and scientific visualization [3D Slicer ® (version 4.11.0)] [15] . Between March and April 2020, 22 CT and MRI technologists volunteered to participate in this project. These technologists were not familiar with the 3D Slicer ® software that was being used by radiologists. Therefore, we decided to train the technologists for pancreas segmentation on our enterprise custom image-viewing software (QREADS). This custom enterprise software is routinely used by the technologists to review images as part of their regular clinical work. However, they were not familiar with the image annotation tools that this software provides. To address this, a standard operating procedure (SOP) document and a 20-min training video that demonstrated steps for image display and review with the use of standard viewing tools (zoom, contrast, scroll, pan, etc.) and image annotation on a slice-by-slice basis using freehand annotation tools were created. The SOP document contained details of the various steps involved such as image retrieval, data organization, links to access the training material, case assignment, data reporting, and quality control. To augment knowledge of the technologists, a curriculum document with infographics focused on pancreas segmentation (Fig. 1) was created by the radiologists over a period of 2 days. The topics covered in the curriculum document included an overview of project goals and an image-rich multiplanar depiction of pancreatic anatomy on CT, common anatomic variations (e.g., variations in the location of the pancreas, lipomatosis, variable pancreatic parenchymal enhancement), and relevant CT artifacts (e.g., partial volume effect, motion artifacts, streak artifacts from embolization coils). These training documents were reviewed with the technologists in four radiologists-led interactive virtual instructional sessions of 1-hour duration each. All these instructional sessions were recorded. A recording of the session along with screencast of the workflow and training module documents were shared with the technologists through a shared folder on the institutional intranet. Each participant technologist was required to document completion of the required training by signing off an online verification form. Finally, the technologists were also given institutional access to an interactive e-anatomy atlas (www.imaio s.com) for additional but optional self-directed learning. Following this training, an initial batch of 188 CT studies was randomly selected from the master dataset of 347 studies and was retrieved on the enterprise software. The technologists performed volumetric pancreas segmentation on a slice-by-slice basis using freehand segmentation tools over a period of 14 workdays. Queries of the technologists during this initial segmentation process were answered by radiologists through emails. These segmentations were saved, exported offline, and converted to NIfTI format. Two radiologists (AP and GS) subsequently reviewed the volumetric CT datasets and the technologists' segmentations on 3D Slicer ® . These two radiologists repeated those pancreatic segmentations that were either an undersegmentation (any Fig. 1 Images from training material: Color-coded depiction of abdominal organs (a) on an axial CT image (pancreas: red; liver: purple; kidneys: light green; stomach: yellow; small bowel: blue, and spleen: cyan). Depiction of pancreas outline in red with labeled subadjacent anatomical structures on axial (b) and coronal (c) CT images. Tracing of pancreas outline on enterprise custom imageviewing software using freehand tools (d). The smaller red squares are artefactually generated by the software with any outline task part of pancreatic parenchyma left out) or an oversegmentation (any part of subadjacent anatomy included) error. Repeat segmentation was done by the radiologists with the use of boundary-points based segmentation mode of the AIassisted segmentation module (NVIDIA) in 3D Slicer ® . As part of this mode, radiologists placed input points at the perimeter of the pancreas on multiple planes (i.e., axial, coronal, and sagittal). The AI-assisted segmentation was then manually fine-tuned by the radiologists. Based on the radiologists' repeat segmentations as the ground truth, the technologists' segmentation errors were also quantified on a pixel-wise basis as either false positives (FP), i.e., the percentage of pixels segmented by technologists but not by radiologists, a measure of oversegmentation, and false negatives (FN), i.e., the percentage of pixels not included by technologists but present in radiologists' segmentations, a measure of undersegmentation (Fig. 2) . Radiologists also subjectively noted the most common causes of segmentation errors. Based on the assessment of segmentations performed in this batch, supplementary training material was created to highlight on common segmentation errors. This material included videos of representative samples of radiologists' corrected segmentations overlaid on the technologists' original segmentations. These segmentations were differently color-coded to highlight the pancreatic region(s) that were commonly being left out or the extra-pancreatic anatomy that was often being included by the technologists (Fig. 2) . Additional presentations depicting the subjacent anatomy using different color codes were also prepared to improve understanding of locoregional anatomy. These supplementary materials were reviewed through virtual video meetings and were also made available to the technologists through the common shared folder. Subsequent to this supplementary training, the technologists segmented the pancreas in the second batch of another 159 CT studies over a period of 9 workdays. Additional queries were addressed via emails. Finally, the second batch of segmentations was reviewed and evaluated by the radiologists similar to the first batch. Both batches of segmentations were performed by the technologists in the downtime during regular clinical duties. There was no provision of additional remuneration for participation in this project. Statistical analyses were performed with Python software (version 3.7.8; Python Software Foundation, Wilmington, Del) by using the Scikit-learn library (version 0.23.1) [16] . For the segmentations performed by technologists that were deemed inaccurate, the segmentations repeated by the radiologists were the ground truth. The original technologists' and revised radiologists' segmentations were compared using similarity metrics such as Dice-Sorenson coefficient (DSC) and Jaccard coefficient (JC). Semantic uncertainty was assessed by FP and FN rates. To evaluate the impact of supplementary training, the proportion of cases that needed no revision, oversegmentation and undersegmentation errors were compared between the two batches of segmentations using the Chi-square test for proportions. The DSC, JC, FP, and FN before and after supplementary training were compared using Kruskal-Wallis tests. Bland-Altman analysis was performed to evaluate the mean pancreatic volume difference (technologists' segmentation minus ground truth segmentation) versus the means of pancreatic volumes before and after supplementary training [17] . A p value < 0.05 was considered statistically significant. Of the initial batch of 188 segmentations, 117 (62%) were deemed accurate by radiologists and 71 (38%) had to be repeated due to segmentation errors. Undersegmentation accounted for the majority of the errors, 45/71 (63%), while the remainder[26/71 (37%)] were oversegmentation errors. Subjectively, the undersegmentation errors were commonly due to missing terminal portions of the head or tail of the pancreas and not including additional lobulations of pancreatic tissue separate from main pancreatic parenchyma. Oversegmentation errors were commonly due to the inclusion of iso-attenuating adjacent duodenum, collapsed jejunum, or the stomach. The DSC was 0.63 ± 0.15 and JC was 0.48 ± 0.15 (mean ± SD). The FP rate was 0.29 ± 0.21 and FN rate was 0.36 ± 0.20 (mean ± SD) ( Table 1) . From Bland-Altman analysis (Fig. 3a) , mean pancreatic volume difference (technologists' segmentation minus ground truth segmentation) was − 2.74 cc (minimum − 92.96 cc, maximum 87.47 cc). Out of the 159 segmentations performed in the second batch after supplementary training, 82 (52%) were deemed accurate and 77 (48%) segmentations had to be repeated. Oversegmentations were seen in 12/77 (16%) cases while 65/77 (84%) were undersegmentations. The causes of oversegmentations and undersegmentations were similar to those in the first batch. The DSC was 0.63 ± 0.16 and JC was 0.48 ± 0.15 (mean ± SD). The FP rate was 0.21 ± 0.10 and FN rate was 0.43 ± 0.19, (mean ± SD) ( Table 1) . From Bland-Altman analysis (Fig. 3b) , mean pancreatic volume difference (technologists' segmentation minus ground truth segmentation) was − 23.57 cc (minimum − 77.32 cc, maximum 30.19 cc). There was no difference in the proportion of accurate segmentations between the first and the second batch of technologists' segmentations (62% in the first batch and 52% in the second batch, p = 0.06). The trend of decline in the proportion of accurate segmentations in the second batch was primarily due to a relative increase in the share of undersegmentation errors (63% in the first batch and 84% in the second batch, p = 0.003). Conversely, there was a decrease in the share of oversegmentation errors (37% in the first batch and 16% in the second batch, p = 0.003). However, the range of mean pancreatic volume difference after supplemental training was lower than in the first batch (− 77.32 to 30.19 cc compared to − 92.96 to 87.47 cc in the first batch). There was no difference in DSC (p = 0.61), JC (p = 0.61), FP (p = 0.07), and FN rates (p = 0.12) between the two batches ( Fig. 4 ). The challenges involved in the curation and labeling of imaging datasets are widely regarded as key barriers for the development and production-scale deployment of reliable AI models in the clinical practice of body imaging. Expert labeling of these datasets is the ideal approach. However, this is often not practical due to the associated costs of time and resources [1] . To the best of our knowledge, training technologists for creation of labeled medical imaging datasets have not been explored. In the literature, experiences with crowdsourcing medical imaging tasks to untrained persons in the community-at-large have been described with variable success. Such tasks include annotations of airways, lung nodules, kidney and liver segmentations, and colon polyp classification on CT colonography images [18] [19] [20] [21] . Most of these studies concentrate on tasks that require little expertise of the crowd, as the objects to identify either have well-defined geometry or can be easily separated from the background. A similar approach for pancreas segmentation has not been attempted, which is likely due to the complex morphology and geometry of the pancreas. Thus, there is an unmet need for alternate approaches to generate labeled datasets for body imaging AI applications. In this study, we explored the feasibility of training radiology technologists for the development of a CT dataset of volumetric pancreas segmentation for AI applications. Specifically, we evaluated their performance vis-à-vis radiologists after initial training and assessed the impact of supplementary training on their performance for volumetric pancreas segmentation. Pancreas morphometrics and radiomics are emerging as biomarkers in both endocrine and exocrine disorders of the pancreas [22] . Accurate pancreas segmentation is essential for further investigation and validation of these biomarkers [8, 22] . A manual approach to pancreas segmentation is cumbersome, inaccurate, and not scalable. Therefore, validated methods for automated segmentation of pancreas in clinical practice are necessary. Automated pancreas segmentation will also have potential applications in surgical and radiation therapy planning, and for early detection of pancreatic cancer [5] . Although technologists gain a working knowledge of key anatomical landmarks during their routine clinical assignments, the skills needed for fine segmentations of organs such as pancreas on cross-sectional imaging are not part of their portfolio. Therefore, in this project, we created an image-rich training curriculum focused on multiplanar pancreatic anatomy on CT, which also included common anatomic variations and relevant CT artifacts. Secondly, we conducted instructional tutorials through multiple videoconferencing sessions for the technologists. All of these sessions had been recorded so that future training could be delivered through as videos or online modules without direct participation by radiologists. After the initial training, 62% of pancreatic segmentations by the technologists were deemed accurate when compared against the ground truth segmentations by radiologists. Given the inherent complexity of pancreas segmentation, we believe this is an encouraging result that justifies the upfront investment of our time and resources in their training. Secondly, the majority of the errors were due to undersegmentation of pancreatic anatomy. A higher proportion of undersegmentation errors suggests that the technologists generally adopted a cautious approach to the segmentation task, which often augurs well for beginners. The performance of technologists should also be viewed in the context of certain other factors. We did not categorize errors into minor and major classes. Any segmentation that was not deemed accurate was redone. Participation in this project was on a voluntary basis. All the segmentations had to be done during the course of a regular clinical assignment. Although the clinical volumes were low due to the COVID-19 containment phase, the tasks of segmentations were not entirely uninterrupted. We also did not structure additional compensation or time-off into this project. In the future, performance-based rewards and, possibly, gamification of segmentation tasks could augment motivation and performance, as has been observed by others [23] [24] [25] . It is also possible that some technologists may not need any subject matter training and could perform reasonably with just instructions on the use of segmentation software and workflow. Secondly, since trainees such as medical students, residents and fellows are also often motivated to participate in medical imaging AI projects, a future prospect is comparison of performance of untrained or trained technologists with that of those trainees, which we plan to undertake in the next phase. Another important consideration is the software platform used for segmentation tasks. The ground truth pancreatic segmentations were done by radiologists with an AI-assisted segmentation module on 3D Slicer ® . This software has to be downloaded on each computer for a given user and requires a certain amount of practice. On the other hand, the technologists used our enterprise custom image-viewing software for their segmentations. This was not a deliberate measure but rather a decision that had to be made in view of the accessibility and their familiarity with the enterprise imageviewing software. This enterprise software is pre-installed on all computers in our institution. Since technologists routinely used this software for their clinical functions, they were well-versed with its basic functions (e.g., loading a study, selecting a particular series, etc.) though they were not aware of its segmentation capabilities. Therefore, our training curriculum and modules included stepwise instructions of the segmentation workflow. This segmentation workflow required the technologists to draw manual regions-of-interest around the pancreas on each slice. This workflow likely made the segmentations cumbersome, which could have also contributed to the observed errors. Our experience highlights the need for cloud-based image annotation platforms with an intuitive interface that can be seamlessly integrated into the routine imaging workflows. After the supplementary training, there was a decrease in the range of mean pancreatic volume difference (minimum − 92.96 cc, maximum 87.47 cc in first batch; minimum − 77.32 cc, maximum 30.19 cc in second batch). However, the proportion of accurate segmentations declined to 52%, though the difference against the first batch was not significant. There was also no difference in the similarity metrics in the two batches. Interestingly, the trend towards a decline in segmentation accuracy was primarily due to an increase in the share of undersegmentation errors (63% in first batch and 84% in second batch, p = 0.003). Conversely, oversegmentation errors significantly reduced (37% in the first batch and 16% in the second batch, p = 0.003). The decline in oversegmentation suggests that supplementary training helped to better distinguish pancreatic anatomy from subadjacent iso-attenuating structures. However, they likely overcompensated for errors by undersegmenting pancreas at its interface with other organs. Accurate delineation of pancreas margins in areas such as near the duodenal groove can be a challenge even for radiologists. Secondly, our training material and approach could have been inadequate. In the future, improved training modules, more frequent training sessions, assessments over a longer period, and, possibly, a more individualized training approach could result in incremental performance improvement. It may not be reasonable to expect that technologists' segmentations or labels could be surrogates for that by radiologists. Instead, trained technologists could increase the efficiency of image annotation projects by creating weak labels, which could be used for weakly supervised learning or could subsequently be improvised upon by radiologists [26] . Trained technologists could also augment project pipelines through a review and revision of annotations initially performed by trained AI models. Finally, such a trained group of technologists can be redeployed towards the development of institutional body imaging datasets during both routine instances of scanner downtimes and during extraordinary decline in clinical imaging volumes as was experienced by our institution during our voluntary COVID-19 containment phase. Our project had limitations. The number and composition of CT scans for this project were based on the ready availability of a curated dataset rather than on statistical considerations. The duration of both initial and supplementary training was relatively short. We also evaluated results for all technologists as a group and could not assess the impact of training on individual performance. We were also unable to capture the time taken per segmentation because these segmentations had been done during the course of the clinical assignment rather than in controlled research settings. In summary, trained technologists had a good performance for volumetric pancreas segmentation on CT scans despite complexity of the segmentation task and justified our upfront investment in their training. Such trained technologists could provide a viable option for the development of labeled datasets for body imaging AI applications. Alternately, they could augment efforts of body radiologists in such development endeavors. The logistics of their engagement will be determined by a given institution's preferences and dynamics of the workplace. There is a need for cloudbased image annotation platforms, validated curriculums, and structured training modules to fully realize the potential of technologists for annotation tasks on body cross-sectional imaging. Investment into these resources could yield a trained workforce that could be gainfully redeployed during routine downtimes as well as during extraordinary circumstances such as COVID-19 containment phase. 
paper_id= ffed51538e59e779838ead9f15f10d906791b4bd	title= MEDICAL MANAGEMENT OF HIV DISEASE IN CHILDREN	authors= Marcel0  Laufer;Gwendolyn B Scott;	abstract= 	body_text= perinatal transmission of HIV. These women and their families should be counseled about the schedule of care visits; diagnostic testing for HIV; and chemoprophylaxis with zidovudine (ZDV) and trimethoprim-sulfamethoxazole (TMP-SMX), which are recommended for at-risk infants ( Table 1 ).2 l9 A DNA polymerase chain reaction for the detection of HIV is recommended before hospital discharge. Diagnostic tests for HIV in the infants detects 30% to 50% of infected infants at birth, with more than 95% diagnosed by 2 months of age, so physicians should follow up these infants closely during the first several months of life. Their mothers should be counseled against breast-feeding their infants because of the additional risk for transmission of HIV to 65 At the authors' institution, the first postnatal visit is done at 14 days of life. At this visit, compliance with the ZDV regimen is assessed, dosing is adjusted as needed, and refills given to ensure that infants have a 6-week supply of ZDV. Also, feeding practices and nutrition'are discussed. A complete blood count is recommended to monitor for hematologic toxicity, most commonly, anemia or leukopenia. If the HIV DNA PCR was not done at birth, it is performed at this visit. During the second visit, at 6 weeks of age, ZDV is discontinued if the HIV DNA PCR tests have been negative. At this visit, complete blood count, T-cell subsets, and DNA HIV PCR results are obtained. TMP-SMX is begun at a dose of 150 mg/m2/ dose given three times a week for prophylaxis against Pneumocystis curinii pneumonia (PCP), and the first set of immunizations is given. PCP prophylaxis is started empirically in at-risk infants less than 1 year of age, independent of the CD4+ cell count and percentage, because HIV status may be uncertain and, in infected infants with rapid pronression of disease, CD4+ cell counts can decrease precipitously in thifirsi m k t h s of life, predisposing to PCP.rn, 74 At-risk infants are seen at 4 and 6 months of age for routine care and immunizations and appropriate immune and diagnostic tests. When an at-risk infant has had two negative HIV DNA PCR test results after 2 months of age and is clinically and immunologically normal, PCP prophylaxis is discontinued. These children are considered presumptively HIV negative, but an HIV enzymelinked immunosorbent assay test is performed after 1 year of age to monitor for the disappearance of maternal antibody. If the HIV enzyme-linked immunosorbent assay and western blot are negative, the ELISA test is repeated and, if the result is negative, the child is considered uninfected and a seroreverter. Because many infants are born to HIV-infected women receiving combination antiretroviral therapy, physicians should carefully note drug exposure on these infants' charts and maintain follow-up on them as long as possible to determine any long-term effects of prenatal or postnatal exposure to these drugs. Also, any congenital anomalies or unusual illnesses occurring in these infants should be  A systematic approach to children with HIV infection is essential. These children should be assessed for symptoms related to HIV and the need for treatment and prophylaxis of opportunistic infections and other HIV-related conditions. Baseline laboratory tests should be performed to assess viral and immunologic status. A complete medical and immunization history should be obtained, with particular emphasis on the mode of transmission, exposure to antiretroviral agents during gestation and after delivery, timing of diagnosis of HIV, and family members who are aware of the diagnosis. The level of understanding of HIV should be assessed in children and their caregivers. If clinical trials are available, these should be discussed with these children and their families. Children with HIV should receive routine pediatric care and monitoring of their HIV disease status. Experts recommend that HIV-infected children have consultation with an HIV specialist. In some cases, pediatricians provide routine care, with referral to an HIV specialist for monitoring of HIV status, whereas in other care settings, HIV specialists provide primary and specialty care. HIVinfected children should be seen at least every 3 months. At each visit, a complete physical examination should be done, with attention to signs and symptoms commonly associated with HIV infection. Growth and development should be evaluated at all stages of development through adolescence. The medications should be reviewed, doses adjusted for growth, and compliance assessed. Children should be assigned a classification (Tables 24), and laboratory studies done to monitor the immunologic and virologic status of the disease. HIV infection is a multisystemic illness. Common clinical manifestations of pediatric HIV infection include generalized lymphadenopathy, hepatomegaly, splenomegaly, oral candidiasis, parotitis, recurrent or persistent diarrhea, failure to thrive, and developmental delay. Multiple organ system involvement is common, and the following HIV-associated conditions have been found: cardiomyopathy, lymphoid interstitial pneumonitis, nephropathy, encephalopathy, enteropathy, hepatitis, and malignancies. Skin manifestations caused by bacterial, viral, and fungal infections; atopic disease; hypersensitivity reactions; and pruritic papular reactions are common. Table 5 outlines the clinical and laboratory studies and assessments that should be done at baseline and thereafter at periodic intervals. These studies are done to identify any am;e problems and to assess the risk for opportunistic infections or other HIV-related complications and monitoring for medication side effects. Studies have shown that prognosis in children is related independently to viral burden and immune ~tatus.5~ Also, a direct relationship exists between the development of certain opportunistic infections and the degree of immune suppression. 18 HIV-infected children should have baseline antibody titers obtained for toxoplasma, cytomegalovirus (CMV), Epstein-Barr virus, varicella-zoster virus, herpes simplex virus (HSV), and hepatitis viruses. The results provide information about these children's exposure and susceptibility to specific infections. For example, if a CMV-seronegative child requires a blood transfusion, then a CMVnegative donor blood is requested or, if unavailable, leukofiltered blood is used. Each year, the following tests and services should be performed for HIVinfected children. Chest radiography. This test identifies mediastinal enlargement, lung lesions, lymphoid interstitial pneumonitis (LIP), and cardiomegaly. Patients with chronic lung disease should have oxygen saturation readings performed at every visit. Baseline plain brain CT. This scan may show changes such as basal ganglia calcifications and brain atrophy.12 A regular-strength tuberculin skin test should be done yearly if a child lives in an area endemic for tuberculosis, because HIV-infected children are at increased risk for tuberculosis. As a control for anergy, candida, mumps, or tetanus toxoid is used, depending on a child's age and immunization status. Visual screening on children capable of cooperating with the examiner. Ophthalmology examination is performed each year, but children with immune category 3 need to be examined by an ophthalmologist every 6 months, especially if they are seropositive for toxoplasmosis or CMV, because they are at high risk for retinitis. Psychometric testing of children with suspected developmental delay or loss of acquired milestones to identify problems that can be treated ea1-ly.2~3 Gynecologic examination. Female adolescents should be referred to adolescent gynecologists or adolescent services. Baseline and annual gynecologic visits should be provided for female adolescents who are sexually active for cervical smears and screening for sexually transmitted diseases. All adolescents should be aware of their diagnoses and receive counseling regarding transmission of HIV, safe sex practices, birth control, the risk Referral to other specialty services depends on the medical status of patients. Ideally, specialists that participate in the care of these children should be aware of the problems affecting HIV-infected children and maintain communication with the primary care physician. Because the care of HIV-infected children is complex, these children's caregivers should have ready access to health care providers to discuss medical problems. A coordinated family-centered comprehensive care team of physicians, nurses, case managers, and social workers facilitates the treatment of these patients and the work of the caregiver. In the authors' program, a case manager or social worker is assigned to each patient to provide community referrals for social services and psychosocial interventions. Home health agencies provide administration of parenteral medication at home, as well as instruction and supervision regarding the medications. A foster-care system is essential to meet the needs of at-risk infants who are abandoned or under protective custody and for HIV-infected children who have lost a parent or caregiver. Prescribed day care that provides nursing services is important for the care of severely ill, medically complex children with HIV. Assessment of growth is an integral part of the care of any pediatric patient. HIV-infected children should have careful monitoring of height, weight, and head circumference during infancy and childhood and evaluation of pubertal changes and growth during adolescence. Several measures of growth can be used in infants and children and include standardized growth charts, incremental growth curves, and computerized calculation of Z scores using the Epi Info Version 6 (Centers for Disease Control and Prevention). Growth delay is common in HIV-infected infants and d a y be the first sign of symptomatic HIV infection. At the other end of the spectrum of disease, weight loss and wasting may occur in older children and adolescents who have progression of HIV disease. The basal metabolism of children with HnT infection is increased compared with noninfected children, and when stressed, caloric needs increase, on average, 12% for each degree Celaus increase in temperature, 25% for acute diarrhea and 60% for sepsis.6, 57, 74 For children who have growth delay or wasting, an evaluation should be done to determine potential causes. The goal is to treat the cause of the growth failure or weight loss and estimate energy needs for children to grow. The following formula can be used to estimate the energy needs of children who need a catch-up growth period? (RDA Kcal for weight age) x (ideal weight for age) actual weight Kcal/ kg = where RDA is the recommended daily allowance, and weight age is the weight at which a patient's present weight would be at the 50th percentile. Useful laboratory parameters for the assessment of nutritional status include albumin (to assess long-term visceral protein), prealbumin (to assess short-term visceral protein), iron, vitamin B,, and folate studies, and micronutrients (i.e., zinc, magnesium, selenium, copper, carnitine, and other vitamin levels). The management of nutritionally deficient children may include one or more of the following interventions: dietary instruction, vitamin supplementation, high-calorie formulas or alimentation using a nasogastric tube, or intravenous (IV) hyperalimentation. Nutritionists are invaluable in assisting the medical team with a dietary history, energy intake, and recommending appropriate interventions. Several high-energy nutritional supplements are available for use in the pediatric population. These formulas contain 4.2 to 6.3 Kcal/mL (1.0-1.5 Kcal/ mL) but differ in their carbohydrate, fat, and protein contents; palatability; electrolytes; and solute load. Patients and their families should understand that supplements are not designed to be the only source of nutrition for patients. If a child has a poor appetite or refuses alimentation, and when medical causes, such as oral ulcers or esophagitis, have been ruled out, physicians have two options: (1) tube feeding or (2) appetite stimulants. Nasogastric tube feedings are easy from a technical standpoint but may increase the risk for sinus infection and gastroesophageal reflux and, in the older children, may affect selfesteem. Gastrostomy tubes, although they require surgical placement, are easy to manage and do not pose a cosmetic problem or increase the risk for sinusitis,5s but in the authors' experience, such tubes should be placed before significant deterioration of the immune and nutritional status occurs because wound healing may be compromised. Two immunosuppressed children cared for at the authors' institution had delayed healing of the tissue around the gastrostomy site and required months of supportive therapy until it finally closed spontaneously. The authors recommend starting with 8-hour to 12-hour infusions of formula overnight at low rates and slowly increasing the volume. If vomiting develops, the tube may be advanced to the jejunum, bypassing the stomach. Megestrol acetate is an orally administered, synthetic, progestational agent initially used to increase the appetite of patients with cancer. This product promotes weight gain but has the disadvantage of increasing fat deposition instead of lean body mass, thus altering the body habitus. An uncontrolled study of megestrol acetate in a small number of HIV-infected children demonstrated weight gain in most, with few other side effects.",The usual dosage is 8 to 10 mg/kg/d, divided into two doses. Recombinant growth hormone has increased lean body mass in adults with HIV infection. Anemia in patients with HIV is multifactorial. The most common causes include nutritional deficiencies; iron; folic acid; and, less commonly, vitamin Bli immune hemolysis; hemorrhage; drug toxicities; and bone marrow suppression caused by the HIV virus, other infectious agents, or malignancy. The workup of anemic patients includes a complete blood count and reticulocyte count, iron level, total iron-binding capacity, transferrin level, vitamin B,, level, folic acid level, and erythropoietin level. Sometimes a lead level is indicated. If hemolysis is suspected, a Coombs test is performed and the haptoglobin level is obtained. An algorithm for the management of anemia is presented in Figure 1 . Anemia with few or absent reticulocytes is common. When secondary to iron deficiency, iron sulfate at 6 mg/kg/d of elemental iron is usually sufficient to correct the hemoglobin level, but sometimes the solution is not that easy. If the erythropoietin level is less than 500 IU/L, a trial of erythropoietin at 100 U/kg/ dose, three times a week, is suggested. A response usually occurs after 2 or 3 weeks of therapy. If the response is unsatisfactory, a higher dose may be given. The usual side effects are bone pain and polycythemia. Patients who are receiving erythropoietin should always receive iron supplementation because of increased utilization of iron. On the other hand, if the erythropoietin is more than 500 IU / L, no evidence of iron, vitamin BI2 or folic acid deficiency is present, and the patient is reticulocytopenic, alternative diagnoses need to be investigated. Parvovirus B-19 infection can cause an aplastic anemia with a low or absent reticulocyte count and normal or high erythropoietin levels. If this infection is confirmed, IV immunoglobulin (IVIG), 1 g/kg/dose for 5 to 10 days, should be administered. The response is usually rapid and results in an increase in reticulocytes and hemoglobin levels and no further need of transfusion. This initial therapy is followed by monthly doses of M G at 400 mg/kg/dose. In the authors' limited experience, if IVIG is discontinued, a relapse of the aplastic anemia may occur. A case report of parvovirus B-19 anemia in an antiretroviral therapy-na'ive patient showed resolution of the anemia with the initiation of HAART. Another common opportunistic infection that causes bone marrow suppres- Figure 1 . sion and anemia is disseminated infection with Mycobacterium avium complex (MAC) infection. When a specific cause for anemia cannot be elucidated, a bone marrow aspiration may be necessary. Megaloblastic changes in the erythrocytes may result from antiretroviral therapy (i.e., ZDV, stavudine, and zalcitabine). Macrocytosis develops within weeks of commencing therapy with some antiretrovirals to a mean corpuscular volume of 110 cells/mL. Levels of vitamin BI2 and folic acid are usually within normal limits. In a few instances an intervention is needed, but if anemia is associated with the antiretroviral use, the options are to decrease the dose of the antiretroviral agent causing the anemia, or eliminating the drug and changing the treatment regimen. Megaloblastic anemia can also be caused by malabsorption or poor nutrition, and if the levels of folic acid and vitamin B,, are diminished, &e anemia must be re-evaluated after appropriate supplements are provided. Anemia with an increased reticulocyte count can be secondary to hemorrhage or hemolysis. A decreased level of haptoglobin characterizes hemolysis. Treatment of this type of anemia is cause-specific and is beyond the scope of this article. Neutropenia is defined as an absolute neutrophil count of less than 1500 cells/mL. This is a common finding in HIV-infected children and may occur secondary to infection, nutritional deficiencies, or drug toxicity. Some drugs used to treat HIV infection, such as ZDV and TMP-SMX, are associated with the development of neutropenia. In some cases, neutropenia responds to initiation or a change of antiretroviral therapy. If the absolute neutrophil count is less than 500 cells/mL, a trial of granulocyte colony-stimulating factor starting at 5 to 10 &kg/dose is begun. Granulocyte colony-stimulating factor is a cytokine that increases circulating neutrophils and improves their functioning. Unlike erythropoietin, the response to granulocyte colony-stimulating factor is immediate, with demargination of the leukocytes, and the dose should be titrated to maintain an absolute neutrophil count of more than 1500 cells/mL.= Thrombocytopenia is a common problem among HIV-infected children. It may occur in the absence of other symptoms and, in some patients, is the initial presenting illness. Although a specific cause is usually not found, it should be inve~tigated.~~ Antibodies against platelet glycoproteins have been identified in some HIV-infected patients with thrombocytopenia." Intervention is not required, especially if the platelet count exceeds 50,000 platelets/mL. In some cases, the thrombocytopenia has responded to the initiation or change of antiretroviral therapy. ZDV has been shown to be beneficial in the treatment of patients with thrombocytopenia.38 The role of other antiretroviral agents is less clear. When the platelet count is less than 20,000 platelets/mL or when bleeding is present, treatment should be instituted. Platelets are shortlived cells, and transfused platelets have even a shorter life span. The use of platelet transfusions is reserved for patients who are actively bleeding. IVIG has been beneficial in the treatment of immune thrombocytopenia. A total of 2 g/ kg is administered over 2 to 4 days. If a response to the WIG occurs, then this is given at 3-week or 4-week intervals.lO, 74 If a patient has maintained a normal platelet count for 6 to 12 months, the interval between doses is increased, and the treatment is eventually discontinued. RHo(D) immune globulin intravenous (WinRho SDF) is an alternative to WIG, but hemolysis may result and is a significant side effect. Corticosteroids are another alternative treatment. Prednisone is usually started at a dose of 2 mg/ kgl d for 2 to 4 weeks and then tapered and discontinued, but the thrombocytopenia may relapse when the steroids have been discontinued. Interferon alfa is reported to be effective in patients with severe thrombocytopenia that is resistant to antiretroviral therapy. Splenectomy is an option to consider if other treatments fail and usually results in the resolution of thrombocytopenia, but the lack of a spleen will predispose these children to encapsulated bacterial infections.55, 6 . 1 If splenectomy is considered, these children should be immunized with pneumococcal, Haemophilus influenzae type B, and meningococcal vaccines. A new approach is the use of recombinant thrombopoietin. Harker et a1% experimentally treated three chimpanzees infected with HIV and thrombocytopenia with pegylated recombinant human megakaryocyte growth and development factor. The chimpanzees had a tenfold increase in platelet number, 30-fold increase in marrow megakaryocyte numbers, and fourfold increase in marrow megakaryocyte progenitor cells. On the other hand, Cole et a P found that thrombocytopenia in HN-infected patients was caused by a shortening of platelet life span, increased splenic sequestration, and ineffective delivery of viable platelets and that levels of thrombopoietin were significantly higher than in control subjectsz3 The eventual role of this agent in the treatment of thrombocytopenia in children or adults infected with HIV is unclear. The occurrence of ulcers in the oral cavity presents a diagnostic dilemma because viral infections, such as HSV, CMV, and Coxsackie A infection, as well as bacteria, may be causative agents. Biopsy and culture of lesions that are persistent or increasing in size or number is recommended. Cultures are tested for viral, bacterial (aerobic and anaerobic cultures), mycobacterial, and fungal infection. If results are negative and the biopsy is not diagnostic, the authors assume that these are aphthous ulcerations and treat patients accordingly. Aphthous lesions may occur in the posterior pharynx, esophagus, perirectal area, and vulva. The treatment of these ulcers is by trial and error. Aphthous ulcers typically occur in severely immunosuppressed patients. For the treatment of oral aphthae, a mixture of viscous lidocaine, antacids (Maalox), diphenhydramine, and mycostatin to swish and swallow may relieve the symptoms. An alternative treatment is the use of clindamycin and steroids. Perirectal or vulvar lesions commonly respond to a mixture of equal parts of Aquaphor (Beiersdorf, S. Norwalk, CT) and cholestyramine, In one case at the authors' institution, fever, weight loss, and oral ulcers developed in an immunosuppressed 15-year old male adolescent. A culture grew CMV, and the patient responded well to treatment with ganciclovir, with a resolution of symptoms. Other alternatives are cimetidine, chlorhexidine, systemic steroids, and thalidomide.34* 39, Most cases of esophagitis are diagnosed by clinical symptoms and confirmed with a barium swallow study. In such cases, treatment is begun empirically for candida esophagitis, especially in the presence of oral candidiasis. Fluconazole is the drug of choice and is given at a dose of 6 mg/kg/d in one daily dose. The choice of inpatient versus outpatient treatment depends on the hydration and clinical condition of the patient and the reliability of the caregiver. If a patient does not respond to empiric treatment after 3 days, then endoscopy with biopsy and culture of the esophageal mucosa should be done. If the biopsy results are compatible with fungal infection, parenteral amphotericin B is the drug of choice because some Candida albicans and Torulopsis glabrata and Candida krusei are resistant to fluconazole. Patients who receive recurrent courses or "prophylactic" regimens of fluconazole are at risk for resistant infections. In difficult-to-treat cases, therapy should be guided by antifungal susceptibility. A few children with recurrent severe oral thrush have developed resistance to most antifungals and are managed at the authors' center with IV amphotericin B infusions two or three times a week to control the pain and infection. Esophagitis secondary to HSV is treated with IV acyclovir, 750 mg/m2/d divided into three doses. If CMV is the cause, then IV ganciclovir is used. Sometimes the cause of esophagitis is uncertain, even after biopsy. The management of these patients is-controversial, and the same options discussed for treatment of aphthous ulcers apply. Patients with recurrent esophagitis are at risk for esophageal stenosis. Development of chronic bilateral enlargement of the parotid glands may occur in as many as 15% of children with HIV infection and usually does not require any treatment, but sometimes parotid pain and fever develop in these patients. In this instance, infection is suspected, frequently with Staphylococcus aureus, although mouth flora may also be present. This complication is easily managed with a course of oral antibiotics, using antistaphylococcal agents. On the rare occasion when suppuration is suspected, an otolaryngologist should be consulted to evaluate the need for surgical treatment. Lymphomas and other tumors should be included in the differential diagnosis, especially in children with increasing parotid sue. Diarrhea is a common manifestation of HIV infection." Although an infectious cause is frequently suspected, other important causes include side effects of medications, malignancies, and HIV enteropathy. Antibacterial agents are indicated in the treatment of diarrhea associated with enteric pathogens, such as shigellae, salmonellae, and campylobacter?, Adult patients commonly improve with fluoroquinolones, but in children, other alternatives (e.g., ampicillin, third-generation cephalosporins, or TMP-SMX) are used. HIV-infected patients, especially those with low CD4 + cell counts and frequent exposure to antibiotics, are at risk for diarrhea caused by Clostridium dificile. The treatment of choice is metronidazole with orally administered vancomycin used as an alternative drug. Relapse should not be considered a treatment failure because as many as 20% of patients relapse, and these patients should be retreated with metronidazole. Many viruses, including rotavirus, calicivirus, adenovirus, coronavirus, and astrovirus, can cause diarrhea. Specific treatment is available only for colitis associated with CMV (ganciclovir) or HSV (acyclovir). The management of parasitic and mycobacterial infections is discussed in the section on opportunistic infections. When pathogens cannot be identified, a consultation with a pediatric gastroenterologist is indicated for possible endoscopy and biopsy. Children who are immunosuppressed and have high viral loads benefit from HAART. Antiretrovirals which may cause diarrhea. In one series, 41% of patients receiving a protease inhibitor had diarrhea. Patients in whom a specific cause of diarrhea is identified may benefit from a lactose-free diet. Abdominal distension with or without diarrhea is another common finding in HIV-infected children. In children without diarrhea, enlargement of the liver, spleen, or both may account for the distension. In patients with diarrhea, bacterial overgrowth of the upper gastrointestinal tract should be suspected. The distension worsens after meals and the breath test is abnormal, even before challenge with carbohydrates. Cultures of the gastric and duodenal fluid commonly are positive, and the stomach pH has decreased acidity. Treatment consists of metronidazole, 30 mg/kg/d, divided into 3 or 4 doses, for 10 days. Some experts suggest using lactobacillus to restore normal bowel flora. Lymphoid interstitial pneumonitis is a chronic lymphocytic infiltrative disease of the lung5r40 The diagnosis of LIP in children is usually based on a typical chest radiograph with persistent reticulondular bilateral infiltrates. In more severe cases, significant clubbing occurs. In asymptomatic children with LIP and normal oxygen saturation, a chest radiograph should be obtained at least once a year, and the oxygen saturation should be monitored using pulse oxymetry at each clinic visit. If a patient is hypoxic, steroids are indicated. Prednisone is given at a dose of 2 mg/kg/d for 2 to 4 weeks, with subsequent tapering to 1 mg/ kg/d, and therapy is continued until the oxygen saturation becomes normal. Most children respond to this treatment within the first few weeks. In general, when an adequate response has occurred, the steroids may be weaned and discontinued. In some cases, repeated courses of steroids may be warranted. A few children with severe, advanced lung disease may not respond to steroid therapy, and if no response occurs after 4 to 6 months, the steroids should be discontinued. In addition, bronchodilators and chest physical therapy are helpful adjunctive therapies. The use of low doses of diuretics may also improve the respiratory status." When dealing with suspected bacterial pneumonia, an aggressive approach should be taken in patients with chronic lung disease. When possible, the treatment should be directed to the specific pathogen causing the infection. Treatment is usually empiric because noninvasive diagnostic procedures, such as blood and sputum cultures, yield an organism in approximately 30% of cases. Therapy is directed at the usual pathogens (i.e., Streptococcus pneumoniae, H. infuenzae, and Staphylococcus aureus), but in patients with chronic lung disease, gram-negative organisms, especially P. aeruginosa, must be considered potential pathogens. The authors have had good experience with the combination ticarcillin-clavulanic acid. The addition of aminoglycosides should be considered in immunocompromised patients and in those infected with resistant strains of gram-negative rods. Aminoglycosides can be delivered by nebulization (tobramycin, 80-160 mg three times a day) to decrease the risk for nephrotoxicity and improve delivery to the affected area. If a patient does not improve or his or her condition deteriorates on treatment, a bronchoalveolar lavage is indicated for diagnosis. The length of therapy is dependent on the cause and severity of the illness. Although the authors usually recommend 10 to 14 days of systemic antibiotic therapy, some children benefit from a longer course of treatment. In patients with chronic lung disease, pulmonary function tests may be beneficial at the onset of treatment and can be used as an indicator for length of therapy. If the pulmonary function tests continue to improve after 14 days of antibiotic therapy, treatment is continued until a return to baseline or a new plateau is reached. Children with HIV infection who have bronchiectasis or frequent episodes of bacterial pneumonia may benefit from daily prophylaxis with TMP-SMX. If this is not successful in decreasing +e frequency of infection, monthly infusions of IVIG at 400 mg/kg/d may be beneficial. HIV-associated nephropathy in children presents as a spectrum of disease that ranges from mild to moderate proteinuria that is persistent, hematuria, renal tubular acidosis, and end-stage renal disease (ESRD). Because a timed urine collection is difficult to perform in children, the authors use the ratio of urine creatinine and urine protein to calculate the degree of proteinuria. A normal creatine-protein ratio is less than 0.2. A ratio more than 0.2 is abnormal and is consistent with nephrotic range proteinuria.' If this finding persists, it can lead to hypoalbuminemia and edema but without elevation of serum triglycerides as occurs in patients with idiopathic nephrotic syndrome. Renal function deteriorates more slowly in children than in adults. Renal sonography shows increased echogenicity an$ provides details of the b c t i o n of the kidneys. Biopsy of the kidney usually shows focal segmental glomerulosclerosis or diffuse mesangial hyperplasia and, less frequently, minimal change or systemic lupus erythematous-like nephropathy. Renal tubular acidosis is corrected with alkalinizing agents. The authors use sodium or potassium citrate, depending on other electrolyte abnormalities. One milli-equivalent of citrate is equivalent to one milli-equivalent of bicarbonate. The authors start with 2 to 3 mEq/kg/d divided in 2 or 3 doses and adjust the dose to maintain a normal serum Ph. Patients with renal tubular acidosis may also need supplements of other minerals, such as calcium, magnesium, and phosphorus. Therapy for patients with proteinuria is divided in two categories: (1) treatment of incipient renal disease to slow the progression to ESRD and (2) the treatment of ESRD. Because the progression of disease in children is not as rapid as in adults, the first approach is to observe these patients over a period of time, monitoring electrolytes, blood urea nitrogen (BUN), and creatinine. Drugs commonly used in HIV Infection that need dose modification in patients with renal impairment include: The use of nephrotoxic agents should be avoided. Since the pathogenesis of the disease has been linked to immune complex deposition, an initial approach has been to use immunosuppressive agents. Steroids represent another option. Several studies suggest that glucocorticoids might slow the progression of HIVassociated nephropathy in adults.63, 69 The use of steroids in children has not been as beneficial as in adults. Diabetic patients with proteinuria benefit from treatment with angiotensinconverting enzyme inhibitors. Kimmel et ar6, 47 compared the progression of renal insufficiency with ESRD in 18 patients with biopsy-proven HIV-associated nephropathy. Nine patients were treated with captopril, and nine patients did not receive an angiotensin-converting enzyme inhibitor. The mean survival of renal function in the captopril group was 156 2 71 days versus 37 days in the control group. Gorriz et a132 described a 36-year-old white homosexual man with HIV-1 infection with nephrotic range proteinuria who was successfully treated with captopril,32 but because proteinuria may persist for long periods without progression in pediatric patients, the authors usually elect to follow these patients without specific treatment as long as they remain asymptomatic. As with every other manifestation of HIV infection, HAART may have a sigruficant role in the prevention or treatment of nephr~pathy.~~ When ESRD has developed, a decision regarding dialysis must be made. The availability of HAART and the improved prognosis and extended survival time mandates an aggressive approach. At the University of Miami, several children with HIV-associated nephropathy have received long-term peritoneal dialysis and are managed in conjunction with the pediatric nephrology team. Cardiac involvement in AIDS was first reported in 1983 in a woman who had Kaposi's sarcoma involving the anterior cardiac wall. Since then, multiple reports have discussed the incidence, etiology, manifestations, and management of cardiomyopathy.7, 8, 51 The spectrum of cardiac disease ranges from clinically silent lesions to fatal disease, and the severity of disease correlates with the degree of immune suppression. Cardiomyopathy seems to affect survival in HIV-infected children.5l. 56 Autopsy studies in HIV-infected adults indicate that as many as 25% have dilated cardiomyopathy and as many as 52% had myocarditis at death? Depressed left ventricular function is common and progressive in children. Serial studies of 88 children with advanced symptomatic HIV infection showed that 21% had decreased left ventricular function and 34% had ventricular dilatation.@ Every structure of the heart can be affe~ted.4~. 51, 52, 56, 75 Cardiac lesions described in patients with HIV include pericardial effusions; pericarditis; myocarditis; dilated cardiomyopathy; endocarditis; and vascular lesions, such as aneurysms, atherosclerosis, and pulmonary hypertension. Infiltrative neoplasm (i.e., Kaposi's sarcoma and lymphoma) has also been reported. The cause of cardiomyopathy is multifactorial. HIV has been found in the heart, but whether its role in pathogenesis is related to a direct effect on the myocardium or secondary to the immune response is a matter of controversy. Other factors that could be involved in the pathogenesis of cardiomyopathy include infections such as Coxsackie B virus, CMV, Epstein-Barr virus, toxoplasma and other opportunistic infections, pulmonary disease, wasting, nutritional deficiencies (i.e., selenium and camipharmacotherapeutic agents (i.e., antiretrovirals, pentamidine, amphotericin B, foscarnet, interferon alfa, TMP-SMX, or steroids), and illicit drug use.4 13, 21 HIV cardiomyopathy starts early in life. In a multicenter study of lung and cardiac pathology in HIV-infected patients, fetal echocardiography was performed in 174 fetuses of HIV-infected mothers. Fetuses of HIV-infected mothers had increased right and left ventricular wall thickness, decreased heart rates, and increased right and left ventricular outflow velocities. The postnatal phase of the study showed increased prevalence of structural heart disease (12.3%), but no significant difference in the prevalence of congenital cardiovascular malformations was found among HIV-infected and noninfected ~hildren.4~. 51 Other studies showed a lower prevalence. The routine use of echocardiograms in children with HIV greatly influences the reported prevalence of HIV cardiomyopathy. The most common clinical manifestation of cardiomyopathy is sinus tachy-~ardia.~l Other associated manifestations include bradycardia, dysrhythmias, abnormal blood pressure (i.e., hypotension or hypertension), left ventricular hypertrophy, pulmonary hypertension, pericardial effusion, and congestive heart failure (CHF). Evaluation of cardiac status includes a thorough history and physical examination followed by specific diagnostic tests when clinically indicated. Initial testing should include chest radiography and ECG. Chest radiography is useful in evaluating heart size and the presence of pulmonary disease (e.g., LIP, pneumonia, or pleural effusions) and other intrathoracic abnormalities. When patients are examined by ECG or during autopsy, cardiac abnormalities are detected more often than expected from physical examination. This raises the question of whether an ECG should be part of the routine examination of HIV-infected children, especially those with low CD4 + cell counts. L i p s h~l t z~~ prospectively followed up 196 HIV-infected children with baseline ECG at enrollment and every 4 months thereafter for 2 years. He found that subclinical cardiac abnormalities were not only common but also persistent and often progressive. Similar data are available on HIV-infected adults. Although introducing ECG as part of the routine care of HIV-infected children implies a significant cost and an additional workload, a periodic ECG assessment may be beneficial. Some centers recommend annual ECG for asymptomatic patients and every 8 months for symptomatic patients.60 Other tests used in the assessment of cardiomyopathy include Holter monitoring and, rarely, cardiac catheterization and endomyocardial biopsy. Noncardiac predictors of hemodynamic abnormalities are AIDS, wasting, LIP, recurrent bacterial infections, encephalopathy, anemia, positive CMV and Epstein-Barr virus serology, and age of less than 1 year. Treatment of HIV cardiomyopathy starts with early detection. Specific therapy of infectious causes of HIV-associated heart disease, such as CMV, Mycobacterium avium complex disease, tuberculosis, toxoplasmosis, and salmonellosis, have significant effect on outcome. Problems such as anemia, nutritional deficiency, hypoxemia, and electrolyte deficiencies need correction. Nonpharmaceutical therapeutic measures include limitation of physical activity, restriction of salt and fluid intake, and oxygen support.30 Pharmacologic treatment should be done in steps. The mildest manifestations of CHF are treated with angiotensin-converting enzyme inhibitors. The two most commonly used agents are enalapril and captopril. Most clinicians favor enalapril because of the advantage of once-or twice-a-day administration. Although the manufacturer does not specify a pediatric dose, two studies started patients on 0.1 mg/kg/d and titrated upward as tolerated to a maximum of 0.5 mg/kg/d. Renal function, serum electrolytes, and blood pressure must be monitored closely when using this class of drug. The next step is diuretics. Loop diuretics (e.g., furosemide) are preferred because thiazides are less potent and their efficiency decreases with decreased glomerular filtration rate. Patients who remain symptomatic despite these measures may benefit from the use of digoxin. Antihypertensive and antiarrhythmic agents are prescribed when appropriate in conjunction with a pediatric cardiologist. IVIG has been reported to be effective in patients with Kawasaki syndrome, CHF refractory to anticongestive therapy, and acute myocarditis. In one study, HIV-infected chldren on monthly infusions of IVIG seemed to have better left ventricular function and ~tructure.5~ Anecdotal reports recommend the use of steroids and other immune suppressor agents in the management of patients with cardiomyopathy. Interferon alfa has also been associated with improvement of myocardial dysfunction in HIV-infected patientsz6 Because pericardial effusion is usually an incidental ECG finding, treatment depends on its severity and cause. When tamponade is present, pericardiocentesis is necessary. Pericardiocentesis is also indicated when bacterial infection is suspected. Nonsteroidal anti-inflammatory agents are useful in the management of patients with pain." Steroids are not recommended. Another issue is the management of patients with hypercholesterolemia and hypertriglyceridemia secondary to the use of protease inhibitor~.'~ In adults, the issue of whether the protease inhibitors cause coronary artery disease is debated.42 So far in pediatrics, no early coronary artery disease has been reported, but hyperlipidemia, lipodystrophy, insulin-resistance diabetes mellitus, and coronary artery disease have been reported in adults. The recommendation for the treatment of these side effects is modification of the diet and pharmacologic treatment with a lipid-lowering agent, if necessary. Some experts recommend the discontinuation of protease inhibitors if these measures are unsuccess-fu1.31, 36, 37, 41, 48 Progressive and static encephalopathy with cognitive, behavioral, and motor manifestations has been described in HIV-infected children.24* 29* 61 Although most children with neurologic impairment have no identifiable pathogen other than HIV, encephalopathy may occur as a result of opportunistic infections, inflammatory disease, vascular disease, or neoplastic changes. Evidence shows that HIV infection of the CNS is associated with typical neuropathologic changesJ8 According to the Centers for Disease Control and Prevention (CDC) Revised Classification System,16 the diagnosis of encephalopathy requires one of the following progressive findings present for at least 2 months in the absence of other identifiable causes: Failure to attain or loss of developmental milestones or loss of intellectual ability, verified by standard developmental scale or neuropsychological tests Impaired brain growth or acquired microcephaly demonstrated by head circumference measurements, or brain atrophy demonstrated by CT or MR imaging, with serial imaging required in children less than 2 years of age Acquired symmetric motor deficit manifested by two or more of the following: paresis, pathologic reflexes, ataxia or gait disturbance Estimates of the prevalence of HIV-related encephalopathy in children are much lower than those reported in the first decade of the pediatric AIDS epidemic. The Woman Infant Transmission Study prospectively followed up 128 HIV-infected children. Of the 128 children, 27 (21%) were diagnosed with HIV encephalopathy. Encephalopathy was more common with advanced disease (89% of the children were category B or C, and 74% were category 2 or 3). The presence of encephalopathy increased the risk for death 28-fold. The presence of hepatomegaly, splenomegaly, or lymphadenopathy in the first 3 months of life and increased viral load was correlated with the presence of encephalopathy. A trend, although, not statistically significant, showed that children with a positive HIV culture in the first week of life were at higher risk for encephalopathy. Levels of HIV RNA in the cerebrospinal fluid may be an important predictor of encephalopathy. Encephalopathy in HIV-infected children can be secondary to HIV or other intercurrent illness, such as infection with other pathogens, neoplasm, and vascular or inflammatory disease. The workup of children with HIV and encephalopathy is as follows. 1. Evaluation by a pediatric neurologist 2. Developmental, audiology, and ophthalmologic evaluation 3. Imaging of the brain by CT or M R imaging. Cerebral atrophy, attenuation of white matter, and cerebral calcifications are the most common findings. The CT scan can easily detect calcifications, but MR imaging is superior in detecting white-matter abnormalities and atrophy. The value of monitoring HIV RNA viral load in cerebrospinal fluid is not yet well defined.67 5. Specific tests for expected pathogens (i.e., Cryptococcus neoformans, Toxoplasma gondii, Mycobacterium tuberculosis and nontuberculosis, CMV, HSV, varicella zoster virus, and Treponema pallidum) 6. Appropriate treatment of identifiable causes. If no cause other than HIV is identified, then the treatment goal is to reduce the viral load. The antiretrovirals with most penetration across the blood-brain barrier are ZDV, stavudine, and nevirapine. Although protease inhibitors may not have good CSF penetration because of their high protein binding, one small study using M R imaging brain studies demonstrated improvement or no progression in white matter disease in 89% of adult patients on protease inhibitors versus worsening in 86% of the patients no€ using these agents?8 Depending on the severity of disease, patients with encephalopathy need a strong support system. Physical therapy, braces, and even surgery are sometimes necessary to minimize contractures. Baclofen has a role in the treatment of hypertonicity. Seizures are treated with antiepileptic agents in conjunction with a neurologist. Because these patients may have a decreased appetite or may have problems swallowing, feeding tubes may be needed. HIV-infected children have a similar immunization schedule to that of HIVnoninfected children ( Table 6 ) except for a few variations that are discussed in this section. The administration of some vaccines has led to a transient increase in HIV viral load, but this effect does not contraindicate the use of vaccines.ll  A series of three doses of inactivated polio vaccine at 2, 4, and 6 to 18 months of age, followed by a booster dose at 4 to 6 years, is recommended. Children who have HIV infection, are at risk for infection, or reside in households with immunocompromised hosts should not receive oral polio vaccine because the vaccine virus is shed through the gastrointestinal tract and could cause vaccine-associated paralytic poliomyelitis in susceptible hosts. In the early years of the AIDS epidemic, infected children in the United States received primary and booster doses of oral polio vaccine without complications, and in developing countries, oral polio vaccine continues to be administered, but because inactivated polio vaccine is readily available and a theoretic risk for vaccine-associated paralytic poliomyelitis exists, this is the recommended vaccine in the United States. The measles-mumps-rubella vaccine is a live attenuated viral vaccine given between 12 and 15 months of age, with a booster dose at 4 to 6 years of age. In areas where a high prevalence of measles exists, monovalent measles vaccine is given between 6 and 12 months of age. In the early years of the AIDS epidemic, children with HIV who developed measles infection had a high risk for complications and death, suggesting that the benefits of administering this live viral vaccine outweighed the potential risks, but in one case report, a 21-year-old hemophiliac man infected with HIV was immunized with the measles-mumpsrubella vaccine, giant cell pneumonia developed 1 year later, and the patient died.17 The measles vaccine virus was isolated from postmortem specimens. Since this report, recommendations have been altered, and children with mild or moderate immunosuppression (i.e., immune category 1 or 2) should receive the measles vaccine, but children with severe immunosuppression (i.e., immune category 3) should be excluded. Whether a child with severe immunosuppression who responds to HAART with an increase in the CD4+ T-cell count and percentage corresponding to immune category 1 or 2 should be immunized with measles vaccine is a matter of controversy. At this time, the functional capacity of the CD4+ T cells that have been restored is uncertain. Until a reliable, quantitative test is available to define the function of these restored cells, immunization with the measles vaccine is not recommended in these situations. Also, the antibody response to measles vaccination in HIV-infected children may vary in amount and duration, depending on the clinical and immune status of these children, so in the case of exposure to measles, HIV-infected children should receive prophylaxis with immunoglobulin, preferably, within 72 hours after exposure. The dose recommended for children with symptomatic HIV infection is 0.5 mL/kg (maximum dose, 15 mL) and for asymptomatic children is 0.25 mL/ kg, given intramuscularly. Children who have received IVIG therapy within 3 weeks of exposure do not need additional prophylaxis. A yearly influenza vaccine is recommended for infants and children with HIV infection who are 6 months of age or older. Only the subvirion, that is, split-virus, vaccine should be used in children less than 13 years of age because this minimizes fever and other adverse side effects. Children less than 9 years of age who have not been immunized previously should receive two doses of the vaccine given 1 month apart. For children previously immunized with this vaccine, one dose is adequate. Physicians should review the official recommendations for influenza vaccine yearly because the composition of the vaccine varies and the dosage for children aged 3 years or less may be different than for older children. The vaccine should be given in early autumn, before the onset of the flu season. In addition, household contacts of high-risk children should also be immunized. The incidence of infection with Streptococcus pneumoniae has increased in HIV-infected children. In addition, these organisms have an increasing pattern of resistance to penicillin and the cephalosporins. The currently available polyvalent pneumococcal polysaccharide vaccine should be given to all HIV-infected children who are aged 2 years or older. A booster dose is recommended after 5 years. A new protein-conjugated heptavalent pneumococcal vaccine is in development that shows safety and immunogenicity and reduces the incidence of otitis media and invasive disease secondary to S. pneumoniae infection in normal infants and cluldren. Limited information on the use of these vaccines in HIV-infected children is available." Varicella infection has been associated with increased morbidity in children with HIV infection. In children with severe immunocompromization, recurrent herpes zoster virus or chronic varicella infection may develop. The Advisory Committee on Immunization Practices does not recommend administering the varicella vaccine to persons with cellular immune deficiencies, but because of the increased risk for morbidity from varicella and herpes zoster, the Advisory Committee on Immunization Practices recommends that the varicella vaccine be considered for children with asymptomatic or mildly symptomatic HIV infection, CDC classification N1 or Al, with an age-specific CD4+ T-lymphocyte percentage of 25% or more.2O The vaccine should be administered in two doses with a 3-month interval between doses for this group of children. This schedule differs from the vaccination schedule for healthy children. It is strongly recommended for HIV-negative siblings and children living in households with HIVinfected adults or children, but this vaccine should not be administered to children with HIV infection who have moderate or severe immunocompromization. Studies to evaluate the safety and tolerance of this vaccine in HIV-infected children are ongoing. In a studyto determine the safety and immunogenicity of the Oka strain varicella vaccine, 42 HIV-infected children aged 1 to 8 years (stage N1 and Al) who had no history of varicella infection and had negative titers received two doses of the vaccine separated by 3 months. No severe adverse effects were reported. Fever occurred in 20% of the recipients after the first dose and in 5% after the second dose. Local reactions were similar to those seen in healthy children. After eight exposures, only one mild case of varicella occurred. Varicella-zoster virus antibody and specific lymphocyte proliferation were respectively detected in 48% and 63% of patients after the first and 56% and 82% of patients after the second dose.= Susceptible children with HIV infection exposed to varicella should receive passive immunization with varicella-zoster immunoglobulin given within 96 hours of exposure. If a child has received IVIG within the 3 weeks before exposure, then varicella-zoster immunoglobulin is not necessary. Hepatitis B, diphtheria, pertussis, tetanus, and H. influenme type b vaccine recommendations are the same for HIV-infected and noninfected children. Bacillus Calmette-Guerin vaccine is a live attenuated vaccine and is contraindicated for children with HIV living in the United States,l5. 33 but the World Health Organization has continued to recommend the administration of this vaccine at birth in countries with a high prevalence of tuberculosis, even if a mother is known to have HIV infection. The rotavirus vaccine has not been evaluated in HIV-infected children and is not recommended for use in this population. A wise physician said that medicine should not give more years to life but more life to years. Quality of life is an important issue when dealing with chronically ill patients, especially during the late stages of HIV infection. Palliative medicine focuses on the management of physical, psychological, and social aspects inherent in ultimately lethal disease^.^, 74 Children with HIV infection may experience pain from their illness and the medical and surgical procedures done to alleviate their illness. An aggressive approach to pain management in children with HIV infection is recommended. The success of therapy is determined by self-reports from these children using age-specific scales. The type and dose of medication used depends on the degree of pain experienced (Table 7) . Certain principles apply to pain management. Opioid therapy is safe in infants and children. Therapy should be individualized, but around-the-clock therapy is preferred to intermittent dosing. Intramuscular and subcutaneous routes should be avoided, and oral and IV routes are preferred. The use of behavioral techniques also has a significant role.74 From the beginning, an open and sincere relationship should exist between physicians, patients, and patients' families. When possible, pediatric patients should be involved in decision making that involves minimizing care and pain management. If a physician believes that therapy would be futile or suffering would be too great, the issue of resuscitation should be discussed with the patient's family. A physician may have difficulty in determining the right time to bring up this issue but this conversation should precede a situation in which the patient requires artificial life suppw. Whether to include these children in the decision making depends on a child's age and maturity and the family's preferences. Psychiatric evaluation may be helpful to assess the level of maturity of these patients. Referrals to hospice for terminal care facilities are appropriate and provide support for families at this difficult time. Children with perinatally acquired HIV are surviving longer than at the beginning of the pediatric AIDS epidemic. As children enter late childhood and adolescence, they should be aware of their diagnoses. Children with other chronic diseases who know their diagnoses have been shown to cope better with their illness than those who do not know their diagnoses. In past years, families have been afraid to tell their children about their HIV infection because of the fear that these children will disclose to others and the family would be stigmatized or because of denial or guilt about transmitting the infection to their children, but an ongoing dialogue with the family regarding the importance of disclosure is important. The age, maturity, and social circumstance of these children must be taken into consideration, and disclosure should be done in a language and at a level that these children understand. Neuropsychological testing can guide the practitioner in determining the developmental age of a child. Simple, uncomplicated explanations can be given to children under 10 years of age, but older children and adolescents should have full disclosure, conveying an understanding of how the infection is transmitted, how to protect others from getting the infection, and a sense of responsibility for their health. These children should become active participants in their health care. As children reach adolescence, issues of compliance arise, and teenagers should be able to express opinions on the type of medication regimen they can accommodate, with full realization of the consequences of not taking their medication. Disclosure may occur through the family or guardian of the child or with assistance of the physician and the social worker. The counseling session should include a discussion of who must or should know about the diagnosis. The authors encourage telling siblings who are mature enough to handle the responsibility of knowing this information so that they can be supportive of the infected child. Physicians should answer questions openly and truthfully to prevent misconceptions about the illness. At each subsequent visit, physicians should allow the children to ask questions regarding the diagnosis. After disclosure, the children should be given the opportunity to speak with another infected child or attend a peer support group so they do not feel isolated?, 54 SUMMARY Significant advances have been made in the understanding of the pathophysiology of HIV infection since the beginning of the epidemic. T h s knowledge has translated into the development of new therapies for HIV and opportunistic infections, laboratory advances in monitoring viral and immune status, and a better understanding of factors affecting patient outcome. Concomitantly, significant progress has been made in the medical management of children with HIV infection in the past 5 years. The number of children reported with AIDS in the United States is decreasing, and efforts are shifting from caring for children with advanced immunosuppression and severe opportunistic infections to early HAART, maintenance of the immune system, and prevention of opportunistic infections. Primary care physicians are now more involved and informed in the care of HIV-infected patients. Although published data are limited, physicians who have been working with this population have observed a dramatic improvement in the quality of life and length of survival of these patients. Unfortunately, this progress is not shared by developing countries where resources are minimal and antiretroviral agents are commonly unavailable. Although efforts to develop a vaccine to prevent HIV infection are ongoing, progress has been slow. Education and awareness continue to be the most powerful weapons against HIV.  
paper_id= ffed5d2a31a0c1a0db11905fe378e7735b6d70ca	title= Supplemental material for the paper "Evidence of Translation Efficiency Adaptation of the Coding Regions of the Bacteriophage Lambda"	authors= Eli  Goz;Oriah  Mioduser;Alon  Diament;Tamir  Tuller;Ramat  Hachayal;Tel  Aviv;Israel  ;	abstract= Israel. *Corresponding author (TT): tamirtul@post.tau.ac.il Ribo-seq reads mapping. Ribosome footprint sequences were obtained from 1 (GSE47509, induction 0-20min). We trimmed the poly-A adaptors from the reads using Cutadapt 2 (version 1.8.3), and utilized Bowtie 3 (version 1.1.1) to map them to the E. coli-lambda transcriptome. In the first phase, we discarded reads that mapped to rRNA and tRNA sequences with Bowtie parameters '-n 2 -seedlen 23 -k 1 --norc'. In the second phase, we mapped the remaining reads to the transcriptome with Bowtie parameters '-v 2 -a --strata --best -norc -m 200'. We attempted to extend alignments to their maximal length by comparing the polyA adaptor with the aligned transcript until reaching the maximal allowed error (2 mismatches across the read, with 3'-end mismatches avoided). We filtered out reads longer than 31-nt and shorter than 21-nt. Unique alignments were first assigned to the ribosome occupancy profiles. For multiple alignments, the best alignments in terms of number of mismatches were kept. Then, multiple aligned reads were distributed between locations according to the distribution of unique ribosomal reads in the respective surrounding regions. To this end, a 100-nt window was used to compute the read count density (total read counts in the window divided by length, based on unique reads) in vicinity of the M multiple aligned positions in the transcriptome, and the fraction of a read assigned to each position was . The location of the A-site was approximated by an 11nt shift from the 5' end of the aligned read. This shift maximized the correlation between MTDR (described below) and the observed read densities per E. coli gene. Ribosome profiling data normalization. We began the analysis by reconstructing ribosome profiles for E. coli and Bacteriophage Lambda expressed genes. The ribosome profiling method produces ribosome footprint counts that are proportional to the time spent in decoding each codon of all translated transcripts in a genome, at single nucleotide resolution. To avoid analyzing ribosomal profiles of genes with many missing read counts (RCs) that may result in a non-reliable estimation of the local ribosome density, genes profiles with fewer than 30 percent non-zero read counts were further filtered. Previous studies indicated an increase of RCs at the beginning of the ORF 4 and for some organisms at the end of ORF 4,5 ; therefore the first and last 20 codons were excluded when determining these thresholds or when calculating the average RCs per ORF. To enable comparison and analysis of RCs of codons of the same type originating from different genes, RCs of each 	body_text= 20min). We trimmed the poly-A adaptors from the reads using Cutadapt 2 (version 1.8. 3) , and utilized Bowtie 3 (version 1.1.1) to map them to the E. coli-lambda transcriptome. In the first phase, we discarded reads that mapped to rRNA and tRNA sequences with Bowtie parameters '-n 2 -seedlen 23 -k 1 --norc'. In the second phase, we mapped the remaining reads to the transcriptome with Bowtie parameters '-v 2 -a --strata --best -norc -m 200'. We attempted to extend alignments to their maximal length by comparing the polyA adaptor with the aligned transcript until reaching the maximal allowed error (2 mismatches across the read, with 3'-end mismatches avoided). We filtered out reads longer than 31-nt and shorter than 21-nt. Unique alignments were first assigned to the ribosome occupancy profiles. For multiple alignments, the best alignments in terms of number of mismatches were kept. Then, multiple aligned reads were distributed between locations according to the distribution of unique ribosomal reads in the respective surrounding regions. To this end, a 100-nt window was used to compute the read count density (total read counts in the window divided by length, based on unique reads) in vicinity of the M multiple aligned positions in the transcriptome, and the fraction of a read assigned to each position was . The location of the A-site was approximated by an 11nt shift from the 5' end of the aligned read. This shift maximized the correlation between MTDR (described below) and the observed read densities per E. coli gene. Ribosome profiling data normalization. We began the analysis by reconstructing ribosome profiles for E.  
paper_id= ffed992cb996dc6628365d51d4e3a3e50a5d78ac	title= A Novel CFD Analysis to Minimize the Spread of COVID-19 Virus in Hospital Isolation Room	authors= Suvanjan  Bhattacharyya;Kunal  Dey;Akshoy Ranjan Paul;Ranjib  Biswas;	abstract= The COVID-19 is a severe respiratory disease caused by a devastating coronavirus family (2019-nCoV) has become a pandemic across the globe. It is an infectious virus and transmits by inhalation or contact with droplet nuclei produced during sneezing, coughing, and speaking by infected people. Airborne transmission of COVID-19 is also possible in a confined place in the immediate environment of the infected person. Present study investigates the effectiveness of conditioned air released from air-conditioning machines to mix with aerosol sanitizer to reach every point of the space of the isolation room so as to kill the COVID-19 virus which will help to protect the lives of doctors, nurses and health care workers. In order to numerically model the laminar-transitional flows, transition SST k- model, which involves four transport equations are employed in the current study. It is found from the analysis that high turbulent fields generated inside the isolation room may be an effective way of distributing sanitizer in entire volume of isolation room to kill the COVID-19 virus. 	body_text= The COVID-19 is a severe respiratory disease originated from the devastating coronavirus family (2019-nCoV) (Zhou et al., 2020; Zu et al., 2020) and has become a pandemic across the globe. It is a contagious virus and transmits by inhalation or contact with droplet nuclei of size Φ < 5μm produced during sneezing, coughing and even speaking by infected persons. Exhaled droplets from confirmed COVID-19 patients or an active carrier of the virus can deposit in the mucosae of noses, mouths, and conjunctiva of eyes of people in close contact. The virus may be transmitted through personal contact with the COVID-19 patient or indirect contact with fomites such as clothes, utensils, furniture, surface etc. used, touched or in the immediate environment of the infected person. Generally, fever, breathlessness, cough, throat pain, weakness are traditional symptoms at the initial stage of the disease (Huang et al., 2020) . The disease causes respiratory illness such as pneumonia and acute respiratory distress syndrome resulting in rapid death of those affected, depending on their age, condition of lungs, immunity and sociodemographic profile. A study (Bukhari et al. 2020) showed that the COVID 19 is highly harmful and transmitted the infection throughout the world. Total of 212 countries and territories around the world are infected by the disease and became a pandemic as declared by the World Health Organization (WHO) (Peng et al., 2020; WHO, 2020a; WHO, 2020b) . It has also been reported (WHO, 2020c ) that airborne transmission of COVID-19 is also possible in specific circumstances like aerosol therapy performed during treatment of pulmonary critical illness such as asthma, bronchoscopy, chronic obstructive pulmonary disease (COPD), tracheostomy and other related diseases. In addition to cough, the droplets produced by sneeze produced aerosols, the droplets containing virus present in the air also constitute a substrate for viruses which transmit the disease through air in an isolated space (Setti et al., 2020; WHO, 2020c) . Furthermore, relative humidity, temperature, rainfall etc. are recognized as factors affecting the infectivity of the virus in the respiratory system (Ahmadi et al., 2020; Bashir et al., 2020; Prata et al., 2020; Monserrate et al., 2020; Taylor, 2019; Xie and Zhu 2020; Qi et al., 2020; Sahin 2020; Shi et al., 2020) . As the characteristics of COVID-19 virus is still not fully understood, there is no particular treatment, medication, therapy or vaccine approved by the medical authorities till date. The mortality rate of the patients affected by this virus is 2 to 3%, but it can be fatal for aged people and children and those with a prolonged illness with less immunity. The medical boards and health administration has implemented travel ban, complete lockdown, containment zone identification, home quarantine of all citizens, strict monitoring of movement of citizens etc. to combat the spread of COVID-19. The transmission can also be reduced by controlling indoor dust level, temperature, humidity, ventilation, improved hygiene, sanitization, wearing mask, using personal protective equipment (PPE) by the healthcare and sanitary personnel etc. It is indispensable to accommodate the confirmed COVID-19 patients and patients with symptoms in isolated rooms or separate ICUs in hospitals for treatment so that spread of this disease can be prevented. These isolated rooms are designated as "Airborne Infection Isolation Rooms (AII)". Also, exhaust air released from AIIs is likely to carry virus particles and hence an effective strategy should be employed to arrest the spread of infections. Recent COVID-19 outbreak in Wuhan province of China found the evidence of COVID-19 virus genetic material in the air about 4 metres from the affected persons in two ICU wards of Huoshenshan Hospital in Wuhan, risking to healthcare personnel (Guo et al., 2020) . Care should therefore be taken to disinfect the exhaust air through various available treatments such as HEPA filtration, sanitization, heating, UV irradiation etc. (European Centre for Disease Prevention and Control, 2020; Ministry of Health & Family Welfare, Government of India, 2020; WHO, 2006; WHO, 2020d ). Very few researchers have however studied the design aspects of an effective ventilation system and influencing factors for room ventilation to reduce the spread of the virus (Li et al., 2007) . The healthcare personnel working in hospitals are at greater risk of getting infected due to outbreak of airborne or droplet contact diseases. Many researchers used Computational Fluid Dynamics (CFD) based models (Memarzadeh et al., 2000) to study air quality inside a room, comfort level, performance of HVAC systems etc. in different types of buildings. It is a very robust and efficient tool to investigate the airflow and contaminant dispersion in rooms where many parameters involved. CFD analysis using complex particle tracking methodologies (Cole et al., 1998; Kowalski et al., 1998; Memarzadeh, 2011; Zhang et al., 2008) with dynamic process variables like velocity, movement, path lines followed by the air in order to determine control strategy for the trajectory of infectious particles moving in air, may be considered and simulated to control the spread the huge number of infectious droplets generated from patients cough, sneeze. As the medical treatments are often inaccurate, besides precautionary measures and supports, it is therefore reasonable to investigate the possibilities to sanitize the confined volume of air to mitigate the spread of COVID-19 virus inside the airborne infection isolation rooms, and ICUs of a hospital. This can be accomplished by designing an aerosolized sanitization system which effectively can sanitize the air inside the room to be used for the treatment of confirmed COVID-19 patients. This is essential to protect the lives of doctors, nurses and health care workers. However, no work is reported in literature on this topic till date. Hence, the present paper depicts aerosol sanitizer delivery systems focusing the effectiveness of conditioned air released from air-conditioning machine to mix with aerosol sanitizer and enabling every corner of the isolation room killing the COVID-19 virus, thereby achieving complete sanitization. Also, results obtained from the research work could be used to help flatten the infection curve of COVID-19. A CFD analysis is carried out considering factors affecting aerosol sanitizer delivery system such as temperature, turbulent kinetic energy and flow dynamics. Tetrahedral and hexahedral elements are used to generate these meshes ( Fig. 2 (b) ). Hexahedral meshing is used to fill the intricate parts, while tetrahedral meshes are used in the remaining parts of the control volumes. On each computational element, governing equations, like mass, momentum and energy equations are solved using finite volume based CFD technique. Boundary conditions constitute an important criterion for any CFD simulation and the present study is of no difference. Inlet conditions are specified at the ceiling of the isolation room with a velocity of 3.91 m/s applied uniformly with an inlet temperature of 24C. With a mass flow rate of 29.97 kg/s. No-slip, no-temperature jump conditions are applied at the exit of the ducting system. Similarly, inlet conditions are specified at the sanitizing machine of the isolation room with a velocity of 1.5 m/s applied uniformly with an inlet temperature of 30C, with a mass flow rate of 1.854 kg/s. Numerical solution to any governing equation which is expressed in a partial differential equation requires discretization. Second-order upwind schemes with Boussinesq approximation (Bhattacharyya et al. 2020 ) are used for this purpose. The purification of air in the isolation room with chemical diffusion requires simulating the diffusion as well as the natural ventilation process, which involves pressure forces, buoyant forces and elements of forced-convection, and conductive as well as convective heat transfer. An unsteady CFD analysis is carried out to get more insight into the flow physics of the system. In order to numerically model the laminar-transitional flows, transition SST k model, which involves four transport equations are employed in the current study (Bhattacharyya et al. 2017 ). This special transition SST k model is formed by a combination of the SST k model along with two additional transport equations-one designed for the transition onset criteria while the other for flow separation induced transition, in terms of momentum thickness Reynolds number (Bhattacharyya et al. 2017 , Rajnath et al., 2020 . (Menter 1994)  CFD simulation is carried out in a 16-core IBM HPC with 64 GB RAM and 27 CPU seconds of processing time (51 days). The CFD models are well accepted and used to investigate thermal comfort, indoor air quality, load of the room, HVAC performance, etc. in numerous buildings. CFD is one of the most resourceful and capable tools to study fluid flow (air as the working fluid) and pollutant spreading in the comfort area. So, computational technique is used in this particular study. This particular study is to computational investigate the flow characteristics of the sanitizer-laden conditioned air inside the room, which is essential for disinfection of the room air and thereby protecting the lives of doctors, nurses and healthcare workers. The inlet and outlet of the portions are shown in Fig. 3 . Before investigating the fluid dynamics/pattern of hospital isolation room geometry, the current numerical model and methodology are validated against published experimental and numerical works (Chung and Hsu 2001, Jacob et al. 2019) . Fig. 4 (a) and Fig. 4 subsequently spreads towards the walls. Another view (seen from the top) is presented in Fig. 6 showing the ever-changing streamlines. It is understood from these figures that the fluid flow which influences the isolation hospital room has originated from the clean air openings (vents) positioned at the top of the isolation room (ceiling) irrespective of the direction of observation. together. It is evident from Fig. 8 (refer to the right side the image) that the sanitizing machine releases sanitizer at relatively higher temperature, which mixes with the cool air coming from the air-conditioning vent. Better mixing is ensured due to velocity as well as the temperature gradient available between the flows of the sanitizing machine and the air-conditioning vent. The cool air coming from the air-conditioning machine from the top of the isolation room exhibits asymmetric pattern due to the influence of the velocity and temperature gradient maintained by the sanitizing machine. An effect with Fig. 8 is demonstrated in Fig. 9 . The velocity vectors are shown in Fig. 9 . with the other flow (sanitizer, horizontal flow). It is also found that both the flows slow a bit when striking the walls. Flow circulation and large scale eddies found due to the mixing of flows between cool air and sanitizer and also due to bounding walls. Due to thorough mixing between cool air and sanitizer it is expected that the entire isolation room air will be sanitized, and due to this sanitizing by volume, it is also expected that the isolation room is fully virus free and the occupants (patients) can stay comfortably. This results and novel idea could be used to help flatten the curve of COVID-19. Moreover, these results could be used to design the effective layout of the air-conditioning ducts. In the context of COVID-19, since there is no specific treatment or established medical protocol, medication and vaccine, the objective should be to control and prevent the transmission of this virus as far as possible. It is absolutely essential to reduce the risk of airborne infection transmission to the lowest possible level in hospital isolation rooms to protect the lives of doctors, nurses and other health care workers, and simultaneously, flatten the curve of COVID- This particular investigation was performed to offer understanding the airflow patterns in the isolation room. The study has been carried out to investigate the effectiveness of conditioned air released from air-conditioning machines to mix with aerosol sanitizer so as to reach every corner of the isolation room and kill the COVID-19 virus. A CFD analysis has been carried out considering factors affecting aerosol sanitizer delivery systems such as temperature, turbulent kinetic energy and flow dynamics. In order to numerically model the laminar-transitional flows, transition SST k model, which involves four transport equations are employed in the current study. It is found from the analysis that high turbulent fields generated inside the isolation room may be an efficient way of distributing sanitizer in a volume of confined isolation room to kill or minimize the COVID-19 virus. G k = generation of turbulent kinetic energy (k)  
paper_id= ffeda348e24cffc27f82cd0249f343790c1bbc12	title= Biometric Technology Today FEATURE The world wants to reopen: will vaccine passes be the key?	authors= Stephen  Davidson;	abstract= 	body_text= As the pandemic raged on, borders opened up here and there but international travel remained severely restricted. As a result, the travel and hospitality industries have suffered significantly, as have countries that rely on seasonal tourism to prop up their economies. In 2019, tourism accounted for around 12% of Spain's economy; by 2020, it was only 4%two-thirds smaller. The pandemic is now turning a corner and vaccination programmes are advancing in most countries. Normality might still be a long way off, but by suppressing the spread of SARS-CoV-2, mass immunisation is a critical step in the journey back to ordinary life. But while the world wants to reopen, it needs a way to do that safely, without triggering new waves of variant infections. Vaccine passes, secured by biometrics, have the potential to unlock international travel once again. Many people and organisations are counting on just that, not least airlines, travel agents, the hospitality industry and weary global citizens, hoping perhaps for their first holiday in two years. But as these vaccine or health pass schemes begin in earnest, fuelled by huge demand from the public and the global economy, there are many questions still to answer. The route back to normality is not yet clear -and the development choices that are made now will be critical. It is important to remember that vaccine passes are not new: the status quo is the World Health Organisation's (WHO) 'Carte Jaune', or Yellow Card, which has been used since the 1930s. Standardised internationally, the Yellow Card is a booklet that your doctor can sign whenever you have a vaccination. It's simple, contains just the required information and is under the control of the individual to show to relevant authorities, such as border officers. Many commentators therefore believe that the best course of action now is to find a way to update the Carte Jaune with modern biometric protections against counterfeiting. This is not a passport aimed at restrictions, it is a Covid-era portable and limited medical record. The development of such updated vaccine passes is well underway, with the same creativity that was seen in the initial pandemic response, leading to a process where regional and international norms will become established for these passes. Individual countries and political blocs are going through their own processes 1 . Israel, for example, which vaccinated its admittedly small population at breakneck speed, already has a 'Green Pass' system in place for domestic activities and has proposed using the same pass to open international travel to certain countries. The African Union Commission is also developing a scheme so that citizens can verify their status to authorities. The Commission intends to extend this to vaccinations, though there is some scepticism. John Nkengasong, head of the Africa Centres for Disease Control and Prevention, told the press 2 in April that: "Our position is very simple. That any imposition of a vaccination passport will create huge inequities and will further exacerbate them." The US Government, meanwhile, has stated that it will not create a central immunisation record system and is instead leaving it to individual states and the private sector to create their own. On 6 April, White House press secretary Jen Psaki said 3 : "Our interest is very simple from the federal government, which is that Americans' privacy and rights should be protected, and so that these systems are not used against people unfairly." State governments in Texas, Florida, Arizona and Utah 4 have banned vaccine passes on the basis that they would violate the privacy and freedom of individual citizens. Meanwhile, New York State has partnered with IBM to provide blockchain-based vaccine passes that allow peo- Covid-19 has halted many things we once thought of as normal. At the beginning of the pandemic, countries quickly closed down their borders in an attempt to staunch the free flow of infections. Given the limited information about the virus at the time, these restrictions were a natural response; after all, health authorities could often trace initial infections in a country back to a handful of international travellers. ple to attend large events. Even communities as small as Bermuda are rolling out vaccine passes in an attempt to restore tourism and social life on the island 5 . The UK Government also began using vaccine passports from 17 May, despite some privacy groups and lawmakers saying that this "divisive and discriminatory" scheme would violate privacy and create a two-tiered society 6 . However, it seems the European Union is offering one of the most promising options globally. After several months of debate about the viability of such a scheme, the European Commission presented its proposals in mid-March, with the intention of unlocking freedom of movement across Europe 7 . It would allow European citizens to once again travel freely across the continent, carrying a QR code certificate that asserts their Covid-19 status. These passes, dubbed the 'EU Covid-19 Certificate', would include a minimum amount of information to securely verify the holder's vaccination details, test result or recovery status. This EU certificate might be the most promising proposal so far, partly because it aims to do something that many current schemes do not: to unlock cross-border travel. In doing so, it could set international standards for such passes. As a supra-national bloc, one of the main political foundations of the EU is freedom of movement across its member countries' borders. This had to be suspended in many ways during the pandemic, but at time of writing the EU Covid-19 Certificate is set to restore that foundational political tenet. As such, it faces certain requirements that other schemes do not. When considering new vaccine passport schemes, many developers have the initial thought of making the identities and data contained within them as secure as possible. That's an entirely understandable aim. Medical records are among the most sensitive information an individual will possess -so the inclination to put it behind a steelplated electronic identity might seem natural. But in this case, it can be counter-productive. eID is an area of brilliant innovation and sometimes of bewildering complexity. Some parties may be inclined to create new regimes which provide secure identities, but can't be rolled out as far and wide as they need to be. These will hamper progress and add more cumbersome characteristics to a process that needs to be streamlined. Others may look to promising new technologies such as blockchain and verified credentials, with examples including South Korea and New York State in the US. However, some feel that these solutions may cause interoperability issues and could be exclusionary in places with modest technological means. Others feel that blockchain technologies could be a bad fit for vaccine passes. Speaking to the press, Matthew Green -a well-known authority on cryptography at Johns Hopkins University -said 8 : "There is zero reason for blockchain to be involved in this problem." He added: "Blockchain solves a very specific problem around not trusting people, and the problem with this vaccine stuff is you do trust people. You have to trust the data being entered into the blockchain is an actual trusted reflection of who's vaccinated or not." Another inclination is to tie this information to either a centralised system of records or to load it with personal information, which might more reliably authenticate the passes. But if the passes have to 'call home' to that system to actively verify live data, then that could bog down a process that needs to be agile. Vaccination is rolling out quickly. And there is pressure to open up even faster. The point we need to underline is that these systems are going to be used by medical professionals, border agents and even event venue operators, but generally not technology experts. That means that each system has to be simple to use; if complexity overtakes usability then the process will be plagued by problems. Given that most vaccine passes are updating the old yellow cardboard with new security features, simplicity is their most important feature. Moreover, unlike most vaccination programmes, which may roll out over years, governments are seeking to credentialise millions of subjects as quickly as possible to restore freedom of movement and to stimulate economic activity. So the best schemes will easily accommodate both paper and digital means. Interoperability is key, which means pinning down the expected uses and reliance on the vaccine passes. What data will be carried in the credential? What technology is required to verify the credentials? Are the security features well-understood and documented? The best vaccine pass schemes must be flexible enough to sit on top of the existing national systems: there is no time for huge integration projects either at the health authorities or at the myriad end points that may rely on the passes. The protection of personal data is also vital. While a traveller previously could keep their Yellow Card in their pocket, users will have concerns with online vaccination tools. Vaccine pass schemes must be transparent about where the data is held, who can access it and who can retain it. Many projects are looking to use tried-andtrue approaches that have been proven at scale, such as the public key infrastructure (PKI) approach. This has already been well proven in similar use-cases, such as for e-passports, and in many IoT deployments that require fast authentication of users, data integrity and privacy, and acceptance/validation by diverse relying parties. Widely understood and supported in consumer software, the use of PKI building blocks can also simplify the rollout and acceptance of vaccine passes. While a myriad of commercial and community proposals are being floated, the EU's scheme represents the first supra-national governmental standard. In many respects, it could set the international standard for vaccine passes going forward. Under the EU's Covid-19 Certificate, the Union's 27 national health authorities will be able to issue the vaccine passes. This makes sense as they are already the custodians of immunisation information. Citizens can store their credentials on a mobile device (with or without an app), or even request a paper version. Both will feature a Believing that Americans' privacy should be protected, the US Government has decided not to create a central immunisation record system. Biometric Technology Today FEATURE QR code containing essential information about the holder and their immunisation status, which can be visually scanned by many mobile devices. The QR code will include a PKI-based electronic signature that asserts the legitimacy/ origin of the credential and the fact that the data has not been tampered with. The PKI architecture is modelled on the one that has been successfully used by the International Civil Aviation Organisation (ICAO) e-passport scheme for years, which is flexible in enabling countries (and their varying health authority structures) to issue the vaccine passes. Using PKI, the European Commission will also build a single gateway that relying parties may use to verify the QR codes' signatures, no matter which country issued them. The personal data encoded in the vaccine pass does not pass through the gateway. The gateway is simply verifying that the QR code was issued by a legitimate authority, and that the data it contains has not been tampered with. Remembering the dominance of WHO's Yellow Card, the EU scheme takes into consideration WHO's guidelines for Smart Vaccination Certificates and has been careful to publicly document its approach, rules for data protection and interoperability, and its data sets. There are defined rules about the retention and use of the vaccination data. Seeking stakeholder buy-in along the way, with the scale of this transparency the EU approach may become the de facto international best practice. In fact, many aspects of its design are already being picked up by other providers, including open-source projects such as the PathCheck Foundation's paper-first vaccine pass 9 . The EU approach could also accommodate -but does not depend on -the use of mobile apps. In fact, other than helping to develop software that authorities can use to check the QR codes, it sidesteps this issue, understanding user worries that their data may be monitored or correlated in some manner. Like the old Yellow Card, the user can choose not to use the EU Covid-19 Certificate. In summary, while there remain problems over vaccination hesitancy, the global consensus is that immunisation provides a valuable risk-control in restoring normal life. For that reason, vaccine passes will likely play an important role. Yet that process could fragment if different countries and communities rush into partial solutions without regard to international standards. That's why the EU's approach seems promising. Europe's approach of designing a simple and flexible system that uses well-known aspects of PKI, makes it possible to roll out at scale. The commitment to documented standards means that it is interoperable and will serve well as an international model. Most of all, the EU Covid-19 Certificate sets an example by recognising that the needs of governments must be balanced against the privacy rights of the individual. 
paper_id= ffedb45af963807de5a85a8151f206ee489c2576	title= The Cultural Psychology of Religiosity, Spirituality, and Secularism in Adolescence	authors= Lene Arnett Jensen;	abstract= Cultural psychology has raised awareness of religiosity, spirituality, and secularism in people's psychological lives. This article takes a cultural-developmental approach by examining the development of religiosity, spirituality, and secularism among culturally diverse adolescents. At the outset, an explanation is provided as to why the valid study of peoples' psychological lives necessitates taking culture into account, and of key implications for theory and methodology. Throughout research on adolescent religiosity, spirituality, and secularism is described, including studies on conceptions of God, afterlife beliefs, the development of an Ethic of Divinity in moral reasoning, recent increases in spirituality and secularism, and the impact of globalization on worldviews and religiously-based puberty rituals. While the focus is on adolescents, the article includes relevant research with children and emerging adults. Concrete future research directions are proposed, including a call to address the extent to which effects of religion on adolescents are dependent on culture and globalization. Publisher's Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. 	body_text= More than 80% of the world's population identify with a religion (Pew-Templeton Global Religious Future Project 2020). At the same time, almost 20% indicate that they are religiously unaffiliated. The proportion of unaffiliated persons is rising in many countries, and this rise is led by adolescents and emerging adults (Bullard 2016; Pew Research Center 2018; Poushter and Fetterolf 2019) . Being religiously affiliated and religious unaffiliated may seem like two distinct categories. However, research that delves into the beliefs and practices of cultural groups and their individual members finds that boundaries are porous between religiosity, spirituality, and secularism. For example, an Irish adolescent can self-identify as atheist and simultaneously be nominated by his community as a spiritual exemplar (King et al. 2014 ). An Australian Aborigine adolescent may take active part in collective puberty rituals that involve the teaching of longstanding religious beliefs, called the "Law," while simultaneously being in the process of developing different beliefs that are more individualistic (Eichelkamp 2013) . A Danish emerging adult may self-identify as agnostic but also profess a belief in life after death (Arnett and Jensen 2015) . The present article will provide a cultural psychology perspective on the development of religiosity, spirituality, and secularism in adolescence. While the focus of this special issue of Adolescent Research Review is on adolescent religiosity and spirituality, this article will also address secularism because of its worldwide growth and the fact that it sometimes intersects with religiosity and spirituality. The focus will be on adolescent development, but relevant research with children and emerging adults will also be included. My theoretical goal is to advance a "cultural-developmental approach" to theory and research on human development. One-size-fits-all theories-popular in the social sciences in the twentieth century-are often too broad and too biased to adequately capture the complexities of human selves and relations. On the other hand, one-for-every-culture raises the specter of theoretical pandemonium. The challenge and opportunity that we face in today's globalizing world is bridging universal and cultural perspectives. My empirical research addresses moral, civic, and cultural identity development in the context of culture, migration, and globalization. I often combine qualitative and quantitative methods. Defining religiosity, spirituality, and secularism in a way that has validity across cultures is a challenge. Here, religiosity and spirituality will be understood to entail belief in the supernatural, or sacred, or "ultimate reality" (King et al. 2020, p. 593) , while secularism will refer to the absence of such belief (Zuckerman and Thompson 2020) . Religiosity involves an affirmative relationship with one or more religions, where a religion typically entails a set of doctrinal beliefs and behaviors that are shared by a community. Spirituality will refer to an individual's search for or sense of connection with the sacred, supernatural, or ultimate reality. Social science definitions of religiosity, spirituality, and secularism are plentiful, and adolescent researchers have been moving toward flexible and multidimensional definitions (Hardy et al. 2019) . The present definitions are intended as a starting point, yet it needs to be acknowledged that they may not apply equally well to all cultural groups (Eller 2017) . Terms and meanings pertaining to religiosity, spirituality, and secularism are varied and multifaceted across cultures. As will be discussed later, investigating such cultural diversity is precisely part of cultural psychology. Cultural psychology aims to understand the diverse psychologies of different peoples. The field of cultural psychology has burgeoned in recent decades. Initially, as cultural psychologists were focused on carving out a discipline, some distinguished cultural psychology from cross-cultural psychology (e.g., Stigler et al. 1990 ). The central argument was that cross-cultural psychology-like cultural psychology-involved the study of peoples of different cultures, but that its central aim-unlike cultural psychology-was to document universal psychological phenomena. Critics of cross-cultural psychology also noted that often concepts and measures developed in the United States were exported without adequate consideration of their relevance to local conditions. By now, the boundaries between cultural and cross-cultural psychology have become less sharp. Cross-cultural psychologists are showing increased attention to cultural diversity and ecological validity, perhaps partly in response to the cultural psychology critique (e.g., Kühnen et al. 2009 ). While this article uses the term cultural psychology, it is important to note that this burgeoning field includes scholars from many disciplines, including not only psychology but anthropology, communications, education, linguistics, political science, social work, sociology, and neuroscience (Jensen 2015a) . Culture is defined here as symbolic, behavioral, and institutional inheritances that are shared and co-constructed by members of a community (Goodnow 2010; Jensen 2015a) . Culture is not synonymous with country or ethnicity, but rather describes communities whose members share key beliefs, values, behaviors, routines, and institutions. Of course, cultural communities include heterogeneity within groups, as scholars addressing cultural issues have long observed (Gramsci 1971) . Variation also exists between cultural communities, including on their degree of heterogeneity and change over time (Weisner et al. 1997; Whiting and Edwards 1988 ). An important source of variation both within and across cultures is access to power. Power differentials occur along lines such as region of the world, nationality, socioeconomic class, ethnicity, gender, and religion (e.g., Abo-Zena and Ahmed 2014; Hammack and Toolis 2015; Kapadia and Gala 2015; Super 2010) . The articles in this journal issue present different disciplinary approaches to the study of religious and spiritual development in adolescence. Since the charge of the present article is to introduce the cultural psychology perspective, rationales as to why this perspective is necessary will be the starting point. Next will follow a section that describes key theoretical and methodological considerations when conducting cultural psychology research. While the key points of these first two section are germane to research with all age groups, the explanations and examples provided will focus on adolescents. The third section highlights additional important cultural psychology studies that have contribute to the study of the development of religiosity, spirituality, and secularism in adolescence. Suggestions for future research will be presented at the end. One reason that cultural psychology is necessary is that humans have evolved to be a uniquely cultural species, capable of inhabiting almost any part of the globe (Tomasello 2011). Our large brains have enabled us to adapt to most environments by inventing new methods of survival and passing them on to our children as part of a cultural way of life. The less mature brain of the human child at birth relative to other species also makes for a longer period of dependency and for extensive brain maturation and learning within local physical and cultural environments (Jensen and Arnett 2020) . Successfully surviving in vastly different environments, from equatorial Africa to the Arctic, requires the highly flexible set of cognitive and emotional skills afforded by the human brain and the ability to create cultural communities. The extent to which religion was a part of this evolutionary process is currently being debated, but it undoubtedly became a salient component of culture over time (e.g., De Waal 2013; Turner et al. 2017) . Apart from adapting to new environments, humans have also become capable of altering their environments, such that it is no longer natural selection alone but also the cultures we create that determine how we live. In short, being cultural is a fundamental part of what it means to be human, and cultural psychology addresses this fact. A second reason for the necessity of a cultural psychology perspective is that today we live in a globalizing world where people from different cultures constantly come into contact with one another, in both the physical and the digital worlds (Jensen 2020) . The flow of ideas, goods, and people across the world is not new, but the current extent and speed of globalization are unprecedented. In many ways, adolescents and emerging adults are at the forefront of globalization with their openness to new ideas, consumption of global brands, facility with use of new global technologies, and migration from rural to urban areas in search of new work opportunities (Jensen and Arnett 2012) . Interestingly, psychological research on young people's orientation to religion in the context of globalization is scarce, although one recent study in Thailand found that urban adolescents with extensive exposure to globalization emphasized individualized views of religion whereas rural adolescents with minimal exposure to globalization regarded religion as a relational experience that includes friends, family members, and teachers . Globalization calls for a cultural psychology perspective that helps us understand people from diverse cultures, including their religious, spiritual, and secular thoughts and behaviors. It is important to note that globalization has an impact across the world, not just in some countries, rendering a cultural psychology perspective relevant for all of us. A third reason for a cultural psychology perspective is that most of the world's population lives outside the West, yet most psychological research only includes samples from North America and Europe (Heine 2020) . There is an urgent need to know more about the psychology of the majority world. Moreover, almost all future population growth is projected to take place in economically developing countries (Population Reference Bureau 2014). Some of the economically developing regions of the world are seeing an increase in religious affiliation in their adult populations. For example, this is the case in Africa, especially sub-Saharan Africa (Bullard 2016) . Does this mean that adolescents in these regions are also becoming more religious? If so, what forms are their religious beliefs and practices taking? And what are the implications for a global world where adolescents and emerging adults in other regions are becoming more secular? From a cultural psychology perspective, social science becomes more valid and more relevant when it includes the full scope of the human population. Psychology has long sought to provide nomothetic and idiographic knowledge; that is, knowledge of what all humans have in common and how individuals are unique, respectively. Cultural psychology adds a point in between these two ends of the spectrum, namely, the aim of knowing the diverse psychologies of cultural groups. Certainly, through the study of different cultures, cultural psychologists may point to human commonalities that carry across cultures. Also, cultural psychologists may provide case studies of individuals within groups or highlight how individuals within a culture vary. Nonetheless, the crucial contribution of cultural psychology is to point out that humans are cultural, and that attention to cultural diversity makes social science better able to accomplish its key goals of contributing knowledge and improving people's lives. In order to reach these social science goals, cultural psychologists employ ecologically sensitive theoretical and methodological approaches. From a cultural psychology perspective, one-size-fits-all theories are insufficient. Psychology has a history of universalistic theories (e.g., Freud, Piaget, Kohlberg, Fowler, Maslow) , and research based on these theories has contributed valuable insights. This research, however, has been ineffective at capturing diversity. For example, a long psychological research tradition on the development of conceptualizations of the supernatural has employed a Piagetian framework (Bassett et al. 1990; Elkind 1971; Goldman 1965; Harms 1944; Nye and Carlson 1984) . Most of this research has examined how children and adolescents think about God, often by asking them to describe God in interviews or depict God in drawings. In line with Piaget's theory of cognitive development, researchers have habitually concluded that children up until the age of about 10 or 11 view God in concrete ways (e.g., a man with a white beard), whereas adolescents have much more abstract images (e.g., God is everywhere). There is a paucity of research on how culture may impact conceptualizations of God, but available studies call into question wholesale Piagetian conclusions and indicate that children from different religious cultures produce different images of God. For example, a study found that children from Latter-Day Saint, Lutheran, and Mennonite families drew pictures of God with more concrete features (God as a person), as compared to children of Jewish background (Pitts 1976) . Another study showed that Hindu children, more than Baptist, Catholic, and Jewish children, described a many-sided God (Heller 1986 ). The Hindu children said they felt close to God while also portraying God as an abstract form of energy. Across these studies, children's drawings and statements reflected the different images of God that distinguish their religions, including notably abstract ideas within some religious cultures. Research that has extended these veins of inquiry to include additional age groups and to inquire about additional supernatural entities has shown that different religious cultures give rise to different psychological realities. For example, one study compared children (7-12), adolescents (13-18), and adults (36-57) from evangelical and mainline American Protestant communities on their conceptions of God and the Devil (Jensen 2009 ). Analyses of interviews showed differences along four dimensions. First, religious culture has an impact in terms of what supernatural entities populate one's psychological reality. For almost all evangelicals, the Devil was real, whereas the majority of mainline participants repudiated the existence of the Devil. Second, religious culture makes a difference in terms of how one characterizes supernatural entities. Evangelicals conceived of God more in terms of power and judgment than mainline Protestants. They were also more likely to see God as male, whereas mainline Protestants often described God's gender in terms of the abstract idea of "something else". Third, religious culture affects how one evaluates supernatural entities. For evangelicals there was a particularly large within-subject difference in their evaluations of God and the Devil, as compared to mainliners. Evangelicals evaluated God far more positively than the Devil, and the Devil far more negatively than God. Fourth, religious culture makes a difference in the relationship that one has with God. The two groups differed strongly on how much control they saw God as having over their lives. This difference is exemplified by an evangelical adolescent who stated that "God controls everything, the kind of people I meet, how I react in situations", and a mainline adolescent who said "I think God leaves ourselves to make our own decisions" (Jensen 2009, p. 142) . In short, religious cultures differ on conceptions of supernatural entities. This includes which supernatural entities are believed to exist, how supernatural entities are characterized, and the nature of the relationship between humans and supernatural entities. These findings illustrate the limitations that one-size-fits-all theories place on obtaining knowledge across cultural groups that is valid and can meaningfully be applied to diverse peoples' lives. In lieu of one-size-fits-all theories, cultural psychology theories typically involve multiplicity. For example, they differentiate between multiple kinds of identities, intelligences, parenting styles, and creativities (e.g., Kağitçibaşi and Yalin 2015; Mourgues et al. 2015; Nsamenang 2011; Sternberg 1985) . The differentiation between religiosity and spirituality can also be taken to represent a movement toward multiplicity, since it aims to better reflect the different ways that cultural groups and individuals believe in the supernatural or sacred or ultimate reality, and the diverse behaviors in which those beliefs find expression. Cultural psychology has called attention to religious and spiritual concepts in research areas where they had not received much attention, thereby introducing multiplicity to these areas. One example is the area of moral development. For many decades most of the research on moral development was based on Kohlberg's universalistic stage theory (Kohlberg 1984) . The coding manual used for research with Kohlberg's theory includes 708 criterion judgments for coding moral reasoning but only two pertain to religiosity, spirituality, or divinity (Walker et al. 1995) . Other moral development theories that arose during the heyday of Kohlberg's theory, such as Gilligan's and Turiel's theories, also did not code divinity considerations as part of moral reasoning (Gilligan 1982; Turiel 1983) . Aiming to broaden the area of moral development, cultural psychology researchers have introduced a tripartite differentiation between three ethics: Autonomy, Community, and Divinity (Jensen 1995; Shweder et al. 1997) . Briefly, the Ethic of Autonomy-which is prominent in Kohlberg's theory-involves a focus on persons as individuals. Moral reasons within this ethic include the interests, well-being, and rights of individuals (self or other), and fairness between individuals. The Ethic of Community focuses on persons as members of social groups, with attendant reasons such as duty to others and concern with the customs, interests, and welfare of groups. The Ethic of Divinity focuses on persons as spiritual or religious entities, and reasons include divine and natural law, sacred lessons, and spiritual purity. Research has shown the presence and reliable differentiation of the three ethics across diverse cultures, including groups from Brazil, Finland, India, New Zealand, the Philippines, Turkey, and the United States (Guerra and Giner-Sorolla 2010; Hickman 2014; Kapadia and Bhangaokar 2015; Padilla-Walker and Jensen 2016; Vasquez et al. 2001) . The broadening of moral development theory to include an Ethic of Divinity has generated new lines of research. Studies have shown that moral reasoning in terms of the Ethic of Divinity is present across many cultures, even as it varies considerably in frequency between cultures (Jensen 2015b) . Research has also shown that some cultural groups use the Ethic of Divinity when reasoning about public issues where their moral judgments apply to everyone, whereas other cultural groups privatize this ethic and use it only when judging their own moral behaviors (Jensen and McKenzie 2016) . Bridging cultural and developmental psychology, theory utilizing the three ethics has also proposed that divinity reasoning may be particularly likely to rise in adolescence. According to the cultural-developmental theory of moral psychology, use of the Ethic of Divinity will often be low among children but will increase in adolescence and become similar to adult use (Jensen 2009 ). This infusion of divinity reasoning in adolescence may especially characterize religious cultures that emphasize scriptural authority, or where people conceive of supernatural entities as largely distinct from humans, such as being omniscient and omnipotent. The reason is that these culturally articulated religious concepts are of such an abstract nature that they may be readily translated into moral reasoning only by adolescents, whose cognitive skills allow for more abstraction than those of younger children (Trommsdorff 2012) . Research has begun to support this cultural-developmental hypothesis regarding the infusion of divinity in adolescents' moral reasoning (DiBianca Fasoli 2018; Jensen and McKenzie 2019; Vainio 2015) . However, in cultures where scriptural accounts of supernatural entities are less salient or where people regard such entities as less distinct from humans, it is possible that Ethic of Divinity concepts are more accessible to and hence used more by children in their moral reasoning from an early age (Saraswathi 2005) . In some Hindu Indian communities, for example, religious devotion finds expression in tangible and recurrent activities (e.g., bathing, dressing, and feeding the gods), there are many places within and outside the home for worship (e.g., household and roadside shrines, temples), and there are a variety of persons seen to have god-like status or special connections with the gods (e.g., gurus, sadhus [renouncers], temple priests) (Jensen 1998; Shweder et al. 1990 ). In such cultures, children may reason about moral issues in terms of Ethic of Divinity concepts from fairly early on because these concepts are tied repeatedly to specific everyday activities and objects. Then, in the course of adolescence and adulthood, additional Divinity concepts may become part of a person's moral reasoning. Research is currently under way to examine this hypothesis (e.g., Pandya et al. 2020) . In sum, from a cultural psychology perspective, theories need to go beyond one-size-fits-all universalism to include cultural diversity. Because of their attention to diversity, cultural psychologists have contributed to highlighting the role of religion and spirituality in people's lives, including how beliefs about the supernatural and divine contribute to different psychological realities and ways of thinking about morality. A cultural psychology perspective has implications for who is studied and how they are studied. With respect to sampling, the still limited amount of psychological research on religiosity, spirituality, and secularism in adolescence (and all other periods of the life course) entails a need for research with samples from all over the world. The role of culture, however, needs to be considered explicitly in the conceptualization and write-up of the research. Conducting research with an American sample, for example, and presuming generalizability of results to everyone else is unscientific. Yet, it is an approach that has been common and continues to be so. Cultural psychologists who seek to publish research from the majority world are routinely asked by reviewers and editors to justify their samples and to explain the cultural context. Research conducted in the United States is not necessarily held to the same standard. Studies are often published that provide no justification for the selection of an American sample and no information about the cultural context. Not only is that unscientific, it misses an opportunity to adequately inform readers from different parts of the world about American culture or cultures. Instead, researchers need to consider and explain how results are shaped by culture. One example of a study in the United States that took a cultural psychology perspective sampled evangelical families. This study aimed to identify "social-communicative" processes that promote the development of the Ethic of Divinity in children's moral reasoning. It purposefully sampled American evangelical families because they belong to a religious culture known for its emphasis on considerations pertaining to divinity (DiBianca Fasoli 2018). The study compared the moral reasoning of parents and children, and it delved into the ways they responded to one another in conversations about moral issues. Findings showed that the evangelical adults reasoned more in terms of Divinity than their children. The results also revealed that the parents regularly sought to reroute their children's reasoning from a focus on the Ethic of Autonomy to the Ethic of Divinity, and that they did so through processes such as "scaffolding" Divinity considerations and "countering" Autonomy considerations. For example, one mother and her daughter discussed why someone should put off an outing to the movies and instead visit a friend who had just been hospitalized (Hickman and Fasoli 2015, pp. 161-162 The author framed the results of the study from the perspective of cultural psychology, stating that they "illuminate the processes through which children rationalize their developing moral outlooks in culturally distinct ways" (p. 1671). The author also specifically suggests that parental socialization by means of everyday moral discourse contributes to the rise in the Ethic of Divinity from childhood into adolescence within conservative religious cultures in the United States. While research attuned to the role of culture within Western societies contributes importantly to our scientific knowledge, there is also a profound and urgent need to know more about the psychology of people from outside Europe and North America (Heine 2020). As noted earlier, the majority world is underrepresented in psychological research. Through the inclusion of understudied samples, we stand to gain new knowledge. For example, a recent study used archival data from the Standard Cross-Cultural Sample (a database of coded variables from approximately 200 highly diverse cultures) to determine the most common religious "traits" among 33 hunter-gather societies and by implication, according to the authors, the origins and evolution of religion (Peoples et al. 2016) . Animism was present in all of the societies, closely followed by belief in an afterlife and shamanism. Ancestor worship and belief in high gods were present in somewhat fewer than half of the societies, and belief in high gods who involve themselves actively in human affairs was rare (15% of societies). If we compare the meanings of religion in these hunter-gather societies with its meanings among other cultures described so far, the sheer diversity of religious beliefs across the world becomes apparent. This demonstrates how invaluable studies of majority world cultures are in broadening and deepening our knowledge of psychology. When working in economically developing societies, it is worth noting that there is often an especially sharp divide between the high-SES urban elite and the relatively poor majority of the population. While the high-SES urban samples are often less difficult to recruit and less time-consuming to work with, they are also the ones most likely to resemble samples from the West because they have the most exposure to globalization. Thus, similarities across these samples cannot be interpreted as pointing to universalism, as sometimes happens. Instead, similarities are more likely to point to ways that urban elites in economically developing countries are becoming Westernized. For example, globalization is contributing to some urban adolescents in Thailand becoming more critical of religion. This cultural shift is vividly exemplified by an adolescent who said: "It's easy to get bored with Buddhism…. There's nothing newthere are no updates in religion. Like a new iPad-iPad 2, iPad 3-there's not Buddhism 1, Buddhism 2" (McKenzie et al. 2019, p. 81) . For psychology to become inclusive of samples across the world and attuned to culture and cultural change, researchers from different countries will benefit from collaborating, something globalization has made easier than ever before. This provides opportunities for research in places that are understudied. Such collaborations can also generate particularly rich insights, as researchers who have insider and outsider perspectives develop and complete projects together. Furthermore, such projects may also lead to explicit consideration of ways that research participants' behaviors may be influenced by the cultural (or religious) backgrounds of the researchers. Cultural psychologists differentiate etic and emic concepts. Etic concepts are the ones that researchers have formulated prior to going into a culture to do research, and which they now intend to use in studying a new culture. In contrast, emic concepts derive from the study of a culture. They are concepts that people in the culture use. While cultural psychologists may use etic concepts, the use of emic ones is necessary in order to achieve ecologically valid knowledge. Returning to the Ethics of Autonomy, Community, and Divinity, described earlier, measurements of the three ethics assess the degree to which a person uses each ethic, and also specific types of reasons used within an ethic. The coding manual for open-ended oral or written responses differentiates 13-16 subcodes for each ethic, such as "Self's Psychological Well-Being" and "Rights" for Autonomy, "Duty to Others" and "Social Order or Harmony Goals" for Community, and "Scriptural Authority" and "Natural Law" for Divinity (Jensen 2015c) . The differentiation of types of reasons within each of the three broader ethics facilitates the valid assessment of all the reasons utilized by persons of different cultural backgrounds. (A questionnaire that measures the use of the three ethics, The Ethical Values Assessment, includes fewer specific types of reasons than the coding manual but provides space for open-ended responses in order to be inclusive of emic concepts; Padilla-Walker and Jensen 2016). The types of moral reasons detailed in the coding manual for the three ethics have been generated over time through research with diverse cultural groups. As new cultures are studied, however, new emic types sometimes become necessary to add. Currently, for example, interviews with children and adolescents from low-and high-SES backgrounds in India are highlighting the importance of the emic concepts of dharma, an idea pertaining to the obligations of an inner self or soul, and paap, an idea pertaining to sin and divine punishment (Pandya et al. 2020) . In order to arrive at emic concepts, it is necessary for researchers to elicit them through their measurements and methods. This may involve having open-ended questions on surveys. It may involve ethnographic research. It may involve naturalistic observations aimed at generating knowledge of culturally-situated behaviors. It may involve using culture-sensitive prompts in experiments or interviews. In the interviews with low-and high-SES Indian children and adolescents, for example, they respond to scenarios constructed on the basis of local knowledge (Pandya and Bhangaokar 2015) . One of these scenarios reads: A group of children is playing soccer in their society garden. The festival of Ganesh Chaturthi is being celebrated and an idol of Lord Ganesha has been installed near the garden. While playing, one of the children-Nikhil-kicks the ball hard and it accidentally hits the idol. As a result, the idol breaks and soon residents of the society come to know it. They wonder who is responsible for this. None of the children speak up. Nikhil has to decide whether to tell that he broke the idol by mistake or stay quiet. In addition to ensuring that their methods and measurements are designed to elicit emic concepts, cultural psychologists will want to consider the extent to which they rely on English in their research. English is becoming an international language. By the year 2050, half of the world's population is projected to be proficient in English (Rubenstein 2017) . Children, adolescents, and emerging adults are particularly likely to learn English as part of their education and through the internet. While relying on English for research simplifies the process for English-speaking researchers and when studying cultures with different native languages, researchers need to bear in mind that use of English may change the nature of participants' responses. For example, a well-known study of Chinese emerging adults who resided in Canada found that they provided different descriptions of themselves depending on the language used (Ross et al. 2002) . Emerging adults provided more individualistic attributes when responding in English (e.g., "I am intelligent") and more descriptions pertaining to their relationships with others when using Cantonese or Mandarin (e.g., "I am an obedient child"). English, perhaps especially as conveyed through popular culture, is not simply a neutral means of communication. English is to some extent tied to certain messages and meanings that originate in an individualistic North American and Western worldview. As multilingual, multicultural, and globalized research participants switch into English, they also switch into a mindset that some extent is different from mindsets linked to their other languages (Hong et al. 2000; McKenzie 2019) . Meanwhile when indigenous languages are not used, there are invariably words and meanings that do not get communicated. Languages have words, expressions, and oral and written products that convey cultural meanings. The Danish word hygge, for example, has no equivalent English translation, and its meaning in Denmark is infused with cultural ideas and habits. The Hindu words paap and dharma, described earlier, are two other examples. From a cultural psychology perspective, words, expressions, stories, and conversations in peoples' local languages-including untranslatable words-are rich sources of understanding emic concepts. This includes what people have to say about religiosity, spirituality, and secularism in their native tongues (c.f., Lomas 2019). Cultural psychology research has called attention to the widespread presence of religion, and how adolescence is an important time for the development of religious and spiritual beliefs and behaviors across cultures. Cultural psychology research has also called attention to the rise in secularism among youth, and ways in which their secular beliefs overlap with religious and spiritual notions. We now turn to a description of some of this research which, in turn, informs subsequent recommendations for future research. Surveys across cultures have documented the presence of puberty rituals across the vast majority of traditional cultures (Schlegel and Barry 1991) . These puberty rituals, which take place in early or mid-adolescence, mark the departure from childhood and the entrance into adolescence. The rituals convey key elements of a culture's worldview to adolescents, and often they confer moral responsibility on adolescents and link that responsibility to knowledge of religious teachings. The rituals involve a lot of time and effort on the part of the adolescents and the community as whole. Sometimes the hope is that ancestors or gods will reciprocate with blessings (Schlegel and Barry 2015) . Certainly, the efforts involved contribute to collective solidarity and to making adolescents feel invested in the worldview of their culture. Researchers have also observed that contemplation of beliefs and values rises as part of identity development during adolescence, and that this process may include religious, spiritual, and secular beliefs and values (King et al. 2014; Levesque 2002; Saroglou 2012; Trommsdorff 2012) . The nature of adolescents' thoughts and questions depends on their culture. For example, research shows that Bahrain, American, and Finnish adolescents ask different questions such as "Do I get a chance to go to Mecca", "Does the Devil exist", and "[In the future], will we still have religion", respectively (Tirri et al. 2005, p. 212) . In cultures with no or relatively limited puberty rituals and without one widely endorsed religious worldview, there is more room for individualization of beliefs. The individual nature of this process in adolescence is evident across economically developed countries. The United States provides one example. While Americans are more religious than people in virtually any other economically developed country, the beliefs of American adolescents do not tend to follow traditional religious doctrines. The National Survey of Youth and Religion (NSYR), which involved over 3000 adolescents ages 13-17 in every part of the United States, found that American adolescents embrace "Moralistic Therapeutic Deism" (Smith and Denton 2005) . This includes the belief that a God exists who created the world and watches over human life; that God wants people to be good, nice, and fair to each other; and that the central goal of life is to be happy and to feel good about oneself. The NSYR, which included not only questionnaires but also interviews with a subset of its sample, found that American adolescents were increasingly likely to say that they are "spiritual, but not religious". They understood spirituality to involve a person's individual quest for meaning and happiness. As we will discuss in the next section on future research directions, more and more adolescents in traditional cultures may also be developing individualized beliefs. In part because identity achievement in economically developed countries is increasingly reached in emerging adulthood rather than adolescence, the NYSR conducted a longitudinal follow-up at ages 18-23 (Smith and Snell 2010) . Beliefs in emerging adulthood remained highly individualized. Few emerging adults accepted a standard religious doctrine; instead, they took pieces of beliefs that they had learned from their parents and many other sources and puzzled them together into their own individual creations. Consequently, religious denomination did not hold much meaning for most of them. They could state they were "Catholic" or "Presbyterian" or "Jewish" without believing much of what is stated in the traditional doctrines of these religions, and without participating in religious activities and rituals. This individualized approach to religion in emerging adulthood has also been found in other studies and is exemplified by an emerging adult man who described himself as Christian, but also said that he believed that "you don't have to be one religion. Take a look at all of them, see if there is something in them you like-almost like an a la carte belief system. I think all religions have things that are good about them" (Arnett and Jensen 2002, p. 459 ). While cultural psychology research shows the salience of religious and spiritual development in adolescence, it also documents a rise in secularism among adolescence in a variety of countries. In American studies, religiosity has been declining among adolescents and emerging adults in recent decades (Pew Research Center 2020; Twenge et al. 2015) . Religiosity also generally declines from adolescence through emerging adulthood. A recent national poll found that half of 18-29-year-old Americans did not consider themselves to be a member of any religious denomination or organization (Pew Research Center 2020). These "Nones", when asked what religion they are, respond with some version of "none"-atheist, agnostic, spiritual but not religious, or just not religious. In general, adolescents and emerging adults in developed countries are more secular than their counterparts in developing countries. Religious beliefs and practices are especially low among adolescents and emerging adults in Europe. For example, in Belgium, only 8% of 18-year-olds attend religious services at least once a month (Goossens and Luyckx 2006) . In Spain, traditionally a highly Catholic country, only 18% of adolescents attend church regularly (Gibbons and Stiles 2004) . A recent interview study in Denmark, one of the least religious countries in the world (Haraldson 2006 ; World Population Review 2020), found that almost 80% of emerging adults described themselves as either agnostic, atheist, or as having no religious beliefs (Arnett and Jensen 2015) . Traditionally, religion has provided answers to questions about the meaning of life and death. How do secular adolescents and emerging adults think about these questions? In that same interview study in Denmark, emerging adults were asked what they believe happens after death (Arnett and Jensen 2015) . Despite unbelief being the norm, 62% of the Danish emerging adults nonetheless believed in some form of afterlife. They were vague about what form it might take. To some, it seemed illogical that death should be the end of existence ("I find it difficult to accept that it's just over and done"). For others, it was emotionally unpalatable to believe that there is no life after death ("I can't tolerate the idea that if someone in your family dies, there's nothing more"). Still others stated a belief in a soul that goes on in some form ("Our soul can't just disappear. It lives on, in one place or another, but I don't know how"). The absence of religion in the worldviews of today's Danish emerging adults can be explained partly on the basis of their society's affluence and stability. Cross-national studies have found that, although within many countries religiosity is associated with well-being and serves a protective function against risk behavior, the countries that are the most prosperous also tend to be the least religious (Diener et al. 2011; French et al. 2012; Lun and Bond 2013; Saroglou and Cohen 2014 ).Yet even for the Danish emerging adults who have had all the advantages of coming of age in a wealthy and stable society, the majority believed in some kind of life after death. As described earlier, belief in life after death is also highly common in hunter-gather societies, whereas belief in high gods-especially high gods who are actively involved in human affairs-is far less common. The everyday lives of Danish emerging adults are very different from the lives of young people in hunter-gather societies, but some of the existential questions they contemplate are similar. From the point of view of cultural psychology, the rise of secularism in many parts of the world merits future research, as does the intersection of secularism with religiosity and spirituality. Cultural psychology research has shown that religiosity, spirituality, and secularism are not as sharply differentiated as might be expected. Research in purposefully selected cultures that examine the meaning of these concepts and their intersections is a promising future direction. This research needs to move beyond a previously common focus on conceptualizations of God and cast a much broader and culturally inclusive net. Clearly, research on beliefs about life after death, animism, and ancestor worship would be useful, as would research on other ideas about the supernatural such as ghosts, witches, and guardian angels. Cultural changes seem to take place more rapidly than ever, in part due to technological advances and globalization. These changes precipitate questions about the beliefs, practices, and values espoused by traditional worldviews, including religious ones. Seldom do adolescent come of age anymore with knowledge of only one traditional worldview. Adolescence and emerging adulthood are periods of life when there is newfound ability to contemplate and decide upon one's worldview. Many examples exist of how individualism is on the rise, even in traditional cultures. For example, the Aborigines of Australia were nomadic hunters and gatherers until about 70 years ago (Burbank 1988) . A key part of traditional childhood socialization among the Aborigines involved the teaching of a set of beliefs and values known as the Law, including an explanation of how the world began. Traditionally, socialization of the Law also included elaborate puberty rituals. To many adolescents today, however, the Law seems irrelevant to the world they live in, which is no longer a world of nomadic hunting and gathering but of schools, a complex economy, and modern media. Aborigine children and adolescents now show increasing resistance to learning and adhering to the traditional beliefs and values, and they develop more individualistic values based not just on the Law but also on their other experiences. How cultural changes are impacting the religiosity, spirituality, and secularism of adolescents is a timely topic of research. In turn, this also raises questions about the extent to which positive contributions of religiosity to adolescent development are dependent on the changing meanings and evaluations of religion within a culture. A meta-analysis of 75 studies conducted with more than 66,000 adolescents and emerging adults between 1990 and 2010 found that religiosity was modestly associated with lower levels of risk behavior and depression, and higher levels of well-being and self-esteem (Yonker et al. 2012) . The 75 studies were conducted in the context of an American society that is fairly high on religiosity. As described, however, American emerging adults are increasingly secular. They also evaluate religion more negatively than older generations. For example, 59% of 18-29-year-olds say religious people are generally less tolerant, in comparison to 34% of persons over 65 years (Cox 2019) . Fifty-five percent of emerging adults also say that religion causes more problems for society than it solves, as compared to 32% of older adults. If religion takes on negative connotations broadly within a society, will religion nonetheless contribute positively to youth development? Finally, for any question that addresses the intersection of development with cultural change, the field is ripe for cross-sequential research which studies different age-cohorts longitudinally. Cross-sequential research is the only way to document both developmental changes as well as the impact of cultural change over time. Such studies are costly, but they would greatly aid in addressing some of the most interesting questions about the future of religiosity, spirituality, and secularism in the lives of culturally diverse adolescents. Cultural psychology has raised awareness of religiosity, spirituality, and secularism in people's psychological lives. Cultural psychologists have also called attention to adolescence and emerging adulthood as important periods for the development of worldviews. As adolescents and emerging adults formulate answers to existential questions, they may draw on the religious, spiritual, and secular ideas and practices within the culture in which they live. They may seek answers collectively or individually. Increasingly, their questions and answers occur in the context of a globalizing and rapidly changing world. While cultural psychology may point to developmental commonalities, cultural psychology research provides the theoretical perspective and research tools to delve into the diverse ways that religiosity, spirituality, and secularism develop among adolescents across the world. 
paper_id= ffee261db2c0dd872b7b1325def14f597a2e7c3f	title= Understanding ST-elevation myocardial infarction in COVID-19: the marriage of bench work and big data Running title: Myocardial infarction in COVID 19 Authors	authors= Jordan  Siscel;Margo  Short;Brigid  Flynn;	abstract= 	body_text= COVID 19 has led to substantial morbidity and mortality around the world and altered healthcare delivery in nearly every hospital in every country. While COVID 19 infection is commonly thought to primarily effect the pulmonary system, the cardiovascular effects cannot be disregarded. [1] [2] [3] Severe Acute Respiratory Syndrome Coronavirus -2 (SARS-CoV-2) is the virus responsible for COVID 19. It is the third beta-coronavirus to cause an infectious outbreak. Each of these epi-or pandemics caused by beta-coronaviruses have involved not only respiratory manifestations among those infected, but also significant effects on the cardiovascular system. The first known beta-coronavirus epidemic was in 2003, when SARS-CoV-1 led to the outbreak of SARS and was noted to commonly cause hypotension and tachycardia. 1 Some patients suffered more severe cardiac manifestations, including arrhythmia, cardiomegaly, diastolic dysfunction, direct damage to the myocardium, vasogenic shock and unstable coronary plaques. In 2012, the Middle East Respiratory Syndrome Coronavirus (MERS) outbreak caused acute myocarditis, myocardial edema and left ventricular dysfunction in some infected patients. 1 Similar to its beta-coronavirus predecessors, COVID 19 leads to cardiac manifestations, however the common presentation differs and the severity is enhanced. With COVID 19, myocardial injury is in the form of STEMI or NSTEMI is the predominant cardiac manifestation with an estimated incidence of 7.2% to 27.8% in patients who have COVID 19. 1 A potential reason for the association of myocardial infarction with COVID may be due to the mechanism in which SARS-CoV-2 enters cells in order to replicate. Both SARS-CoV-1 and SARS-CoV-2 bind to the angiotensin converting enzyme (ACE)2 receptor. However, SARS-CoV-2 has a 10-20 fold greater affinity for this receptor compared to SARS-CoV-1. 4 ACE2 receptor is expressed on many tissues, including the upper airway, which allows for inhalational and droplet entry, the lungs, leading to pulmonary manifestations, and the heart. 4 Not only does SARS-CoV-2 likely lead to direct myocardial injury within myocytes, it also has negative effects on the renin angiotensin aldosterone system (RAAS) due to ACE2 receptor binding. As SARS-CoV-2 binds to the ACE2 receptor, the available number of ACE2 receptors decrease. This is important since the ACE2 receptor is responsible for downregulation of the RAAS system. While the mechanisms are still being elucidated, it is possible this binding results in unhindered RAAS action leading to vasoconstriction and aldosterone release, which could both have negative effects on cardiac function. 4 RAAS activation also activates endothelium to upregulate von Willebrand factor, leading to platelet and complement activation. Activated platelets recruit neutrophils to release neutrophil extracellular traps (NETs) which are webs of chromatin, microbicidal proteins and oxidant enzymes. 5 While NETs are meant to contain infections, NETs can also promote excessive inflammation and clotting and have been associated with arterial and venous thrombosis, disseminated intravascular coagulopathy and vasculitis. Importantly, NETs have been found to be elevated in patients with COVID. 6 All of these components of unbridled RAAS activation may be one of many etiologies behind the notable incidence of thrombotic complications in patients with COVID 19 infection. While most viruses lead to an inflammatory state, COVID appears to be extremely prothrombotic with 34% of critically ill patients with COVID suffering thrombotic complications despite being on anticoagulant thromboprophylaxis. 7 This large percentage does not include the unmeasured microthromboses, including those in the coronary vasculature, that are likely occurring simultaneously. For patients who do suffer myocardial infarctions, the COVID pandemic presented particular challenges in the diagnosis and management of acute coronary syndrome. The reasons for this are two-fold: 1) difficulty in correct diagnosis due to the overlap of symptoms with COVID and myocardial infarction, especially dyspnea and chest pain, and 2) the immediate need for reorganization and triage of every hospitalized patient in order to treat patients with COVID and to mitigate the spread of COVID 19. 8 Thankfully, many researchers were able to quickly organize collaborations and create large registries and databases aimed at understanding how the pandemic affected the healthcare system and specific patient populations. Several such registries took a granular approach and focused on patients who suffered ST-elevated myocardial infarction (STEMI). Authors have recently begun to publish initial registry and database findings concerning the implications of the COVID 19 pandemic on the diagnosis, treatment and outcomes of patients who sustained STEMI. The largest such North American registry was created by three major cardiovascular societies (Society for Cardiovascular Angiography and Interventions (SCAI), Interventional Council) which collaborated to establish the North American COVID-19 Myocardial Infarction (NACMI) Registry. The aim was to clarify the effect of the pandemic on the clinical management of acute coronary syndrome. 9 The NACMI registry is a prospective, investigator-initiated, multicenter observational registry of patients specifically hospitalized with STEMI and who have confirmed or suspected COVID infection. It includes 64 sites, of which 52 are located in the United States while 12 are located in Canada. Initial findings were published for patients admitted between January 1st, 2020 to December 6th, 2020. 10 In this analysis, demographic characteristics, management strategies and outcomes of patients in the registry (230 patients with confirmed COVID and 495 patients under investigation (PUI)) were compared to a control group of STEMI patients without COVID infection treated 5 years before the pandemic (460 patients) and included in the Midwest STEMI Consortium. Patients were matched based on age and gender in a 2:1 ratio of control (pre-pandemic) to case-matched NACMI registry patients. Patients who had a STEMI in the setting of infection with COVID were typically male (71%) and between 56 and 75 years of age. The majority of COVID positive patients were ethnic minorities (61%). The COVID positive cohort was more likely to have diabetes, but less likely to have other coronary artery disease risk factors compared to the control group, including dyslipidemia, a history of smoking, previous percutaneous intervention (PCI) or previous myocardial infarction. Additionally, presentation was more atypical with the COVID positive STEMI patients presenting with a higher percentage of dyspnea rather than chest pain. When compared to historical non-COVID controls with STEMI, the COVID positive STEMI patients were more likely to have pulmonary infiltrates on chest x-ray and more likely to be sicker pre-PCI as evidenced by a higher rate of cardiac arrest and cardiogenic shock. The primary endpoint of the study was the difference in a composite of in-hospital death, stroke, recurrent infarction and unplanned revascularization which was statistically higher in the COVID positive group with a rate of 36% compared to the control group rate of 5%. In-hospital mortality rate of the COVID group was 33% which was significantly higher than the control group (4%). It remains unclear if the COVID STEMI patients died due to pulmonary, cardiovascular or other etiology of death. This is an important distinction as the reported incidence of death from COVID in intensive care unit patients, not considering any other morbidities, is nearly 39%. 11 Central discussion points of this initial publication concerned the variance in management of the COVID STEMI patients when compared to control patients who suffered STEMI prior to the pandemic. During the pandemic, only 78% underwent angiography (compared to 100% of the control) and when they did receive angiography the door-to-balloon time was prolonged (79 minutes vs. 66 minutes). Interestingly, the COVID positive patients were less likely to receive PCI than control patients (71% vs. 93%). This may have been due to the high rate of no culprit lesions found on angiography in patients with STEMI in the setting of COVID infection (23% vs. 1%). Medical treatment alone was prescribed to 20% of patients with COVID, whereas only 2% of STEMI patients prior to the pandemic were prescribed medical management without intervention. The NACMI registry is just one of the many ongoing registries attempting to provide insight on the management of cardiovascular disease during the COVID 19 pandemic. The differences in these registries highlight potential limitations in the NACMI registry. Recently, Lala, et al. examined patients from the five hospitals in the Mt. Sinai Health System in New York. 12 Rather than analyzing based on therapy offered for STEMI patients, this study focused on incidence of increased troponin levels and outcomes in hospitalized COVID 19 patients. The study found that 36% of hospitalized COVID 19 patients had elevated troponin levels, representing myocardial injury. Furthermore, elevated troponin levels in COVID patients were significantly associated with higher in-patient mortality. When compared with the NACMI registry, which included both known and suspected COVID patients, databases such as the one described by Lala, et al., that limit enrollment to COVID patients only, instead of analyzing PUI's, offer less confusing results. In addition, comparing patients with acute coronary syndrome who were positive for COVID with contemporaneous patients who were COVID negative may have benefit. In other words, the NACMI comparison of pre-pandemic STEMI patients to pandemic STEMI patients may lead to confounding findings. This is due to the possibility that STEMI patients during the pandemic actually fared differently -even if they were COVID negative -than historical STEMI patients from five years ago. In other words, the North American healthcare system encountered challenges in treating all patients, regardless of COVID status. Thus, STEMI patients who did not have COVID also had their care impacted during the pandemic compared to previous cohorts. 13 Furthermore, the effect of delayed presentation for coronary syndromes during the pandemic likely had an equally negative compounding effect on patients with and without COVID. Hence, the outcome analysis by Garcia, et al., would be altered if, indeed, contemporary, non-COVID STEMI patients also had poor outcomes that could not be attributed to COVID infection, but were an unfortunate sequelae of limitations on medical care incurred during to the pandemic. This notion is corroborated by another registry titled the International Study on Acute Coronary Syndromes (ISACS) STEMI COVID-19 registry which was created in Europe with similar intentions to the NACMI registry. 14 The ISACS registry is much larger than the NACMI registry, including 6,609 patients versus 1,185 patients, respectively. Unlike the NACMI, the ISACS STEMI registry included only STEMI patients who underwent interventional treatment with PCI. Also different is that the ISACS STEMI registry included patients regardless of COVID status. In other words, contemporary non-COVID controls were included in the registry. These key differences allow for secondary and sub-analyses via alternative angles on how the pandemic effected cardiovascular care in STEMI patients. Similar to the NACMI registry study, the ISACS STEMI analysis compared patient outcomes utilizing two finite time points of intra-versus pre-pandemic. And, similarly, the ISACS STEMI researchers observed a significantly higher mortality in 2020 as compared with pre-pandemic mortality in 2019 (6.8% vs. 4.9%, OR: 1.41). Furthermore, the mortality rate was much higher among patients testing positive for COVID with 29% of COVID positive patients succumbing to death versus 5.5% of COVID negative patients dying (OR: 7.0). Importantly, the poor outcomes observed in patients with STEMI treated during the pandemic persisted after correction for all potential confounding factors (geographic area, direct access by ambulance, ischemia time, door-to-balloon, radial access, type of stent). This disparity in outcomes following PCI remained even after exclusion of patients who tested positive for COVID 19. These findings illustrate that it may not be prudent to compare STEMI outcomes during the 2020 pandemic with STEMI outcomes from historical controls as was conducted by the NACMI registry analysis. The ISACS STEMI registry team also identified that during the pandemic, as expected, there was a significant reduction in PCI as compared with pre-pandemic PCI. Interestingly, this reduction was not related to the incidence of COVID disease within the particular institution or to the number of COVID positive patients, at both local and national levels in these European centers. Unfortunately, the incidence of PCI within the specific Canadian and U.S. sites in the NACMI registry was not reported, but would be an interesting data point as this, too, could affect STEMI outcomes. While there is no perfect registry, database or study, researchers are beginning to identify important signals from these analyses. Equally of import, understanding distinct mechanisms related to the notable incidence of myocardial infarction in patients with COVID, especially concerning the ACE2 receptor and RAAS dysregulation, is advancing rapidly. Bridging of this molecular work alongside the registry and database findings is important in order to understand the clinical presentations and the optimal treatment options for patients with COVID and cardiovascular dysfunction. The collaborations that quickly formed among countries and hospitals in order to achieve a large volume of patient data during the pandemic is commendable. Researchers who partook in these efforts will aid the on-going treatment of COVID patients. Unfortunately, these impressive efforts in studying COVID 19 may need to be revised and reinvented in a future pandemic. Epidemiologists fear that zoonotic diseases, such as COVID 19, will continue to occur. If past holds true, the severity of each outbreak, including cardiac manifestations, may be unfathomably worse than the one before. And, just as in the past, the weapon that scientists and physicians will need most, will be data collection and analysis in order to save lives. 
paper_id= ffee9b5aa941373afcb67928225bda51c2ca9fb2	title= 	authors= Mark P Breazzano;Junchao  Shen;Aliaa H Abdelhakim;Lora R Dagi Glass;Jason D Horowitz;Sharon X Xie;C  Gustavo De Moraes;Alice  Chen-Plotkin;Royce W S Chen;Martin  Helen;  Kimmel;	abstract= 	body_text= From March 2-April 12, 2020, New York City (NYC) experienced exponential growth of the COVID-19 pandemic due to novel coronavirus (SARS-CoV-2). Little is known regarding how physicians have been affected. We aimed to characterize COVID-19 impact on NYC resident physicians. IRB-exempt and expedited cross-sectional analysis through survey to NYC residency program directors (PDs) April 3-12, 2020, encompassing events from March 2-April 12, 2020. From an estimated 340 residency programs around NYC, recruitment yielded 91 responses, representing 24 specialties and 2,306 residents. 45.1% of programs reported at least one resident with confirmed COVID-19: 101 resident physicians were confirmed COVID-19-positive, with additional 163 residents presumed positive for COVID-19 based on symptoms but awaiting or unable to obtain testing. 56.5% of programs had a resident waiting for, or unable to obtain, COVID-19 testing. Two COVID-19-positive residents were hospitalized, with one in intensive care. Among specialties with >100 residents represented, negative binomial regression indicated that infection risk differed by specialty (p=0.039). Although most programs (80%) reported quarantining a resident, with 16.8% of residents experiencing quarantine, 14.9% of COVID-19-positive residents were not quarantined. 90 programs, encompassing 99.2% of the resident physicians, reported reuse or extended mask use, and 43 programs, encompassing 60.4% of residents, felt that personal protective equipment (PPE) was suboptimal. 65 programs (74.7%) have redeployed residents elsewhere to support COVID-19 efforts. Many resident physicians around NYC have been affected by COVID-19 through direct infection, quarantine, or redeployment. Lack of access to testing and concern regarding suboptimal PPE are common among residency programs. Infection risk may differ by specialty. For the first time in over a century, the United States (US) is part of a global pandemic known as COVID-19, 1 with anticipated impact comparable to the Spanish flu of 1918. The causative novel coronavirus (2019-nCoV, SARS-CoV-2), first described in Wuhan, China, 2,3 has spread worldwide, particularly in New York City (NYC), which is currently the US epicenter of cases and mortality. 4 The first case was confirmed in NYC on March 1, 2020; 5 six weeks later, hundreds of patients are dying from COVID-19 daily. 6 Healthcare workers (HCW) are on the front lines of this pandemic. 2, 7 However, although at least 4,500 peer-reviewed articles have been published on this topic between January 1, 2020 and April 18, 2020, comparatively little is known about the toll of COVID-19 on healthcare workers directly occupied with patient care. Notably, the first physician to sound the alarm about the novel coronavirus causing severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) was the Chinese ophthalmologist Li Wenliang, who died after infection by a pre-symptomatic patient. 8 Anecdotally, HCW in NYC have experienced unique challenges in combatting the illness, including close contact with the sickest patients, exposure to high viral loads, redeployment to clinical duties outside of their ordinary responsibilities, and severe shortages in personal protective equipment (PPE). 7, 9, 10 Among those at highest risk are resident physicians, who are commonly stationed in high-acuity settings and comprise a substantial part of the healthcare workforce in the United States. 11 The activities of resident physicians are standardized among residency training programs throughout the US via accreditation with the Accreditation Council for Graduate Medical Education (ACGME), with each residency program supervised by an appointed program director. 12 The structure of residency programs, with many resident physicians reporting to one program director responsible for their activities and well-being, makes the resident physician population practical for study through collection of data from residency program directors. However, to our knowledge, no primary peer-reviewed data has addressed implications of COVID-19 for resident physicians, whose situation has only been described in editorials. 13, 14 We also sought to explore whether specialty-specific risks existed for COVID-19 infection. By surveying residency program directors among all departments within NYC from April 3-12, 2020, we captured the immediate features and impact of COVID-19 among resident physicians during the exponential phase of the COVID-19 pandemic in NYC. As future or recurrent outbreaks are likely, such knowledge may help tailor future interventions to mitigate the burden of COVID-19 among HCWs. directors, and contact electronic mail addresses were retrieved from either previous correspondence or publicly available search tools with ACGME via hyperlink (https://apps.acgme.org/ads/Public/Programs/Search). The survey was first distributed to 12 ophthalmology residency program directors in the greater NYC area, who expanded distribution to 188 additional non-ophthalmology training programs within their own institutions. As a second method, 303 programs identified separately in the ACGME database by two authors (M.P.B., A.H.A.) were also contacted electronically. Ultimately, at least one contact attempt was made at every known residency training program in the greater NYC area (approximately 340 total), as our two approaches may have overlapped. An anonymous survey (Supplemental Content) eliciting de-identified information was included in circulated electronic mail message by hyperlink with SurveyMonkey® cloudbased software (SurveyMonkey®, San Mateo, CA, USA). More than one survey completion by the same user was prohibited, both by request within the recruitment electronic message and based on internet protocol address. The need for subject consent was waived due to minimal risk, anonymous nature, and lack of sensitive information in the study design as per Columbia University Institutional Review Board expedited exemption protocol IRB-AAAS9946. All procedures were reviewed and in accordance with the tenets of the Declaration of Helsinki. Diagnosis or suspicion of COVID-19 among residents was elicited in our survey based on clinical presentation with symptoms including: sore throat, cough, fever, shortness of breath, chest pain, myalgia, malaise, conjunctivitis, anosmia, or gastrointestinal symptoms. Survey questions pertained to 3 distinct groups among resident doctors: (1) "confirmed" -defined as resident physicians with COVID-19 symptoms and positive test results; (2) "presumed" -defined as resident physicians with COVID-19 symptoms without test results, and (3) "suspected" -defined as resident physicians with COVID-19 symptoms and negative test results. Suspected cases were tallied in our analysis due to the relatively high false negative rate of reverse transcription polymerase chain reaction (RT-PCR) testing for active infection by this virus 15, 16 as well as high pre-test probability for COVID-19 in the context of suggestive symptoms, due to HCW status and NYC location. Responses were reviewed for inclusion based on specific training program. Fellowship programs were excluded from the analysis. Because certain specialties have programs that exist as a residency-fellowship continuum, these training programs with ACGME accreditation were included. We did not distinguish between these integrated programs and residency-only programs. All programs included were ACGME-accredited, with the exception of oral maxillofacial surgery (OMFS), which was included as many OMFS programs offer clinical experience through ACGME-accredited rotations such as general surgery, ultimately leading to medical licensure with or without an M.D. degree, in All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint this version posted April 28, 2020. . https://doi.org/10.1101/2020.04.23.20074310 doi: medRxiv preprint addition to pre-existing dental licensure. Programs were included if within or immediately adjacent to NYC. All queried programs but one were centralized within 30 miles of Central Park in Manhattan, verified by Google Maps with hyperlink: https://www.google.com/maps (Google Inc., Mountain View, CA, USA) for distance calculations which used mailing addresses from primary affiliations for each recipient of the survey. Proportions are reported as percentages with 95% confidence interval (CI) calculated using the Clopper-Pearson approach. Specialties with representation by 100 or more residents were selected for further between-specialty analyses. Because the number of COVID-19 positive residents by individual programs were count outcomes and non-normally distributed, Poisson regression and negative binomial regression were fitted to determine whether specialty, program size, or date of survey response affected the number of residents with positive COVID-19 tests. Likelihood ratio (LR) testing was used to determine the appropriateness between Poisson regression and negative binomial regression. Fisher's exact test was used to assess the overall effect of specialties on the proportion of residents with confirmed COVID-19. Pearson's chi-squared test was used to compare infection rate and redeployment rate between departments. Correction for multiple comparisons was made with Bonferroni procedures. Statistical analyses were performed in the R programming language (Version.1.2.5042). Type 1 error was defined at the 5% level for hypothesis testing with two-tailed probabilities. Study sample 102 program director responses were received between April 3 and April 12, 2020, 10 of which were excluded based on residency and ACGME-accreditation criteria, and one of which was removed as it was incomplete and reported zero residents in the program. Thus, 91 programs representing 2,306 residents from 24 different specialties were included in this study (Figure 1) . Average program size was 25 residents (standard deviation [S.D.] = 21), with a range of 1 -98 residents per program. 49 programs (53.8%, 95% CI 43.1-64.4) reported that residents provided services for >3 different hospitals. All 91 program directors reported numbers for symptomatic residents who had tested positive for COVID-19 ("confirmed" cases). 90/91 program directors reported numbers for symptomatic residents who were awaiting or unable to obtain testing ("presumed" All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint this version posted April 28, 2020. . https://doi.org/10.1101/2020.04.23.20074310 doi: medRxiv preprint cases) and symptomatic residents who had tested negative for COVID-19 ("suspected" cases). In total, 41/91 (45.1%, 95% CI 34.6-55.8) programs reported at least one confirmed case, 49/90 programs (54.4%, 95% CI 43.6-65.0) reported at least one presumed case, and 36/90 programs (40%, 95% CI 29.8-50.9) reported at least one suspected case. Among all residents from all programs pooled together, 101 residents were confirmed cases, 163 were presumed cases, and 76 were suspected cases (Figure 2) . The total number and proportion of affected residents by specialty are shown in the Table. 86/91 program directors reported knowing how many residents were tested for COVID-19. Among the 2,088 residents in these 86 programs, a total of 242 residents (11.6%, 95% CI 10.  To determine whether any specific medical specialties were more likely to have a COVID-19 positive resident, all specialties with more than 100 residents in our sample were compared. Programs that met this criterion included anesthesiology, emergency medicine, general surgery, internal medicine, ophthalmology, pediatrics, and psychiatry (Figure 1) . Three specialties (anesthesiology, emergency medicine, ophthalmology) appeared to cluster as high-risk specialties by proportion of residents with confirmed COVID-19, compared to the remaining specialties (p=0.015, Fisher's exact test). In negative binomial models adjusted for the size of the residency program and date of survey completion, specialty remained significantly associated with the number of confirmed positive residents (p= 0.039). Using anesthesiology as the reference group (as this specialty had the highest number of positive residents), anesthesiology was significantly more likely to have a COVID-19 confirmed resident, compared to both internal medicine (p= 0.020) and pediatrics (p = 0.029, Figure 3) . Symptom onset was reported to occur as early as or prior to the week of March 2-8, 2020 for 5 residents (1.5%) with confirmed (n=1), presumed (n=3), or suspected (n=1) All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint this version posted April 28, 2020.  The majority of programs, encompassing 1,832 residents (79.4%, 95% CI 77.7-81.1) used either N95 or surgical masks during patient encounters, depending on the context. Nineteen programs, encompassing 323 residents (14%, 95% CI 12.6-15.5) used only surgical masks during patient encounters; and 8 programs, encompassing 31 residents, (5.7%, 95% CI 4.8-6.7) used an N95 respirator for all patient encounters. Excepting one radiology program, all programs, encompassing 99.2% of residents in this study, reported reuse or extended use of their masks (vs. single-use). Protocols mandating universal wearing of surgical masks were introduced as early as the week of March 2-8, 2020 in only 3 programs (3.5%), and as late as March 30-April 5, 2020 in 20 programs (23.5%, Figure 4) . 43/87 program directors (49.4%, 95% CI 38.5-60.4) representing 1,314 residents answered "yes" when asked whether their residents had had to work with suboptimal PPE. There was no correlation between the mask type used by residents (surgical, N95, or both) to perceived shortage of PPE. Among the 101 residents with confirmed COVID-19, 57 (56.4%, 95% CI 46.2-66.3) presented to clinic or primary care, 17 (16.8%, 95% CI 10.1-25.6) visited the emergency department, 2 (2.0%, 95% CI 0.2-7.0) were hospitalized, and 1 (1%, 95% CI 0-5.4) had care escalated to the intensive care unit (ICU). The 163 residents with presumed COVID-19 presented to primary care or clinic in 40 cases (24.5%, 95% CI 18.1-31.9) and the emergency department in 6 cases (3.7%, 95% CI 1.4-7.8). Among the 76 residents with suspected COVID-19, 38 (50%, 95% CI 38.3-61.7) were evaluated in clinic or by primary care, 5 (6.5%, 95% CI 2.2-14.7) presented to emergency department, and 1 (1.3%, 95% CI 0-7.1) was hospitalized. In total, among the 340 residents with confirmed, presumed or suspected COVID-19, 3 (0.9%, 95% CI 0.2-2.6) were hospitalized (1 each from emergency medicine [who was also hospitalized and went to the ICU], ophthalmology, and psychiatry programs; 2 were confirmed, and 1 suspected COVID-19). There were no deaths reported in any of the completed surveys. One program (pediatrics) of 58 residents did not report any quarantine data. Of the remaining 90 programs encompassing 2,248 residents (including 339 residents with All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint this version posted April 28, 2020. 6 ) reported at least one asymptomatic, but exposed, resident, who was quarantined. Among 34 asymptomatic but exposed residents with known duration of quarantine, the time ranged from 1 -14 days. 15 residents (14.9%, 95% CI 8.6-23.3) from 2 programs with confirmed COVID-19, 26 residents (16.0%, 95% CI 10.8-22.6) from 5 programs with presumed COVID-19, and 5 residents (6.6%, 95% CI 2.2-14.7) from 2 programs with suspected COVID-19 were not quarantined. Redeployment 87/91 program directors responded to questions about residents redeployed to other departments or locations to support COVID-19 efforts. 65 programs (74.7%, 95% CI 64.3-83.4) reported at least one resident redeployed, with 35 (40.2%, 95% CI 29.9-51.3) programs redeploying more than one-third of their workforce. 594 residents (27.3% of 2,176 residents for whom redeployment information is known, 95% CI 25.4-29.2) were reported to be redeployed. Anesthesiology had the highest redeployment rate, with 158 (56.0% of 282 total anesthesiology residents, 95% CI 50.0-61.9) residents being redeployed to other services (p<0.001, Pearson's chi-squared test). Of programs that redeployed residents, 53 programs (81.5%, 95% CI 70.0-90.1) instituted redeployment between the fourth and fifth weeks of March, approximately 1 month after the first case in NYC was confirmed. Among residents redeployed to duties beyond their usual clinical responsibilities, the majority went to the ICU (283/594 redeployed residents, 47.6%, 95% CI 43.6-51.7), followed by hospital floors (176/594, 29.6%, 95% CI 26.0-33.5), and the emergency department (85/594, 14.3%, 95% CI 11.6-17.4). As of the date of our survey's close, NYC is the epicenter of the COVID-19 pandemic in the US, and the daily death toll continues to rise. 6 Here, we report the impact of COVID-19 on NYC resident physicians, as reported by their residency program directors, surveyed between April 3-12, 2020. Many of these residents have been directly infected (101 confirmed positive), quarantined (16.8% of residents), or redeployed (27.3% of residents) to duties outside of their usual clinical activities in support of COVID-19 efforts. 101 residents were reported to have confirmed COVID-19 in our sample. While this is 4.4% of the 2306 residents whose program directors participated in our study, the true rate in our sample may be higher, since 242 resident physicians were tested for COVID-19, and only 177 had received their test results at the close of the survey. We highlight a few points found in our study. First, program directors reported 15 confirmed COVID-19 residents and 26 presumed COVID-19 residents who were not quarantined. Whether this was due to these residents being initially asymptomatic, workforce need, delay in obtaining testing, or some other reason is not known. All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint this version posted April 28, 2020. . https://doi.org/10.1101/2020.04.23.20074310 doi: medRxiv preprint However, we do note that 57.4% of residency program directors reported at least one resident awaiting or unable to obtain COVID-19 testing. Second, 49.4% of residency directors answered "yes" to the question of whether resident physicians for whom they were responsible had suboptimal PPE. While this might reflect selection bias with respect to which residency directors chose to answer the survey, we note that 90/91 programs reported reuse or extended use of masks that are ordinarily disposable after a single use. Third, we find that some specialties may be at greater risk for contracting COVID-19 compared to others. In particular, anesthesiology had significantly higher numbers of confirmed COVID-19 residents than several other specialties. It is possible that the higher infection rates may be due to the critical skill of intubation provided by anesthesiologists, which comes with high probability of aerosolization and exposure to viral particles. 17 We recognize limitations to our current study. While not all presumed and suspected cases have COVID-19, we present these numbers given the high pre-test probability of infection in HCW with suggestive symptoms, as well as known limitations of RT-PCR detection of the virus. 15, 16 Future work using serological testing may provide a more accurate census of confirmed positives, as recent studies have shown. 18 Selection bias may have affected our findings, as fields such as ophthalmology may have been overrepresented due to the authors' connections to colleagues in this field, while other specialties may have been under-represented because of significant stress and lack of time to complete the survey. It is also possible that program directors whose residents have been affected by COVID-19 would be more likely to respond. However, we capture 91 NYC residency programs (out of an estimated 340 total residency programs) during a period of exponential pandemic growth, offering a unique perspective on the impact on resident physicians during what may be the height of COVID-19 in NYC. Indeed, capturing the experience as it happens avoids recall bias after the fact. It is our hope that this insight may allow locations not yet as substantially affected by COVID-19 to better anticipate the needs of resident physicians, who are truly at the front lines of an unprecedented challenge. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint this version posted April 28, 2020. . https://doi.org/10.1101/2020.04.23.20074310 doi: medRxiv preprint All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. 25 (6.5%) 3 (5.4%) 9 (5.1%) 2 (5.0%) 3 (4.8%) 4 (4.5%) 4 (4.4%) 3 (3.7%) 1 (3.7%) 9 (3.6%) 5 (3.4%) 2 (2.4%) 1 (2.1%) 1 (2.1%) 2 (1.7%) 1 (1.1%) 1 (0.7%) 0 (0.0%) 0 (0.0%) 32 (8.4%) 3 (5.4%) 17 (9.6%) 3 (7.5%) 17 (27.4%) 4 (4.5%) 7 (7.7%) 1 (1.2%) 1 (3.7%) 16 (6.3%) 10 (6.8%) 3 (3.6%) 1 (2.1%) 4 (8.3%) 5 (4.2%) 5 (5.6%) 2 (1.6%) 7 (12.1%) 4 (30.8%) 12 (3.1%) 2 (3.6%) 7 (4.0%) 2 (5.0%) 0 (0.0%) 1 (1.1%) 1 (1.1%) 7 (9.2%) 0 (0.0%) 13 (5.2%) 5 (3.4%) 7 (8.4%) 4 (8.3%) 1 (2.1%) 0 (0.0%) 0 (0.0%) 0 (0.0%) 0 (0.0%) 1 (8.6%) Nuclear Medicine 5 0 (0.0%) 1 (20.0%) 1 (20.0%) Oral/Maxillofacial Surgery 18 0 (0.0%) 0 (0.0%) 0 (0.0%) Medical Genetics 2 0 (0.0%) 0 (0.0%) 0 (0.0%) Total 2,306 101 (4.4%) 163 (7.1%) 76 (3.3%) All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint this version posted April 28, 2020. . https://doi.org/10.1101/2020.04.23.20074310 doi: medRxiv preprint 
paper_id= ffef5d5de315ab0701f7c1240be589b1590b720a	title= 	authors= 	abstract= 	body_text= CVD has been observed among patients with COVID-19, and these comorbidities are associated with increased mortality [17] [18] [19] [20] [21] [22] . Furthermore, COVID-19 seems to promote the development of cardiovascular disorders, such as myocardial injury, arrhythmias, acute coronary syndrome (ACS) and venous thromboembolism [23] [24] [25] . Children with COVID-19 have also been reported to develop hyperinflammatory shock with features akin to Kawasaki disease, including cardiac dysfunction and coronary vessel abnormalities 26 . Together, these data indicate the presence of a bidirectional interaction between COVID-19 and the cardiovascular system, but the mechanisms underlying this interaction remain elusive. The high burden of systemic inflammation associated with COVID-19 has been proposed to accelerate the development of subclinical disorders or cause de novo cardiovascular damage 5, [12] [13] [14] . ACE2, which is a key surface protein for virus entry and part of the reninangiotensin-aldosterone system (RAAS), is also thought to be involved in this interaction on the basis of findings from animal models [12] [13] [14] [15] . The fast-moving nature of this research field necessitates the integration of available biological data with clinical findings of COVID-19 to improve our understanding of the pathophysiology of the disease and to contribute to the development of potential therapies. In this Review, we summarize our current knowledge of SARS-CoV-2 from a biological viewpoint, with an emphasis on the interaction between the viral S protein and human ACE2. Furthermore, we provide an overview of the clinical findings related to the effects of COVID-19 on the cardiovascular system. Finally, we discuss the possible link between common cardiovascular drugs and susceptibility to COVID-19 and the potential cardiovascular effects of drugs used to treat COVID- 19. Of note, several limitations of this Review need to be acknowledged. First, given the fast-moving nature of this research field, we will discuss and cite data from preprint reports on bioRxiv or medRxiv in addition to peer-reviewed articles that have cited preprint reports. These findings need to be interpreted with care and require validation in larger studies. Second, the majority of clinical COVID-19 data mentioned in this Review are from China, given their early experience with the disease. Finally, the clinical data on COVID-19 are predominantly derived from non-randomized studies. Therefore, potential biases and confounding factors associated with observational data, such as differences in patient background, diagnostic methods and health-care systems, should be taken into account. Since the emergence of SARS-CoV-2, extensive efforts have been made to characterize the features of this novel coronavirus through genomic sequence studies [1] [2] [3] and the evaluation of viral protein structure [9] [10] [11] 27, 28 . Coronaviruses, which are a large family of single-stranded enveloped RNA viruses, were not recognized as being highly pathogenic in humans until the outbreak of SARS caused by SARS-CoV in 2002-2003 (refs 29,30 ). A decade after the SARS pandemic, an outbreak of MERS was detected in Saudi Arabia, caused by MERS-CoV, another highly pathogenic coronavirus 29 . In the ensuing years, extensive studies of SARS and MERS have contributed to our understanding of coronavirus biology. On the basis of phylogenic analyses, both SARS-CoV and MERS-CoV are thought to have originated in bats, which are likely to be a major natural reservoir of coronaviruses 29 . A number of genetically diverse coronaviruses that are related to SARS-CoV or MERS-CoV have been discovered in bats worldwide 1, 29, 31 . SARS-CoV-2 has been shown to have 79.6% genomic sequence identity with SARS-CoV and 96.0% with the bat coronavirus RaTG13 (refs [1] [2] [3] 8 ). Given this genomic sequence homology, SARS-CoV-2 is thought to share many biological features with SARS-CoV, suggesting that we can apply, at least in part, our rich knowledge of SARS-CoV biology and pathogenesis to understanding SARS-CoV-2 (refs 4,29,30 ). For example, both SARS-CoV and SARS-CoV-2 use ACE2 as an attachment receptor to enter host cells, whereas MERS-CoV uses dipeptidyl peptidase 4 as the attachment receptor 1, 2, 29, 30 . Coronaviruses have a crown-like morphology, consisting of four structural proteins known as spike (S), envelope (E), membrane (M) and nucleocapsid (N) proteins 29, 30, 32 (fig. 1a) . The viral genome surrounded by the N protein is a positive-sense, single-stranded rNA that functions as both a genome and an mRNA 29, 30, 32 . Coronaviruses can be divided into four genera: α, β, γ and δ, of which only α and β-coronaviruses are known to infect humans. Phylogenetic studies have revealed that all three highly pathogenic coronaviruses (SARS-CoV, MERS-CoV and SARS-CoV-2) belong to the genus Betacoronavirus 1-3, 29 . Like other coronaviruses, the • Severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), the virus that causes coronavirus disease 2019 (COVID- 19) , shares many biological features with SARS-CoV, the virus that causes severe acute respiratory syndrome, owing to 80% genomic sequence identity. • The interaction between the viral spike (S) protein and angiotensin-converting enzyme 2, which triggers entry of the virus into host cells, is likely to be involved in the cardiovascular manifestations of COVID-19. • The presence of underlying cardiovascular comorbidities in patients with COVID- 19 is associated with high mortality. • COVID-19 can cause cardiovascular disorders, including myocardial injury, arrhythmias, acute coronary syndrome and venous thromboembolism. • Several medications used for the treatment of COVID-19 have uncertain safety and efficacy profiles. The ability of a given pathogen to infect specific cell or tissue types of a host. An acute febrile illness of unknown cause that primarily affects children aged <5 years and can cause coronary artery aneurysms. Fig. 1 | Structure, genome and life cycle of SarS-CoV-2. a | Coronaviruses form an enveloped spherical particle that consists of four structural proteins (spike (S), envelope (E), membrane (M) and nucleocapsid (N)) and a positive-sense, single-stranded RNA (ssRNA) genome that is 30 kb in length. b | The 5′-terminal two-thirds of the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) genome encodes polyproteins pp1a and pp1ab, which are cleaved into 16 different nonstructural proteins. Structural proteins are encoded in the 3′-terminal one-third of the genome. The S protein consists of two subunits; the S1 subunit contains a receptor-binding domain (RBD) that binds to angiotensin-converting enzyme 2 (ACE2) on the surface of host cells, whereas the S2 subunit mediates fusion between the membranes of the virus and the host cell. Compared with the S protein of SARS-CoV, the S protein of SARS-CoV-2 has two notable features. First, within the RBD of the S1 subunit, five of the six residues that are crucial for binding to human ACE2 are mutated. Second, an insertion of four amino acid residues at the boundary between the S1 and S2 subunits is present in SARS-CoV-2 but not in SARS-CoV, which introduces a novel furin cleavage site. c | SARS-CoV-2 infection is triggered by the binding of the S protein to ACE2 on the surface of host cells, and the viral complex is incorporated into the cytoplasm either by direct fusion with the cell membrane or via endocytosis with later release into the cytoplasm from the endocytic vesicle. The S protein is cleaved at the S1/S2 boundary and the S2 subunit facilitates membrane fusion. The viral genome RNA is released into the cytoplasm, and the first open reading frame (ORF) is translated into polyproteins pp1a and pp1ab, which are then cleaved by viral proteases into small, non-structural proteins such as RNAdependent RNA polymerase (RdRP). The viral genomic RNA is replicated by RdRP. Viral nucleocapsids are assembled from genomic RNA and N proteins in the cytoplasm, whereas budding of new particles occurs at the membrane of the endoplasmic reticulum (ER)-Golgi intermediate compartment. Finally, the genomic RNA and structural proteins are assembled into new viral particles, leading to their release via exocytosis. 3CL, 3-chymotrypsin-like protease. Nature reviews | Cardiology genome of SARS-CoV-2 is approximately 30 kb in length and contains ten open reading frames (ORFs) that encode 24-27 genes 1,2 . The 5′-terminal two-thirds of the genome encodes polyproteins pp1a and pp1ab, which are cleaved into 16 non-structural proteins, such as RNA-dependent RNA polymerase (RdRP). The S, E, M and N structural proteins are encoded in the 3′-terminal one-third of the genome 29, 30, 33 (fig. 1b) . Among the structural proteins, the S protein has pivotal roles in virus attachment and entry and disease pathogenesis [9] [10] [11] 29, 30 . In SARS-CoV and SARS-CoV-2, the binding of the viral S protein to ACE2 triggers virus entry into the host cell. Therefore, the interaction between the S protein and ACE2 has been considered to be a promising therapeutic target for the development of vaccines 34, 35 , neutralizing antibodies 36, 37 and antiviral compounds 38, 39 for SARS-CoV and SARS-CoV-2. The sequence similarity between the S protein of SARS-CoV and that of SARS-CoV-2 is approximately 76% for the whole protein, 73% for the receptor-binding domain and 50% for the receptor-binding motif 11 . The S protein of SARS-CoV-2 contains two distinctive features that are not present in the S protein of SARS-CoV 4, 8 . First, the S protein of SARS-CoV-2 maintains a high binding affinity to human ACE2 even though five of the six residues present in the SARS-CoV receptor binding motif that are critical for binding to human ACE2 are mutated in the S protein of SARS-CoV-2 (L455, F486, Q493, S494 and N501) 4, 8 . Cryogenic electron microscopy studies have demonstrated that the S protein of SARS-CoV-2 can bind directly to human ACE2 with a similar or even higher affinity than that of SARS-CoV 9-11 . This high binding affinity is likely to be related to the high transmissibility of SARS-CoV-2 and the severity of COVID-19. Second, the S protein of SARS-CoV-2 has an insertion of four amino acid residues (12 nucleotides) at the boundary between the S1 and S2 subunits, which introduces a novel furin cleavage site 4, 8 . This novel cleavage site has not been observed in SARS-CoV or other SARS-related coronaviruses originating from bats and seems to facilitate the processing of S protein at the S1 and S2 subunit boundary by ubiquitously expressed furin-like proteases for preliminary activation 4 . Although the function of this novel cleavage site is unknown, similar cleavage sites have been described in highly pathogenic avian influenza viruses and the Newcastle disease virus 4, 8 . This notable feature has been proposed to have a role in expanding cell or tissue tropism of SARS-CoV-2 (ref. 4 ), contributing to the multiorgan effects of COVID-19 ( fig. 1b ). Infection with either SARS-CoV or SARS-CoV-2 involves binding of the viral S protein to ACE2 on the surface of the host cell. The receptor-binding domain on the surface subunit S1 of the S protein is responsible for attachment of the virus to ACE2. After binding, the S protein is cleaved at the S1/2 and S2′ regions (in a process known as S protein priming) by the transmembrane serine protease TMPRSS2, which in turn facilitates the fusion of the viral membrane with the membrane of the host cell and direct entry of the virus into the cytoplasm 4, 29, 30, 40 (fig. 1c ). Respiratory tract epithelial cells express both ACE2 and TMPRSS2 on their surface, and this direct or 'early' entry pathway seems to be the predominant mode of in vivo entry by SARS-CoV and, probably, SARS-CoV-2 into the respiratory tissue 40 . Alternatively, SARS-CoV-2 can also use an endosomal entry pathway, whereby the ACE2-virus complex is translocated to endosomes and S protein priming is performed by the endosomal cysteine proteases cathepsin B and cathepsin L, after which the virus is released from the endosome into the cytoplasm. This endosomal entry pathway, which can be blocked by either lysosomotropic agents (such as hydroxychloroquine) or cathepsin inhibitors, might be the predominant entry pathway used by coronaviruses in the infection of cells cultured in vitro 40 , but the importance of this pathway for infection in vivo remains unclear. After the release of the viral genomic RNA into the cytoplasm, the first ORF is translated into polyproteins pp1a and pp1ab, which are then cleaved by viral proteases into small non-structural proteins such as RdRP. The viral genomic RNA is then replicated using viral RdRP, and the four structural proteins (S, E, M and N) are translated through the endoplasmic reticulum and Golgi complex of the host cell. Finally, the genomic RNA and structural proteins are assembled into new viral particles, leading to their release through exocytosis 29, 30 ( fig. 1c ). Each step of the viral life cycle described here is a potential therapeutic target, including S protein priming by TMPRSS2 (a target of the serine protease inhibitor camostat mesylate), membrane fusion and endocytosis (a target of the antimalarial drug chloroquine and anti-influenza drug umifenovir) and RNA replication by RdRP (a target of the antiviral agents favipiravir, remdesivir and ribavirin) 41,42 . Underlying cardiovascular comorbidities CVD is a common comorbidity observed in patients infected with SARS or MERS (with a prevalence of 10% and 30%, respectively) [12] [13] [14] 43, 44 . A series of reports on the clinical characteristics of patients with COVID-19 have also described similar findings [12] [13] [14] [15] . Early reports from China found that CVD and its risk factors, such as hypertension and diabetes mellitus, were common pre-existing conditions in patients with COVID-19, but the definition of CVD used in each study was vague 17, 18, 21, 22, 45 (TAble 1) . In an early report from Wuhan involving 41 patients who were hospitalized with COVID-19 by 2 January 2020, the prevalence of any comorbidity was 32% and the most common underlying diseases were diabetes (20%), hypertension (15%) and other CVDs (15%) 17 . The high prevalence of these comorbidities was confirmed in subsequent studies [18] [19] [20] [21] [22] [45] [46] [47] . Importantly, the prevalence of these pre-existing conditions was higher in critically ill patients (such as those admitted to the intensive care unit (ICU)) and in those who died. In a single-centre cohort study of 138 patients hospitalized with COVID-19 in Wuhan, 46% of patients had any comorbidity (72% of patients in the ICU), 31% of patients had hypertension (58% of patients in the ICU), 15% of patients had other CVDs Orfs. Continuous stretches of nucleotide sequences in genomic DNA or mrNA between a start codon (usually ATg in DNA) and a stop codon (usually TAA, TAg or TgA in DNA) that are potentially translated into proteins. A specific peptide sequence in precursor proteins that can be cleaved by an enzyme furin to facilitate conversion to a biologically active state. Drugs that are taken up selectively into lysosomes. www.nature.com/nrcardio (25% of patients in the ICU) and 10% of patients had diabetes (22% of patients in the ICU) 18 . Similarly, in a multicentre cohort study involving 191 patients hospitalized with COVID-19 in Wuhan, 48% of patients had any comorbidity (67% of those who died), 30% of patients had hypertension (48% of those who died), 19% of patients had diabetes (31% of those who died) and 8% of patients had coronary heart disease (24% of those who died) 19 . Furthermore, in a report involving 1,099 patients with COVID-19 from mainland China, 24% of patients had any comorbidity (39% of critically ill patients), 15% of patients had hypertension (24% of critically ill patients), 7% of patients had diabetes (16% of critically ill patients) and 3% of patients had coronary heart disease (6% of critically ill patients) 20 . The overall case fatality rate of COVID-19 reported by the Chinese Center for Disease Control and Prevention as of 11 February 2020 was 2.3% (1,023 deaths among 44,672 confirmed cases) 21, 45 . The individual case fatality rate of patients with CVD was 10.5% (highest among those with any comorbidities, including chronic respiratory disease (6.3%) or cancer (5.6%)), the case fatality rate of patients with diabetes was 7.3% and that of patients with hypertension was 6.0% 45 . Of note, these early approximations of case fatality rate are likely to be overestimated given that the estimates did not account for the many people who had the virus but were not tested. A similar trend in the prevalence of comorbidities has been reported by researchers in other countries [46] [47] [48] [49] [50] [51] . In a report involving 1,591 patients with COVID-19 who were admitted to the ICU in Italy, 49% of patients had pre-existing hypertension, 21% had CVD and 17% had diabetes 46 . Furthermore, in a report of 393 consecutive patients hospitalized with COVID-19 in New York, USA, up to 50% of patients had hypertension (54% of ventilated patients), 36% had obesity (43% of ventilated patients), 25% of patients had diabetes (28% of ventilated patients) and 14% of patients had coronary artery disease (19% of ventilated patients) 47 . Of note, this study from New York highlighted the high prevalence of comorbid obesity among patients with COVID-19, which had not been reported in the studies on patients in China probably owing to differences in the background prevalence of obesity between the USA and China. Investigators in this study suggest that obesity might also be a risk factor for respiratory failure and the need for invasive mechanical ventilation 47 . Although the predominant clinical manifestation of COVID-19 is viral pneumonia 1,2,48,49 , COVID-19 can also cause cardiovascular disorders such as myocardial injury, arrhythmias, ACS and thromboembolism [12] [13] [14] [15] ( fig. 2 ). Some patients who present without the typical symptoms of fever or cough have cardiac symptoms as the first clinical manifestation of COVID-19 (refs 52,53 ). Myocardial injury during the course of COVID-19 is independently associated with high mortality 23 . Furthermore, a possible link between COVID-19 and a Kawasaki disease-like syndrome has been described in children 26 . Acute myocardial injury, as evidenced by elevated levels of cardiac biomarkers or electrocardiogram abnormalities, was observed in 7-20% of patients with COVID-19 in early studies in China [17] [18] [19] [20] [21] . The presence of myocardial injury was associated with a significantly worse prognosis 23 . In the initial report of 41 patients with COVID-19 in Wuhan, 5 patients had myocardial injury with elevated levels of high-sensitivity cardiac troponin I (>28 pg/ml), and 4 of these 5 patients were admitted to an ICU 17 . In a multicentre cohort study of 191 patients with COVID-19, 33 patients (17%) had acute cardiac injury, of whom 32 died 19 . In a subsequent study of 416 patients hospitalized with COVID-19, 82 patients (20%) had evidence of cardiac injury, which was associated with a 5-fold increase in the need for invasive mechanical ventilation and an 11-fold increase in mortality 23 . Of note, cardiac injury was found to be an independent risk factor for inhospital mortality 23 . Another study confirmed this finding and reported that the rate of death in patients with elevated levels of cardiac troponin T was 37.5%, whereas, in patients with underlying cardiovascular comorbidities plus elevated levels of cardiac troponin T, it was almost double (69.4%) 24 . Furthermore, a subsequent study demonstrated that markers of myocardial injury were predictive of the risk of in-hospital mortality in patients with severe COVID-19 (ref. 25 ). The area under the receiver operating characteristic curve of the initial cardiac troponin I level for predicting in-hospital mortality was as high as 0.92. Other predictors of myocardial injury include advanced age, presence of comorbidities and high levels of C-reactive protein. Whether typical clinical features of myocarditis were present in patients who had elevated levels of cardiac troponins during the course of COVID-19 is unclear because most of the early studies did not include echocardiography or MRI data [12] [13] [14] 54 . In a cohort study involving 112 patients with COVID-19, the 14 patients with myocardial injury who had elevated high-sensitivity levels of cardiac troponin I (>0.12 ng/ml) plus abnormalities 51 Prevalence of comorbidity among critically ill patients is shown in parentheses. a Prevalence of coronary artery disease specifically. b Prevalence of comorbidity among patients who died. COVID-19, coronavirus disease 2019; ICU, intensive care unit; NR, not reported. The proportion of people who die from a certain disease among all individuals diagnosed with the disease over a certain period of time. Nature reviews | Cardiology on echocardiography and/or electrocardiogram did not have typical signs of myocarditis such as segmental wall motion abnormality or reduced left ventricular (LV) ejection fraction (LVEF), suggesting that myocardial injury was secondary to systemic causes rather than a result of direct viral infection of the heart 53 . By contrast, several case reports have described typical signs of myocarditis in patients with COVID-19. A woman aged 53 years with myocardial injury, as evidenced by elevated levels of cardiac biomarkers and diffuse ST segment elevation on the electrocardiogram, had diffuse biventricular hypokinesis on cardiac MRI, especially in the apical segments, in addition to severe LV dysfunction (LVEF = 35%) 55 . MRI data also revealed marked biventricular interstitial oedema, diffuse late gadolinium enhancement and circumferential pericardial effusion, features that are consistent with acute myocarditis. Furthermore, in a man aged 37 years with chest pain and ST segment elevation, echocardiography revealed an enlarged heart (LV diastolic dimension = 58 mm) and LV dysfunction (LVEF = 27%) 56 . This patient was diagnosed with COVID-19-induced fulminant myocarditis and treated with methylprednisolone. Cardiac size and function recovered to normal after 1 week (LV diastolic dimension = 42 mm, LVEF = 66%). Histological evidence of myocardial injury or myocarditis in COVID-19 is also limited. An autopsy of a patient with COVID-19 and ARDS who died of a sudden cardiac arrest showed no evidence of myocardial structural involvement, suggesting that COVID-19 did not directly impair the heart 57 . By contrast, another case report described a patient with low-grade myocardial inflammation and myocardial localization of coronavirus particles (outside of cardiomyocytes), as measured by endomyocardial biopsy, suggesting that SARS-CoV-2 might infect the myocardium directly 58 . Drugs used to reduce cardiovascular risk such as angiotensin-converting enzyme (ACE) inhibitors and angiotensin II receptor blockers (ARBs) have numerous effects that might influence susceptibility to or the severity of COVID-19. Furthermore, although the main presentation of COVID-19 is viral pneumonia, COVID-19 can also induce cardiovascular manifestations including myocardial injury, myocarditis, arrhythmias, acute coronary syndrome and thromboembolism. Among these cardiovascular manifestations, myocardial injury has been independently associated with high mortality among patients with COVID-19 (ref. 23 ). Finally, medications that have been proposed as treatments for COVID-19 such as hydroxychloroquine and azithromycin have pro-arrhythmic effects. AF, atrial fibrillation; VF, ventricular fibrillation; VT, ventricular tachycardia. www.nature.com/nrcardio have also revealed the presence of mild inflammation and viral RNA in the hearts of patients with COVID-19 (refs 59,60 ). However, whether these patients had myocarditis or whether the findings were a consequence of systemic inflammation remains unclear. Our understanding of the pathophysiology underlying SARS might help to determine whether SARS-CoV-2 can infect cardiac cells directly, given that SARS-CoV and SARS-CoV-2 share the same mechanisms of entry into the host cell 4, 12, 43 and that the heart expresses high levels of ACE2 (refs 15,61 ). In a report that described autopsy samples from ten Canadian patients with SARS, the viral RNA of SARS-CoV was detected in 35% of the heart samples, but the infected cell types were unknown 62 . A marked increase in macrophage infiltration with evidence of myocardial damage was also detected, suggesting that SARS-CoV can infect the heart directly 62 . Taken together, these findings suggest that myocardial injury is not only a common manifestation of COVID-19, but also a risk factor for poor prognosis. At present, we do not understand the mechanisms underlying COVID-19-related myocardial injury. However, on the basis of the available clinical evidence, myocardial injury seems to be largely attributable to advanced systemic inflammation. SARS-CoV-2 might also infect the myocardium directly, resulting in viral myocarditis in a small proportion of patients with COVID-19. As with other infectious diseases, including SARS and influenza, COVID-19 can trigger ACS 44, [63] [64] [65] [66] . In early studies from China, a small proportion of patients with COVID-19 presented with chest pain on admission to hospital, but the characteristics of the chest pain were not described 17, 18 . In a case series from New York involving 18 patients with COVID-19 and ST segment elevation, which is indicative of potential acute myocardial infarction, five of the six patients with myocardial infarction required percutaneous coronary intervention 66 . In a case series from Italy involving 28 patients with COVID-19 and ST segment elevation myocardial infarction, assessment by coronary angiography showed that 17 patients had evidence of a culprit lesion that required revascularization 52 . Of note, ST segment elevation myocardial infarction was the first clinical manifestation of COVID-19 in 24 of these 28 patients who had not yet received a positive test result for COVID-19 at the time of coronary angiography. These observations suggest that COVID-19 can cause ACS even in the absence of substantial systemic inflammation. However, the incidence of ACS in patients with COVID-19 is still unknown. Considering the overwhelmed health-care facilities of many cities during the COVID-19 outbreak, the number of cases of acute myocardial infarction among patients with COVID-19 might be underestimated in early studies. The mechanisms underlying COVID-19-induced ACS might involve plaque rupture, coronary spasm or microthrombi owing to systemic inflammation or cytokine storm 67, 68 . For example, activated macrophages secrete collagenases that degrade collagen, a major constituent of the fibrous cap on atherosclerotic plaques, which can lead to plaque rupture 67 . Activated macrophages are also known to secrete tissue factor, a potent procoagulant that triggers thrombus formation when the plaque ruptures 67 . Direct endothelial or vascular injury caused by SARS-CoV-2 infection might also increase the risk of thrombus formation and ACS 69 . Despite the potential for COVID-19 to induce ACS, the number of reported cases of ACS during the COVID-19 outbreak in Italy, Spain and the USA was actually significantly lower than during pre-COVID-19 periods, with a reported 42-48% reduction in hospitalizations for ACS and a 38-40% reduction in percutaneous coronary interventions for ST segment elevation myocardial infarction [70] [71] [72] [73] . By contrast, the incidence of out-of-hospital cardiac arrest increased during the COVID-19 outbreak in Italy, which was strongly associated with the cumulative incidence of COVID-19 (ref. 74 ). This observation is in accordance with the finding that the number of patients with myocardial infarction seeking urgent hospital care declined by >50% during the peak of the COVID-19 outbreak, as reported in an extensive global survey by the ESC 75 . Heart failure. In an early study from Wuhan involving 799 patients, heart failure was one of the most commonly observed complications of COVID-19, with a reported incidence of 24% in all patients and 49% in patients who died 76 . Elevated levels of amino-terminal pro-B-type natriuretic peptide were identified in 49% of all patients (85% of those who died) 76 . Similarly, in another study of 191 patients in Wuhan, heart failure was identified in 23% of all patients and in 52% of patients who died 19 . The aetiology of acute or decompensated heart failure in COVID-19 has not been studied 77 . Given that patients with COVID-19 are likely to be older and to have pre-existing comorbidities such as coronary artery disease, hypertension and diabetes, heart failure might be the result of an exacerbation of these pre-existing conditions, whether already diagnosed or unknown, or the uncovering of subclinical cardiac dysfunction. In particular, elderly patients with reduced diastolic function might develop heart failure with preserved EF during the course of COVID-19, which can be triggered by high fever, tachycardia, excessive hydration and impaired renal function 77 . In patients with heart failure with preserved ejection fraction, cardiac MRI might help to detect changes induced by COVID-19 (refs 78,79 ). Acute myocardial injury and ACS triggered by COVID-19 can also aggravate pre-existing heart disease or provoke contractile dysfunction. In the advanced stages of COVID-19, the response of the immune system to infection might trigger the development of stress-induced cardiomyopathy or cytokine-related myocardial dysfunction, as with sepsis-associated cardiac dysfunction 80, 81 . Given that COVID-19 primarily causes respiratory symptoms and viral pneumonia with bilateral, peripheral and lower lung distribution, the pulmonary oedema that is observed in these patients, which is usually accompanied by ARDS, is mainly regarded as non-cardiogenic. However, given that approximately 25% of patients hospitalized with COVID-19 develop heart failure, the potential contribution of pulmonary congestion by heart failure should be taken into Nature reviews | Cardiology consideration 77 . Additional haemodynamic data from patients with COVID-19-related respiratory failure are needed to validate this involvement. Arrhythmias and sudden cardiac arrest are common manifestations of COVID-19. Heart palpitations have been reported to be the main presenting symptom of COVID-19 in patients without a fever or cough 82 . In a cohort of 138 patients with COVID-19 in Wuhan, China, the presence of cardiac arrhythmia was reported in 17% of all patients (44% of patients in the ICU), but the specific types of arrhythmia were not recorded 18 . In another study in Wuhan involving 187 patients hospitalized with COVID-19, those with elevated levels of troponin T were more likely to develop malignant arrhythmias, such as ventricular tachycardia and fibrillation, than those with normal levels of troponin T (12% versus 5%) 24 . In-hospital and out-of-hospital sudden cardiac arrests have also been reported in patients with COVID-19 (refs 57,66,74 ). However, the exact contribution of COVID-19 to cardiac arrhythmias remains uncertain given that arrhythmias, such as atrial and ventricular tachycardia and fibrillation, can be triggered by myocardial injury or other systemic causes such as fever, sepsis, hypoxia and electrolyte abnormalities 24, 83 . Furthermore, patients with advanced COVID-19 are often treated with antiviral medications and antibiotics that are known to induce arrhythmias in some patients (described in detail below) 13, 41 . is associated with coagulation abnormalities, which can result in thromboembolic events 73 . Patients with COVID-19 often have elevated levels of d-dimer, modestly reduced platelet counts and slightly prolonged prothrombin time. In an early study of 1,099 patients with COVID-19 from China, elevated levels of d-dimer (>0.5 mg/l) were observed in 46% of all patients (60% of those with severe illness) 20 . Similarly, another study in patients with COVID-19 in Wuhan showed that d-dimer levels were elevated (>1 mg/l) in 42% of all patients (81% of those who died), which, if detected at admission to hospital, was associated with an 18-fold increased risk of death 19 . By contrast, the changes in platelet counts and prothrombin time were modest. Among 41 patients with COVID-19 in Wuhan, only 5% had a low platelet count (<100 × 10 9 cells per litre) and the prolongation of prothrombin time was mild even in patients admitted to the ICU (11.1 s versus 12.2 s) 17 . Moreover, levels of fibrinogen and factor VIII were elevated in these patients, indicating a hypercoagulable state 84, 85 . These findings show that a substantial proportion of patients with COVID-19 have coagulation abnormalities that typically do not meet the criteria of disseminated intravascular coagulation established by the International Society on Thrombosis and Haemostasis 86 , but nevertheless might contribute to the development of the diverse cardiovascular manifestations of COVID-19. Clinical observations of increased thromboembolic events in patients with COVID-19 suggest the presence of a hypercoagulable state. Venous thromboembolism, which includes deep vein thrombosis and pulmonary embolism, is a common complication in critically ill patients with COVID-19. An autopsy study revealed that deep vein thrombosis was present in 7 of 12 patients who died with COVID-19 in whom venous thromboembolism was not suspected before death, whereas pulmonary embolism was identified in 4 of the 12 patients 59 . Arterial thrombotic events have also been reported. A case series from New York described five patients aged ≤50 years who presented to the same hospital with large-vessel ischaemic stroke and who all tested positive for SARS-CoV-2 infection 87 . Furthermore, acute limb ischaemia was also reported in 20 patients with COVID-19 (90% men, mean age 75 ± 9 years) in a case series from Italy 88 . All 20 patients were diagnosed with COVID-19-related pneumonia before acute limb ischaemia was detected. The mechanisms underlying these coagulation abnormalities, particularly hypercoagulation, in the setting of COVID-19 are unclear. One hypothesis is that the severe inflammatory response and endothelial damage induced by COVID-19 in combination with underlying comorbidities might predispose patients to a hypercoagulable state 6 . Of note, certain antiviral medications and investigational therapies given to these patients might promote thrombosis or bleeding events through drug-drug interactions with antiplatelet agents and anticoagulants 41 . A retrospective study in New York showed that systemic anticoagulation was associated with prolonged survival in patients hospitalized with COVID-19 (ref. 89 ). Among 2,773 patients, 786 (28%) received systemic anticoagulation. The median survival time of patients who were treated with anticoagulation was longer than in those who were not treated (21 days versus 14 days), although overall mortality between the two groups remained similar (22.5% versus 22.8%) 89 . The differences in median survival time and mortality were more pronounced among patients who required mechanical ventilation (21 days versus 9 days and 29.1% versus 62.7%, respectively) 89 . Another retrospective study in China also showed reduced mortality in patients with COVID-19-associated coagulopathy who were treated with prophylactic heparin 90 . Importantly, findings from these retrospective studies are limited by potential selection bias, the presence of confounding factors and the undefined indication of anticoagulation treatment. In addition, the optimal anticoagulation agent to prevent thromboembolic events in these patients is not known (for example, low-molecular-weight heparin, unfractionated heparin, direct oral anticoagulants or others). Prospective, randomized trials are needed to validate the protective effect of anticoagulation therapy in patients with COVID-19. Kawasaki disease. Children are thought to be less susceptible than adults to COVID-19, and the majority of children with COVID-19 are asymptomatic or present with only mild symptoms 91 . However, COVID-19 has been reported to cause severe inflammatory symptoms in a small proportion of paediatric patients 26, 92 . A case series from the UK reported an unprecedented cluster of eight children (aged 4-14 years) presenting with a www.nature.com/nrcardio hyperinflammatory syndrome with features of Kawasaki disease, five of whom tested positive for SARS-CoV-2 or were potentially exposed to SARS-CoV-2 from family members 26 . Clinical presentations included fever, variable rash, conjunctivitis, peripheral oedema, extremity pain and severe gastrointestinal symptoms. A common finding on echocardiography was echo-bright coronary vessels, which progressed to a giant coronary aneurysm in one patient. Furthermore, researchers in Bergamo, Italy, found a 30-fold increase in the incidence of Kawasakilike disease among children during the peak of the pandemic 92 . These paediatric patients were older and had a higher rate of cardiac involvement than patients diagnosed with Kawasaki disease before the pandemic. Together, these early clinical findings are suggestive of a new phenomenon caused by SARS-CoV-2 infection in children that can lead to a hyperinflammatory syndrome with features that are similar to those of Kawasaki disease, including coronary artery abnormalities. In general, patients with depressed immunity are at a higher risk of infectious diseases. The effect of COVID-19 on the cardiovascular system in immunocompromised patients, such as those with cancer or those who have undergone organ transplantation, is largely unknown. Heart transplantation recipients might be at higher risk of COVID-19 owing to their immunosuppressed state combined with their baseline cardiovascular disorders 14, 93 . COVID-19 has been reported in two heart transplantation recipients from China, both of whom made a full recovery 94 . The clinical presentations of these two patients were not distinct from those of non-immunosuppressed patients. In a retrospective case series of heart transplantation recipients with COVID-19 admitted to hospitals in Michigan, USA, between 21 March 2020 and 22 April 2020, 13 patients were identified, all of whom were African American men 95 . Six patients required admission to the ICU and two patients died during hospitalization. Of note, the clinical presentation and laboratory markers of disease severity of these patients were not distinct from those of the general population, despite immunosuppression use to preserve allograft function. However, another case series involving 28 heart transplantation recipients with COVID-19 in New York reported higher mortality and an increased incidence of severe complications in these individuals than in the general population 96 . A total of 22 patients were hospitalized, of whom 7 patients required mechanical ventilation and 7 patients died (case fatality rate 25%). Moreover, 13 out of 17 patients had myocardial injury, as evidenced by elevated levels of troponin T (>0.022 ng/ml). These findings suggest that recipients of heart transplantation are at high risk of severe complications from COVID-19. Whether organ transplantation recipients are more susceptible to COVID-19 and whether immunosuppressive treatments have harmful (or protective) effects on disease progression needs to be assessed in large-scale studies. Another important point regarding heart transplantation is the need to screen for COVID-19. Given that individuals with SARS-CoV-2 infection might be asymptomatic, routine screening of donor tissues is necessary during the pandemic. Screening of recipients before transplantation will also be required to avoid worsening of subclinical infection after starting immunosuppression. As with patients with CVD, patients with cancer are thought to be at higher risk of severe COVID-19 symptoms than the general population [97] [98] [99] . This high risk of poor outcomes might be attributable to suppressed immunity by chemotherapy, the presence of cardiovascular risk factors (such as hypertension and diabetes), cardiotoxicity of cancer treatment and/or cardiovascular damage by COVID-19, combined with their impaired baseline condition 97 . Therefore, a major consideration in the delivery of care to these patients during the pandemic is to balance the risk of SARS-CoV-2 infection with the need to provide timely cancer treatment. Clinicians need to determine the optimal timing of treatment in patients with cancer and cardiovascular comorbidities, especially if they are infected with or exposed to SARS-CoV-2. Routine COVID-19 screening might be necessary before cancer treatment to avoid worsening of a subclinical infection. The mechanisms underlying the development of COVID-19-related cardiovascular injury are not known. ACE2 expression is thought to be one of the major factors involved in the biological mechanism underlying tissue-specific infection. As with SARS-CoV, SARS-CoV-2 infection is triggered by binding of viral S protein to human ACE2, whereas TMPRSS2 induces S protein priming 4 . The interaction between S protein and ACE2 has gained much research interest given that ACE2 is known to have crucial roles in both the cardiovascular system and the immune system 61,100 . ACE2 is a part of the RAAS and is involved in the development of diabetes, hypertension and heart failure. At the tissue level, ACE2 is highly expressed in the lungs, kidneys, heart and blood vessels 61, 100 . According to bulk RNA sequencing data in the Genotype-Tissue Expression (GTEx) project V8, the expression of ACE2 in the heart and coronary arteries is even higher than in the lungs 101 . At the single-cell level, ACE2 is highly ex pressed in pericytes of adult human hearts 102 . Single-cell RNA sequencing data have also revealed that cardiomyocytes (especially those in the right ventricle) express ACE2 at a lower level than pericytes and that neither pericytes nor cardiomyocytes express TMPRSS2 (ref. 103 ). However, both cell types have a high expression of cathepsin B and cathepsin L, which facilitate S protein priming and might promote entry of the virus into the cell via the endocytic pathway. Therefore, SARS-CoV-2 might be capable of directly infecting multiple cardiovascular cell types, including cardiomyocytes, endothelial cells and pericytes. Importantly, however, expression of ACE2 is not in itself sufficient for entry of the virus into a cell, and the efficiency of viral replication and release might also have a role in host cell infection. To date, clinical evidence of direct viral infection of cardiomyocytes has not been found. Given that myocarditis related to SARS-CoV-2 infection is rare 23, 53 , the interaction between SARS-CoV-2 and ACE2 might affect the cardiovascular system in an indirect manner 61,100 . Genome-wide association studies would help to facilitate the identification of novel pathways involved in SARS-CoV-2 pathogenesis 104, 105 . SARS-CoV entry into cells has been shown to downregulate ACE2 expression 62, 106, 107 . In a mouse model of SARS, ACE2 levels in the heart were significantly reduced after SARS-CoV infection 62 . In addition, a separate study reported that knockout of Ace2 in mice resulted in a significant reduction in cardiac contractility 108 . Furthermore, Ace2 -/-Apoe -/mice had greater atherosclerotic plaque accumulation and upregulated expression of genes encoding adhesion molecules and inflammatory cytokines such as IL-6 and CCL2 compared with Apoe -/mice 109 . These results support a cardioprotective role of ACE2 (ref. 61 ). Similarly, ACE2 has a protective effect in the lungs. ACE2 is expressed primarily in alveolar epithelial type II cells in the normal adult lung [110] [111] [112] . These cells produce surfactant proteins that reduce surface tension, preventing the alveoli from collapsing. In a mouse model of ARDS, Ace2 knockout exacerbated acute lung injury, whereas treatment with recombinant ACE2 rescued lung damage 106 . Therefore, like SARS-CoV, SARS-CoV-2 infection might result in the downregulation of ACE2, which can lead to cardiac dysfunction and progression of atherosclerosis, as well as exacerbated lung damage. ACE2 as a therapeutic target. Angiotensin II, the main effector molecule in the RAAS, is upregulated in many diseases and is a common treatment target for various cardiovascular disorders 61, 100, 113 . ACE2 inactivates angiotensin II by converting angiotensin II to angiotensin (1-7) 61,100 . Cryogenic electron microscopy studies have demonstrated that the S protein of SARS-CoV-2 can directly bind to human ACE2 with a similar or even higher affinity than the binding of the S protein of SARS-CoV to human ACE2 (refs 9-11 ). In this context, a study has shown that exogenous administration of recombinant human ACE2 (rhACE2) can prevent SARS-CoV-2 infection by acting as a decoy 114 . The investigators demonstrated that clinical-grade rhACE2 can reduce SARS-CoV-2 infection in cell culture and in engineered human blood vessel organoids and kidney organoids 114 . Given that rhACE2 has been shown to be protective against various CVDs 61, 115, 116 , rhACE2 therapy might be a promising approach to treat patients with COVID-19-related cardiovascular disorders 114 . Although SARS-CoV-2 preferentially infects the lungs and respiratory tract like other respiratory viruses, COVID-19 can cause a diverse range of extrapulmonary manifestations including CVD, stroke, seizures, liver damage, renal dysfunction and gastrointestinal symptoms 117 . Systemic hyperinflammation induced by viral pneumonia is likely to have an important role in the development of these varied manifestations of COVID-19, but numerous studies have also reported histological evidence of direct viral infection in non-respiratory organs such as the heart, brain, liver and kidney 59, 69, 118 . In an autopsy case series involving 12 patients with COVID-19, in which SARS-CoV-2 RNA was detected by quantitative reverse transcription PCR in the lungs at high concentrations in all patients, 5 patients also had high viral RNA titres in the heart, liver or kidney 59 . Similarly, SARS-CoV-2 RNA was detected in the lungs, heart, brain, liver or kidneys in an autopsy analysis of 27 patients who died with COVID-19 (ref. 118 ), indicating a broad organotropism of SARS-CoV-2. Although the specific cell types infected by SARS-CoV-2 in each organ are unknown, these preliminary data support the possibility that the extrapulmonary manifestations and multiorgan failure observed in patients who died with COVID-19 are not just a consequence of systemic inflammation or cytokine storm, but might also be caused by direct infection of numerous organ systems by SARS-CoV-2. Mechanistically, the broad tissue tropism of SARS-CoV-2 might be a result of the instability of the S protein of SARS-CoV-2 related to the presence of the novel furin cleavage site, as mentioned above 4 . Preliminary histological data from a case series of three patients with COVID-19 have shown that endothelial cells might be a direct target of SARS-CoV-2 infection 69 . In a patient with a history of renal transplantation who died from COVID-19-induced multiorgan failure, viral particles were detected by electron microscopy in endothelial cells in the kidney 69 . A prominent endotheliitis (inflammation within the endothelium) with recruitment of inflammatory cells was identified on histological assessment, as well as an unusually high concentration of apoptotic bodies in numerous organs including the lungs, small bowel and heart 69 . In another patient who died from COVID-19-related multiorgan failure and ST segment elevation myocardial infarction, lymphocytic endotheliitis was observed in the lungs, heart, liver and kidneys 69 . In the third patient who had mesenteric ischaemia and underwent resection of the small intestine, histological assessment of the small intestine revealed prominent endotheliitis of the submucosal vessels in addition to a large concentration of apoptotic bodies 69 . Findings from this case series suggest that SARS-CoV-2 can infect endothelial cells directly, which can lead to inflammation in the endothelium 69 . Given that endothelial cells are an important component of every organ and have a high level of ACE2 expression, inflammation in the endothelium caused by infection with SARS-CoV-2 might underlie the diverse clinical manifestations of COVID-19. The potential drug-disease interactions in patients with COVID-19 have become a highly researched topic [12] [13] [14] ( fig. 2) . First, whether antihypertensive agents such as ACE inhibitors and angiotensin II receptor blockers (ARBs) are involved in the progression or prevention of COVID-19 is unknown 113, 119 . Second, some of the potential antiviral drugs used to treat patients with COVID-19 are known to induce cardiotoxicity 41 . Given that ACE2 is a receptor for SARS-CoV-2, clinicians have expressed concern that medications that upregulate the cell surface expression of ACE2 might be harmful 113, 119, 120 . ACE inhibitors and ARBs have been shown to increase the expression of ACE2 in animal models 121 . Given that both ACE inhibitors and ARBs are commonly used worldwide for the treatment of hypertension and other CVDs, whether these drugs should be discontinued during the COVID-19 pandemic has become a pertinent question. Importantly, indiscriminate withdrawal of these drugs could harm high-risk patients. Several medical societies including the ACC, AHA, Chinese Society of Cardiology, ESC and the Heart Failure Society of America have issued statements recommending continuation of RAAS antagonists for those who are currently prescribed these agents [122] [123] [124] . Findings from clinical studies support this recommendation. In four studies from Spain (n = 1,139) 125 , Italy (n = 6,772) 126 and the USA (n = 5,894 and 1,735) 127, 128 , the use of RAAS inhibitors was not associated with a positive COVID-19 test, suggesting that these agents do not affect susceptibility to SARS-CoV-2 infection. Furthermore, the use of these agents was not associated with a substantial increase in the risk of severe or fatal illness among patients with COVID-19 (refs 126,127 ). Two main points need to be discussed on this topic. First, the effect of RAAS inhibitors on ACE2 is controversial 113 . As shown in fig. 3 , the targets of ACE and ACE2 are different despite high structural similarity between the two enzymes. ACE converts angiotensin I to angiotensin II, whereas ACE2 degrades angiotensin II to angiotensin (1) (2) (3) (4) (5) (6) (7) . ACE inhibitors prevent the conversion from angiotensin I to angiotensin II, whereas ARBs inhibit the angiotensin II receptor type 1, which is downstream of angiotensin II 61, 113 . Therefore, neither class of drugs affects ACE2 directly. Although some animal studies have shown that treatment with ACE inhibitors or ARBs can increase the expression of Ace2 (refs 121,129 ), findings from other preclinical studies were more mixed 113, 119, 120, [130] [131] [132] . In clinical studies, long-term exposure to the ACE inhibitor captopril 133 or the ARB olmesartan 134 was associated with increased plasma levels of angiotensin (1) (2) (3) (4) (5) (6) (7) or urinary levels of ACE2, respectively, indicating increased ACE2 activity. However, other studies have not shown evidence of upregulated ACE2 by RAAS inhibitors in patients with heart failure 135 , aortic stenosis 136 , atrial fibrillation 137 or coronary artery disease 138 . These inconsistent results are likely to be attributable to the indirect effects of ACE inhibitors or ARBs on ACE2, which depend on conditions such as baseline expression levels of ACE2, dosing and treatment periods. Second, whether potential upregulation of ACE2 is harmful or protective is uncertain. Even if ACE inhibitors or ARBs can upregulate ACE2, no direct evidence has been found to show that upregulation of ACE2 affects susceptibility to viral infection 113 . Furthermore, as shown in Ace2-knockout mice [106] [107] [108] [109] and rhACE2 studies [114] [115] [116] , ACE2 is considered to be protective rather than harmful in settings of lung injury and CVDs 61, 100 . Together, these findings indicate no benefit of withdrawal of ACE inhibitors or ARBs in protecting against COVID-19. At present, many research teams worldwide are focused on the development of drugs for the prevention and treatment of COVID-19 (ref. 41 ). Of note, the development and testing of new drugs are time-consuming processes 139 and not a viable strategy during this COVID-19 pandemic. Drug repurposing 139 , in which existing medications that have already been approved for a disease are tested for a new condition, is currently the main approach in the search for new drugs for COVID-19 (refs 41,42,140,141 ). However, some of the drugs under investigation have known or unknown cardiovascular adverse effects 142 or might be involved in drug-drug or drug-disease interactions 41, 140 (TAble 2) . Chloroquine and hydroxychloroquine have been widely touted as potential treatment strategies for COVID-19 (refs 143-145 ). Chloroquine and hydroxychloroquine can potentially block virus entry into cells, particularly via the endosomal pathway, by inhibiting the glycosylation of host receptors, proteolytic processing and endosomal acidification 41, 145 . These agents can also mediate immunomodulatory effects through attenuation of cytokine production and inhibition of autophagy and lysosomal activity 41 . Both drugs are used in the treatment of malaria and chronic inflammatory diseases such as systemic lupus erythematosus and rheumatoid arthritis 41, 146 . A French observational study involving 36 patients with COVID-19 reported improved virus clearance with hydroxychloroquine treatment 144 . Furthermore, combining hydroxychloroquine with azithromycin in six patients resulted in even better virus clearance than with the use of hydroxychloroquine alone 144 . Importantly, however, several concerns have been raised regarding the characteristics of the control group (which consisted of patients recruited from a separate hospital) and ethics approval 147 . In a randomized study involving 62 patients with COVID-19 in China, patients in the treatment group received hydroxychloroquine (400 mg per day) for 5 days, whereas patients in the control group received standard treatment (oxygen therapy, antiviral agents, antibacterial agents and immunoglobulin, with or without corticosteroids) 143 . Hydroxychloroquine improved the time to clinical recovery, body temperature recovery time, cough remission time and pneumonia-related symptoms compared with standard treatment alone. Furthermore, in a retrospective study from Wuhan involving 550 critically ill patients with COVID-19, mortality was significantly lower among patients treated with hydroxychloroquine plus standard treatment (which included other antiviral drugs and antibiotics) compared with standard treatment alone (18.8% versus 47.4%) 148 . However, in a large observational study involving 1,376 patients from New York City, hydroxychloroquine treatment did not alter the risk of the composite end point of intubation or death 149 . These inconsistent results show that the efficacy of hydroxychloroquine is still controversial and needs to be validated in large, randomized studies 150 . Although chloroquine and hydroxychloroquine have a long history of clinical use for numerous conditions, these agents are also known to induce arrhythmias 41, 142, 151 . Azithromycin, which has been assessed in combination with hydroxychloroquine as a treatment for COVID-19, is also known to prolong the QT interval 152 . In a cohort study of 90 patients with COVID-19 who received hydroxychloroquine (with or without azithromycin), those who received a combination of hydroxychloroquine and azithromycin had greater QT interval prolongation than those taking hydroxychloroquine alone 151 . Furthermore, in a retrospective cohort study of 1,438 patients hospitalized with COVID-19 in New York, treatment with hydroxychloroquine, azithromycin or both was compared with neither treatment 153 . None of the groups had an increase in in-hospital mortality, but the secondary outcome of cardiac arrest was more likely in patients receiving both hydroxychloroquine and azithromycin than in patients receiving neither drug 153 . Given that some patients with COVID-19 might have impaired renal function owing to systemic illness, frequent electrocardiographic evaluation should be strongly considered in patients treated with hydroxychloroquine and/or azithromycin. Remdesivir is a promising investigational nucleotide analogue for the treatment of COVID-19 that has broad-spectrum antiviral activity and functions by targeting RdRP 27, 154, 155 . Remdesivir was originally developed for the treatment of Ebola virus disease. Prophylactic and therapeutic administration of remdesivir has been shown to improve pulmonary function and to decrease viral load in a mouse model of MERS 156 . In a randomized, double-blind, placebo-controlled trial involving 237 patients with COVID-19 in China, remdesivir was associated with a numerically (but not statistically significant) faster time to clinical improvement than was the placebo 155 . Preliminary results from a double-blind, randomized, multicentre, placebo-controlled trial involving 1,063 patients with COVID-19 indicate that those who received remdesivir had a 31% faster time to recovery than those who received placebo (median time to recovery 11 days versus 15 days) 157 . Of note, in light of these preliminary findings, the FDA granted emergency use of remdesivir for COVID-19 in May 2020 to meet the urgent demand for treatment of hospitalized patients 158 . The optimal dosing and duration of treatment is still under investigation. Under the emergency use authorization, a 10-day treatment regimen (200 mg on day 1 followed by 100 mg per day for 9 days) is suggested for patients requiring invasive mechanical ventilation and/or extracorporeal membrane oxygenation, and a 5-day treatment course is suggested for patients with milder symptoms. Although prominent cardiovascular adverse effects associated with remdesivir have not been reported so far, these might become apparent with more widespread use 158 . Lopinavir-ritonavir is a fixed-dose drug combination used for the prevention and treatment of HIV infection and works by inhibiting protease activity 159 . Lopinavir is available only in combination with ritonavir, which functions to slow the breakdown of lopinavir by inhibiting cytochrome P450 3A4 (ref. 41 ). In a randomized, controlled, open-label trial involving 199 patients with COVID-19, no benefit was observed with lopinavir-ritonavir treatment compared with standard care 159 . Gastrointestinal adverse effects were more frequently reported in the lopinavir-ritonavir treatment group than in the standard group, but adverse 159 . Nevertheless, lopinavir-ritonavir should be used with caution in patients with COVID-19 because this drug combination might interact with common cardiovascular drugs that are metabolized by cytochrome P450 3A4, including antiarrhythmic agents, antiplatelet drugs and anticoagulants 41, 160 . Given that numerous studies have demonstrated that SARS-CoV-2 shares many biological features with SARS-CoV, our knowledge of the pathophysiological mechanisms underlying SARS can be used to understand the disease processes involved in COVID-19. Mechanistically, the interaction between the S protein and ACE2 is likely to have a central role in disease pathogenesis, especially in cardiovascular manifestations of this disease, and this interaction is a potential target for the prevention and treatment of COVID-19. Several hurdles need to be overcome in the study of the mechanisms underlying COVID-19. First, biological experiments using SARS-CoV-2 can be performed only in laboratories with a biosafety level 3 rating 161, 162 . Second, the use of animal models to mimic the disease process is associated with numerous challenges [163] [164] [165] . Given that cellular or tissue tropism is likely to be an important factor contributing to the diverse phenotypes of COVID-19 (ref. 4 ), mouse or rat models are not ideal to study host tropism because they are not as susceptible to SARS-CoV-2 as humans owing to differences in the amino acid sequence of ACE2 (ref. 166 ). To use mice or rats, human ACE2 needs to be introduced artificially. Transgenic mice expressing ACE2 infected with SARS-CoV-2 have been reported to show signs of pneumonia, but the overall symptoms experienced by these mice are much milder than those in humans 163 . Therefore, alternative platforms might involve genome-edited mouse or rat models in which Ace2 is replaced by human ACE2, other animal species that are naturally susceptible to SARS-CoV-2 infection (such as ferrets, hamsters and non-human primates) 164, 165, [167] [168] [169] [170] or in vitro models such as induced pluripotent stem cells [171] [172] [173] and organoids 114, 174, 175 . The COVID-19 pandemic is changing our lives in unprecedented ways. Given the lack of safe and effective vaccines or proven treatments for COVID-19, our main strategy to combat the pandemic is social distancing. The capacity of health-care systems globally has been severely tested (and in some countries completely overwhelmed), and the effect of this pandemic on social interactions, health-care delivery and the global economy continues to mount. Reduced physical activity owing to lockdown measures might also contribute to poor control of cardiovascular risk factors. Vaccine development is expected to take 12-18 months 34 . To meet the urgent need for effective treatment and preventative strategies, a concerted effort must be made by researchers globally to investigate and integrate biological and clinical findings related to COVID-19 (bOx 1).  
paper_id= ffef7c62ad4059e44bf00507244af1e964fbf36a	title= The Intersection of HIV and Syphilis: Update on the Key Considerations in Testing and Management	authors= Melody  Ren;Thomas  Dashwood;Sharon  Walmsley;	abstract= Purpose of Review To highlight recent trends in the epidemiology of HIV and syphilis, the impact of the COVID epidemic, our approach to care of co-infected patients, and our views on important next steps in advancing the field. Recent Findings HIV and syphilis co-infection has been on the rise in recent years although since the COVID pandemic there is a decrease in new diagnoses-it remains unclear if this represents a true decline or inadequate testing or under-reporting. Standard HIV care should include regular syphilis serology .Treatment and serological follow-up of syphilis in HIV positive and negative patients can be conducted similarly. Challenges remain in the diagnosis and management of neurosyphilis. New models for testing and prevention will be crucial next steps in controlling co-infection. Summary The intersection of HIV and syphilis infections continues to pose new and unique challenges in diagnosis, treatment, and prevention. 	body_text= Our understanding of the interaction between syphilis and HIV continues to evolve and has increasing clinical importance given the rising rates of co-infection and the unique synergy between the two sexually transmittable infections. Syphilis facilitates the transmission and acquisition of HIV, and HIV has been documented to accelerate the natural history of syphilis [1] [2] [3] [4] [5] . Furthermore, HIV infection has been associated with false positive and negative syphilis serology complicating our diagnoses and management [6, 7] . Whether standard definitions of the serologic response and cure of syphilis after adequate therapy should be applied to those with HIV coinfection is unclear [8] [9] [10] . The purpose of this review is to highlight recent trends in the epidemiology of HIV and syphilis, the impact of the COVID epidemic, our approach to care of co-infected patients, and our views on important next steps in advancing the field. The Epidemiology of Syphilis and HIV The rates of HIV and syphilis co-infection globally have been on a meteoritic rise in recent years. In the late 1990s, the introduction of syndromic treatment for STIs, widespread sexual behavioral change, and sexual network disruption consequent to the HIV epidemic and fears of infection, contributed to a decreased prevalence in syphilis in many regions [11] [12] [13] [14] . However, with the introduction of successful antiretroviral therapy and improved survival from HIV infection, and the demonstration of U=U (undetectable =untransmittable), observed rates of syphilis have been increasing over the last two decades, most notably documented in the USA, Europe, Canada, Australia, and China [15] [16] [17] [18] [19] [20] [21] [22] [23] . The most recently available data indicate the number of cases in the USA have tripled from 2013 to 2018, have increased~50% from 2009 to 2018 across Europe (the data influenced by the changing list of individual countries reporting statistics to the European CDC), have more than doubled from 2008 to 2017 in Canada, and have increased by 135% from 2013 to 2017 in Australia. Regional data from China demonstrates a general trend of increasing rates with some regions reporting higher rates than the USA. In high-income countries, the re-emergence of syphilis is overwhelmingly disproportionate in men with a significant proportion self-identifying as MSM [24] . Data from 2017 and 2018 indicate that over 80% of infectious syphilis cases in the USA, Europe, Canada, and Australia were diagnosed in men. In low-middle-income countries (LMIC), syphilis remains endemic in the general population, and accounts for over 90% of syphilis cases worldwide [25] . Although the MSM, transgender women, and sex worker populations remain at high risk in LMIC, data on the overall epidemiology is sparse due to under-reporting and may also be influenced by stigma and marginalization leading to limited access to health care [26] . Consequently, most epidemiological data in LMIC for syphilis are derived from World Health Organization (WHO) antenatal monitoring in pregnant women [27] . Rates of syphilis in pregnancy and congenital syphilis have increased in both high-income countries and LMIC, echoing the increase in primary and secondary syphilis in reproductive age women [28] [29] [30] . Globally, congenital syphilis is a major cause of neonatal and fetal mortality and the leading cause of stillbirth [31] . However, there has been some hope in this realm with five additional countries declared by the WHO to have successfully eliminated mother to child transmission of HIV and syphilis since 2015: Cuba, Thailand, Belarus, Armenia, and the Republic of Moldova [32, 33] . The reasons for the increase in syphilis rates are thought to be multi-factorial. Improved HIV management with highly effective anti-retroviral therapy (ART) has led to reestablishment of sexual networks and declines in previously adopted safer sex behaviors including unprotected sex [34] [35] [36] [37] . The impact of condom-less sex has been further compounded by the introduction of pre-exposure prophylaxis (PrEP) and "serosorting" behaviors (selecting sexual partners based on concordant HIV serostatus) [36, [38] [39] [40] [41] [42] [43] [44] . Growing use of the Internet to find sexual partners has also been associated with increased high-risk sex including multiple sex partners and substance abuse during sex [45] . Moreover, syphilis prevalence is inversely proportional to the intensity of public health efforts and the current rise in syphilis has mirrored the decrease in public health funding in many jurisdictions [36, 37, 46, 47] . Increasing rates of syphilis across a wide variety of populations in diverse geographical settings demand that modernday clinicians have a heightened awareness of syphilis, especially in high-risk patients including those who are MSM or have risk factors for HIV acquisition. How Do We Define "Cure" of Syphilis in Those Co-infected with HIV Given our lack of ability to grow the organism, diagnosis and response to therapy is based on serology. HIV and syphilis management has been plagued by a lack of evidence to inform the expected response in syphilis serology after treatment in PLWH. An adequate serologic response after syphilis treatment is defined as a four-fold decrease in the RPR titer. This definition was adopted after mathematical models were used by Brown et al. to graph the decline in the Venereal Disease Research Laboratory test titers after treatment in 818 patients with primary and secondary syphilis [48] . The ongoing debate surrounds whether the time to the serologic response should differ in PLWH with infection in the various syphilis stages. There remains a wide variability among guidelines from the USA, Canada, European, and UK [49] [50] [51] [52] . Many guidelines indicate that PLWH require "more time" to reach criteria for an adequate serologic response but do not delineate a specific timeline. The WHO guidelines on syphilis management do not comment on the expected timeline for serologic response post-treatment [53, 54] . Certainly, the imperfect nature of the current serologic methods for diagnosis complicates our ability to interpret testing after treatment. Defining serologic cure on indirect measures of disease activity continues to challenge health care providers who base clinical decision making on those tests. This is clouded by rates of syphilis re-infection, loss to follow-up, and serofast states, whereby titers remain low indefinitely, when some recommend consideration of re-treatment for syphilis [10, 55] . Further confounding factors include the variability in HIV status including viral load, CD4 counts, and the evolving environment of treatment with increasingly efficacious antiretroviral therapies [56] . An improved understanding of the serological response after adequate treatment in PLWH is critical in defining management. To try to understand this in the real-world setting, we undertook an evaluation of the serologic response to syphilis in 171 persons co-infected with HIV in our tertiary care clinic. We demonstrated that over 90% of our patients across all syphilis stages achieved an adequate serological response after appropriate treatment as defined by the 2015 CDC guidelines. It remains unclear how to monitor or manage the remaining 10% [57] . First-line standard therapy for syphilis is treatment with penicillin or doxycycline. Barriers to using penicillin include the need for injections, drug shortages, and allergies while doxycycline use is not recommended in pregnant women. The appropriate management for syphilis is additionally complicated by the emergence and proliferation of macrolide resistant strains. Benzathine penicillin G (BPG) is off patent with a market price of pennies per dose but is expensive to manufacture as a sterile injectable medication. Several pharmaceutical manufacturers have stopped producing BPG due to these economics [58] . From 2014 to 2016, the WHO collected country level data and found 39 of 95 responding countries and territories were experiencing BPG shortage, including in high syphilis morbidity regions [58] . Brazil notably had an increase in congenital syphilis cases from 4.0 per 1000 live births in 2012 to 6.5 per 1000 live births in 2015 after a manufacturer ended production on 2013. In Rio de Janerio, the BPG shortage was associated with a 2.23-fold increase in the risk of congenital syphilis [58, 59] Azithromycin can be used in patients at high risk for STIs to symptomatically treat for gonorrhea and chlamydia and in the 1990s was a simple one dose alternative treatment to penicillin for syphilis treatment [60] . The first cases of azithromycin clinical failure were described shortly after in the early 2000s. Retrospective analyses in Canada and the USA have shown the proportions of samples containing one of the two single nucleotide polymorphisms responsible for conferring resistance, A2058G and A2059G, have increased over time [61] [62] [63] [64] [65] . To date, macrolide-resistant syphilis has been described in Australia, Canada, China, Europe, and the USA with some regions reporting macrolide resistance in over 80-90% of clinical isolates [26, [66] [67] [68] . This has eliminated azithromycin as a second-line option for syphilis treatment in some geographical regions and has caused very cautious use elsewhere [69] . There is ongoing pressure for international co-operation to ensure adequate supply of penicillin for syphilis treatment in addition to judicious monitoring of the effectiveness of alternate, less studied non-penicillin antimicrobials. Since the start of the COVID-19 global pandemic, the impact on the management of patients with HIV and syphilis has been closely monitored. To date there has been no evidence that PLWH hospitalized for COVID-19 fair any worse when compared to similar demographic groups [70, 71] . However, there have been increasing concerns about the effects of social distancing and lockdown measures on maintaining care for chronic HIV and STI management. This includes ongoing prevention and treatment efforts in socially and economically vulnerable populations who are at high risk of COVID-19 and HIV, ensuring minimal disruptions to regular HIV care and antiretroviral therapy access, and juggling limited resources in over-burdened health systems as priorities shift towards the management of COVID-19 [72, 73] . Modelling studies in high burden HIV settings have shown dire consequences should HIV care and prevention work fall to the wayside; deaths due to HIV could increase by up to 10% and a 6-month interruption of ARV could lead to more than 500,000 more deaths from AIDS-related illnesses in sub-Saharan Africa in 2020-2021 compared to the 470,000 AIDS-related deaths in the region in 2018 [74, 75] . Survey data has indicated that COVID-19 has prevented patients from accessing their chronic medications [75] . Social distancing and lockdown measures have led to decreased rates of diagnosis of syphilis in Rome and Madrid and syphilis and HIV in Taiwan compared to same time periods in previous years [76] [77] [78] [79] [80] [81] [82] [83] [84] [85] [86] [87] [88] . This phenomenon has also been noted in public health reports comparing 2020 data for HIV and syphilis in regions in Canada and in the USA [79, 80] . It is unclear if the reduction is due to less frequent sexual encounters and a true decline in syphilis and HIV rates or if patients are postponing diagnosis and care leading to under-reporting as seen in other specialties. Encouraging people to seek care for STIs during the COVID-19 pandemic continues to be important. It remains to be seen if COVID-19 has any persistent substantial effect on STI epidemiology, whether that is a decrease in rates due to disrupted sexual networks, an increase in rates due to lack of prevention and treatment of STIs, or no effect at all. As the COVID-19 global pandemic continues, we must be vigilant and ensure that prevention and health care services for HIV and syphilis remain accessible and be flexible in meeting patients' needs in this unprecedented time. Similar to the international and Canadian trends, our HIV clinic in a tertiary care center in Toronto, Canada, has experienced an increase in syphilis diagnoses among our patient population. From 2000 to 2008, the average number of patients diagnosed with syphilis was 11.3/year. For the total number of patients seen in our clinic, this translated to 1413 cases/100,000 population compared to the national Canadian rate of 4.2 cases/100,000 population in 2008 [81] . From 2009 to 2017, there was an average of 38.8 patients diagnosed with syphilis each year translating to 3527 cases/100,000 population compared to the national Canadian rate of 79.5 cases/ 100,000 population in 2017 [81] . From 2000 to 2017, there was a 250% increase in syphilis diagnosis in our clinic. In our study of the serologic response after syphilis treatment, we observed that 17.5% in our included cohort had neurosyphilis compared to 9.4% with primary, 31% with secondary, 15.2% with early latent, and 26.9% with late latent [57] . This may be explained by the tertiary nature of our center; our patient population exclusively includes PLWH who may be at higher risk for neurosyphilis [82, 83] . In our HIV clinic, syphilis serology has become a part of the standard panel of bloodwork completed on patients during each visit since 2018 when our research demonstrated this to be an effective approach [84] . Previous studies have also indicated that in our context implementing routine syphilis testing is cost effective and supported by patients [85, 86] . We diagnose and monitor patients for syphilis based on the 2015 Centre of Disease Control guidelines for syphilis management and have described that over 90% of our patients across all syphilis stages achieved an adequate serological response after appropriate treatment [57, 86] . Our first-line therapy is benzathine penicillin for primary, secondary, early latent, and late latent syphilis and IV penicillin G for neurosyphilis with only 4% of patients receiving doxycycline for documented or reported penicillin allergy [57] . Diagnosing and managing neurosyphilis in PLWH presents several challenges, including the diagnosis and management of asymptomatic neurosyphilis (ANS) and limited evidence to guide optimal therapy. There is ongoing controversy as to whether ANS is a true entity, and whether it constitutes an increased risk of neurological complications if left untreated. Asymptomatic neurosyphilis in PLWH became a concern after several case reports were published during the height of the HIV epidemic, showing that patients who had been treated for syphilis with conventional regimens later developed "neurorecurrence" of the disease [87] [88] [89] [90] . However, there is no unifying definition for ANS, which is typically defined based on specific cerebrospinal fluid (CSF) abnormalities. Regardless, these reports spurred clinicians to consider whether PLWH who are diagnosed with syphilis should undergo lumbar puncture (LP) routinely to assess the CSF. Investigators then attempted to risk-stratify patients in order to determine who would benefit most from CSF examination. Reliable criteria for determining this risk have not been established, and a consensus on which patients should undergo examination for neurosyphilis remains elusive. The European guidelines on syphilis, updated in 2020, state that CSF assessment is not indicated in early syphilis in the absence of neurological symptoms, and regardless of HIV status [51] . Furthermore, despite benzathine penicillin G not reaching reputed treponemicidal levels in the CSF, a recent study by Tomkins et al. showed that among 64 PLWH who were deemed "high risk" for the development of symptomatic neurosyphilis (RPR ≥1:32, and/or CD4 <350/ mm 3 ), and treated with a single dose of intermuscular benzathine penicillin, one participant developed ANS, and none developed symptomatic neurosyphilis [91] . Despite this, some experts continue to recommend CSF assessment in asymptomatic patients to exclude neurosyphilis in the following circumstances: in PLWH who are diagnosed with late syphilis and have CD4 cells ≤ 350/mm 3 and/or a serum RPR titer >1:32, in those who have serologic failure, or in those given non-penicillin therapy for late syphilis [92] . Our practice is consistent with the European guidelines in that we do not conduct CSF assessments in patients without neurologic symptoms. In patients who have an inadequate response to syphilis therapy, we offer to conduct an LP, or to treat for presumptive neurosyphilis. Similarly, in cases where serologic testing raises the possibility of neurosyphilis, and a patient has vague or non-specific symptoms, we engage in a discussion of the risk and benefits of conducting an LP, treating for neurosyphilis presumptively, or treating for late syphilis, which many argue is adequate therapy for neurosyphilis, with frequent reassessment and the option to escalate investigations if necessary. In terms of assessing response to therapy, we do not routinely repeat an LP, and instead we rely on serological response as a strong predictor of CSF response, as previously demonstrated [93, 94] . The challenges described above in syphilis therapy are exacerbated in neurosyphilis due to a reliance on penicillinbased therapies. Intravenous aqueous penicillin has long been the mainstay of therapy; despite few studies to support its use, we know it reaches treponemicidal concentrations in the CSF, and has been a reliable therapy for decades. Intramuscular procaine penicillin, administered with probenecid, is also a recommended alternative regimen by the CDC and European guidelines [50, 51] , and a recent study by Dunaway et al. suggested it is equivalent to intravenous aqueous penicillin G, regardless of HIV status [95] . Ceftriaxone is a common alternative regimen that can be used in most people with penicillin allergy, but a recent Cochrane review identified minimal evidence to support its use [96] . While other drugs such as doxycycline and azithromycin have been used for syphilis therapy, and others, such as cefixime [97] , are being investigated, none has been evaluated for neurosyphilis specifically, leaving us with few alternative therapeutic options. Given the intensive resource requirements of intravenous therapy, and penicillin desensitization when required, evaluating alternative agents will be increasingly important as syphilis rates continue to soar. The current landscape in HIV and syphilis co-infection has two major areas of improvement that could help advance clinical management: improved testing and diagnostic methods for syphilis and increased efforts towards prevention of HIV and syphilis. HIV diagnostic testing was recognized as a crucial contributor to the medical and public health response to the HIV epidemic. Since 1985, when the US Food and Drug Administration (FDA) approved the first HIV detection systems, the evolution in laboratory technologies led to the development of the highly sensitive screening test and highly specific confirmatory assay used today. The improvement of HIV diagnostics has revolutionized the management and prevention of HIV. Contrary to HIV, there remains ample opportunity for improvement in syphilis testing to minimize morbidity, mortality, and transmission. Traditional darkfield microscopy is no longer routinely available and current serologic tests rely on the patient humoral immune response to diagnose syphilis. These indirect measures of syphilis are fraught with issues in interpretation. Rapid testing is available but is limited to treponemal antibody results and does not differentiate between active and previously treated infection [53] . The 2016 WHO syphilis guidelines suggest using the rapid treponemal antibody assay in antenatal screening in high-burden areas as the turnaround time is 10-15 min and the assay does not require refrigerated storage or lab equipment. Reported sensitivity and specificity range from 85 to 98% and 93 to 99%, respectively, compared to the TPPA as the reference standard. However, isolated treponemal antibody may lead to unnecessary treatment and stigma associated with syphilis diagnosis. Rapid testing combining non-treponemal and treponemal antibodies has been researched but not sufficiently validated or field tested and is not recommended in 2016 WHO syphilis guideline [98] . No combination rapid tests are currently available in the USA or Canada. PCR testing for syphilis is also an ongoing area of interest. There are various types of PCR testing; however, none are approved by the FDA and PCR testing for syphilis remains costly [99] . PCR is best for detecting primary syphilis with swabs from chancres although it would amplify both live and dead organisms. There is conflicting data regarding the sensitivity of PCR in secondary syphilis, and it is not felt to be suitable for screening asymptomatic individuals as PCR sensitivity drops to 24-32% in blood and CSF [99, 100] . The resurgence of syphilis has spurred interest in expanding syphilis prevention methods beyond traditional measures. Investigation into pharmacologic prevention strategies has increased recently with the advent of pre-exposure prophylaxis (PrEP) for HIV. Due to observations of high incidences of bacterial STIs in several HIV PrEP studies, post-exposure prophylaxis (PEP) for syphilis was investigated in the openlabel phase of the ANRS IPERGAY trial for HIV PrEP [101] . This sub-study of 232 participants showed that taking 200 mg of oral doxycycline within 72 h of condomless sex (without exceeding 600 mg total per week) reduced the incidence of syphilis and chlamydia, with a hazard ratio of 0.30 (95% CI: 0.13-0.70, p=0.006) and 0.27 (95% CI: 0.07-0.98, p<0.05) respectively. Further studies are ongoing to investigate the role of doxycycline as syphilis PrEP, such as in the Dual Daily HIV and Syphilis Pre-exposure prophylaxis trial [102] . Preliminary results have been promising. In patients using HIV PrEP, taking doxycycline 100mg orally daily immediately compared to deferring doxycycline (starting by 24 weeks) resulted in a syphilis incidence of 0 compared to 8.74 per 100 person years, and a chlamydia incidence of 0 versus 69.9 infections per 100 person years. Another pilot study was able to show reduced STIs in participants taking doxycycline 100 mg daily (versus a contingency management strategy with financial incentives to avoid STIs), but not in syphilis specifically [103] . In addition, a systematic review of studies assessing the efficacy of periodic presumptive treatment for bacterial STIs among sex workers showed efficacy for reducing the incidence of chlamydia, gonorrhea, and ulcerative STIs, but not syphilis [104] . Further evidence is required to support the use of PEP, PrEP, and presumptive treatment strategies for syphilis and other bacterial STIs, and investigations into the possible unintended effects of these prevention strategies, such as antimicrobial resistance, or Clostridium difficile superinfection will be important. An effective and accessible vaccine for syphilis could turn the tide on syphilis globally. Recently, Lithgow et al. identified a promising vaccine target, Tp0751, a vascular adhesin implicated in the dissemination of Treponema pallidum [105] . They have shown efficacy in animal models, but efficacy and safety in humans is yet to be shown. While a vaccine is likely years away from discovery, it could have a significant impact on curbing syphilis worldwide. Integrating HIV care with syphilis prevention and care is crucial for several reasons, especially with the advent of PrEP for HIV. We know that HIV and syphilis can increase the risk of becoming infected with the other [103] , and that certain populations are at high risk for both infections. Therefore, anyone presenting with a new diagnosis of HIV should be screened for syphilis, and vice versa. Furthermore, the guidelines for HIV PrEP in Canada and the USA now consider a new diagnosis of syphilis as an indication to consider starting HIV PrEP [106, 107] . In addition, frequent healthcare contact for PLWH and patients using PrEP could theoretically lead to increased rates of STI screening, which may result in earlier diagnosis of STIs, earlier treatment, and reduced onward transmission. While this has not been shown definitively, there is growing data to suggest this is possible [108] . Other novel strategies for improving STI screening rates include home testing kits for asymptomatic individuals, such as used by Sexual Health London in the United Kingdom [109] , and coupling syphilis screening serology with other routine tests, such as viral loads for PLWH [84] . Adapting these various tools and strategies to the needs of local communities worldwide is crucial to preventing both syphilis and HIV. The intersection of HIV and syphilis infections continues to evolve in the modern era, posing new and unique challenges in diagnosis, treatment, and prevention. Here we have explored the impacts of rising syphilis rates globally, the advent of HIV PrEP, and the COVID-19 pandemic. In addition, we have outlined our approach to caring for the patient with both HIV and syphilis. Moving forward, there continues to be a significant and growing need for more accessible testing strategies, alternative treatment regimens, and improved approaches to the prevention of both HIV and syphilis. Conflict of Interest Melody Ren and Thomas Dashwood declare that they have no conflict of interest. Sharon Walmsley reports grants, personal fees, non-financial support, and other from Merck; grants, personal fees, non-financial support, and other from Gilead Sciences; grants, personal fees, non-financial support, and other from ViiV Healthcare; grants, personal fees, non-financial support, and other from GSK; and grants and personal fees from Janssen, outside the submitted work. Human and Animal Rights and Informed Consent This article does not contain any studies with human or animal subjects performed by any of the authors. 
paper_id= ffef8194e52de95fe345db7dd12fe3185d786978	title= Virology Journal Evidence of HIV-1 adaptation to host HLA alleles following chimp-to-human transmission	authors= Nobubelo K Ngandu;Cathal  Seoighe;Konrad  Scheffler;	abstract= The cytotoxic T-lymphocyte immune response is important in controlling HIV-1 replication in infected humans. In this immune pathway, viral peptides within infected cells are presented to T-lymphocytes by the polymorphic human leukocyte antigens (HLA). HLA alleles exert selective pressure on the peptide regions and immune escape mutations that occur at some of the targeted sites can enable the virus to adapt to the infected host. The pattern of ongoing immune escape and reversion associated with several human HLA alleles has been studied extensively. Such mutations revert upon transmission to a host without the HLA allele because the escape mutation incurs a fitness cost. However, to-date there has been little attempt to study permanent loss of CTL epitopes due to escape mutations without an effect on fitness. Here, we set out to determine the extent of adaptation of HIV-1 to three wellcharacterized HLA alleles during the initial exposure of the virus to the human cytotoxic immune responses following transmission from chimpanzee. We generated a chimpanzee consensus sequence to approximate the virus sequence that was initially transmitted to the human host and used a method based on peptide binding affinity to HLA crystal structures to predict peptides that were potentially targeted by the HLA alleles on this sequence. Next, we used codon-based phylogenetic models to quantify the average selective pressure that acted on these regions during the period immediately following the zoonosis event, corresponding to the branch of the phylogenetic tree leading to the common ancestor of all of the HIV-1 sequences. Evidence for adaptive evolution during this period was observed at regions recognised by HLA A*6801 and A*0201, both of which are common in African populations. No evidence of adaptive evolution was observed at sites targeted by HLA-B*2705, which is a rare allele in African populations. Our results suggest that the ancestral HIV-1 virus experienced a period of positive selective pressure due to immune responses associated with HLA alleles that were common in the infected human population. We propose that this resulted in permanent escape from immune responses targeting unconstrained regions of the virus. 	body_text= Phylogenetic analysis indicates that the human immunodeficiency virus type 1 (HIV-1) originated from simian immunodeficiency virus infecting chimpanzees (SIVcpz) through a chimpanzee-to-human zoonotic transmission [1] [2] [3] [4] . Until recently [5] , the natural hosts of the virus, the chimpanzee, have been thought to remain asymptomatic throughout infection despite high viral loads [6] [7] [8] In humans, however, an increase in viral load is usually associated with progression to the acquired immuno-deficiency syndrome (AIDS) and subsequently death [9] [10] [11] [12] [13] . The causes of the difference in disease progression may involve either differences in the host and/or between the HIV-1 and the SIVcpz viruses. A zoonotic (i.e. cross-species) event is expected to be accompanied by mutations that enable the pathogen to adapt to the new host environment, (e.g. as observed in a study by Baric et al [14] ). Indeed, sequence changes have been identified in HIV-1 that are evidence of selective pressure associated with the genetics of the human host [15] [16] [17] . In particular, the human cytotoxic T-lymphocyte (CTL) immune response directed against foreign antigens plays a major role in exerting selective pressure on antigenic proteins, including those of HIV-1. The activation and characteristics of the immune responses against the virus have been found to differ remarkably between human and chimpanzee [7, [18] [19] [20] : an elevated anti-HIV immune response upon infection is characteristic in humans, but the chimpanzee generally maintains a low level of immune activation. The human immune response may therefore exert higher selective pressure on the virus sequence compared to immune responses of the natural host. However, the virus is capable of overcoming the immune response, leading to AIDS. The CTL immune response is mediated by Human Leukocyte Antigen (HLA) molecules that bind to endogenous antigenic peptides known as epitopes, and transport them to the surface of the infected cell for recognition by CTLs resulting in killing of the infected cell [21] . The HLA gene is highly polymorphic and each HLA molecule binds to peptides that contain specific sequence motif patterns (known as anchor residue motifs) [22, 23] . For binding to occur between a peptide and the HLA binding groove, only limited amino acid variation at the main anchor positions of the peptide is allowed [21, 24, 25] . Successful binding, efficient transport and presentation of a peptide to a CTL depend on the presence of the appropriate anchor residue motif and the overall affinity between the HLA binding groove and the epitope [26, 27] . The strength of selective pressure varies between specific CTL immune responses directed by different HLA alleles [28] . Some HLA molecules have been associated with immune escape mutations at anchor sites which enable the virus to adapt to the host, thus increasing viral load [8, [29] [30] [31] . Investigation of the evolutionary dynamics of immune escape has focussed primarily on escape mutations that incur a fitness cost and consequently revert to wild type, upon transmission to a host that mounts different immune responses. This can result in a pattern of toggling between escape and wild-type amino acids that is detectable using evolutionary modelling [32] . In this study the focus is on escape mutations that do not incur a cost in terms of viral fitness. Such escape mutations do not experience selection pressure to revert to the wild-type state following transmission to a new host. Consequently, they are associated with episodic selection, rather than the ongoing rapid evolution associated with escape and reversion. Upon transmission to human, SIV is likely to have experienced selective pressure to escape from common human immune responses. Some of these escape mutations would not have had a significant effect on the fitness of the virus and thus would not have experienced strong selection to revert. Consequently, we hypothesized that the branch of the SIV-HIV-1 phylogenetic tree leading to the ancestor of the HIV-1 sequences would include evidence of episodic selection to escape from common HLA alleles. To investigate the evidence of episodic selection for CTL escape along this branch, we predicted epitopes for HLA alleles, using the SIV consensus sequence to approximate the sequence that was transmitted to humans. We used a structure-based method that estimates the strength of binding between a viral amino acid sequence and an HLA molecule from amino acid pair-wise potentials for the epitope prediction. We selected regions where known anchor residue motifs were present and which had high binding affinity, limiting our analysis of selective pressure to these regions. Finally, we used models of codon sequence evolution to quantify the selective pressure, inferring positive selection from the ratio of nonsynonymous substitution rates (dN) to synonymous substitution rates (dS) for individual branches in a phylogeny. Branchspecific analysis of selective pressure enabled us to investigate selective pressure along the branch ancestral to the HIV sequences, and hence to study how HIV-1 adapted to the human host upon transmission from chimpanzee. We downloaded an alignment of HIV-1 group M reference genome sequences and chimpanzee sequences from the Los Alamos database [33] . Previously, we found that some synonymous sites of the nucleotide sequence are highly conserved due to purifying selective pressure acting upon them, and that such conservation of synonymous sites can cause errors in the prediction of positive selection [34] . Therefore, in this study we removed the conserved regions identified in that study from the alignment (see Additional File 1). The resulting alignment consisted of 9 chimpanzee sequences and 32 HIV-1 sequences starting from codon 1 of the gag gene and ending with the nef stop codon, but excluding regions listed in Additional File 1. We used HyPhy [35] to build a phylogenetic tree ( Figure  1 ) from the alignment using a neighbour joining method and the pairwise distances calculated using maximum likelihood. We used PREDEP [36] , a structure-based method for predicting HLA binding peptides to determine potential binding regions across the genome. We used the consensus chimpanzee sequence to predict the best HLA binding regions because it has not been exposed to selective pressure resulting from human HLA and approximates the sequence that was transmitted to human. Consequently, it may be possible to detect epitopes in the chimpanzee sequence that were eliminated from HIV-1 shortly after transmission to humans. PREDEP does not require knowledge of known HLA-binding peptides. The program requires solved crystal structures of the HLA molecules as well as knowledge of amino acid residues on the HLA binding groove that interact with each position of the antigenic peptide sequence. Amino acid pair-wise potentials between the peptide and the amino acids in the HLA binding groove are calculated based on backbone and side-chain interactions. A score for each HLA-peptide interaction is calculated as the sum of amino acid pair-wise potentials between each peptide residue and the interacting residues of the HLA binding groove. The lower the score, the better the peptide binds to the HLA binding groove, i.e., the higher the binding affinity. Peptides with strong binding affinities to the HLA molecule are most likely to be successfully presented to CTLs in-vivo. A test of PREDEP performance showed that 80% of the top 15 percentile best binders were known optimal HLA binding peptides [36] . In this study, we determined the binding energy of all possible peptides across the SIVcpz consensus sequence to each of the six HLA alleles with known crystal structures available in PREDEP. For each available HLA allele, we first selected peptide regions that had binding scores in the best 5% (a conservative threshold chosen to ensure minimal false positives) of those obtained across the chimpanzee sequence for that particular HLA allele. Next, we discarded regions that did not contain the amino acid residues known to give optimal binding at the major binding pockets, i.e. peptides that matched the anchor residue motifs of the HLA allele. For each HLA allele, we generated a new alignment consisting of only the sites in the potential binding regions identified for that allele for further analysis. We used the BranchAPriori [37] and GABranch [38] algorithms implemented in HyPhy [35] to analyse branchspecific selective pressure exerted by each HLA allele. We were specifically interested in selective pressure along the SIVcpz branch ancestral to the HIV-1 sequences (labelled 'B' in Figure 1 , and referred to below as the HIV ancestral branch), because we expect that this reflects the evolution of the virus around the time of transmission from chimpanzees to human. We therefore investigated whether there is higher selective pressure on this branch at sites that are potential targets for each HLA allele under study. These two approaches calculate the average selective pressure acting upon all regions potentially targeted by an HLA allele, thus combining evidence from multiple sites. We expect that this should result in more powerful tests than can be obtained via site-specific analysis. For each HLA-related alignment described in the previous section, we compared the selective pressure along the HIV ancestral branch to the rest of the branches in the tree using the BranchAPriori algotithm. The program outputs a p-value derived from the difference in the log likelihood between the null and the alternative models. The null model assumes a single global dN/dS ratio (ω) across the tree; in the alternative model, ω is allowed to have a different value for the HIV ancestral branch. The a priori analysis has the disadvantage of assuming that all the branches in the rest of the tree are under uniform selective pressure. This could result in the analysis having reduced power, for instance when there is strong amongbranch heterogeneity of selective pressure in the rest of the tree [39] . In order to construct a more realistic null model, we therefore considered models that allow selective pressure to vary across all branches of the tree. We used GABranch [38] , a genetic algorithm implemented in HyPhy, to infer branch-specific selective pressure across the entire phylogeny of SIV and HIV-1 sequences and determine, for each of the potential HLA binding regions, whether it evolved under positive selection in the HIV ancestral branch. As input, the GABranch analysis requires an underlying nucleotide model -we determined the best fitting nucleotide model using a maximum likelihood-based tool available in HyPhy [35] . GABranch then searches through a range of possible codon models with varying dN/dS rate The phylogenetic tree of the 32 HIV-1 group M reference genome sequences and 9 SIVcpz sequences from the Los Alamos sequence database [31] Figure 1 The phylogenetic tree of the 32 HIV-1 group M reference genome sequences and 9 SIVcpz sequences from the Los Alamos sequence database [33] . The chimpanzee sequence names start with 'CPZ' and the group M sequences start with the subtype name. The branch lengths are scaled in reference to the scale given at the top of the tree. The zoonosis event is located on the branch marked "B", referred to in the text as the HIV ancestral branch. classes, starting with a single rate model, i.e. a model that assumes uniform selection across all the branches of the tree. It tests models with more than one rate class, with the evolutionary rate of each branch being assigned to the best fitting rate class. An Akaike Information Criterion (AIC) value is calculated for each model, based on its fit to the data compared to the single rate model. The model with the best fit to the data, as indicated by the lowest AIC score, is selected and each branch is assigned to a dN/dS rate class (indicated on the phylogeny in figures 2, 3 and 4). Additionally, GABranch provides the proportion of tested models that show support for dN > dS for each branch. Only six HLA alleles with solved crystal structures (given in Table 1 ) were available for analysis within the PREDEP program. Of these six, only HLAs A*0201, A*6801 and B*2705 showed strong binding to regions of the SIVcpz consensus sequence, that is, regions with scores that were within the top five percentile and also contained the preferred anchor residue motifs. The total length of the overlapping peptide regions predicted to be the best binders for each HLA molecule across the chimp genome are given in Table 2 . We used the BranchAPriori analysis [39] to test, in the predicted binding region for each HLA allele, for evidence of differential selective pressure between the HIV ancestral lineage and the rest of the tree branches. For HLA A*0201 and HLA A*6801, the nonsynonymous-synonymous rate ratio ω was higher in the HIV ancestral branch than in the rest of the tree ( Table 3) . The difference was significant only for HLA A*6801, with a ω of 3.5 in the HIV ancestral branch and 1.6 in the rest of the tree (p value = 0.04). Selective pressure acting along the HIV ancestral branch for sites associated with A*0201 (ω = 2.5) was also high, but failed to differ significantly from the rest of the tree (ω = 1.2, p value = 0.08). For sites associated with B*2705 there was no significant difference between the HIV ancestral branch and the rest of the tree, with no indication of strong selective pressure in either case (ω = 1.0 in the HIV ancestral branch and 1.1 in the rest of the tree, p value = 0.65). We ran the GABranch analysis on the sequence alignments of predicted binding regions from each HLA allele. The ω rate categories for the best fitting model as well as the number of branches assigned to each rate category are given in Table 4 . Also shown, are the model-averaged val-ues obtained for ω and estimated probabilities that ω>1 on the HIV ancestral branch. The proportion of models that have support for dN>dS for each branch is given in Figures 2, 3 and 4 . The mean omega values and model support data for all 79 branches of the three trees are given in Additional files 2 (A*0201), 3 (A*6801) and 4 (B*2705). The tree in Figure 1 was made from the HIV and SIV full length sequences before selecting binding regions for each HLA allele, while those of Figures 2, 3 and  4 were generated from the screened alignments of binding regions for individual HLA alleles. We therefore do not expect these trees to have exactly the same topology. For HLA A*6801 (Figure 3 ), positive selective pressure was inferred along the HIV ancestral branch (ω = 1.15). A very high proportion of the tested models (0.996) supported dN>dS along this branch. We also found positive selective pressure along the HIV ancestral branch for HLA A*0201 (ω = 1.14, Figure 2 ) and again 0.996 of models supported for dN>dS. The best fitting model for the HLA B*2705 predicted binding sites had only a single rate class under weak purifying selection, and support for dN>dS was not obtained on any branch of the phylogeny ( Figure  4 ). These results are consistent with those of the Branch-APriori analysis and suggest that HLA A*0201 and HLA A*6801 exerted positive selective pressure on the HIV-1 sequence in the period immediately following zoonosis, whereas HLA B*2705 did not exert strong selective pressure on the HIV-1 sequence at any point in the phylogeny. The PREDEP program provides binding predictions for a limited number of HLA molecules with solved crystal structures and preferred binding anchor residue motifs that were predicted from HLA-peptide structural conformations. Only six such HLA alleles known to mediate cytotoxic T-lymphocyte immune responses are available for analysis. Amongst these, we only observed HLAs A*0201, A*6801 and B*2705 to bind strongly to some regions of the consensus SIVcpz genome. Our analysis was therefore restricted to selective pressure potentially exerted by each of these three alleles following the chimpanzee-to-human zoonosis event of HIV. It is interesting that neither the a priori nor the GABranch analysis found evidence for positive selection in the HLA B*2705 alignment, whether along the ancestral HIV-1 branch or any other branch. This is surprising because B27 alleles have been associated with delayed progression to AIDS in HIV-1 infected individuals [40] , which in turn is associated with persistent strong positive selection at specific sites [41] . Also, delayed progression is a result of reduced viral replication -this indicates that these sites are important for the fitness of the virus. One possibility that could explain the observed HLA B*2705 result is that it Branch-by-branch selective pressure for regions predicted to be targeted by HLA-A*0201 Figure 2 Branch-by-branch selective pressure for regions predicted to be targeted by HLA-A*0201. The ω classes for each branch are shown in colours given in the legend, along with the ω value for each class and the percentage of branches falling in each category. The percentage of models that support dN>dS are written above each branch. Branch-by-branch selective pressure for regions predicted to be targeted by HLA-A*6801 Figure 3 Branch-by-branch selective pressure for regions predicted to be targeted by HLA-A*6801. The ω classes for each branch are shown in colours given in the legend, along with the ω value for each class and the percentage of branches falling in each category. The percentage of models that support dN>dS are written above each branch. may have caused positive selection on only a few sites. Such selection is hard to detect because ω is averaged over all sites of the sequence. Selection may also have been weak due to the fact that this is a rare HLA allele (1%) [42] . In the HLA A*0201 dataset, both the a priori and the GABranch analyses inferred positive selection on the HIV-1 ancestral branch, with very high support for dN>dS. Of the alleles available for analysis in this study, HLA A*0201 is the most frequent in African populations (see Table 1 ). It is also the most frequent HLA allele in Caucasian populations and many studies have been carried out to determine its effect on HIV disease progression [42] . Even though the allele recognizes immunodominant peptide regions of the HIV-1 sequence, it fails to exert strong selective pressure on some virus peptides [43] . Some studies have also shown that the outcome of an immune response does not only depend on the HLA molecule but also on the specific peptide sequences that are targeted [44] [45] [46] [47] [48] . Our results suggest that immune escape mutations that occurred for HLA A*0201 mediated CTL responses may have been selected for in the period immediately following zoonosis. If these adaptations subse-quently became fixed in the viral population they would no longer be under diversifying selection today. HLA A*6801 (another common allele in African populations) appears to have exerted strong selective pressure on the HIV-1 ancestral branch compared to the rest of the tree. High support (99.6% of the tested models) for ω > 1 was observed at the ancestral HIV branch. This allele has anchor residue motif restrictions that are shared within the HLA A3 supertype, the second most frequent supertype in the human population [49] . The HLA A*6801 allele itself targets the Tat protein, which is expressed in the early stages of the HIV-1 lifecycle, and CTL responses to this protein cause a significant reduction in disease progression rate [50] . Escape mutations from the CTL immune response have also been identified within Tat at the population level, causing reduced viral load [51, 52] . The virus may have adapted well to the A*6801 responses early after the cross-species transmission event at sites that do not affect the replication of the virus. The recently observed association with a reduction in viral load indicates that there were also functionally important sites contained in A*6801 epitopes -this would have made it difficult for these regions to adapt to the immune response. This is the first study that analyses HLA-associated selective pressure following the transmission from chimpanzee to human across all potential target sites of the HIV-1 genome. We identified regions of the HIV-1 sequence that were initially targeted by the CTL immune response immediately after the cross-species transmission of HIV-1 from chimpanzee to human using the chimpanzee consensus sequence. Of the six HLA alleles with crystal structures available for analysis, we found strong binding regions; this could imply successful immune responses in vivo, for HLAs A*0201, A*6801 and B*2705. We determined the average extent of selective pressure exerted by each HLA allele along the branch leading to HIV-1 sequences. This branch represents the sequences that first encountered human immune response-directed selective pressure immediately following the zoonosis event. Our results suggest that HIV-1 adapted to CTL responses HLA alleles with solved crystal structures that are available for analysis in the PREDEP program, with allele frequencies as estimated in [42] . (Table 1) . It is therefore likely that the virus was frequently exposed to selective pressure exerted by common immune responses during initial exposure to the human host following transmission of the virus from chimpanzees. As observed from the results, we did not find evidence for strong selective pressure exerted by the HLA B*2705, which has extremely low frequencies in the African populations (Table 1 ) [53, 54] . In this study we focussed specifically on epitopes that we infer were likely to have been present in the viral sequence that first infected humans. We propose that the selection we observe at these positions along the branch of the phylogenetic tree leading to all of the HIV-1 sequences reflects episodic selection to evade human cytotoxic immune responses. Episodic selection has been proposed to be an important aspect of cross-species pathogen transmission and, in fact, observed in a laboratory setting previously [14] . However, this is the first time, to our knowledge, that evidence has been presented of transient positive selection associated with human immune responses against unconstrained regions of the virus shortly after transmission to human.  
paper_id= ffeffafec5c5db2f5bd2472bac2f7c5cabe13557	title= Stability analysis of a novel epidemics model with vaccination and nonlinear infectious rate	authors= Defang  Liu;Bochu  Wang;Songtao  Guo;	abstract= In this paper, by considering pathogen evolution and human interventions behaviors with vaccines or drugs, we build up a novel SEIRW model with the vaccination to the newborn children. The stability of the SEIRW model with time-varying perturbation to predict the evolution tendency of the disease is analyzed. Furthermore, we introduce a time-varying delay into the susceptible and infective stages in the model and give some global exponential stability criteria for the time-varying delay system. Finally, numerical simulations are presented to verify the results. 	body_text= Infectious diseases result in 14.7 million deaths, 26% of global mortality in accordance with WHO estimates in 2001 [1] . Mathematical analysis and modeling operation of infectious diseases are critical to study virus spreading dynamics which can state clearly the origination and evolution of viruses. In recent years, many mathematical models have been proposed for the transmission dynamics of infectious diseases [2] [3] [4] [5] [6] [7] [8] [9] such as, SI (susceptible-infective), SIR (susceptible-infectiveremoved), SEIR (susceptible-exposure-infective-recovered), SEI (susceptible-exposure-infective), SIRS (susceptible-infective-removed-susceptible), SEIS (susceptible-exposure-infective-susceptible). The development of such models is aimed at exploring the transmission dynamics of epidemic virus, investigating the evolution of resistance to antibiotics and the evolutionary cost of resistance, and designing the programs for disease control. However, a model's ability to achieve the above goals depends greatly on whether the assumptions made in the modeling process are consistent with the actual evolution of the epidemic diseases. Understanding and predicting the actual transmission dynamics of the epidemic diseases is, therefore, an important pursuit in mathematical epidemiology, which is one of our motivations for this work. At the same time, the stability analysis of epidemic models has also attracted much attention of biologists, mathematicians and ecologists. The underlying reason behind such attention is that through stability analysis for the epidemic models, the tendency of the infectious diseases can be found by the basic reproductive number R 0 (the average number of secondary cases produced by a typical primary case in an entirely susceptible population) and the generation time (the average time from symptom onset in a primary case to symptom onset in a secondary case), which determine to a large extent the speed of epidemic outbreaks. Based on the stability principles, the conditions of the infectious diseases persistence or extinction were obtained [10] [11] [12] [13] [14] [15] . Huang [3] presented an SEIR model, which is suitable for the eradication of diseases by mass vaccination or the control of diseases by case isolation combined with contact tracing, incorporating the vaccine efficacy or the control efficacy into the model. An SIRC model with a fourth class, i.e., the cross-immune individuals, was proposed [9] . Via bifurcation analysis of the model, they discussed the effect of seasonality on the epidemiological regimes. In this model, the incubation period of virus is not considered, nevertheless, a majority of infectious diseases have incubation period. Buonomo et al. [16] studied the global behavior of a non-linear SIR epidemic model with a non-bilinear feedback mechanism, which describes the influences of information and information-related delays on a vaccination campaign. Wan and Cui [17] proposed an SEIS epidemic model to study the effect of transport-related infection on the spread and control of infectious disease. Furthermore, through stability analysis, they found that it is very essential to strengthen restrictions of passengers once infectious diseases appearance is known. In [18] , McCluskey studied the complete global stability for the SIR models with distributed delay and with discrete delay. Lahrouz et al. [19] analyzed the complete global stability for an SIRS epidemic model with generalized nonlinear incidence and vaccination. In [20] , Kuniya studied the global asymptotic stability for an age-structured multigroup SIR epidemic model with a discretization approach. Muroya et al. [21] investigated the global asymptotic stability of a disease transmission model of SIRS type with latent period and the specific non-monotone incidence rate. The results of these studies, however, do not take into account the whole process of the epidemic diseases such as the exposed fraction and the partial immunity individuals, which are to a great extent owing to deficiency of the genetic variation behavior of the virus and the epidemiology theory. Consequently, they are limited on reflecting the actual transmission dynamics of the epidemic diseases and offering the practical proposal for government to draw up the policy. On the other hand, human interference behaviors such as enhancing the host's immune system with drugs or vaccines that alter pathogen genetic diversity, carrying out public health educational campaigns and vaccination in adolescents have been incorporated into the epidemic diseases models [22] [23] [24] [25] . Mathematical models for the spread of infectious diseases are an important tool for investigating and quantifying such effects of these behaviors. However, few efforts have been made to quantify and capture the effects in a systematic way. In this paper, to overcome the disadvantages above, by considering more complete processes of the epidemic diseases, we first build a novel susceptible-exposed-infective-recovered-partial immunity (SEIRW) model, which incorporates pathogen evolution and human interventions behaviors with vaccines or drugs that alter pathogen genetic diversity, aiming to reflect the transmission dynamics of infectious diseases more realistically. Our model differs from the existing ones in [17] [18] [19] [20] [21] in the following respect: (i) our model additionally takes into account the exposed fraction and the partial immunity individuals; (ii) our model is coupled with evolutionary-epidemic strategy and human interference behaviors such as vaccination to a proportion of all newborn children; (iii) in particular, the force of infection of the infectious disease in the existing work is multiplied by a constant such as survival probability while in this paper, it is governed by a nonlinear time-varying growing curve of pathogens, which reflects the viruses' evolution. Subsequently, we transform the SEIRW model into the ordinary differential equation with time-varying perturbation. Then we carry out a complete stability analysis of the transformed SEI-RW model and establish its stability criteria. Furthermore, we introduce the time-varying delay into the susceptible and infective stages of the SEIRW model and provide the globally exponential stability criteria for the SEIRW model with the time-varying delay. Finally, the numerical simulations verify the results. One of our purposes for this work is to investigate the effect of the treatment on the long term dynamics of the disease, and show that the treatment or rational immunization to prolong the susceptible or infective stages can hasten the disease dying out. To the best of our knowledge, this work is the first one that builds a novel SEIRW model by considering pathogen evolution and human interference behaviors, and further establishes sufficient conditions for the asymptotic stability of the model with time-varying perturbation and the global exponential stability of the model with time-varying delay. The rest of this paper is organized as follows: in Section 2, the infectious rate function of pathogens is obtained. The SEI-RW model is provided in Section 3. In Section 4, the stability of the SEIRW model with time-varying perturbation is analyzed. In Section 5, we discuss the global stability of the SEIRW model with time-varying delay by means of suitable Lyapunov functionals. Simulation to verify analysis for the model is gained in Section 6, concluding remarks. Section 7 concludes this work. Mathematical modeling of the evolution of pathogens responding to the viruses' genetic variation plays an important role in clinical epidemiology and statistics of infectious diseases. In the existing literatures, the mathematical descriptions of the contact rate which implies the infectivity depend on the analysis for the empirical data. Nevertheless, the contact process described by empirical data could be questionable on account of the incomplete statistical methods, e.g., the methods of the investigation based conversational contacts, which describe the spread of respiratory infections, the routine surveys of travel. Furthermore, the contact rate served as the infectious power cannot reflect the evolution of pathogens, especially the viruses' genetic variation in the available studies. The growing curve of pathogens indicates the speed of viruses spread as it is the coordinate figure of time-viruses number. The one-step growth curves, which reflect pathogen genetic diversity under the condition of interpretation of interventions with drugs or vaccines, have been studied by many biologists [26] [27] [28] [29] . Usually, the viruses' growth simulating the viruses' growth curve is expressed by: Eq. (1a) is a classical equation which is widely used in life science to describe the growth of the organism such as leaves, bacterium. The parameters a, b and k have specific biological meanings and are only positive real numbers in (1a). The unit of time t is day. Here, the derivation of Eq. (1a) is given by. which denotes the infectious rate function of viruses and the speed of the viruses growth. Since the transmission of the viruses among the populations is related with the infectious rate of the viruses; furthermore, it is direct proportion to the infectious rate, therefore, in this paper, the speed of the viruses' transmission among the populations is expressed by the function x(t) below. In (1c), the product of k 1 k is constant and it can be represented by k just as well. Consequently, the infectious rate of transmission among the populations can be given by (2) . is constant and it can be represented a; b; k > 0: ð2Þ A general flow process diagram of the SEIRW epidemic mathematical model is shown in Fig. 1 . From an epidemiological point of view, the normal community of people can be divided into five compartments at any time t for the majority infectious diseases such as influenza A, SARS and AIDS: the susceptible individuals S(t), who may be infected by the viruses, and do not have specific immune defenses against the infectious disease; the exposed fraction E(t), which denotes hosts whom viruses have entered into while the viruses are in incubation period; the infective individuals I(t), who are infected by the specific viruses and are infectious to the susceptible individuals; the recovered fraction R(t), which denotes those individuals who have recovered from the infection but have partially immune to the specific pathogen. However, after a period of time, a part of the individuals in R(t) enter the next group W(t), the partial immunity individuals, because they have only partial immunity to the viruses owing to their variation in the new environment [26, 30] , as in the case of SRAS [26] , influenza [31] and pertussis [32] . The environment is much important for the survival of intermediate hosts and for the survival of the pathogens outside the host, which can affect the efficiency of transmission. The environmental component of infectiousness can result in an unusually large number of secondary infections, which motivates that the environmental effect should be considered in our model. For example, the 'superspreading' event led to various secondary severe acute respiratory syndrome (SARS) infections among residents of the Amoy Gardens estate in Hong Kong in 2003 [26] . In Fig. 1 , the susceptible class is generated from individuals at a net birth ZN, where N denotes the total population size and Z denotes the birth rate. Similarly, it is assumed that the natural death rate of all the classes is l which is not equal to the birth rate. g denotes the death rate caused by the special infectious diseases. Infectious rate function x(t) represents the viruses' state of genetic variation. Those from the susceptible individuals to latency ones are proportion to the susceptible, the infective and the infectious rate function. Therefore, the incidence of disease can be considered as S(t)I(t)x(t) per unit time, and I(t)x(t) is often used to represent the force of infection. The rate of transformation from incubation period individuals to infective individuals per unit time is given by e. The cure rate (i.e. the rate at which the infective people recover per unit time) is denoted by a. In this model, vaccination strategies are the focus soul and they are the influential measures to control the outbreak or extinction of the infectious diseases by the government. In order to evoke the given immune response with clinical infectious disease rather than natural infection, vaccines can provide a foreign antigen to the immune system and the subsequent immune stimulation to natural infection is gained more rapidly so as to prevent the attack of severe disease viruses. In this study, vaccination is applied to a ratio of all newborn children. Because of newborn children's highly immature immune systems, it is difficult to generate antibody to the special infectious disease after they have been injected vaccination or drug. Even so, it is the forceful strategies for the government to prevent viruses such as AIDS and influenza A transmission. We assume that the ratio of vaccination to newborn children is Bv, and the newborns successfully gain the vaccine and obtain immunity to infection. Then there are an inflow of susceptible individuals ZN(1 À Bv) and an inflow of vaccinated individuals ZNBv to the vaccinated compartment. Though the recovery individuals obtain the specific immunity to the infection in the original period, due to the variability and evolution of the viruses, the ability of the hosts to evade natural immune protection is diminished or died away in a new environment. In this study, it is assumed that a proportion n of the recovered population enter the partial immunity group W(t) in some given period. After some time, a proportion c of the hosts become incubation period individuals, and a proportion 1 À c of the vaccinated hosts become recovered people. d is the contact rate. b is the rate from the partial immunity group W(t) to the susceptible individuals S(t). The relationship expression e > c is obvious. Formally, the SEIRW model can be illustrated by the following set of five ordinary differential equations. where xðtÞ ¼ abke Àat ð1þbe Àat Þ Let T denotes the transpose of a matrix and R 5 be the 5-dimensional Euclidean space. For . For a positive number r 6 + 1, we define X r ¼ fz 2 R 5 : kzk 6 rg.  Since G(t,0)-0, it is obvious that system (4) can be regarded as the non-zero time-varying perturbation to linear autonomous system. Take the initial condition A solution of problem (4) is a function x : ½0; 1Þ ! R 5 , having a locally bounded derivative and satisfying (5) and (3) for all t 2 [0,1). Existence of solution is assumed. Furthermore, the following assumptions are satisfied: In addition, function G(t, x) can be referred to as a continuous mapping ½0; 1Þ Â X r ! R 5 . In order to analyze the stability of system (4), we need show that the following lemma holds. Lemma 1. For "t P 0, "x 2 X r , the perturbation function G(t, x) satisfies the following condition: where d is a non-negative constant number, i.e., d ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi Proof. From (4), we have Because only a proportional of susceptible individuals are infected by the viruses and enter into the eclipse group, the condition below is of existence, lN(1 À B v ) P xSI, i.e., / À xSI P 0, it is not difficult to obtain that ðxSIÞ 2 þ ð/ À xSIÞ 2 6 ðxSI þ / À xSIÞ 2 ¼ / 2 : In addition, because Sdenotes the size of the susceptible group, i.e., S P 1, we have kxk P 1. Therefore, kG(t, x)k 2 6 / 2 + u 2 6 (/ 2 + u 2 )kxk 2 . Obviously, there exists a non-negative constant number d ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi This completes the proof. h In the first place, we shall investigate the stability of the linear part of system (3): We can reasonably arrive at the following theorem. Theorem 1. For l > 0, e > 0, g > 0, a > 0, n > 0 and b > 0, the linear system (7) is asymptotically stable. Proof. It is well known that the trivial solution of the system (7) is asymptotically stable if all roots of the characteristic Eq. (8) have negative real parts, and unstable if one root has positive real part. The characteristic equation of system (7) can be given by  In order to study the stability of the equilibrium of system (7), we need to investigate the distribution of roots of Eq. (8) . For the first term in the left-hand side of (8), we have k = Àa 1 < 0. It is clear that the equilibrium point of system (7) is asymptotically stable. Therefore, we only need to consider the following characteristic equation: According to Routh-Hurwitz criterion, all roots of the characteristic Eq. (8) will have negative real parts provided that the following conditions are satisfied: and In the following, we will prove the conditions (9a) and (9b) hold. It follows from (8) that It is clear that the condition (9b) holds, i.e., M 2 > 0. This completes the proof. h Remark 1. We can conclude from Theorem 1 that the value of c does not effect on the stability of system (7). In other words, the stability of system (7) is unrelated to the proportion of the partial immunity individuals to the exposed ones, or to the recovered ones. The stability criterion of system (7) is much relaxed. Furthermore, we shall study the stability criteria of the system (4) with non-zero time-varying perturbation term. where a 1 and a 2 are K-class function, #(x) is continuous positive definition function. Let r > 0 make B r & D and assume t < a À1 2 ða 1 ðrÞÞ. Then for every initial state x(t 0 ) such that xðt 0 Þ k k6 a À1 2 ða 1 ðrÞÞ, there exists a KL -class function u and T P 0, which is not related with x(t 0 ) and t, to make the solutions of system (4) satisfy kxðtÞk 6 uðkxðt 0 Þk; t À t 0 Þ; 8t 0 6 t 6 t 0 þ T; ð10aÞ kxðtÞk 6 a À1 1 ða 2 ðtÞÞ; 8t P t 0 þ T: By using the definitions and lemma above, we have the following theorem. Theorem 2. The system (4) is asymptotically stable if there exists a symmetrical positive definite matrix P such that A T P þ PA ¼ ÀI and / 2 þ u 2 < 1=ð2k max ðPÞÞ 2 ; where I is the unit matrix, k max (Á) and k min (Á) denote the maximum and the minimum eigenvalues, respectively. Proof. We can choose the candidate Lyapunov function to be Then the following relationship holds: which indicates that the Lyapunov function is bounded. It is not difficult to see that (12) is the function about t and x. Calculating derivative of this function along the trajectories of system (4) yields The first two terms of the right side of (14) constitute the derivation of V(t, x) along the trajectories of the linear part of system (4), and the third term is the result of perturbation. Taking the derivative of (12) along the linear part of system (4), we have _ Vðt; xÞ ¼ x T ðAP þ PAÞx: By comparison with the first two terms of the right side of (14), it is easy to obtain that We know from Theorem 1 that the matrix A is Hurwitz matrix and the linear system (7) is asymptotically stable. Thus A T-P + PA is positive definite. Without loss of generality, we let A T P + PA = ÀQ, where Q is a symmetrical positive definite matrix. Then we can get Moreover, the following relationship can be obtained: According to Lemma 1 and the relationships (15) and (16), it follows that _ Vðt; xÞ ¼ @V @t þ @V @x Ax þ @V @x Gðt; xÞ0 6 Àk min ðQ Þkxk 2 þ @V @x kGðt; xÞk 6 Àk min ðQ Þkxk 2 þ 2dk max ðPÞkxk 2 ¼ ðÀk min ðQ Þ þ 2dk max ðPÞÞkxk 2 : Thus, if d < k min ðQ Þ=ð2k max ðPÞÞ; _ Vðt; xÞ < 0, i.e., the system (4) is asymptotically stable. Obviously, the bound of d depends on the choose of Q. Therefore, it is a challenge that how to chose Q makes k min (Q)/ k max (P) maximum. It has been proven in [34] that when Q = I, i.e., A T P + PA = ÀI, the goal can be reached. From Lemma 1, we have that if / 2 + u 2 < 1/(2k max (P)) 2 , the system (4) is asymptotically stable. This completes the proof. h Remark 2. It can be concluded from Theorem 2 that the system (4) is asymptotically stable if the condition is satisfied: / 2 + u 2 < 1/(2k max (P)) 2 . The condition implies that if the quadratic sum of the inflow of susceptible individuals and the inflow of vaccinated individuals to the vaccinated compartment, is below a constant value, the disease will tend to be stable, that is to say, the infectious disease will not be diffuse infinitely but will become endemic disease. This means that we can control the value of Bv, the ratio of vaccination to newborn children, to postpone the outbreak of the disease and the epidemic disease becomes disappeared gradually. In the proof of Lemma 1 and Theorem 2, we use kGðt; xÞk 2 6 / 2 þ u 2 6 ð/ 2 þ u 2 Þkxk 2 : The right inequality can result in the larger bound of k G(t, x)k, which can further induce the conservativeness of stability criterion of Theorem 2. In order to overcome this weakness, we can give the following result by only using kGðt; xÞk 6 r ¼ ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi Theorem 3. If there exists a symmetrical positive definite matrix P and a T P 0, and for all t P 0, 0 < h < 1 and x 2 X r ¼ fx 2 R 5 : x k k 6 rg, the condition r < hrk 2kmaxðPÞ holds, then for all kx(t 0 )k 6 rk, the system (4) is asymptotically stable, whose solutions are uniformly ultimately bounded and satisfy: kxðtÞk 6 1 k expðÀfðt À t 0 ÞÞkxðt 0 Þk; 8t 0 6 t 6 t 0 þ T; Proof. Similar to the proof of Theorem 2, we can also choose a candidate Lyapunov function to be Vðt; xÞ ¼ x T Px and let A T P + PA = ÀI, here, I denotes the unit matrix. The derivative of this function along the trajectories of system (4) can be given by _ Vðt; xÞ ¼ @V @t þ @V @x Ax þ @V @x Gðt; xÞ 6 Àk min ðA T P þ PAÞkxk 2 þ @V @x kGðt; xÞk 6 À x k k 2 þ 2rk max ðPÞ x k k ¼ Àð1 À hÞkxk 2 À hkxk 2 þ 2rk max ðPÞkxk: Thus for any kxk P 2rk max (P)/h, we have _ Vðt; xÞ 6 Àð1 À hÞkxk 2 : According to Lemma 2, we can get a 1 (kxk) = k min (P)kxk 2 , a 2 (kxk) = k max (P)kxk 2 , # (x) = (1 À h)kxk 2 and t = 2rk max (P)/h. Thus it follows that if the conditions (13) and (17) are satisfied, for each initial state x(t 0 ) such that kxðt 0 Þk 6 a À1 2 ða 1 ðrÞÞ ¼ rk, there exist a KL-class function u and a T P 0 to make the solutions of system (4) satisfy kxðtÞk 6 uðkxðt 0 Þk; t À t 0 Þ; 8 t 0 6 t 6 t 0 þ T; kxðtÞk 6 a À1 1 ða 2 ðtÞÞ; 8 t P t 0 þ T: From Lemma 2 and the converse function of a 1 and a 2 , we can get a À1 1 ða 2 ðtÞÞ ¼ 2rk max ðPÞ hk : Without loss of generality, we can choose uðkxðt 0 Þk; t À t 0 Þ ¼ 1 k expðÀfðt À t 0 ÞÞkxðt 0 Þk, which obviously belongs to a KL-class function According to t < a À1 2 ða 1 ðrÞÞ, i.e., 2r k max (P)/h < rk, we can obtain r < hrk 2kmaxðPÞ . This completes the proof. h Remark 3. Compared with the results in Theorem 2, the stability condition obtained in Theorem 3 is much relaxed, i.e., the condition of postponing the outbreak of the disease is easy to be satisfied. More importantly, the attractive region of the feasible solutions is provided. Biologically, the ultimate bound of the ratio of susceptible people, the exposed fraction, the infective individuals and the recovered fraction is given. In this section, by introducing time-varying delay s(t) to the susceptible individuals S(t) and the infective individuals I(t) in the SEIRW model (3), we investigate the effect of time delay on the stability of the SEIRW model. s(t) > 0 is assumed to be a variable incubation period during which the infection of the susceptible and clinical manifestation of infective is postponed because of the therapy treatment. The system (3) can be transformed as follows: which can be rewritten in the form of compact matrix as _ xðtÞ ¼ AxðtÞ þ xðtÞHðxðt À sðtÞÞÞ þ C; where H(x(t À s(t))) = BS(t À s(t))I(t À s(t)). Other parameters are the same as in (4) . In this section, we introduce the following assumption: (S1) Function s(t) is nonnegative, bounded and continuously differentiable defined on R + and inf t2Rþ f1 À _ sðtÞg > 0, where _ sðtÞ denotes the derivative of s(t) with respect to time t. (S2) There exist two diagonal matrices L = diag{l 1 , . . . , l 5 } and F = diag{f 1 , . . . , f 5 } such that for any x, y 2 R and xy, the following inequalities hold l i 6 H i ðy 1 Þ À H i ðy 2 Þ y 1 À y 2 6 f i ; i ¼ 1; . . . ; 5: Let s = sup{s(t):t 2 R + }. We denote by C 5 [ À s,0] the Banach space of 5-dimensional continuous functions / (s) = [/ 1-(s), . . . , / 5 (s)] T :[ À s,0] ? R 5 with the norm k/k = max Às6s60 j/ (s)j. In this section, we always assume that all solutions of system (21) satisfy the following initial conditions where / = [/ 1 , . . ., / 5 ] T :C 5 [ À s,0]. It is well known that by the fundamental theory of functional differential equations, system (21) has a unique solution x(t) = [x 1 (t), . . . , x 5 (t)] T satisfying the initial condition (22) . (21) is said to be bounded on R + , if the following condition holds: where -> 0 is a constant. jxðtÞ À yðtÞj 6 Mk/ À wk expðÀetÞ for all t P 0: In the following, we will use Young inequality and Dini derivative to prove the boundedness of solutions. On the Young inequality and Dini derivative, we have the following lemmas [35] . Lemma 3. Assume that a P 0, b P 0, p > 1, q > 1 with 1/p + 1/q = 1. Then we have the inequality Lemma 4. Assume that u(t) is a differentiable function defined on R + . Then for any t 2 R + the Dini upper right derivative D + j u(t)j of function ju(t)j exists and has the following expression:  For model (21) to be mathematically tractable and epidemiologically meaningful, it is important to prove that all the state variables are nonnegative and bounded for all time. We prove that all solutions of system (21) with positive initial data will remain positive for all time t P 0. Proof. Let x(t) = [x 1 (t), . . . , x 5 (t)] T be any solution of system (21) with the initial functions u 2 C 5 [ À s,0] at t = 0. System (21) can be transformed into the following form A ij x i ðtÞ þ xðtÞH i ðx i ðt À sðtÞÞÞ þ C i : Calculating the upper right derivative D + (jx i (t)j r ), i = 1,. . . , 5, we have Note that the first inequality in (22a) holds, which is due to that x i (t) and H i (x i (t À s(t))) are the ith elements of vectors x(t) and H(x(t À s(t))), respectively, that is, both of them are real number. As a result, it is not difficult to obtain that x i (t) 6 jx i (t)j and H i (x i (t À s(t))) 6 jH i (x i (t À s(t)))j. The second inequality in (22a) holds, which is since jx(t)j 2 [0, ka/4]. By using Young inequality, we further have ½ðr À 1Þjx i ðtÞj r þ jH i ðx i ðt À sðtÞÞÞj r ; for all t P 0 and i = 1,. . . , 5. According the assumption (S2), we can choose a constant p such that max tÀs6s6s jH i ðx i ðtÞÞj r 6 pjx i ðtÞj r : Furthermore, we can obtain Thus there exists a sufficient large constant w such that for all t P 0 and i = 1,. . . , 5, and k/k 6 w. Then we can obtain jx i (t)j < w for all t P 0 and i = 1,2, . . . ,5. In fact, if it is not true, then there exist some i and time t 1 > 0 such that jx i (t 1 )j = w, D + jx i (t 1 ) j r P 0 and jx j (t)j 6 w, for all Às 6 t 6 t 1 and j = 1,2,. . . , 5. However, from (22b) and (22c), we obtain This is a contradiction. Hence, jx i (t)j 6 w, for all t P 0. Therefore, the solution x(t) = [x 1 (t), . . . , x 5 (t)] T of system (21) is defined and is bounded on R + . This completes the proof. h In this subsection, by constructing new Lyapunov functional and using the technique of matrix analysis, we will establish new criteria on the global exponential stability of system (21) . Theorem 5. Assume that (S1) and (S2) hold. System (21) is globally exponentially stable if there exist 5-dimensional symmetrical positive definite matrix P, diagonal matrix a = diag(a 1 , . . . , a 5 ) > 0, F = diag{f 1 , . . . , f 5 }, and a constant a > 0 such thatk min (D(t)) P a, for all t 2 R + , where DðtÞ ¼ ÀA T P À PA À 1 1 À _ sðtÞ F T aF À xðtÞPa À1 xðtÞP: . , x i5 (t)] T , i = 1,2 be any two solutions of system (21) satisfying the initial conditions . . , 5, then system (21) is transformed into the following form _ zðtÞ ¼ AzðtÞ þ xðtÞHðzðt À sðtÞÞÞ; where H(z(t À s(t))) = [H 1 (z 1 (t À s(t))), . . . , H 5 (z 5 (t À s(t)))] T , and H i (z i (t À s(t))) = H i (x 1i (t À s(t))) À H i (x 2i (t À s (t))). Let e be a constant. We construct the Lyapunov functional as follows: By Theorem 4, x(t) is defined for all t 2 R + and is bounded, which means that z(t) is bounded on R + . Furthermore, we obtain that V(t, z t ) is also bounded on R + . The derivative of this functional along the solution of system (21) is H T ðzðtÞÞaHðzðtÞÞÀH T ðzðt À sðtÞÞÞaHðzðt À sðtÞÞÞ i Since 2z T ðtÞPxðtÞHðzðt À sðtÞÞÞ À H T ðzðt À sðtÞÞÞaHðzðt À sðtÞÞÞ 6 z T ðtÞPxðtÞa À1 xðtÞP T zðtÞ and H T (z(t))aH(z(t)) 6 z T (t)F T aFz(t). we further obtain _ Vðt; z t Þ 6 e et z T ðtÞðA T P þ PAÞzðtÞ þ ez T ðtÞPzðtÞ þ e esðtÞ 1 À _ sðtÞ H T ðzðtÞÞaHðzðtÞÞ þ z T ðtÞPxðtÞa À1 xðtÞPzðtÞ ! ¼ Àe et z T ðtÞ ÀA T P À PA À eP À e esðtÞ 1 À _ Hence, we have lim e!0 D 1 ðt; eÞ ¼ DðtÞ uniformly for all t 2 R + . It follows that there exists a constant e > 0 such that k min (D 1-(t, e)) P a, for all t 2 R + . Therefore, we finally obtain _ Vðt; z t Þ 6 Àae et z T ðtÞzðtÞ < 0 for all t 2 R þ ; which implies that VðtÞ 6 Vð0Þ for all t P 0: ð25aÞ Directly from (24) and assumption (S2) we have for all t P 0 and where M 0 = M/k min (P) P 1 is a constant and is independent of any solution of system (21) . We further obtain that system (21) is globally exponentially stable, and the proof is thus completed. h In Theorem 1, if we choose a = rE, where r > 0 is a constant and E is a unit matrix, then we have Thus we obtain the following corollary as a special case of Theorem 1. Corollary 1. Suppose that (S1) holds. If there exist 5-dimensional symmetrical positive definite matrix P and constants r > 0 and -> 0 such that k min ðÀA T P À AP À 1 1 À _ sðtÞ F T F À rx 2 ðtÞP 2 Þ P -; for all t 2 R þ ; then system (21) is globally exponentially stable. Since x(t) belongs to [0, ka/4] and achieves maximum when the time t = lnb/a, it follows from (25) Pa À1 P T : Hence, we have lim e?0 D 1 (t, e) = D(t) uniformly for all t 2 R + , where Pa À1 P: Thus we have the following corollary. Corollary 2. Suppose that (S1) holds. If there exist 5-dimensional symmetrical positive definite matrix P, diagonal matrix a = diag(a 1 , . . . , a 5 ) > 0 and a constant -> 0 such thatk min (D(t)) P -, for all t 2 R + , where Pa À1 P; then system (21) is globally exponentially stable. In this section, we shall verify the correction and effectivity of the stability conditions in Theorems 2 and 3. In the first place, we shall show that the system (4) is asymptotically stable when the stability conditions in Theorem 2 are satisfied. According to the biologically practical meanings, without loss of generality, we let l = 1.5 Â 10 À3 , e = 0.85, g = 0.65, a = 0.88, n = 0.85, d = 0.85, b = 0.35. It is not difficult to obtain that there exists a symmetrical positive definite matrix P such that A T P + PA = ÀI: ; whose eigenvalues are 0.2212, 0.3088, 0.4920, 1.4456, and 13.8226. Thus we have k min (P) = 0.2212 and k max (P) = 13.8226. Since the bound of x(t) is less than 1, it follows that ak/4 6 1. In this case, we might as well let a = 0.45, b = 0.25, k = 8, N = 1000, z = 2.0 Â 10 À3 . The simulation results are depicted in Fig. 2 . From the results of numerical computation, we can see that if the stability conditions in Theorem 2 are satisfied, the system (4) is asymptotically stable. Furthermore, we shall verify the effectivity of Theorem 3. We assume that r = 500, h = 0.65. Then we have  We further find that the bound of Bv is 0.3375 < Bv < 0.6625, which implies that we can control the ratio of vaccination to newborn children, to interrupt the outbreak of the disease or to slow the transmission of the infectious disease. According to kx(t 0 ) k 6 63.25, we assume xðt 0 Þ ¼ 40 20 15 10 20 ½ T . It is not difficult to see that the system (4) is asymptotically stable when the stability conditions in Theorem 3 are satisfied. More importantly, we let r = 1.425 and the ultimate bound of the solutions of system (4) can be given by b = 479.1, which implies that the value of b works upon the transmission of the infectious disease, that is the growth function of the pathogens has a very close connection with the disease spread. The values of r and b are the conditions of influencing the stability of the system (4) and the ratio of vaccination to newborn children. Practically, government can control Bv by vaccinating the newborn children to delay the outbreak of the disease. From Fig. 3 , we can also see that during the initial stage, owing to vaccination to newborn children, the initial values of W and S do not equal to 0. As time goes on, if the conditions are satisfied, the system will be stable. At last, the infected individuals will achieve minimal number and the disease will be endemic diseases rather than diffused. It is shown from Fig. 4 that the speed of the viruses' growth descends faster as parameter a increases while it decreases and trends to zero as time t goes on. Furthermore, we can find that in the case of 0 < a < 1, when time t is about 2 days, the infectious rate of viruses is a constant, 0.3, which is independent on the parameter choice of x(t). Thus, we have t = 2, which implies that if we let t 0 = 0, the ultimate bound of the solutions of system (4) is time-varying when time t is less than 2, otherwise, it is a constant. This time point is the best one that the governments control the infectious rate of viruses. It is shown from Fig. 5 that in the initial time, when 0 < t < 1, as time t goes on, the individuals in incubation time, E(t), increases rapidly. After E(t) goes to the apex of the curve, it decreases rapidly and trends to zero as time t goes on. On the point of the apex, the individuals E(t) in incubation period increases with the increase of the parameter a. From Fig. 5 we can also conclude that it is the inflection of the curve when time t is about 2 years. E(t) decreases slowly and trends to stability when time t is greater than 2 while E(t) decreases as parameter a increases. We can conclude that the number of infected individuals is related with the value of a, the index of the infectious rate function. In the following, we shall verify the correction and effectivity of the stability conditions in Theorem 5 and its corollaries. We let s (t) = 1 + 1/2sin (t), a = 0.045, Z = 3⁄10 À3 and F = diag{ZN, ZN, ZN, ZN, ZN}. Other parameters are the same as the verification for Theorem 2. We obtain that inf t2Rþ f1 À _ sðtÞg P 1=2. Furthermore, it is not difficult to see that the assumptions (S1) and (S2) are satisfied. We choose _ : Hence, we can see that there exists a constant -> 0 such that k min (D(t)) P -, for all t 2 R + . This shows that from Corollary 2, system (21) is globally exponentially stable, as shown in Fig. 6 , which implies that the treatment or rational immunization for the susceptible or infective individuals may hasten the disease dying out. In this paper, we present a new epidemic model, SEIRW model, with human interference behaviors such as vaccination and therapy treatment. Furthermore, the stability of the model with variable coefficients in five dimensions is analyzed so as to predict the evolution tendency of the disease. One of our goals is to obtain the relatively accurate conditions to ensure the asymptotic stability of the SEIRW model, in other words, to give the practical conditions that the infectious disease will not be diffuse infinitely and disappear gradually. This study can quantify the effects of human interference behaviors like vaccination on the diseases spread. Furthermore, we introduce the time-varying delay into the susceptible individuals and the infective individuals in the SEIRW model and analyze the global exponential stability of the SEIRW model with time-varying delay. Our results show that rational immunization for the susceptible or infective individuals may hasten the disease dying out. Finally, the numerical simulations verify the results. Therefore, one of the main aims of our research is to activate epidemiological surveys for purpose of deepening current knowledge of vaccinating behavior to the newborn children and perform therapy treatment to the infective. The study takes into account such human interference behaviors and quantifies their effects on the spread of the infectious diseases. Thus, some key efforts might be made to public healthy authorities, for example, rational immunization could be acceptable only in the case where the spontaneous baseline vaccination rate fulfills the reverse of inequality. The challenge to the infectious disease epidemiologist is to resolve the concealed determinants of the human interference behaviors of the transmission of the infectious disease and discriminate rules that could enable effective controls to be identified and this behavior to be predicted. 
paper_id= fff0a264abb537842ffceba9933f2f6f6cca19b4	title= Comment	authors= Oliver W Morgan;Ximena  Aguilera;Andrea  Ammon;John  Amuasi;Gabriel M Leung;Barbara  Mahon;John  Nkengasong;Farah  Naz Qamar;Anne  Schuchat;Lothar H Wieler;Scott F Dowell;Omorgan@who  Int;	abstract= 	body_text= The COVID-19 pandemic has exposed weaknesses in disease surveillance in nearly all countries. Early identification of COVID-19 cases and clusters for rapid containment was hampered by inadequate diagnostic capacity, insufficient contact tracing, fragmented data systems, incomplete data insights for public health responders, and suboptimal governance of all these elements. Once SARS-CoV-2 became widespread, interventions to control community transmission were undermined by weak surveillance of cases and insufficient national capacity to integrate data for timely adjustment of public health measures. 1,2 Although some countries had little or no reliable data, others did not share data consistently with their own populations and with WHO and other multilateral agencies. The emergence of SARS-CoV-2 variants has highlighted inadequate national pathogen genomic sequencing capacities in many countries and led to calls for expanded virus sequencing. However, sequencing without epidemiological and clinical surveillance data is insufficient to show whether new SARS-CoV-2 variants are more transmissible, more lethal, or more capable of evading immunity, including vaccine-induced immunity. 3, 4 Public health decision making relies on real-time, accurate surveillance. 5 As communities and economies struggle to recover from the consequences of these surveillance deficiencies, now is the time for countries and multilateral agencies to take a hard look at what failed and to act boldly to implement the necessary improvements to disease surveillance. Future disease surveillance should comprise well integrated national systems based on five principles (table) . First, a strong surveillance foundation should monitor the population in a systematic, consistent, and statistically sound way. Second, surveillance systems must incorporate laboratory confirmation appropriately scaled for different diseases and risks. Third, surveillance systems must be digitised, with unique health identifiers to connect individual-level data and with privacy safeguards. Fourth, surveillance programmes must use standardised case definitions and common data elements, with appropriate access for the public, local and national health authorities, regional bodies, and WHO. Fifth, disease surveillance must be adequately financed. Interpretation of disease surveillance data needs population representativeness, denominators, and historical baseline data. Civil registration and vital statistics (CRVS) systems are important for population estimation and understanding excess mortality but have historically taken years to build. Many countries that lack or have inadequate CRVS systems need to accelerate their development in alignment with the recommendations in the WHO SCORE for Health Data Technical Package report. 6 In the meantime, representative sample registration systems can provide denominator and mortality data and can be designed to support the development of CRVS systems. Such sample registration systems are established in several middle-income countries and are being implemented in some lowincome countries, such as Mozambique and Sierra Leone. 7 Multiple surveillance systems can be integrated on such a population-representative foundation, according to the priorities of the country and leveraging internal resources, such as surveillance programmes run by academic and non-governmental institutions. A fully integrated surveillance system could include integrated disease surveillance and response, including COVID-19 case reporting; pathology-based cause of death surveillance; 8 electronic health and laboratory record data transfer; serological surveillance; vaccine adverse events reporting; epizootic and food safety surveillance systems on the One Health model; participatory community surveillance; and disease-specific systems for HIV, tuberculosis, malaria, vaccine-preventable diseases, and many others. For data linkage it is crucial that all systems are digital and that unique health identifiers are assigned to everybody in the population. Privacy protection, including review by privacy watchdogs, must be established. Surveillance data reviews should trigger rapid public health action locally. National public health institutes (NPHIs) should be charged with collating and analysing data nationally and coordinating or undertaking modelling of disease patterns and pathogen evolution to guide public health suppression measures, border policies, vaccine development and deployment, and treatment protocols. NPHIs should have the mandate and systems to share information about transnational health threats with international bodies under the International Health Regulations (2005), and these bodies must commit to full transparency of the data they receive. Additionally, NPHIs should monitor key performance indicators, such as time to detect, report, investigate, and control disease outbreaks. Adequate financing and the creation of a sustainable market will be needed for the establishment and continual maintenance of surveillance infrastructure. Countries should expect to spend about US$1-4 per capita annually on disease surveillance infrastructure and personnel. 9 For low-income and middle-income countries, substantially more start-up investment is likely to be required to strengthen laboratory capacities, data systems, and human resource capacity, as part of larger investments in health systems strengthening; some of this cost will need to be met by donors and high-income countries. Dedicated investments will also be needed to ensure that high-risk populations, especially in humanitarian contexts, are not excluded from improved surveillance systems. The amounts are considerable but represent a small proportion of the $249 per person average annual military spending and the more than $10 trillion in estimated economic costs from inadequate disease surveillance. 10 Piecemeal, antiquated public health surveillance must be robustly transformed into a modern system. As the COVID-19 pandemic has shown, weak surveillance limits the ability of countries to detect and rapidly respond to health threats and harness the benefits from innovations such as pathogen genomic sequencing, mRNA vaccines, and novel antivirals. Bold changes to implement fully interconnected disease surveillance are needed to manage the risks posed by SARS-CoV-2 variants and future pandemics. OWM  
paper_id= fff0dfd4a0507d95bb42d0af8d9a5f4fac0927bd	title= Uniform infection screening allowed safe head and neck surgery during the coronavirus disease 2019 pandemic in Japan	authors= Akiko  Ito;Kenya  Kobayashi;Mika  Shiotsuka;Tetsufumi  Sato;Go  Omura;Yoshifumi  Matsumoto;Atsuo  Ikeda;Azusa  Sakai;Kohtaro  Eguchi;Tomonari  Takano;Fumihiko  Matsumoto;Osamu  Kobayashi;Satoshi  Iwata;Seiichi  Yoshimoto;	abstract= Background: The purpose of this study was to determine whether a uniform infection screening protocol could be used to safely perform head and neck cancer surgery during the coronavirus disease 2019 pandemic and clarify how surgical treatment changed compared with the prepandemic period. Materials and methods: During the unprecedented coronavirus disease 2019 pandemic in Tokyo, we continued providing head and neck cancer care, guided by our own uniform screening protocol. In this study, medical records of 208 patients with head and neck malignancy, who underwent surgical treatment at our hospital during the first and second wave of pandemic for each 2-month period (first wave: ), were analysed. Results: A total of 133 patients were admitted for surgical treatment and all, except six patients with emergency tracheostomy, were screened according to the protocol. As a result, all 127 patients received surgical treatment as planned, and all 1247 medical staff members involved in the surgeries were uninfected by severe acute respiratory syndrome coronavirus 2. During the first wave of pandemic, 20% reduction of head and neck surgery was requited; however, restrictions of surgery were not necessary during the second wave. Surgical procedure, length of hospitalization, postoperative complications and number of medical staff were unchanged compared with prepandemic period. Conclusion: Our data indicate that continuation of head and neck anticancer surgical treatment in an epidemic area during the coronavirus disease 2019 pandemic were safe and feasible, if adequate and strict preventive measures are vigorously and successfully carried out. 	body_text= The coronavirus disease 2019 , the novel infection caused by the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), has spread quickly around the world. In March 2020, the World Health Organization officially declared a pandemic. In response, a drastic and unprecedented change has been required for head and neck clinical practice. Because the highly concentrated SARS-CoV-2 viral particles have been reported in the nasal cavity, nasopharynx and oropharynx, there is concern for intraoperative transmission by aerosolization of the viral particles during head and neck surgery (1) . Inhalation of the virus in the form of aerosols in the air originating from the nasopharyngeal mucosa is regarded as a high infection risk during surgical procedures, especially with use of a high-speed drill for resecting bone, which generates significant airborne aerosols (2) . Several head and neck surgeries require mandibular, maxillary and nasal bone osteotomies using a drill and are therefore considered SARS-CoV-2 high-risk exposure surgeries for medical staff. In addition, it has been suggested that patients with SARS-CoV-2 infection have higher rate of 30-day mortality and pulmonary complications after surgery (3) . Against this background, delayed elective head and neck surgery has been permitted to promote safety of medical staff and patients. However, the justification for postponing surgery in cancer patients is not clear as any delay in cancer treatment might affect prognosis. There were several data on surgical management and outcome or clinical guideline of patients with head and neck cancer during the COVID-19 pandemic (4) (5) (6) (7) (8) , although the available guideline is not specific and strict due to varying infection rates between the countries and regions. Moreover, the COVID-19 pandemic is likely to continue for some time. Hence, we are faced with difficult decisions on how to safely perform elective surgery in various pandemic situations. The purpose of this study was therefore to determine whether a uniform infection screening protocol could be used to safely perform head and neck cancer surgery during the COVID-19 pandemic and clarify how surgical treatment changed compared with the prepandemic period. The inclusion of the infection background in the analysis was an original study, unlike previous reports. The National Cancer Center Hospital of Japan is located in the centre of Tokyo. During the unprecedented COVID-19 pandemic in Tokyo, we continued providing head and neck cancer care. In this retrospective cross-sectional study, we analysed medical records of patients with head and neck malignancy who underwent surgical treatment at our hospital during the first and second wave of COVID-19 pandemic for each 2-month period (first wave: 30 March 2020-30 May 2020, second wave: 14 July 2020-14 September 2020) and the 2-month pre-pandemic period (30 October 2019-30 December 2020). In our department, 475 surgical cases (general anaesthesia: 418 cases, local anaesthesia: 57 cases) were performed between 1 January 2019 and 31 December 2019, with 79 surgical cases performed per 2 months. As the control for the COVID-19 pandemic period, we randomly sampled 2 months in 2019: 30 October 2019-30 December 2019. Patients characteristics, primary site, clinical stage, surgery, use of drilling system, operation time, blood loss, cumulative total number of medical staff, intensive care unit hospitalization, length of hospitalization and postoperative complications were evaluated. Head and neck surgery was classified as follows: maxillectomy/mandibulectomy, glossectomy/other oral surgery, partial pharyngectomy, transoral videolaryngoscopic surgery, total laryngectomy/total laryngopharyngectomy, skull base surgery, endoscopic sinus surgery, emergency tracheostomy and other neck surgery (thyroidectomy, parotidectomy and neck dissection). Postoperative complications were defined as adverse events needing any medical intervention or prolonging the length of hospital stay. Medical staff included all individuals who participated in a surgery for >10 min. Medical staff consisted of head and neck surgeons, plastic surgeons, general surgeons, anaesthesiologists and operating room nurses. The eighth edition of the TNM classification from the UICC was adopted for clinical staging (9) . Ethical approval for this retrospective study was obtained from our ethics committee (approved number 2018-179). General measures. After the start of the epidemic in Tokyo, the Tokyo governor requested the public to stay home unless necessary, on 26 March 2020. Because the infection got out of control, the Prime Minister declared a state of emergency between 7 April 2020 and 25 May 2020. Contrary to lockdowns in the West, our state of emergency is not legally binding, but relies on voluntary co-operation of the public. The resulting infection trends in Tokyo are shown in Fig. 1 , from official Tokyo data (10) . Specific hospital measures. Thermography was set up in front of the hospital entrance, and all incoming persons were triaged by this system. Detailed instructions about hand hygiene, cough and sneeze hygiene, as well as disinfecting agents were provided. The hospital also denied access to visitors except families receiving informed consent. In addition, wearing medical face masks became mandatory for all medical staff. Specific measures at department of head and neck surgery. Oto-Rhino-Laryngological Society of Japan provided useful screening protocol guidelines based on region and facilities (11). We also developed our own protocols while adhering to the guidelines. The COVID-19 preoperative screening protocol is shown in Fig. 2 . First, as 'basic screening', we asked about fever (≥37.5 • C), olfactory dysfunction, contact with COVID-19 patients or close contact and travel history on admission. As objective findings, we also checked chest computed tomography (CT) and SpO 2 (<95%) to check for symptomatic/asymptomatic pneumonia. A positive basic screening would lead to postponement of surgery at that point and examination for COVID-19 infection. If basic screening was negative, we proceeded to 'advanced screening'. Advanced screening was performed using a real-time reverse transcription polymerase chain reaction (RT-PCR) on respiratory samples from nasopharyngeal and trachea stomal swab. We asked our patients to admit 1 or 2 days before the surgeries, and the patients were required not to go out during the hospitalizations. During the pandemic, especially in the first wave, the medical system was disrupted and medical supplies became limited. In particular, the efficient use of N95 masks and RT-PCR tests were required. We overcome this problem by performing surgery at 80% of the conventional rate and by gradually expanding the indication for PCR test according to the COVID-19 exposure risk from head and neck surgery. The range of indications for PCR testing was expanded in three stages every 3 weeks, according to our capacity of in-hospital examination. The changes in the protocol over time and COVID-19 exposure risk classification of surgeries are shown in Figs 1 and 3, respectively. In Version (Ver) 1 protocol (1 April 2020-24 April 2020), high exposure risk surgery such as skull base surgery, sinonasal surgery, surgery using drilling system, surgery with massive haemorrhage and difficult intubation cases were eligible for PCR. In Ver.2 protocol (24 April 2020-17 May 2020), in addition to high exposure risk surgery, intermediate exposure risk surgery such as surgery with free flap or pedicle flap reconstruction and surgery (oral cavity/pharynx/larynx) lasting >2 h without reconstruction (e.g. total laryngectomy, partial pharyngectomy) were eligible for PCR. In final Ver.3 protocol (18 May 2020-), in addition to the previous two risk groups, low exposure risk surgery (oral cavity/pharynx/larynx) lasting <2 h without reconstruction (e.g. glossectomy, transoral video surgery) were eligible for PCR. Finally, it was possible to perform PCR testing for all but neck surgery (neck dissection only, thyroidectomy and parotidectomy) that did not involve the oral or nasal cavity, with negligible exposure risk. Finally, if preoperative screening was negative, surgery was performed as scheduled. Medical staff wore normal personal protect equipment (PPE), including surgical mask and level-3 gown. In high-risk surgery, we set the transparent screen around the surgeon to protect the anaesthesiologist. For emergency tracheostomy, the medical staff operated with full PPE (N95 respirator, goggles, level-3 gown) because there was no time to perform PCR testing. During the second wave of the pandemic, there were few shortages of medical supplies because of the experience of the first wave; PCR testing was adequate because of the expanded testing capacity; however, the N95 respirator was still inadequate. Hence, the restrictions on surgery were removed while adhering to the screening protocol. We overcome this problem by performing surgery at 80% of the conventional rate and by gradually expanding the indication for PCR test according to the COVID-19 exposure risk from head and neck surgery. Postoperative management was carried out as before in accordance with standard precautions.  Difference between any two groups were analysed using Fisher's exact test or Wilcoxon signed-rank test. Statistical analysis was performed using SPSS Statistics software (version 22.0.0, IBM, Armonk, NY, USA). The level of significance was set at P < 0.05. According to our public database, the population of Tokyo in 2020 was estimated at 13.98 million. A total of 448 persons were infected with COVID-19 at the beginning of this study (30 March 2020), representing 0.32 infections per 10 000. At the end of the first wave (5 June 2020), the total number of infected persons had risen to 5338, or 3.8 infections per 10 000. At the end of this study during the second wave (14 September 2020), the total number of infected persons had risen to 23 274, or 16.6 infections per 10 000. A total of 133 patients (first wave: 62 patients, second wave: 71 patients) were admitted for surgical treatment and all, except six patients (first wave: four patients, second wave: two patients) for emergency tracheostomy, were screened according to the protocol (Table 1)  The number of surgeries during the first and second wave of pandemic each period and control period was 62, 71 and 75, respectively. The number of surgeries decreased by 20% in the first wave period, while it decreased by only 5% in the second wave period. Table 2 shows the changes between each pandemic and control periods. There was no significant difference in age, sex, primary site, stage, surgical procedure and operation time. We aimed to improve the efficiency of the number of medical staff involved in the surgeries, but, in fact, there was no significant difference between pandemic and control period. Postoperatively, there was no difference in length of hospital stay or complication rate between groups. Therefore, apart from the ∼20% reduction in surgery during the first wave pandemic period, other parameters did not differ significantly from the control period. In this study, during the COVID-19 pandemic in Tokyo, we provided safe and efficient head and neck cancer care using our own screening protocol. A total of 564 medical staffs were involved in the surgeries, and no medical staff was infected with SARS-CoV-2. With an infection background of 16.6 infections per 10 000, this protocol was considered useful. Our institution is located in Tokyo, Japan. Tokyo has a population of 13.95 million, >10% of the total population of Japan. As of the first wave (5 June 2020) and the second wave (14 September 2020), 81 596 and 447 471 individuals were tested for COVID-19 in Tokyo, with registration of 5338 and 23 274 laboratory-confirmed COVID-19-positive cases, representing a 6.5 and 5.2% positivity rate, respectively (10) . According to the report by Fong et al. (12) in northern Italy, they tested 219 cancer outpatients by RT-PCR screening and the positivity rate was 1.8%. A cumulative total of 3075 persons were tested by RT-PCR in their healthcare district over the same period and 200 (6.5%) were positive. We tested 81 asymptomatic cancer patients based on our protocol of preoperative screening and none of them were positive by PCR. Asymptomatic cancer patients might have significantly lower positivity rate compared with their healthcare district's rate because generally PCR testing was performed on symptomatic patients or their close contacts. Another reason may be that patients scheduled for cancer surgery are educated on proper infection prevention and tend to avoid behaviours that put them at risk for infection. On the other hand, in a retrospective study on 1524 patients with cancer, Yu et al. (13) estimated an infectious rate of 0.79% (12/1524) compared with 0.37% within the general population. This study suggested that cancer patients have high risk of SARS-CoV-2 infection because of immunocompromised status or highly invasive surgery. Therefore, screening protocols are needed to protect patients and medical staff. According to a report from Hong Kong, with their initial measures in surgery management, which reduced elective surgeries by 80% and infection control, no medical staff developed nosocomial infection (14) . Morrison et al. (15) reported that they stratified surgical cases and saw a 55% reduction in surgical volume during 18 March-18 April 2020 in AL, USA. Brar et al. (16) in London, UK, reported their 47 elective head and neck surgeries from 23 March to 20 May 2020 and concluded that careful preoperative screening and postoperative care in a COVID-19 clear ward allowed head and neck surgeries to proceed safely during the epidemic. We used unified COIVD-19 screening protocol and control measures and performed 133 head and neck surgeries during extracted 4 months of the COVID-19 pandemic. Our hospital specializes in cancer treatment and performs highly invasive surgeries including skull base surgery, radical surgery with massive haemorrhage and reconstructive surgery requiring a large number of medical staff and long operation time. Such surgical procedures are considered SARS-CoV-2 high risk. Because the screening protocol was effective, we performed all surgeries safely without any infection to the medical staff. The decrease in surgeries was restrained to 20% in first wave and only 5% in second wave, and surgical procedures, postoperative course and other parameters were similar to the pre-pandemic period in 2019. At the beginning of the outbreak, it was found that while most COVID-19 patients rarely infected others, there were clusters suspected of contracting COVID-19 from specific patients (17) . In Japan, infection preventive measures by tracking such clusters were adopted at the early stage of the epidemic. And it was concluded that the risk of occurrence of clusters is particularly high when the 'Three Cs' overlap: close spacing with insufficient ventilation, crowded conditions with other people and conversation over short distances (18, 19) . As operation rooms meet these conditions to some extent, we should minimize the number of medical staff entering the operation room. However, in fact, there was no significant difference in number of medical staff involved in surgeries between pandemic and control period in this study. The most common reported symptoms of COVID-19 include fever and cough, leading to dyspnea and fatigue (20) . Sore throat, sputum production, olfactory and taste disorder are also identified as symptoms (20, 21) . Therefore, we checked these symptoms at the point of hospitalization. The difficulty in managing COVID-19 is due to the high number of asymptomatic persons infected with the disease. Some studies showed that >50% of patients were asymptomatic at the time of testing (22) . In order to expose asymptomatic patients, CT scans and SpO 2 screening were used as routine objective tests. It is reported that chest CT abnormality is found in 86.2% of COVID-19 patients, whereas chest X-ray abnormality is found in 59.1% (20) . Moreover, asymptomatic patients could have CT changes before symptom onset (23) . In addition, we used nasopharyngeal swabs and tracheal stomal swabs for RT-PCR, as 'advanced screening'. The detection rate of nasal swabs was reported to be 63% (24) , implying around 40% false negatives. We expected chest CTs to augment RT-PCR results. However, no matter how accurate the screening protocol, it is difficult to completely eliminate false negatives. Infection screening is not perfect, so medical staff continue to wear normal PPE. In high-risk surgeries, we set the transparent screen around the surgeon to protect the anaesthesiologist. Furthermore, any delays to performing emergency surgeries can be life threatening, and therefore a full screening on patients prior to the surgery may not be practical. In order to mitigate the risk of infection from those patients, (1) all patients are subject to screening on admission and (2) medical staff involved in emergency surgeries is equipped with full PPE. In this study, all six patients of emergency tracheostomy had tested negative during screening on admission and therefore they did not proceed to receive chest CT or RT-PCR. Screening on admission involves looking for symptoms such as fever (37.5 • C) or olfactory dysfunction, any history of contact with COVID-19 patients, as well as travel history. If a patient tests positive during screening on admission, our policy is to perform the necessary emergency surgery with full PPE, followed by private room management and PCR testing until it is confirmed that the patient tests negative for COVID-19. Screening on admission helps us mitigate the risk of posthospitalization horizontal transmission of COVID-19. Our study suggested that proper infection screening and protection through the first and second wave of the COVID-19 pandemic may allow head and neck surgery to be performed safely without restriction. Because SARS-CoV-2 is silent and persistent virus, longterm measures considering the infection situation in surrounding healthcare district are required. Further research is needed in the future. The major limitation of this study was selection bias due to the retrospective setting with a limited number of cases. Also, the lack of PCR-positive patients is a weakness in terms of demonstrating the usefulness of infection screening. However, during a pandemic, it is difficult to conduct clinical studies, and the fact that a large number of head and neck cancer surgeries continued to be performed with a unified infection screening is very valuable information. In conclusion, we performed uniform COVID-19 screening of 133 preoperative head and neck cancer patients and implemented safe perioperative management for both patients and medical staff. Although medical supplies were scarce during the pandemic, surgeries could be performed safely by expanding the screening indications in stages. Our screening protocol was considered valid for the infection background of 16.6 infections per 10 000 persons. During the first wave of pandemic, 20% reduction of head and neck surgery was requited; however, restrictions of surgery were not necessary during the second wave. The nature of surgery, length of hospital stay, postoperative complications and number of medical staff were unchanged compared with the pre-pandemic period. 
paper_id= fff0eda8a5ad88b27dbafaa7a814eef9472fbe4c	title= COMMUNICATION Please do not adjust margins Please do not adjust margins Regeneration of lung epithelial cells by Fullerene C 60 nanoformulation: A possible treatment strategy for acute respiratory distress syndrome (ARDS)	authors= Nabodita  Sinha;Ashwani Kumar Thakur;	abstract= Fullerene nanoformulation can be used for the regeneration of cells treated with apoptosis-inducing molecules, suggeting its potential for ARDS therapy. 	body_text= Acute respiratory distress syndrome (ARDS) is one of the pathological effects of the SARS-CoV-2 infection which severely damages the lungs. This syndrome also occurs in lung injury, trauma, viral and bacterial infections. However, SARS-CoV-2 induced ARDS is reported to be severe compared to pneumonia or sepsis-induced ARDS. 1 Lung capillary endothelial cells, type I pneumocytes, and alveolar epithelia get damaged in ARDS, leading to dyspnea and hypoxemia. 2 Neuromuscular blocking agents and steroids are currently used to reduce inflammation and to increase patientventilator synchrony for respiration. [3] [4] [5] Inhaled nitric oxide is another strategy to reduce pulmonary vascular resistance and to facilitate better response to the ventilator. However, these approaches show transient improvement in the patients and exhibit no significant reduction in mortality. 6, 7 While approaches, such as mesenchymal stem cells (MSCs) and liposomal drug delivery systems have shown some promise in endotoxin (LPS) induced lung injury, 8-10 novel therapeutic approaches for lung repair and improvement of mortality rate in ARDS is an unmet need in the current times of Covid-19 pandemic. Our lab has recently developed a Fullerene nanoformulation by dispersing 200-400 µg/ml Fullerene powder in cell culture media which results in ~170-230 nm (FP170-230) sized nanoparticles. This nanoformulation accelerates the proliferation of lung epithelial cells (A549), as well as BMSC, HepG2, NIH-3T3, MDCK and SH-SY5Y cell lines. The nanoformulation was also used in repairing cellular scratch and mice skin wound. 11 In ARDS, the lung epithelial cells generally undergo apoptosis which ultimately results in the loss of functional cells and consequently impairment of lung function. Regeneration of cell can therefore be one of the therapeutic approaches for ARDS. The basic underlying mechanism of cellular death in ARDS can be successfully modeled in-vitro. Cellular death can be induced in-vitro by hydrogen peroxide which is a known apoptosis-inducing molecule at certain concentrations. 12 Toxicity-inducing nanoparticles can be another approach for ensuing cell death. Earlier we have prepared a cytotoxic Fullerene nanoformulation in the size range of 20-50 nm (FT20-50) by dispersing Fullerene at a concentration range of 1-10 µg/ml in cell culture media. 11 Hence we have used 30 nm sized cytotoxic Fullerene nanoformulation (FT30) and hydrogen peroxide to model cellular death in ARDS condition and evaluated the proliferation effect of the Fullerene proliferative (FP170-200) nanoformulation for regeneration of lung epithelial cells. Effect of the proliferative nanoformulation (FP170 and FP200) on cell death induced by toxic nanoparticles (FT30) Earlier we had prepared two types of Fullerene nanoformulations and tested them on A549, BMSC, HepG2, NIH-3T3, MDCK and SH-SY5Y cell lines. The toxic nanoformulation of size ˜20-50 nm (FT20-50) showed significant cell death while the proliferative nanoformulation of size ˜170-230 nm (FP170-230) showed 3-4-fold cell proliferation within 24 hours. 11 (ESI S1 and S2) Here the regenerative effects of the proliferative nanoformulation (FP170 and FP200) on the cells already treated with the toxic nanoformulation was checked. This would test the capability of the proliferative Fullerene nanoformulation to increase cellular viability after insult with a toxic treatment. (Figure 1 a) First, the lung epithelial cells (A549) were treated with toxic Fullerene nanoparticles of size 30 nm (FT30) since in our earlier work these particles have shown significant toxicity on the cell Viability was normalized to control cells treated with only fresh media for 48 hours (not shown in graph). The entire experiment was repeated thrice (n=3) for statistical analysis. The types of second treatments and the results obtained are as follows: (Figure 1 a)  In the first case (Media bar of Figure 1 ), the second treatment was only media which was applied after the first treatment of FT30. As shown in figure, the viability of the cells is about ˜40% as compared to 100% viability of the untreated cells. This signifies that after first toxic treatment, treatment with media does not have any significant reparative effect. In the second case (FT30) nanoformulation were used as the second treatment. Here the viability was reduced to ˜15%. Since in the first treatment with FT30, viability due to toxic effect reduces to ˜35%, 11 the value of 15% viability suggests continuous toxic effect of FT30 nanoformulation during second treatment. In the third case, FP170 nanoformulation were used as the second treatment. In our earlier work, it was shown that 170 nm particles (FP170) can enhance cell proliferation but to a lesser extent than 200 nm nanoformulation (FP200) . 11 Here treatment with 170 nm particles (FP170) as a second treatment, improved cell viability but not significantly with respect to the case where fresh media was given as the second treatment. (t-test, p>0.05) In the fourth case, FP200 nanoformulation was given as the second treatment. This treatment shows significant increase in cell viability up to ~2.3 folds (t-test; p<0.05) even after cell death has been ensued by initial toxic treatment. Thus here we have shown that even after cellular death is initiated by toxic Fullerene nanoparticles (FT30), the cell viability can increase by treatment with proliferative Fullerene nanoformulation suggesting regeneration potential of the proliferative nanoformulation (FP200). Next we induced cellular death by a known apoptosis and inflammation-inducing agent hydrogen peroxide and assessed the capability of the proliferative nanoparticles to improve the viability of these damaged cells. Hydrogen peroxide at a concentration of 0.5 mM predominantly leads to cell apoptosis. 13 It is used to induce cell damage and for evaluating the reparative effect of a potential drug candidate. Here we analysed the effect of hydrogen peroxide on the cells with or without the presence of proliferative Fullerene nanoformulation. FP170 nanoformulation were not used since it did not show significant proliferative effect in the previous experiment. Figure 1 a. The regenerative potential of the proliferative Fullerene nanoformulation on A549 cells after inducing cell death by toxic nanoformulation. First, the cells were treated with 30 nm toxic nanoformulation (FT30) for 24 hours. Control cells were untreated for 48 hours and their viability was set at 100% (not shown in the graph). When only media was applied as second treatment, the cell viability became ˜40% due to initial toxicity treatment. When second treatment was FT30, cell viability further reduced to ˜15%. FP170 as second treatment increased cell viability but not significantly compared to control. (t-test; p>0.05) Treatment with 200 nm nanoformulation (FP200) showed a significant increase in cell viability (t-test; p<0.05) even after initial toxic treatment (n=3). b. Treatment with FP200 nanoformulation alone increased the cell viability up to 300% compared to control. Induction of apoptosis by 0.5 mM hydrogen peroxide decreased the cell viability to 35%. Treatment with both hydrogen peroxide and 200 nm Fullerene nanoformulation (FP200) simultaneously increased cell viability up to 125% depicting the potential of FP200 to increase cell viability in presence of apoptosis inducing chemical (t-test; p<0.01) (n=3). The control cells were treated with only media for 24 hours. In the second set, cells were treated with the proliferative Fullerene nanoformulation (FP200). When the viability was checked after 24 hours, cells showed an expected 300% increase in cell proliferation as compared to control. In the third set, 0.5 mM hydrogen peroxide was added to the cells. As a result, the viability decreased to ˜40%. But in the fourth set, when both hydrogen peroxide and proliferative nanoformulation were added simultaneously, the viability improves to ˜125%. This results in a ~3-fold increase in proliferation compared to treatment with only hydrogen peroxide (t-test; p<0.01) (Figure 1 b) . These results suggest that the proliferative Fullerene nanoformulation can increase the cell viability even after cellular damage by hydrogen peroxide. Similar results were observed in fluorescence microscopy (Figure 2 a-d) For initial drug development, primary lung epithelial cells, or lung epithelial cell lines can be used to simulate ARDS condition. 14 Since one of the leading outcomes of ARDS is apoptosis of alveolar epithelial cells, 15 the end-point of our study was to obtain high cell viability in two experimental Please do not adjust margins Please do not adjust margins models of cellular damage. Here we have shown that the proliferative Fullerene nanoformulation has the potential to increase cell viability after induced cell damage by toxic nanoparticles (FT30) or hydrogen peroxide. The improved viability might play a role in mitigating the alveolar epithelial death pathways and might help in restoration of cellular functioning. In future, rodent models of ARDS would be developed and proliferative nanoformulation would be applied to check lungs repair and mortality of the animals. A thorough toxicological analysis of acute or chronic toxicity of the Fullerene nanoformulation for pulmonary delivery would also be investigated. Encouraging results would further lead to design of an aeroformulation suitable for pulmonary delivery through nebulizer. 15 The Covid-19 pandemic and resulting ARDS has led to a significant rate of mortality among patients and novel therapeutic approaches in this regard is the need of the hour. The proof-of-concept developed here if successful might prove to be a novel therapeutic approach in reducing ARDS-induced mortality. It would also establish the regenerative potential of Fullerene nanoformulation and would open multiple avenues for exploration of its potential. 
paper_id= fff0f7d4f6d319f14ff23aaa04b8d83274b589fe	title= Commentary Physical Activity for Oncological Patients in COVID-19 Era: No Time to Relax	authors= Daniela  Tregnago;Joachim  Wiskemann;Massimo  Lanza;Michele  Milella;	abstract= Whereas the coronavirus disease storm is relentlessly progressing worldwide, a great effort from scientific societies has been made in order to give recommendations for safely continuing oncological care, prioritizing the interventions according to patient's condition, type and stage of tumor. Nevertheless, to date no specific suggestions regarding physical activity and exercise in cancer patients during the COVID-19 era have been released, neglecting the potential deleterious effects of quarantine and sedentary (imposed as containment measures against COVID-19), particularly in these subjects. Moreover, literature is constantly consolidating the crucial impact of regular physical activity in cancer in reducing recurrence and mortality risk. In this Commentary, we discuss possible adaptations of the recently published exercise guidelines to the current pandemic emergency, proposing various modalities to prevent or mitigate the physical inactivity risk in cancer patients. 	body_text= Since the new coronavirus , also known as SARS-CoV-2 (severe acute respiratory syndrome coronavirus 2) was announced in Wuhan in late December 2019, it has rapidly spread worldwide, prompting the World Health Organization (WHO) to declare the pandemic on March 11, 2020 [1] . Due to the high contagiousness and aggressiveness of this disease, on July 28, more than 16,341,920 cases and 650,805 related deaths have been reported around the world [1]. COVID-19 patients can be completely asymptomatic (around 18% of cases) [2] or manifest several symptoms, ranging from mild to severe, mainly including respiratory manifestations (e.g., rhinorrhea, sneezing, sore throat, cough, ground-glass opacities, pneumonia, hypoxemia, dyspnea, acute respiratory syndrome), but also systemic disorders (e.g., fever, fatigue, headache, coagulation disorders, lymphopenia, and other blood alterations, gastrointestinal symptoms as diarrhea and nausea) [3] . Preliminary data suggest that elderly subjects [4] or affected by chronic comorbidities [5] may be at higher risk of COVID-19 incidence occurring with a worse outcome [4, 5] . In particular, cancer patients seem to represent a high-risk category to experience COVID-19 disease with more severe manifestations, mainly due to compromised immune defenses and sequelae of antineoplastic treatments [5] . Thus, given the current pandemic emergency, a great effort from scientific societies was performed in order to provide recommendations for safely continuing oncological care, prioritizing the interventions according to patient's condition, type, and stage of tumor [6] . Nevertheless, the emergency may unequivocally lead to postponing some anticancer treatments [5, 6] , further increasing patients' anxiety and distress levels and therefore lowering compliance to therapy. Considering that up to now, no vaccine or specific treatments against COVID-19 are available, the only way to keep the spread of the infection under control is the social distancing, i.e., keeping people at home as much as possible, for as long as possible, until COVID-19 outbreak will be under control. Indeed, several countries around the world have adopted various containment measures [7] . In Italy, for example, the national quarantine, i.e., the prohibition for all people to move except for work, necessity, or medical needs, began on March 9, 2020, and it lasted until May 18, 2020, when a gradual reopening of commercial, productive and social activities was allowed [8] . Although these measures are strictly necessary, social distancing and quarantine may also have negative effects. A recent review has explored the impact of quarantine on psychological status, describing a high risk of post-traumatic stress symptoms, confusion, and anger [9] . Moreover, this homestay period may lead to reduce physical activity (PA) and, thus, increase sedentary behaviors. In general population, PA and sedentary time are respectively associated with positive and negative effects on body systems, mainly on muscle mass and cardiorespiratory fitness. Recent evidence highlighted the correlation between sedentary and risk of several chronic conditions as metabolic syndrome, osteoporosis, cardiovascular and respiratory disease, stroke, cognitive function, and type 2 diabetes [10, 11] . Despite WHO underlined the importance to be as active as possible during this quarantine period, it is reasonable to speculate that one of the groups decreasing its PA level is the oncological population. In this regard, patients usually reported a reduction in PA level after cancer diagnosis [12] , with only approximately one-third of patients satisfying PA recommendations [13] . In our experience, this proportion seems to be even smaller [14] . Due to the current restrictions, this number could further diminish, amplifying the deleterious effects of sedentary behavior and physical inactivity. PA is emerging as a key element in the oncological trajectory. A growing body of literature demonstrated the association between PA levels after a cancer diagnosis and survival [15] . Collectively, these data reported a consistent, inverse correlation with all-cause mortality (ranging from 21% to 45%) and cancer-specific mortality (ranging from 26% to 69%) risk [15] . Furthermore, some physical fitness components harbor a relevant impact in terms of both prognosis and recurrence risk. Cardiorespiratory fitness and muscular strength are prognostic factors in cancer patients [16] . In addition, muscle mass wasting has been connected with a worse treatment tolerance, higher risk of recurrence, overall, and cancer-specific mortality [17, 18] . PA and exercise interventions are shown to be safe and feasible in oncological patients [19] . A recent meta-analysis, including 48 randomized controlled trials with a total of 3,632 patients, found that exercise increases the peak of oxygen consumption by +2.80 mLO2*kg -1 *min -1 in the interventional group compared with no changes in the control one [20] . Padilha and colleagues have investigated the role of resistance training or a combined exercise program (aerobic + resistance) on muscle mass, strength, and body fat. The results have demonstrated that exercise was effective in improving muscular strength, regardless of the treatment type, concomitantly increasing lean body mass and decreasing body fat [21] . Over the years, the role of PA and exercise as a strategy to improve health-related quality of life in cancer patients has been established [13, 19] . This could be partially associated with the efficacy of exercise in alleviating or preventing cancer-and treatmentrelated adverse events, such as cancer-related fatigue, lymphedema, anxiety and depression levels, bone health, sleep quality, as well as cardiotoxicity risk, cognitive function, sexual function, chemotherapy-induced peripheral neuropathy, and nausea [19] . Finally, limited data also exist regarding treatment tolerance, i.e., the adherence to a planned therapy. In fact, exercise may improve the chemotherapy completion rate in patients physically active during adjuvant treatments comparing to the control group [19] . The American College of Sports Medicine (ACSM) has released the new exercise guidelines for cancer survivors [19] . ACSM suggests that an effective exercise prescription should include moderate-intensity aerobic training at least three times per week for 8-12 weeks. Moreover, the exercise program should add resistance training activities, at least two times per week, using two sets of 8-15 repetitions at least 60% of one maximum repetition [19] . According to the current pandemic emergency, these guidelines should be adapted to a home-based setting since supervised sessions are not possible. A reliable solution can be represented by home-based exercise programs. The home-based exercise programs can exploit the telehealth (or telemedicine), i.e., the remote delivery of health care as well as a range of other services, including patient education and wellness promotion through technology [22] . Telehealth programs do not have a formal structure to deliver information and can utilize different technologies, therefore allowing the exercise prescription and monitoring in several ways [22] . For example, in cancer survivors telephone counseling, short message services, digital media (e.g., DVD), tailored and/or mailed materials, and/or computer/web-assisted programs, were applied [23] . Moreover, the home-based exercise programs are feasible, usually well accepted, and can be facilitated through the social support deriving from the patient's family and the possibility to selforganize the free time, choosing when to perform the activities [25] . If well structured, including, for example, an initial phase to educate patients (e.g., to self-monitor the intensity), home-based programs have been demonstrated to be efficacious in improving lifestyle in cancer population. In this regard, Demark-Wahnefried and colleagues have proposed a randomized trial, including 519 newly diagnosed breast and prostate cancer survivors, with the aim to improve diet and exercise practice using a tailored mail print intervention [26] . The intervention included personalized workbooks followed by a series of newsletters (at 6-week intervals) that were tailored to barriers, stage of readiness, and progress towards goal attainment of exercising and nutritional aspects [26] . Patients also received a survey on the current health practices and the willingness of starting and maintaining a lifestyle change, which was used to adapt the periodic newsletters [26] . The study increased the weekly time spent in exercise, improved the overall diet quality, the daily intake of fruits and vegetables, decreasing fat intake and weight [26] . In the recent years, thanks to the advent of technology in the PA context, a growing number of studies have tested different internet approaches for PA and exercise programs, as web-based system [27] , mobile application [28] or social media (e.g., Facebook) interventions [29] , finding positive and meaningful results. Along these lines, a recent randomized trial tested a web-based exercise program in 68 breast cancer patients undergoing chemotherapy, to determine the effectiveness in preventing the impairment of functional capacity, muscular strength and anthropometric parameters, usually experimented during chemotherapy periods. The intervention consisted in an 8-week, web-based exercise program, with three sessions per week, which were organized in warm-up, aerobic and strength activities. The web system also permitted the communication between patients and research staff and weekly contacts with the aim to assure the correct performance and to tailor the program according to patients' needs [27] . The results demonstrated the intervention effectiveness in terms of both cardiorespiratory fitness and muscular strength, ameliorating the detrimental effects of treatments [27] . Nevertheless, the application of telehealth should be also considered outside the pandemic emergency. Due to the constant improvement in prevention, diagnosis and treatments, the number of cancer patients and survivors is continuously increasing, and the financial resources available for supporting exercise program could be limited. The home-based exercise program can offer a lowcost and sustainable alternative, especially when the costs are borne by the patients [24] . In this Apart from the limited cost, telehealth offers the opportunity of easily spreading the access to exerciseprograms to cancer patients. For example, patients living in rural or remote communities are at high risk of being under-served in terms of healthcare and health-related services. Indeed, a recent study has reported that rural cancer survivors are 2.6 times less likely to meet aerobic PA guidelines than urban cancer survivors [31] . This population should face the burden and discomfort of travel time to reach the services, thus decreasing the willingness to start a supervised exerciseprogram. This statement is also confirmed by interesting data evaluating the exercisepreferences in rural breast cancer survivors. Seventy-six percent of patients were interested in participating in an exerciseprogram, the majority preferred to perform exerciseat home (63%), and almost half (47%) of the participants favored an unsupervised program, endorsing the hypothesis that a remote exerciseprogram could be well accepted by a rural cancer population [32] . Telehealth can allow overcoming these barriers and indirectly diminish the disparity in survival and disease-related outcomes existing between non-metropolitan and metropolitan patients [33] . According to the aforementioned evidence and with the current containment measures, several modalities are available to support an effective home-based exerciseintervention (Figure 1) . The COVID-19 outbreak makes necessary to remotely perform all the steps of exerciseprescription, which are usually carried out face-to-face. The health-related physical fitness can be hard to test in this framework. Nevertheless, an initial evaluation may be proposed at distance through a videoconference, especially for those patients starting an exerciseprogram (Figure 1) . Ideally, this phase should include different assessments. From one side, patients' health history (including cancer characteristics and comorbidities), current treatments, presence and severity of side effects, and screening tools to assess the exerciserisk are essential to prescribe a safe program [19, 34] . On the other hand, understanding exercisepreferences, barriers, facilitators, availability of resources to support exerciseengagement, and patients' exercisehistory can be useful to build a tailored and feasible program [19, 34] . Several and validate tools are available for the exercise physiologist or kinesiologist to achieve this phase. For example, the Physical Activity Readiness Questionnaire can help defining an initial risk profile of the subject [35] , whereas the European Organization for research and Treatment of Cancer QLQ-C30 can measure the quality of life and the severity of some symptoms and treatment-related side effects [36] . Paradoxically, the social distancing period may be a good time to start an exerciseprogram, because some barriers that usually interfere with an active lifestyle adoption, e.g., distance from gym, lack of time, traffic and fixed time for lessons are missing. Setting goals and track progresses (Figure 1 ) using different instruments (e.g., wearable technology and/or a personalized diary) can be an ideal strategy to stimulate patients to maintain adherence to the prescribed exerciseprogram [34] . Goals should be established with the subjects, according to the following characteristics: specificity, measurability, achievability, realistic goals, and time-availability. Cancer patients have unique needs related to their disease, therefore choosing the most appropriate goals should take all of them into consideration (e.g. symptom control, improving mood, bodyweight, increasing exerciselevel), selecting those that are remotely assessable and most important for the subject. Moreover, the kinesiologist or physiotherapist should help the patients to identify those exerciserelated goals that are realistic and achievable. This aspect is crucial because failure in achieving the proposed goals can be extremely demotivating, particularly for oncological patients, with the possible consequence of exerciseprogram drop out. Finally, goals should be time-based, remembering that the exerciseprescription objectives may be influenced by the change in disease and treatment-related toxicities over time. Another component that should be included into a home-based program, especially during the COVID-19 pandemic period, is the periodic follow-up (Figure 1 ) [34] . This is important to maintain high engagement [37] and can be delivered by several modalities, e.g. telephone, videochat platforms (i.e. Skype), or email. The aims of the follow-up can be various: educate subjects to manage the exercisetraining, supervise the program, support patients to maintain an active lifestyle stimulating their motivation, reassess the situation, and modify the prescription. Follow-up time depends on several factors, e.g., the modality to deliver the program and the patients' needs. These revaluations could be preformed within a short-interval at the beginning of the program to maximize patient's support, and longer later to favour the subject' autonomy. The exercise-program components should reflect guidelines, including type, frequency, duration, and intensity of the activities [19] . Aerobic and strength exercises should be a key component of the exerciseprescription, and their balance should be determined according to the patient's goals and needs. Whereas strength activities require small spaces and limited equipment (e.g., elastic bands, bodyweight exercises), the aerobic exercises could be hard to be included into a home-based program. If it is not possible to get outside or if the patient does not hold a specific machine (e.g., treadmill or cycle ergometer), a valid alternative could be represented by adapted exercises as dancing, or walk up and down the stairs. Moreover, the program should also include flexibility and proprioceptive training, especially for patients with specific symptoms or treatmentrelated side effects. Proprioceptive exercises could improve the chemotherapy-induced neuropathy, ameliorating the balance control, whereas regaining the joint range of motion through flexibility activities, could be beneficial for patients undergone surgery and presenting a limited range of joint extension. In the home-based program, patients must be educated to self-monitoring exercise-intensity because, even if low-intensity may be appropriated for deconditioned patients, in other cases it may be insufficient to modulate the body homeostasis and increase the functionality, while high intensity may be unsafe. Thus, it is important to educate the patients to understand the intensity level using some practical tools, such as the heart rate or the perceived exertion scale. Frequency, i.e., the number os session per week, and duration, the time or the sets/repetitions per session/activity, are also essential to be defined. Although the ACSM guidelines suggest a frequency of 2-3 times per week of 90 minutes of aerobic activities and two sets of 8-15 repetitions for strength training, it may be necessary to adapt these parameters to the peculiar patient's clinical situation and disease. During the quarantine, patients have more free time to spend in exercising, but they may be sedentary or deconditioned, thus increase the frequency and diminish the duration may be a strategy to adopt. Nevertheless, it is necessary to remember that the "dosage" ofexercise, in terms of type, frequency, duration and intensity, recommended by ACSM, may be not appropriate at the beginning for cancer patients and should be progressively reached, balancing the exercise-prescription components with the patient's capacity. Taking all these factors into consideration may allow to develop effective tailored exercise programs during COVID-19, which can be potentially carried on beyond the quarantine period, in order to reduce the negative effect of sedentary, increase benefits related to PA andexercise, and ameliorate the psychological impairment due to the isolation and the outbreak emergency. The COVID-19 outbreak is a major challenge for global public health. Until a vaccine or specific therapies against COVID-19 are available, physical distancing and homestay remain the most effective approaches to slow down the spread of the infection. Nevertheless, these restrictive measures may decrease PA levels in cancer patients, with consequent deleterious long term outcomes. In this light, promoting a remote home-based lifestyle intervention in cancer population is an urgency, because if social distancing is necessary to stay healthy "today", the physical inactivity that may be experienced, will have negative and lethal effects "tomorrow", especially in cancer patients. This research did not receive any specific grant from funding agencies in the public, commercial, or not-for-profit sectors. 
paper_id= fff1194fa9a0e18d13a565a7a2355a584cb860b1	title= 	authors= W-M  Chan;Dtl  Liu;Dsc  Lam;	abstract= 	body_text= assessment but for clinical admission, patients are only screened by a resident with clinical history taking only. I do not see why there should be a difference and why emergency cases cannot be screened by residents before admission to the appropriate ward. For a suspected case in the out-patient clinic, the authors advocated attending the patient in the last time slot. It is very unusual to leave the suspected case in the crowded clinic for an unnecessary long period of time to increase the risk of cross-infection. The usual practice is to see the patient in a special room as early as possible and to discharge the patient to the appropriate destination. The authors placed too much emphasis on the personal protective equipment (PPE). Although PPE plays an important role in the prevention of infection, the authors only mentioned lightly and failed to highlight the importance of the proper technique in donning and sequential removal of the PPE in areas designed for such purposes. There is a high risk of being infected during removal of the PPE especially when they have been contaminated with the SARS coronavirus. It is therefore extremely important for the hospital to provide, besides adequate stock of PPE, proper and adequate areas for putting on and removal of PPE, training courses and regular refresher courses for the technique, as well as audit on the practice of the proper technique. It is interesting to know what PPE was used by the authors in their wound revision operation on the suspected SARS case. The use of enhanced PPE including positive air-powered respirator is advised when operating on a suspected SARS case. 2 Although not impossible, it is certainly painstaking to use the operating microscope and the indirect ophthalmoscope after wearing the respirator and its helmet. We would like to thank Dr Lai for the invaluable comments. Some of the issues raised are interesting and we will take this opportunity to make further clarifications. In our article, we said 'yBeing the Ophthalmology Department of the only hospital in the world that has just gone through the largest outbreak of SARS, we would like to share our strategy, measures and experience of preventing SARS infection'. This statement was based upon scientific evidence available at the early stage of SARS outbreak in 2003. Subsequent careful epidemiological and infection control study had shown that the principle index case responsible for SARS outbreak in Hong Kong community came from Prince of Wales Hospital. Moreover, Prince of Wales Hospital, Hong Kong was vetted to be the only hospital with the largest number of SARS cases under intensive care during early outbreak. 1,2 SARS is a highly infectious disease. It is not surprising that once there is a community outbreak, all the regional hospitals of the territory will have to tackle the multiplying suspect/probable cases. United Christian Hospital did admit a number of suspect and probable SARS cases, but significant number of them represented mutant strains during the second wave of 'super-spreading' infection. 2 Concerning the issue about direct ophthalmoscope, we were referring to the special and temporary infection control measure during the SARS outbreak in substituting direct ophthalmoscope by other safer examination techniques such as binocular indirect ophthalmoscope or fundus photography ('In real life, the ophthalmic practices in the midst of the SARS outbreak have been changed. The ophthalmologists in Hong Kong have abandoned the direct ophthalmoscopic examination in view of its short working distance. In ultrahigh risk patients proven to have SARS, safer and easily accessible investigative toolsy'). 3 It is conceivable that continuous usage of direct ophthalmoscope will constitute an imminent threat for SARS infection via droplets spread. 3 Regarding the proposed infection control measures in admitting patients to hospital, the underlying principle is a balance between the infection containment risk and the ophthalmic need for timely care. 3 In the midst of SARS infection, a well-planned infection and ophthalmic triage system is mandatory. Our proposal has taken into consideration of these two competing interests. For instance, it is unethical to decline patients from proper ophthalmic care for the sake of infection containment need. There was cancellation but not to a 100% of clinical admission during SARS outbreak. To our knowledge, only a small number of nonurgent or nonessential operations were suspended in most of the eye service teams in Hong Kong. On the other hand, lowering the admission threshold while jeopardizing the infection control is equally unwise. The keys to this dilemma are judicious exercise of one's clinical judgment and close liaison with the hospital infection control team in the implementation of measure. Emergency ophthalmic patients obviously differ from the clinically admitted patients in term of the severity and urgency of their ocular conditions. Emergency eye patients are in need of almost immediate ophthalmic attention such as the acute angle closure glaucoma patients. They have to be admitted the sooner the better. Most of the time, investigation or treatment is instituted swiftly after the admission and owing to the complexity of the ocular problem, these patients are likely to stay at the hospital for a few days. In other words, if the emergency eye patients happen to have been infected by SARS coronaviruses but in incubation period, the risk of a major hospital outbreak should not be understated. Therefore, for emergency eye patients, an early isolation and observation in the infection triage ward are in conformity with the major infection control guidelines. 4 As for the clinically admitted patients such as those coming for cataract operation, their time of stay inside hospital is comparatively shorter and they carry a lower risk of crossinfection in contrast to the emergency eye patients. Obviously, admitting all patients to the infection triage ward for observation is neither practical nor feasible. Screening by house eye surgeons is a realistic alternate without significant compromise of the infection containment principle. Dr Lai may have mistaken our suggestions regarding the ophthalmic consultation catered for suspect cases. In order to minimize the period of contact between the suspect cases and other patients in eye clinic, we were advocating seeing the suspect cases at the last time slot and they were strongly advised not to come earlier. 3 Arrangement and proper prior notice were given to patients concerned so they the running was very smooth. As mentioned in our paper, 'Under circumstances such as between cases and immediately following a high-risk procedure, all health-care workers have to abide by the decontamination process consisting of removal of all potentially contaminated protective wear in proper sequence and putting on clean protective weary', a proper knowledge and usage of PPE is universally accepted as the bread and butter for infection control. 5 Dr Lai has simply echoed our point in his letter. 
paper_id= fff12f3234c8b3a240073bd4000de83089946fe7	title= G-TADOC: Enabling Efficient GPU-Based Text Analytics without Decompression	authors= Feng  Zhang;Zaifeng  Pan;Yanliang  Zhou;Jidong  Zhai;Xipeng  Shen;Onur  Mutlu;Xiaoyong  Du;	abstract= Text analytics directly on compression (TADOC) has proven to be a promising technology for big data analytics. GPUs are extremely popular accelerators for data analytics systems. Unfortunately, no work so far shows how to utilize GPUs to accelerate TADOC. We describe G-TADOC, the first framework that provides GPU-based text analytics directly on compression, effectively enabling efficient text analytics on GPUs without decompressing the input data. G-TADOC solves three major challenges. First, TADOC involves a large amount of dependencies, which makes it difficult to exploit massive parallelism on a GPU. We develop a novel fine-grained thread-level workload scheduling strategy for GPU threads, which partitions heavily-dependent loads adaptively in a fine-grained manner. Second, in developing G-TADOC, thousands of GPU threads writing to the same result buffer leads to inconsistency while directly using locks and atomic operations lead to large synchronization overheads. We develop a memory pool with thread-safe data structures on GPUs to handle such difficulties. Third, maintaining the sequence information among words is essential for lossless compression. We design a sequencesupport strategy, which maintains high GPU parallelism while ensuring sequence information. Our experimental evaluations show that G-TADOC provides 31.1× average speedup compared to state-of-the-art TADOC. 	body_text= Text analytics directly on compression (TADOC) [1] - [4] has proven to be a promising technology for big data analytics. Since TADOC processes compressed data without decompression, a large amount of space can be saved. Meanwhile, TADOC reuses both data and intermediate computation results, which results in that the same contents in different parts of original files can be processed only once, thus saving significant computation time. Recent studies show that TADOC can save up to half of the processing time and 90.8% storage space [1] , [2] . On the other hand, GPU as a heterogeneous processor shows promising performance in many real applications, such as artificial intelligence. It is popular to use heterogeneous processors, such as GPU, to accelerate data analytics systems [5] - [9] . Therefore, it is essential to enable efficient data analytics on GPUs without decompression. Applying GPUs to accelerate TADOC brings three key benefits. First, GPU performance is much higher than CPU performance, so applying GPUs with proper designs can greatly accelerate TADOC performance, which means that users can feel no delay in data analytics towards massive data. Second, previous TADOC mainly focuses on distributed systems. If we develop a GPU-based solution on a single HPC server while achieving higher performance, tremendous resources, including equipment cost and electricity cost, can be significantly saved. Third, many data analytics applications, such as latent Dirichlet allocation (LDA) [10] and term frequency-inverse document frequency (TFIDF) [11] , have been ported to GPUs, while TADOC has proven to be suitable for these advanced data analytics applications. Hence, providing a GPU solution would remove the last barrier to apply TADOC to a wide range of applications. Although it is both beneficial and essential to develop TADOC on GPUs, building efficient GPU-based TADOC is very challenging. Applying GPUs to accelerate TADOC faces three challenges. First, TADOC organizes data into rules, which can further be represented as a DAG. Unfortunately, the amount of dependencies among the rule-structured DAG of TADOC is extremely large, which is unfriendly for GPU parallelism. For example, in our experiments, the generated DAG for each file has 450,704 dependent middle-layer nodes on average, which greatly limits its parallelism. Even worse, a node in the DAG of TADOC can have multiple parents, which makes this problem more complicated. Second, a large number of GPU threads writing to the same result buffer inevitably cause tremendous write conflicts. A straightforward solution is to lock the buffer for threads, but such atomicities lose partial performance. In the worst case, the parallel performance is lower than that of the CPU sequential TADOC. Third, maintaining and utilizing the sequence information on GPUs is another difficulty: the original TADOC adopts a recursive call to complete sequential traversal on compressed data, which is similar to a depth-first search (DFS) and is extremely hard to solve in parallel. Currently, none of the TADOC solutions can solve the challenges of enabling TADOC on GPUs mentioned above. Zhang et al. [2] first proposed TADOC solution but it is designed in a sequential manner. Although TADOC can be applied in a distributed environment, TADOC adopts coarsegrained parallelism and the processing for each compressed unit is still sequential. Zhang et al. next developed a domain specific language (DSL), called Zwift, to present TADOC [1] , and further realized random accesses on compressed data [3] . However, the parallelism problems still exist. Zhang et al. [4] then provided a parallel TADOC design, which provides much better performance than the sequential TADOC. Unfortunately, such parallelism is still coarse-grained: it only divides the original file into several sub-files, processes different files separately, and then follows a merge process, which cannot be utilized by GPUs efficiently. We design G-TADOC, the first framework that provides GPU-based text analytics directly on compression, effectively enabling efficient text analytics on GPUs without decompressing input data. G-TADOC involves three novel features that can address the above three challenges. First, to utilize the GPU parallelism, we develop a fine-grained thread-level workload scheduling strategy on GPUs, which allocates thread resources according to the load of different rules adaptively and uses masks to describe the relations between rules (Section IV-B). Second, to solve the challenge of write conflict from multiple threads, we enable G-TADOC to maintain its own memory pool and design thread-safe data structures. We use a lock buffer when multiple threads update the global results simultaneously (Section IV-C). Third, to support sequence sensitive applications in G-TADOC, we develop head and tail data structures in each rule to store the contents at the beginning and end of the rule, which requires a light-weight DAG traversal (detailed in Section IV-D). We evaluate G-TADOC on three GPU platforms, which involve three generations of Nvidia GPUs (Pascal, Volta, and Turing micro-architectures), and use five real-world datasets of varying lengths, structures, and content. Compared to TADOC on CPUs, G-TADOC achieves 31.1× speedup. In detail, TADOC can be divided into two phases: initialization and DAG traversal. For the initialization phase, G-TADOC achieves 76.5% time saving, while for the DAG traversal phase, G-TADOC achieves 82.2% time saving. As far as we know, this is the first work enabling efficient text analytics on GPU without decompression. In summary, we have made the following contributions in this work. • We present G-TADOC, which is the first framework enabling efficient GPU-based text analytics directly on compressed data. • We unveil the challenges for developing TADOC on GPUs and provide a set of solutions to these challenges. • We evaluate G-TADOC on three GPU platforms, and demonstrate its significant benefits compared to state-ofthe-art TADOC. II. BACKGROUND In this section, we introduce TADOC and GPUs, which are the background and premises of our work. TADOC [1] - [4] is a novel lossless compression technique that enables data analytics directly on compressed data without decompression. In detail, TADOC adopts dictionary conversion to encode original input data with numbers, and then uses context-free grammar (CFG) to recursively represent the numerical transformed data after conversion into rules. Repeated pieces of data are transformed into different rules in CFG, and the data analytics tasks are then represented as rule interpretations. To leverage redundant information between files, TADOC inserts unique splitting symbols for file boundaries. Moreover, the CFG can be represented as a directed acyclic graph (DAG), so the interpretation of the rules for data analytics can be regarded as a DAG traversal problem. Currently, TADOC extends Sequitur [12] - [14] as its core algorithm. We use Figure 1 to show how TADOC compresses data by CFG representation, which is an example used in [2] . Figure 1 (a) shows the original input data, which consists of two files: file A and file B, and "wi" represents a unique word. Figure 1 (b) shows the dictionary conversion, which uses an integer to represent an element. Note that the rules "Ri" and file splitters "spti" are also transformed into numerical forms. Figure 1 (c) shows the TADOC compressed data, which are sequences of numbers. The TADOC compressed data can be viewed as CFG shown in Figure 1 (d), which can be further organized as a DAG shown in Figure 1 (e) for traversals. We use Figure 2 to show how to utilize the TADOC compressed data to perform a simple data analytics application word count. In Step 1, R2 transmits its accumulated local word frequencies to its parents, which are R0 and R1. In Step 2, R1 receives the word frequencies from R2 and merges these frequencies to R1's local frequency table. In Step 3, R1 transmits its accumulated word frequencies to its parent R0. After R0 receives the word frequencies from all its children, which are R1 and R2, R0 merges all received word frequencies into R0's word count results, which are also the final word counts. GPU is a specialized device targeting graphics and image processing originally. Due to its high parallelism R0: R1 R1   R2 w1   R1: R2 w3 R2 w4   R2: w1 w2   <w1,6>, <w2,5>, <w3,2>, <w4,2>  CFG Relation  Information Propagation   spt1 Step 1 Step 2 Step 3   <w1,1>, <w2,1>   <w3,1>, <w4,1>, <w1,2>, <w2,2>   <w1,1>, <w2,1>   <w1,2>, <w2,2>,  <w3,1>, <w4, design, GPUs have now been applied to a wide range of applications, including data management systems [5] - [9] . A server equipped with GPUs can offer unprecedented computing power within a single machine. Previous TADOC mainly targets distributed environments without GPUs. If we provide a GPU solution for TADOC, enabling efficient GPUbased data analytics without decompression, not only the number of machines, but also the electricity budget can all be saved. GPUs are different from CPUs in various aspects. First, different from CPUs, GPUs include a large number of light-weight cores, grouped into different streaming multiprocessors (SM). GPUs utilize high throughput to hide memory access latency. Second, each SM has controllable shared memory, which has similar performance to caches. Therefore, good use of shared memory is critical for GPU performance. Third, the execution model on GPUs is also quite different from that on CPUs. The basic thread scheduling unit is a warp, which includes 32 threads for Nvidia GPUs. The threads within the same warp execute the same instructions in a lock-step manner, so called single instruction, multiple threads (SIMT). Developing GPU-based TADOC needs to adapt to the GPU execution model. In this part, we revisit previous traversal-based techniques to show our motivation of a new GPU-based TADOC design. Why GPU-based BFS does not apply? The DAG traversal in TADOC is unique and cannot be replaced by a BFS traversal. First, many data analytics tasks require sequence maintenance of words, where BFS cannot be used directly [2] . Second, TADOC involves complicated data processing and complex data structures during traversal. For example, each rule, which is a node in DAG, needs to maintain a local word table and a rule table, and all rules write to the same global buffer, which generates write conflicts in G-TADOC among GPU threads. Third, the DAG traversal in TADOC involves dynamic data transmission. For example, the traversal can transmit accumulated word frequencies among rules. Unfortunately, the amount of data transferred between nodes cannot be obtained in advance, which has not been involved in BFS on GPUs. The uniqueness of TADOC is that each node requires complicated text-related intra-and inter-node operations. This uniqueness does not need to be considered in previous GPU traversal solutions. In detail, within a node, a dynamic buffer needs to be maintained to receive intermediate results from parents and to transmit data to children. Between different nodes, cross-rule sequence needs to be considered. In this section, we mainly discuss the challenges of enabling efficient text analytics on GPUs without decompression. Challenge 1: GPU parallelism for TADOC. The high performance of GPU relies on the high throughput from thread-level parallelism. First, as presented in [2] , there exist massive dependencies among the DAG, which leads TADOC difficult to be parallel. Accordingly, TADOC utilizes coarsegrained parallelism that mainly processes different compressed files in parallel: each CPU thread handles a separate file [4] . We cannot apply such coarse-grained parallelism on GPUs because a GPU supports thousands of threads and it is inefficient to split the compressed data into that large number of partitions. Second, if we use one GPU thread for one rule, there is a workload unbalancing problem because the numbers of elements in different rules vary significantly. GPUs launch threads in warp level, and the threads within a warp have to release resources simultaneously. The workload imbalance problem decreases the parallelism degrees. Third, we cannot simply decide the number of threads for rules, because of the various rule length. Challenge 2: TADOC final result update conflict of massive GPU threads. The update conflict is a serious problem when we develop TADOC on GPUs. First, the update conflict of multiple threads writing to the same result buffer is not a serious problem on CPUs because the number of CPU threads is limited. However, on a GPU server, when a large number of GPU threads write to the same result buffer, we have to use atomic operations to guarantee correctness, which incurs massive conflicts. Second, the complicated data structures used in TADOC cannot be applied in GPU environment. For example, TADOC uses an unordered map data structure for results such as word counts; we need to develop our own similar data structures on GPUs with atomicity and consistency considered. Third, the amount of memory required by TADOC is unknown until runtime. Even worse, for TADOC on GPUs, the memory sizes of different threads are also various, which makes the update problem with thread conflicts more difficult. Challenge 3: sequence maintenance of TADOC compressed data on GPUs. How to keep the sequence information on GPUs is also challenging. Sequence maintenance is essential for sequence sensitive applications, such as counting three continuous word sequences. First, to keep the sequence information, TADOC originally traverses the DAG in a DFS order [2] , which is hard to be parallel. Second, a word sequence can span several rules and these rules can be controlled by different GPU threads. Currently, threads across different GPU blocks have no mechanism for synchronization. Third, TADOC uses map data structures to store sequence counts. For these sequence-based applications, we need to develop special data structures in GPU memories to store sequences and perform basic comparisons between threads. Based on the analysis, designing a GPU-based TADOC is very rewarding, but full of challenges. In this section, we show our G-TADOC framework. G-TADOC consists of three components, a module for data structures, a parallel execution module, and a sequence support module. We first show the G-TADOC overview and then the different modules. We show the overview of G-TADOC in Figure 3 . The inputs are TADOC compressed data and user program, and the outputs are the results, which are similar to previous non-GPU TADOC implementations [1] , [2] . Modules. G-TADOC consists of three major modules. The parallel execution module is responsible for the G-TADOC parallel execution on GPUs, which decides how to partition workloads for thread parallelism. The data structure module provides necessary data structures for G-TADOC execution, including a self-controlled memory pool, thread-safe data structures, and head and tail structures for sequences. The sequence support module is used for applications that are sensitive to sequence orders. Phases. After receiving the TADOC compressed data and program, G-TADOC execution mainly consists of two phases: initialization phase and graph traversal phase. In the initialization phase, G-TADOC prepares necessary data structures according to the user program and launches a lightweight scanning to fulfill related values. In the graph traversal phase, G-TADOC analyzes different traversal strategies and chooses the most suitable one based on both data and tasks. Before the end of the graph traversal, G-TADOC performs a merging process for final results. Solutions to challenges. G-TADOC can handle the challenges mentioned in Section III-B. To address the first GPU parallelism challenge, G-TADOC adopts a threadlevel workload scheduling strategy for GPU threads, which partitions the DAG in a fine-grained manner for parallelism (Section IV-B). To address the second TADOC update conflict challenge, we develop a memory pool on GPUs and maintain necessary data structures so that all threads manage the same memory objects with consistency guaranteed (Section IV-C). To address the third challenge of sequence sensitivities on GPUs, G-TADOC scans the DAG for recording the cross-rule content in a light-weight manner in the initialization phase. Then, G-TADOC performs a rule-level processing and result merging process in the graph traversal phase (Section IV-D). We show our G-TADOC parallel execution engine in this part. In developing our parallel partitioning strategy, we consider two possible designs, as shown in Figure 4 . The first design is to partition the DAG vertically from the root: different parts are traversed by different threads, as shown in Figure 4 (a). This design can leverage the GPU parallelism, but at the same time, some rules can be scanned by different threads. For example, R2 and R4 are scanned by both thread0 and thread1. Even worse, when the DAG is very deep and complicated, the problem that massive rules are repeatedly scanned by different threads can be serious. Hence, we abandon this design. The second design is fine-grained threadlevel scheduling: we assign a thread for each node except the root; the root rule usually includes a large number of elements so we allocate a group of threads based on the rule length to handle it. Note that when a rule includes a large number of elements (the default threshold is 16 times the average number of elements per thread), such as R4, more threads should be allocated for the rule. To traverse the DAG, each rule is associated with a mask to indicate whether a rule is ready to be traversed or not. This design ensures the dependency for correctness in the DAG traversal and retains great parallelism simultaneously. Therefore, we adopt this fine-grained design. Moreover, as discussed in [4] , the optimal traversal strategy depends on both input data and analytics tasks, so we develop both top-down and bottom-up traversals and use the strategy selector in [4] for such decisions. Next, we show our detailed top-down and bottom-up designs in G-TADOC. Top-down traversal. We show our top-down traversal design in this part. 1) General design. The general design of top-down traversal transmits required data, such as file information, from the root to sub-nodes for processing. Then, G-TADOC gathers local results from different nodes as the final result. First, in root, 2) Detailed algorithm. We show our top-down DAG traversal design in Algorithm 1. The general control is performed on the CPU side by the topDown function, as shown in Lines 1 to 8, which calls different GPU kernels, including initTopDownMaskKernel, topDownKernel, and reduceResultKernel. The initTopDownMaskKernel is executed on GPUs, which initializes the nodes whose in-edges are from only the root. In detail, we consider the topology of the rules except the root, and accordingly, the initial weights of these rules are their frequencies in the root. Additionally, rule.numInEdge stores the number of in-edges of each rule, and only the rules with zero numInEdge can start the DAG traversal initially. In G-TADOC, we mark the masks of the rules that can be processed as true. The topDownKernel is the main body of the top-down traversal and is executed on GPUs. We set the devStopFlag to true in Line 4. If the devStopFlag is still true after the topDownKernel execution, which means that the DAG has no update and has been fully traversed, then G-TADOC stops traversal. In topDownKernel, for different applications, only the for-loop from Lines 15 to 17 is different. Here, we take word count as an example. For a given rule, it first transmits its accumulated weights to all its subrules (Line 17). If the number of current in-edges subRule.curInEdge is equal to a subrule's full number of in-edges subRule.numInEdge, then we mark the subrule's mask to true, indicating that the subrule is ready to be traversed in the next round (Line 20). Note that when any masks are changed, stopFlag shall be set to false. Moreover, rule.mask should be set to false in Line 22 so that the rule will not be involved in the next round. The reduceResultKernel merges the word frequencies from all rules multiplied by their corresponding accumulated rule weights on GPUs. Algorithm 1 Top-Down Traversal 1: function topDown(rules) Executed by host; use word count as an example 2: initTopDownMaskKernel(rules) with at least rules.size threads 3: do Repeated top-down traverse until all rules' weight generated 4: cudaMemSet devStopF lag ← true 5: topDownKernel(rules, devStopF lag) with at least rules.size threads 6: cudaMemCpy devStopF lag to stopF lag 7: while stopF lag is f alse 8: reduceResultKernel ( if subRule.curInEdge is full then 20: subRule.mask ← true Sub-rule then can be traversed 21: devStopF lag ← f alse 22: rule.mask ← f alse 3) Complexity analysis. Algorithm 1 can be divided into three stages. The first stage is mask initialization (Line 2), in which each thread checks the corresponding rule's number of in-edges and then sets its mask. Assuming sufficient parallel resources, the complexity is O(1). The second stage is topdown traversal (Lines 3 to 7). Assuming that the DAG has k layers, the number of loops is not greater than k. Then each thread in topDownKernel traverses the corresponding rule's sub-rules. Suppose in the i th loop, the maximum number of sub-rules of a rule is e i,max , then the total complexity of this stage is O( k i=1 e i,max ), which can be represented as O(kē max ). The third stage is to reduce results (Line 8). Each thread needs to merge the corresponding rule's local words from the local table to the global table, so the complexity is O(w max ), where w max is the maximum number of local words among all rules. Therefore, the overall complexity of Algorithm 1 is O(kē max + w max ). Bottom-up traversal. We show our bottom-up traversal design in this part. 1) General design. The bottom-up traversal transmits required data, such as local word counts, from leaves to upper-level nodes. After transmission, the root and its directly connected nodes (called 2nd-layer nodes) store the gathered result. Note that we do not accumulate the results to the root because the root contains file information. In detail, first, each leaf transmits the required data from its local tables to its parents. Second, during traversal, each node accumulates the transmitted data from children and then transmits the accumulated results to its parents. Note that data consistency needs to be guaranteed since different rules are controlled by different threads. Third, after traversal, G-TADOC analyzes the local buffers in the root and 2nd-layer nodes in parallel to generate the final results. 2) Detailed algorithm. We show our bottom-up DAG traversal design in Algorithm 2. The general control is performed by the function bottomUp from the CPU side, which calls different GPU kernels. Different from Algorithm 1, the bottom-up design in Algorithm 2 first generates the pointers from children to parents (Lines 2 to 3), initializes masks (Line 4), and generates the local tables' bound in a light-weight bottom-up manner (Lines 5 to 9), so that the local tables in rules can be allocated (Line 10). Then, it initializes masks again (Line 11) and traverses the graph in a comprehensive bottom-up direction with a result merging process (Lines 12 to 17). The initBottomUpMaskKernel set the rule masks. The leaves are set to true so that they can be traversed initially. The genLocTblBoundKernel is used to calculate the memory size limit for local tables, and is called by the bottomUp function repeatedly. Its kernel execution is similar to that of topDownKernel in Algorithm 1, except the use of out-edge rather than in-edge during traversal. When a rule is traversed, G-TADOC sums the upper limits of its local words and all its children's local tables as the amount of space that should be allocated. Then, the rule increases all its parents' out-edges. When a parent's number of current out-edges is equal to its number of subrules, G-TADOC sets its mask to true for the next-iteration execution. After calculating the memory limit of each node, we uniformly allocate the corresponding buffer for each rule in rules.locT bl (Line 10). The genLocTblKernel is used for DAG traversal with the allocated memory space from the bottomUp function. Its traversal order is controlled by the traversed out-edges, which is the same as genLocTblBoundKernel. However, the kernel's computation task is much heavier. Here, we use the word count example for illustration. When a rule is traversed, it first reduces its local word frequencies, and then merges all its subrules' local word frequencies into its own local table. The reduceResultKernel merges the word frequencies from the root and its children where the root is directly connected (called level-2 nodes in [2] ) on GPUs. In detail, G-TADOC merges 1) the word frequencies in the root, and 2) the frequencies in the local tables of the root's direct children multiplied by their corresponding rule frequencies in the root. This is different from the reduceResultKernel in Algorithm 1. 3) Complexity analysis. Different from Algorithm 1, Algorithm 2 consists of five stages. The first stage is to generate the parents of rules (Lines 2 to 3). Each thread in genRuleParentsKernel stores the corresponding rule's ID in all its sub-rules' parent table. The complexity is O(e max ), where e max is the maximum number of sub-rules of all rules. The second stage is mask initialization (Line 4 and 11). cudaMemCpy devStopF lag to stopF lag 9: while stopF lag is f alse 10: allocate device memory to rules.locT bl 11: initBottomUpMaskKernel(rules) with at least rules.size threads 12: do 13: cudaMemSet devStopF lag ← true 14: genLocTblKernel(rules, devStopF lag) with at least rules.size threads 15: cudaMemCpy devStopF lag to stopF lag 16: while stopF lag is f alse 17: reduceResultKernel(rules) with at least root.size threads Similar with the mask initialization in Algorithm 1, the complexity is also O(1). The third stage is to generate rules' local table bound (Lines 5 to 9). Each thread in genLocTblBoundKernel traverses the corresponding rule's subrules and parents. Suppose in the i th loop, the maximum numbers of sub-rules and parents of these rules are e i,max and p i,max respectively. Then, the complexity is O( where k is the number of layers in the DAG. The fourth stage is to generate rules' local table (Lines 5 to 9). Besides traversing corresponding rule's sub-rules and parents, each thread in genLocTblKernel also merges all sub-rules' local tables and its own words. For a given rule i, suppose its local table size is t i and its number of words is w i , then its computation load is w i + j∈i.subRules t j . The complexity of this stage is O( k i=1 C i,max ), which is O(kC max ). C i,max is the maximum computation load among rules in the i th loop. The fifth stage is to reduce results (Line 8). This stage scans the root and merges all level-2 nodes. In detail, each thread is responsible for one level-2 node, so the complexity is O(t lv2,max ), where t lv2,max is the maximum size of level-2 nodes' local tables. Therefore, the overall complexity of Algorithm 2 is O(k(ē max +p max + C max ) + t lv2,max ). Parameter selection. G-TADOC involves a few parameters to adjust, such as the threshold of GPU thread resources allocated to a rule. The current solution is to extract a sample set of input and then use a greedy strategy to set each parameter in turns. If the input is unavailable until runtime, then the parameters are set according to our training set (a small extracted dataset from Wikipedia [15] ). The data structures in G-TADOC include a self-maintained memory pool, thread-safe structures, and sequence support. G-TADOC maintained memory pool. As discussed in Section IV-B, we need to provide each thread a separate memory space during DAG traversal. Because 1) the required memory size is unknown until runtime, and 2) allocating memory dynamically for all threads is inefficient, we develop a global memory pool to manage the GPU memory by G-TADOC itself. First, each rule calculates its own required memory size for necessary data structures. Second, with data transmission in the initialization phase in Figure 1 , each rule transmits its memory requirement to its parents in a bottomup traversal, or to its children in a top-down traversal. This memory requirement transmission process can be recursive. Third, after the whole range transmission in the initialization phase, each rule determines its maximum memory requirement and we can allocate related resources of different rules from the memory pool. Thread-safe data structures. After we introduce the memory pool in G-TADOC, we next describe the thread-safe data structures used in the memory pool for GPU threads. The most important data structure in TADOC is the hash structure [2] , which can be used to store the results both locally and globally. Hence, we use the hash structure for illustration in G-TADOC thread-safe design, as shown in Figure 5 . The original state of the hash table is shown in Figure 5 (a). The lock buffer is for locking entries (1 means locked, and 0 means unlocked). The entry buffer is for hashing (default -1). The key and value buffers are for the key-value pairs. The next buffer is for the next entry if multiple key-value pairs are mapped into the same entry. Figure 5 (b) shows the state after inserting <126,1>, assuming the key-value pair is hashed to 1. Because there is no conflict in this insertion, the related value in the next buffer is -1. Figure 5 (c) shows the hash table state after inserting <163,1>, assuming the key-value pair is hashed to 3. Accordingly, G-TADOC just stores the key-value pair <163, 1> after the first <126,1>. Figure 5 (d) shows a hash conflict situation: the hash table state after inserting <78,1>, assuming the key-value pair is hashed to 1. Because <126, 1> has already been inserted to the first entry, we update its "next" buffer pointing to a new place for the newly inserted <78,1>. Note that the lock buffer is used only when all threads writing to the same buffer location. Moreover, if the hash table is private and owned by one thread, we do not need to create the locks. Head and tail structures for sequence support. The head and tail structures are used to support sequence sensitive applications, such as sequence count [2] . Because G-TADOC traverses the DAG in parallel, some rules may involve crossrule sequence (a word sequence spanning multiple nodes in the DAG). We design head and tail data structures for each rule to store the content of the beginning and end of the rule, which are provided to the parents. We show an example in Figure 6 . In the root, the first sequence, <w1,w2,w3>, is a sequence that does not span across rules. However, for the next three-word sequence, <w2,w3,w4>, it spans across the root and R1. For this sequence, we store the partial content of <w4,w5> in the head buffer of R1, so that this cross-rule sequence can be processed by the parent, which is the root. Similarly, we store <w6,w7> in the tail buffer of R1 so that R1's parent can quickly process the sequences containing the words in R1's tail buffer. Note that the first few elements and the last few elements in the subrule can also be a rule. For example, in Figure 4 , the first element of R2 is also a rule, so the sequence from the root can span more than two rules, which is complicated. In our design, each rule can be handled by different threads. If we can provide the head and tail buffers of all rules, we can avoid multi-rule scanning by looking into only the head and tail buffers of different subrules directly. In summary, the parents are responsible to process cross-rule sequences, and the problem can be solved by scanning the head and tail buffers of the direct children. More details are presented in Section IV-D.  In this part, we discuss the sequence support in G-TADOC for sequence sensitive applications. The sequence support in TADOC [2] is developed by function recursive calls, which is inefficient and hard to be parallel on GPUs. To improve the sequence support of TADOC and parallelize it on GPUs, we have the following insights. First, to fully parallelize the rule processing, each rule needs to include the head and tail buffers mentioned in Section IV-B to remove the sequence dependency across rules. Second, a first-round initialization phase is required to fulfill the head and tail buffers for all rules. Third, the original recursive design in TADOC [2] is inefficient and thus shall be abandoned; a more efficient parallel graph traversal needs to be developed. Based on the analysis, we develop a two-phase sequence support design for sequence sensitive applications. Initialization phase. The first initialization phase is to prepare the head and tail buffers for each rule with a lightweight scanning. The upper limit of memory space for each rule is shown in Equation 1, where wordSize denotes the size of the word elements, l denotes the sequence length, and subRuleSize denotes the number of subrules. The detailed process to generate the head and tail buffers of each rule is shown in Figure 7 . The CPU side uses a whileloop to continuously check whether all the head and tail buffers have been fulfilled. To generate the head buffers, G-TADOC traverses the rules, and puts a given number of continuous words at the beginning of the rule in the head buffer. Within such a process, if G-TADOC encounters a subrule, G-TADOC first checks the related mask. If the mask is set, which implies that the subrule's head buffer is ready, then G-TADOC can put the content from the subrule's head buffer to the current rule's head buffer; otherwise, the calculation fails and needs to be conducted in the next round. The generation of the tail buffers is similar to the generation of the head buffers. Graph traversal phase. The second phase of sequence support is shown in Figure 8 . Similar to the first phase shown in Figure 7 , the CPU part uses a while-loop to control the DAG traversal process. We use sequence count [2] as an example, which uses the hash tables described in Figure 5 . For sequence support, we need to reduce the intermediate results in the local tables from the rules. We use parallel hash tables to merge these results, as discussed in Section IV-C. First, we distribute each key-value pair a mask, and each entry a lock. Second, each thread is responsible for one key-value pair. Third, each thread needs to justify whether it is necessary to insert a keyvalue pair. If not, G-TADOC returns directly; otherwise, G-TADOC obtains the entry based on hash functions, and then verifies if the same key already exists on this entry. If the key exists, G-TADOC uses atomic additions directly, and then sets the mask to true; otherwise, G-TADOC tries to obtain the lock of the entry. If the lock is occupied by other threads, G-TADOC sets the stop flag to false and returns directly; if G-TADOC obtains the lock, G-TADOC needs to verify whether the same key coexists. If the same key coexists, G-TADOC uses atomic additions to avoid this issue; otherwise, G-TADOC obtains a new node and sets the entry accordingly. Finally, G-TADOC unlocks the table, sets the mask to true, and returns. Note that the CPU part continuously launches this process until the stop flag is set to true. V. IMPLEMENTATION We integrate our G-TADOC into the CompressDirect (CD) [2] library, which is an implementation of TADOC. G-TADOC in CD includes two parts: 1) the CPU part that is used to input data and program, and to handle the GPU module, and 2) the GPU part that is used for GPU-based TADOC acceleration. We use the same interfaces as TADOC in CD, including word count, sort, inverted index, term vector, sequence count, and ranked inverted index, so users do not need to change any code in this GPU support. In this section, we measure the performance of G-TADOC and compare it with TADOC [2] for evaluation. We show our experimental setup in this part. Methodology. The baseline in our evaluation is TADOC [2] , which is the state-of-the-art data analytics directly on compression, denoted as "TADOC". Our method that enables TADOC on GPUs is denoted as "G-TADOC". In our evaluation, we measure TADOC [2] performance and G-TADOC performance for comparison. Moreover, we assume that small datasets can be stored and processed in GPU memory directly without PCIe data transmission; large datasets are stored on disk with PCIe data transmission required to be involved in time measurement. Platforms. We use three GPU platforms and a 10-node Amazon EC2 cluster in our evaluation, as shown in Table I . We evaluate G-TADOC on three generations of Nvidia GPUs (Pascal, Volta, and Turing micro-architectures), which are used to prove the adaptability of G-TADOC. Since GPU architectures are constantly changing, if we can achieve high performance on all these platforms, then it is very likely that G-TADOC can achieve promising results for future GPU products. The 10-node cluster is a Spark cluster on Amazon EC2 [16] for large datasets of TADOC. Datasets. The datasets used in our evaluation are shown in Table II , which include various real-world workloads. Datasets A, B, and C are used in [1] - [4] . Dataset A is NSF Research Award Abstracts (NSFRAA) downloaded from UCI Machine Learning Repository [17] , and is composed of a large number of small files. Dataset B is a collection of four web documents downloaded from Wikipedia [15] . Dataset C is a large Wikipedia dataset [15] . To increase the diversity of test data, we add datasets D and E compared to previous works [1] - [4] . Dataset D is COVID-19 data from Yelp [18] , and dataset E is a collection of DBLP web documents [19] . Note that only dataset C is evaluated on the 10-node cluster.  In this part, we measure the speedups of G-TADOC over TADOC and show their time breakdowns. Overall speedups. We show the speedups that G-TADOC achieves over TADOC [2] in five datasets in Figure 9 . In detail, Figure 9 (a) shows the speedups on Pascal platform, Figure 9 (b) shows the speedups on Volta platform, and Figure 9 (c) shows the speedups on Turing platform. We have the following observations. First, G-TADOC achieves significant performance speedups over TADOC in all cases. On average, G-TADOC achieves 31.1× speedup over TADOC. The reason is that the GPU device for G-TADOC provides much higher computing power and bandwidth than the CPU device for TADOC. For example, on the Pascal platform, the theoretical peak performance of the GPU is about 185.3× over the theoretical peak performance of the CPU. Moreover, the bandwidth provided by the GPU memory is about 8.3× over the memory bandwidth provided by the CPUs. The performance speedups achieved by G-TADOC further prove the effectiveness of our solutions to handle the dependencies in our parallel design for GPUs. Second, the speedups of G-TADOC over TADOC on single nodes for processing small datasets are higher than the speedups of G-TADOC over TADOC on clusters for processing large dataset C. The average speedup of G-TADOC over TADOC on a single node is 57.5×, while the average speedup of G-TADOC over TADOC on a tennode cluster is 2.7×. The reason is that when processing the large dataset, TADOC adopts coarse-grained parallelism in distributed environments to improve the data processing efficiency. However, due to the data exchange overhead between nodes in the distributed environment of TADOC, our G-TADOC is still more efficient than TADOC. Third, the speedups G-TADOC achieves for sequence count and ranked inverted index are much higher than the speedups of the other applications in most cases. In detail, the average speedups of sequence count and ranked inverted index are 111.3× and 112.0×, which are much higher than the full range average speedup. The reason is that sequence count and ranked inverted index of TADOC in [2] is of low performance: as described in [2] , the performance behaviors of sequence count and ranked inverted index of TADOC are close to those of the original implementations on uncompressed data without compression. As to G-TADOC, sequence count and ranked inverted index reuse the partial results of duplicate data and execute in parallel on GPUs. We analyze G-TADOC acceleration in different phases and the traversal strategies on GPUs in this part. Speedups in different phases. We show the separate speedups of G-TADOC over TADOC in different phases in Figure 10 . First, the average speedup in the second phase is 64.1×, which implies that most G-TADOC performance benefits come from the acceleration in the second phase of DAG traversal. The second phase also has a relatively long execution time in TADOC [2] , which provides more parallel optimization opportunities. Second, for the first phase, although the execution time is relatively short, G-TADOC still achieves clear performance speedups: on average, G-TADOC is 9.5× faster than TADOC on CPUs. Third, in large dataset C, the speedups in the first initialization phase are extremely high, which shows that the data structure preparation for massive large files is time-consuming in TADOC [2] and is essential to be accelerated by G-TADOC. Top-down vs. bottom-up traversals. We develop topdown and bottom-up traversals in G-TADOC, but the optimal traversal strategy for each application can be input dependent. For example, for term vector in dataset A, the top-down traversal takes 14.04 seconds, but the bottom-up traversal takes only 1.56 seconds. In contrast, for term vector in dataset B, the bottom-up traversal takes 0.43 seconds, but the topdown traversal takes only 0.11 seconds. In detail, dataset B involves only four files. If we traverse the DAG in a top-down strategy, we only need to maintain a small buffer of 16 bytes in each rule indicating its file information, and the transmission for file information in DAG traversal is also marginal. In contrast, for dataset A, which involves a large number of small files, the top-down traversal with file information would be time-consuming and drags down the overall performance. Therefore, we should select the bottom-up traversal strategy in dataset A, and top-down strategy in dataset B. We apply the TADOC adaptive traversal strategy selector on GPUs, as discussed in Section IV-B, which can help select the optimal traversal strategy. We summarize our findings and insight as follows. First, we find that GPUs are very suitable for text analytics directly on compression, but need special optimizations. For example, G-TADOC needs fine-grained thread-level workload scheduling for GPU threads, thread-safe data structures for parallel updates, and head and tail structures for sequence sensitive applications. Second, the GPU platform is both cost-effective and energyefficient, which can be applied to a wide range of data analytics applications directly on compression, especially in large data centers. Experiments show that a GPU server can have much higher performance on data analytics directly on compressed data than a ten-node cluster does. Third, although the GPU memory is limited, our work can help put much larger content directly in GPU memory. The frequent data transmission between the CPU and GPU drags down the performance advantages of GPUs when large workloads fail to be loaded to the GPU memory at once. Our work sheds light on the GPU acceleration design for such big data applications. We next show the importance of our paper and future work. Importance of our work. As the first work enabling efficient GPU-based text analytics without decompression, G-TADOC provides the insights that are of interests to a wide range of readers. Currently, G-TADOC involves only the applications in TADOC [2] , but other data analytics tasks can all benefit from G-TADOC. Furthermore, the series of optimizations on GPUs for TADOC can be directly applied to other advanced data analytics scenarios. Comparison with GPU-accelerated uncompressed analytics. In our evaluation for the six data analytics tasks with the five datasets, G-TADOC reaches 31.1× of the performance of the state-of-the-art TADOC on CPUs. A common question is how the G-TADOC performance differs from the performance of GPU-accelerated uncompressed analytics. Currently, there is no implementation about the six analytics tasks on GPUs, so we develop efficient GPU-accelerated uncompressed analytics for comparison. Experiments show that G-TADOC still achieves an average of 2× speedup. Applicability. G-TADOC has the same applicability as TADOC [4] . In general, G-TADOC targets the analytics tasks that can be expressed as a DAG traversal problem, which involves scanning the whole DAG. How far is the performance from the optimal? Although G-TADOC already achieves high performance, it still has room for performance improvement. The reasons include 1) dependencies in DAG traversal, 2) random accesses on large memory space, and 3) atomic operations on global buffers. When these issues are solved, G-TADOC can achieve at least 20% extra performance improvement. Future work. Currently, G-TADOC supports data analytics directly on compression on GPUs. This research is headed for high-performance and efficient data analytics methods. The future possible avenues of exploration include architecture optimizations or multi-GPU environments, which can further accelerate G-TADOC. As far as we know, G-TADOC is the first work that enables efficient GPU-based text analytics without decompression. In this section, we show the related work of grammar compression, compression-based data analytics, and GPU data analytics. Grammar compression. There are plenty of works on grammar compression [1] - [4] , [20] - [28] . The closest work to G-TADOC is TADOC, which is the text analytics directly on compression in single-node and distributed environments [2] . TADOC extends Sequitur [12] - [14] as its compression algorithm for data analytics. After TADOC being proposed, Zhang et al. [1] proposed Zwift, which the first TADOC programming framework, including a domain specific language, TADOC compiler and runtime, and a utility library. Then, Zhang et al. [4] applied TADOC as the storage to support advanced document analytics, such as word co-occurrence [29] , [30] , term frequency-inverse document frequency (TFIDF) [11] , word2vec [31] , [32] , and latent Dirichlet allocation (LDA) [10] . Furthermore, Zhang et al. [3] enabled random accesses to TADOC compressed data, and at the same time, supported insert and append operations. In this work, we enable TADOC on GPUs, which improves the performance of TADOC significantly. Index compression. The compression-based data analytics is an active research domain in recent years. However, typical approaches mainly use suffix trees and indexes [33] - [38] , [38] - [41] . Suffix trees are traditional representations for data compression [33] , [42] but incur huge memory usage [43] , [44] . Suffix arrays [45] and Burrows-Wheeler Transform [35] , [36] are the development of these compression formats, but still generate high memory consumption [43] . Compressed suffix arrays [46] - [50] and FM-indexes [36] , [51] - [54] are more efficient than the previous compression techniques. Furthermore, Agarwal et al. proposed Succinct [34] , which targets queries on compressed data. Moreover, there are many works about inverted index compression [55] - [61] . For example, Petri and Moffat [55] developed compression tools for compressed inverted indexes. Different from these works, G-TADOC targets text analytics directly on compressed data on GPUs. GPU data analytics. GPUs have been applied to various aspects of data analytics, including structured data analytics, stream data analytics, graph analytics, and machine learning analytics [5] - [9] , [62] - [66] . For example, MapD (Massively Parallel Database) [5] is a popular big data analytics platform powered by GPUs. Most current analytics frameworks, such as Spark, have supported GPUs [6] . SABER [7] is a stream system that schedules queries on both CPUs and GPUs, and Zhang et al. [9] further developed FineStream, which enables fine-grained stream analytics on CPU-GPU integrated architectures. Gunrock [8] is an efficient graph library for graph analytics on GPUs, and for large graphs, multi-GPU graph analytics have been explored [62] . For machine learning data analytics, parallel technologies have been extensively applied to various aspects, especially for deep learning applications [63] . Currently, most machine learning frameworks, such as TensorFlow [64] , support GPU. VIII. CONCLUSION In this paper, we have presented G-TADOC enabling efficient GPU-based text analytics without decompression. We show the challenges of parallelism, result update conflicts from multi-threads, and sequence sensitivities in developing TADOC on GPUs, and present a series of solutions in solving these challenges. By developing an efficient parallel execution engine with data structures and sequence support on GPUs, G-TADOC achieves 31.1× speedup on average compared to state-of-the-art TADOC. 
paper_id= fff1300883abf1228c0dde1c7adfdce76d921ce2	title= Detection of Enteric Adenoviruses With Synthetic Oligonucleotide Probes	authors= Tim H Scott-Taylor;Gurmuk  Ahluwalia;Magdy  Dawood;Gregory W Hammond;	abstract= The abilities of hybridization probes to detect all human adenovirus types and to identify enteric adenovirus types were evaluated. The efficiency of hybridization was compared to other tests currently in routine laboratory use on clinical specimens from young children with gastroenteritis. Probes were derived from various regions of the adenovirus types 2 and 41 genomes, and were evaluated by hybridization with a series of DNA quantities from 1 Fg to 10 pg of one adenovirus type from each human subgenus, lambda phage, and HEp 2 cells. The sensitivity of hybridization with the HPll probe (92.7%), containing the conserved hexon gene, compared well with EM (54.6%), culture and neutralization (45.5%), and enzyme immunoassay (61.8%). The sensitivity of detection of enteric adenovirus isolates by the cloned BgllI D fragment probe (92.9%) and by a synthetic probe (85.7%), manufactured from type-specific sequences of the Ad41 hexon gene were comparable to Ad40/Ad41 specific enzyme immunoassay (84.6%). Hybridization was found to be a sensitive method of adenovirus detection in comparison to traditional methods of laboratory diagnosis. Synthetic oligonucleotides enable specific detection of individual enteric adenovirus types. Hybridization had additional advantages over other tests in identifying cases of infection with more than one adenovirus type and in allowing an estimate of the concentration of adenovirus in the specimen. 0 1993 Wiley-Liss, Inc. 	body_text= Despite the specific detection of adenoviruses in 5 1 7 % of young children with gastroenteritis [Christiensen, 19891 , determination of the involvement of adenoviruses in the aetiology of gastroenteritis has been difficult. Adenoviruses have been found consistently in the stools of apparently healthy children during surveillance programs [Fox et al., 1977; Rodriguez et al., 19851. The ubiquity of adenoviruses makes it difficult to establish criteria to define adenoviral agents of gastroenteritis and prevents unequivocal substantiation of adenoviral causation of diarrhoea [Madeley, 19831 . The problem is compounded by differences in pathogenicity and cultivability between adenovirus types. The readily grown, lower numbered adenovirus types can be carried asymptomatically [Fox et al., 1977; Kidd et al., 1982; Rodriguez et al., 19851 , while isolates of the enteric adenovirus types 40 and 41 that cause the majority of clinical disease [Uhnoo et al., 1984; Wigand et al., 19831 have fastidious growth characteristics and tend to escape identification. Additionally, enteric adenoviruses present in dual infections are frequently not observed [Brown, 19851. Adenoviruses of a different type or viruses of most other groups tend to overgrow enteric adenoviruses in culture, even when present at a lower concentration [Brown, 19901 , and fastidious adenoviruses are underdiagnosed. Adenoviruses are found as the only pathogen present in a high proportion of stools of sick children [Christiensen, 19891 and should be included in a diagnostic protocol for pediatric gastroenteritis. An adenovirus test should operate directly on the initial specimen, to avoid difficulties with culture. The capacity of the test to distinguish the enteric adenovirus types, which have a specific association with gastroenteritis, would be advantageous. In this study we examined cloned and synthesized sequences for the detection of all adenovirus types and for the specific identificationof enteric types by hybridization. The sensitivity and specificity of the hybridization probes were evaluated in comparison to other conventional methods of adenovirus detection on clinical specimens from children with gastroenteritis. strain Dugan, and Ad41 strain Tak from each subgenus A to F were obtained from American Type Culture Collection (ATCC, 12301 Parklawn Drive, Rockville, MD). Viruses were cultured, purified on CsCl density gradients, and extracted as previously described [Scott-Taylor and Hammond, 19921 . Fragments of the adenovirus genome Ad41 strain Tak were amplified in competent E . coli strain Jm109 in plasmid pGem 32 (Promega Biotech, Mississauga, Ontario) . EcoRI fragments A, B, and C were cloned in plasmids p41EAC (containing both A and C fragments), p41EA, p41EB, and p41EC. Ad41 hexon gene sequences were electroeluted from p41EA after digestion with SalI and HindIII. An attempt was also made to isolate the type-specific hexon gene sequences by cloning Hind11 fragments of the p41EA plasmid in the blunt-ended SmaI site in the pGEM 32 vector. Useful transformants were identified by colony hybridization with the SalID fragment of the Ad41 genome. The BglIID fragment probe was kindly donated by Howard Takiff in the form of a n insert in vector pAT153 [Takiff et al., 19851 . The HPII probe, a HindIII-PuuII fragment enclosing the complete sequence of the Ad2 hexon gene, was evaluated in previous experiments as containing the most conserved adenovirus sequences, which enable this probe to detect all adenovirus subgenera with uniformity [Scott-Taylor et al., 19921 . Adenovirus DNA sequences were analysed with Pustell Sequence Analysis software (IBI, New Haven, CT) on genomic sequences recorded by Genbank. The suitability of oligonucleotide sequences for use as probes was determined with the program Oligo (National Bioscience, Hamel, MN). to Ad41 or Ad40 LWood et al., 19891 , supplied by J a n de Jong, were used in a blocking immunoassay as a further means of identification of a sample of 5 uncultivable isolates. Identification was determined by the ability of the specific antisera to block the binding of the isolates to microtitre wells coated with a capture antibody (Ahluwalia et al., in preparation) . The specificity of probes was evaluated with dilutions of DNA of one type of adenovirus from each subgenus, lambda phage, and HEp 2 cells, spotted a t 10 pg to 100 pg per ml. DNA preparations were denatured with the addition of 0.1 volume of 3 M NaOH, neutralized after 30 min incubation with 0.1 volume of 3 M ammonium acetate, and equilibrated to physiological salt conditions with the addition of 0.3 volumes of 20x SSC ( I x SSC = 0.15 M NaC110.015 M Na citrate). Then 150 p1 of each dilution was applied by a slot blot apparatus (Schleicher and Schuell, no. 03431, Keene, NH) under very low vacuum to nylon membrane prewetted in 6 x s s c . Preliminary investigation demonstrated that the protein extraction of the clarified stool suspension improved the clarity and completeness of hybridized spots, as noted by Kidd et al. [19821. Stool suspensions in 450 pl aliquots were incubated with 50 pl of 10% SDS and 250 pg of proteinase K for 30 min at 37°C before extraction with phenol and chloroform. Extracted samples were then boiled and cooled in ice water before 150 pl of sample were applied to prewetted nylon membranes with the slot blot manifold under low vacuum. Membranes were washed twice in 6X SSC, air dried, and baked at 80°C for 2 h prior to hybridization. Hybridization was carried out a t 68°C as previously described [Scott-Taylor et al., 1992al with a t least lo7 cpm/ml random prime labelled probe (Boehringer Mannheim, kit no. 1004 760). The melting temperatures of some hybridizations were lowered by the addition of formamide to the hybridization solution, according to the estimate that 1% formamide lowers the melting temperature by 0.72"C [McConaughy et al., 19691 . Enteric Adenovirus Ad41 EcoRI fragments A, B, and C, together comprising 84% of the genome [Scott-Taylor et al., 19921 , were used to evaluate the specificity of Ad41 sequences for the detection of enteric adenoviruses. The EcoRI fragments, cloned in plasmid vectors, were hybridized with a series of adenovirus DNA preparations of each subgroup. The reaction of the DNA dilutions with a genomic Ad41 DNA probe is shown in the first panel of Figure 1 as the standard to which other probe reactions were compared. The reaction of plasmid p41EA with the subgroup DNAs is shown in panel ii. This large plasmid, containing over 50% of the Ad41 genome from 8 to 61 map units, reacted more strongly with the DNA Between July 1990 and June 1991, 1,071 stool specimens received a t the Cadham Provincial Laboratory were examined for the presence of adenovirus by electron microscopy (EM), tissue culture, and enzyme immunoassay (EIA). Ten percent stool suspensions were made by emulsification of approximately 1 g/10 ml phosphate buffered saline (PBS) containing antibiotics in polypropylene tubes with glass beads over a Vortex mixer. Suspensions were clarified by centrifugation at 3,020 x g (5,000 rpm in a Sorval RT600 centrifuge) for 20 min. EM examination was enhanced by ultracentrifugation of clarified suspensions onto formvar-coated grids by means of an airfuge [Hammond et al., 19811 . Two commercial immunoassay kits were employed for detection of all adenoviruses and for identification of enteric types according to the manufacturer's instructions (Cambridge Bioscience, Worcester, MA). Culture was performed by inoculation of 100 p1 of clarified suspension applied to semiconfluent monolayers of 293 cell, primary RMK and HEp 2 cell lines. Specimens which grew virus in the conventional HEp 2 or RMK cells were further tested with neutralizing antisera (obtained from ATCC) to the 6 lowest numbered species. Non-neutralized virus isolates were identified by restriction analysis. Two monoclonal antibodies specific of other subgroups than the whole Ad41 genomic probe, detecting lower amounts of heterologous subgroup DNA relative to the quantity of Ad41 DNA detected in the same autoradiographic time interval. Various PuuI fragments were electroeluted from the p41EA plasmid to assess the specificity of isolated central sequences of the Ad41 genome as probes. PuuI fragments B, D, E, and F, extending from map units 26 to 48,48 to 59,17 to 21, and 21 to 26, respectively (Fig. 1, RE map) , all demonstrate reactions (Fig. 1, panels iii, iv, v, and vi) comparable to the parent plasmid. Isolation of segments of the EcoRI A fragment by cleavage with PuuI did not demarcate any area able to better distinguish between the DNA of Ad41 and other types. The PuuI F fragment shown in panel vi may be useful as a subgenus F specific probe. This fragment detected Ad41 DNA within one log dilution of the reaction with Ad41 DNA and distinguished between other subgroups by at least 3 log dilutions. PuuI F fragment probe has an approximately equal reactivity with enteric types and would not detect other adenovirus types unless present a t 1,000 times the concentration of Ad41 virions. Type-Specific Probe A number of fragments from different areas of the Ad41 genome were examined for their ability to differentiate between enteric adenovirus types. The reactions of these various probes are shown in a succession of panels in Figure 2 as tested against both enteric adenovirus DNA preparations on membrane spotted with DNA of species of each subgroup. In comparison to the whole Ad41 DNA probe in the uppermost panel, the EcoRI B and C fragments inserted in pGEM 32 vector have a relatively insensitive reaction with the DNA of other subgroups (panels ii and iv), corresponding with their position at the nonconserved right-hand end of the genome. The BgZII D fragment, derived from the portion of the EcoRI B fragment nearer the right terminus of the Ad41 genome, hybridized with greater relative intensity with homologous Ad41 DNA than the parent plasmid. The reaction of the BgZII D fragment with Ad40 DNA in panel iii of Figure 2 is highly equivalent in sensitivity to the homologous DNA reaction. The difference in sensitivity for Ad41 and Ad40 DNA, apparently only 2 to 4 fold, is the least of any of the Ad41 fragments tested, and the BglII D fragment was the best prospect for use as a n enteric adenovirus specific probe defined. Further attempts to distinguish an Ad41 specific probe were made by testing small restriction fragments from within the hexon gene that code for the type-specific epitopes of the capsid [Roberts et al., 19861 . The reaction of electroeluted SaZI D and Hind111 I fragment probes (Fig. 2 , panels v and vi) do not adequately distinguish Ad41 DNA from DNA of other types for use as specific probes. The reaction of a cloned Hind11 hexon fragment probe, called plasmid p41HH in panel vii of Figure 2 , varies with the different subgroup DNA prep-arations. No Ad41 specific probe was isolated by cloning or electroelution of Ad41 DNA fragments. Published Ad41 sequences were compared to Ad2 and Ad40 genes to determine exact sequences unique to Ad41. The longest stretches of unique Ad41 sequence in the available sequences were found in the hexon gene. The L1 surface epitope sequence [Roberts et al., 19861 was divided into 4 sequences of variation of 30 base pairs or more that could serve as diagnostic probes. The most suitable sequence, however, was a fifth unique stretch of 84 nucleotides from residues 1225 to 1308 of the Ad41 hexon sequence [Toogood and Hay, 19881 forming the L2 epitopic loop. This sequence was synthesized as two 40 base oligomers, designated Hex5A and Hex5B, signifying the fifth unique hexon region, with the following sequences: The sensitivity of the two probes was tested empirically by hybridization with dilutions of adenovirus, A phage, and HEp 2 DNA in conditions of increasing stringency as shown in a series of panels in Figure 3 . Hex5A has a greater content of guanine and cytosine residues and could be used in conditions of greater stringency (Fig.  3A) . Hex5B sequence is unique to Ad41. This is reflected in the loss of reactivity with Ad40 and nonhomologous DNA in reactions carried out above 35°C (Fig.  3B ). The HexEiB probe was used in evaluation of the hybridization test with clinical samples. Electron microscopy, EIA, and viral culture of stool specimens were performed routinely through the period of study. Adenovirus was detected by a t least one conventional test in 55 specimens of the 1,071 stool samples examined. Isolates were identified by neutralization or restriction analysis. All identified enteric isolates had restriction patterns of the Ad41A strain [Scott-Taylor et al., 19901 . The positive samples were spotted in random order among 200 samples on a nylon membrane for hybridization. Suspensions used in prior evaluations of 9 adenovirus positive specimens were spotted to determine whether virus in original suspensions had deteriorated. Samples 81 and 88 were from a single specimen, spotted twice to ensure reproducibility. The 200 spotted samples consisted, therefore, of 55 unique adenovirus positive specimens, 28 of which were assessed as enteric isolates, 10 duplicates, and 135 adenovirus negative specimens. Twelve of the adenovirus negative samples contained rotavirus, 8 grew enteroviruses, and small round virus particles were seen by electron microscopy in 6 and coronavirus in 1 more. The specimens reacting with the various diagnostic Table I in the order spotted on the hybridization membrane. A total of 60 of the samples, including 51 of the unique specimens, reacted with the HPII probe in the first panel of Figure 4 . The BgEII D fragment and synthetic Hex5B probes in the two panels below reacted with 26 and 24 specimens, respectively. The amount of viral DNA in the specimen could be estimated from the dilution series of control DNA below the specimens. The Ad41 genomic DNA probe and plasmid p41EC, containing the Ad41 EcoRI C fragment, hybridized with specimens containing adenoviruses other than enteric types (reactions not shown). Cloned probes p41EC and BgZII D reacted with negative specimens 122,124, and 166. Specimens 27 and 199 reacted strongly with all Ad41 DNA probes, although types Ad2 and Ad5 emerged from culture. These specimens were subsequently tested with the subgenus F-specific EIA and were found to harbour a conventional type as well as a n enteric adenovirus in concurrent infection. A sample of 5 of the 9 uncultivable specimens not identified to type was tested in the blocking assay. Preincubation with the Ad41 monoclonal antibody reduced binding to the capture antibody by between 12% and 37%. The Ad40 antibody had no effect and for the purposes of calculation of test performance all the uncultivable specimens were presumed to contain Ad41. The sensi-tivity and specificity of the diagnostic tests are compared in Table 11 . The 92.7% sensitivity of the HPII probe in hybridization, detecting 51 of 55 unique specimens, compares favourably with conventional diagnostic tests. The specificities of the genomic Ad41 DNA, p41EC, andBgZII D probes were evaluated on the detection of enteric adenovirus types and were reduced by reactions with unrelated adenovirus types or negative specimens. The synthetic DNA probe specificity was evaluated for Ad41 specimens alone. No false positives were attributed to HPII or Hex5B probes. The predictive values of the tests (Table 11) indicate that most hybridization probes had greater reliability in reporting a positive or negative test result than the conventional diagnostic methods. Hybridization demonstrated a higher sensitivity than the methods of adenovirus detection currently employed. Results indicate that hybridization could improve the efficiency of diagnosis of adenovirus infection in gastroenteritis by one and a half times or more over individual methods in routine use. The technique has great flexibility and can utilize cloned or synthetic sequences to diagnose groups or individual types of virus. Oligomeric probes can evidently form effective means of diagnosis, and selection of the appropriate shared or unique sequence can enable differentiation of groups or individual adenovirus types according to the degree of specificity required. The type-specific hexon sequences provide a means to differentiate between closely related adenoviruses by hybridization with DNA probes or neutralisation with antipeptide sera [Toogood et al., 19921 . Hybridization had several additional advantages over other diagnostic methods in enabling the detection of dual infections and allowing an estimate of the concentration of viral particles present in the specimen. Dual infections are probably not uncommon judging from the numbers of specimens that yield more than one adenovirus type upon careful culture [Brandt et al., 1986; Brown, 1985; Kidd et al., 1982; Wigand et al., 19831 , and none of the traditional methods in use are capable of identifying more than one isolate in a specimen. The intense hybridization associated with most enteric adenovirus isolates indicated that these types were excreted in greater concentration than the conventional types. The most reactive of the enteric specimens were defined from the control dilution series as present in excess of 100 ng of viral DNA in 150 pl of the 10% stool suspension spotted. Since the Ad41 genome comprises 34,600 base pairs [Scott-Taylor and Hammond, 19921 and has a molecular weight of approximately 23 x lo6, it can be calculated that there are about 2.5 x 10" molecules of the Ad41 genome per microgram of DNA. Therefore, in the 150 pl of stool suspension, containing 15 mg of stool and 0.1 pg of viral DNA, there are more than 2.5 x lo6 genomes, and there are more than 1.7 x 10l1 virus particles per gram in the most reactive stool specimens. This is in close agreement with the previous evaluation that enteric adenoviruses can be excreted in excess of 10l1 particles per gram of stool [Takiff et al., 19811 and demonstrates that hybridization can define viral concentration with some reliability. An association between virus concentration and disease is not clear at present. Examination of the relationship between viral burden and prognosis with this technique could yield significant results. Additionally, evaluation of viral concentration may be helpful in determining a critical level for defining causation of gastroenteritis by certain adenovirus types. The loss of specificity of some hybridization probes was probably due to reaction with plasmid DNA from bacterial flora, as several false-positive specimens reacted only with probes amplified in bacteria. The reaction of vector DNA with plasmids derived from alimentary flora has previously been observed [Huang and Deibel, 1988; Takiff et al., 19851 . The lack of falsepositive reaction of the HPII probe, however, demonstrates that extensive electroelution can render probes sufficiently free of plasmid DNA contamination for use with faecal specimens. It may be advisable to saturate the probe with unlabelled plasmid DNA to completely eliminate this source of false results in hybridization. The uncultivable specimens tested with the blocking assay were reduced in binding by less than the 50% critical value that identifies a n adenovirus type by the test methods. The low reduction in binding may signify that either these specimens were grossly denatured or that commercially available Ad41 monoclonal antibodies [Herrmann et al., 1987; Wood et al., 19891 are not able to react efficiently with local variant strains of Ad41 specific to Manitoba. The prevalent Ad41 strain in Manitoba was not detected by the first commercial enzyme immunoassay [Scott-Taylor et al., 19901 marketed with monoclonal antibodies developed to the prototype strain Tak of Ad41 [Herrmann et al., 19871. These observations suggest that DNA hybridization tests, less affected by the variation found in circulating strains of adenovirus types, may have more long-term efficacy than highly specific serological detection methods. 
paper_id= fff17ea9203b59fb5fcd1d7dd9caf3e4a4021200	title= The cyclophilin-inhibitor alisporivir stimulates antigen presentation thereby promoting antigen-specific CD8 + T cell activation	authors= Katharina  Esser-Nobis;Julia  Schmidt;Katja  Nitschke;Christoph  Neumann-Haefelin;Robert  Thimme;Volker  Lohmann;	abstract= Background & Aims: Cyclophilin-inhibitors have potent antiviral activity against Hepatitis C virus (HCV) and are promising candidates for broad-spectrum antiviral therapy. Cyclosporine A (CsA) acts immunosuppressive by blocking T cell activation and antigen presentation. Alisporivir, a non-immunosuppressive CsA analog in clinical development, does not inhibit T cell activation. In this study we explored the impact of alisporivir on antigen presentation. Methods: Hepatoma cells endogenously expressing the epitoperestricting major histocompatibility complex-class I (MHC-I) allele HLA-A2 and constitutively expressing a viral antigen were established to study the impact of cyclophilin-inhibitors on antigen presentation. Antigen-specific CD8 + T cell activation and MHC-I surface expression were measured to quantify antigen presentation. Results: Our work establishes a novel cell culture model to study antigen presentation in liver-derived cells. Authentic regulation of antigen presentation was ensured by the action of pro-and anti-inflammatory cytokines. Alisporivir pretreatment stimulated antigen presentation by hepatoma target cells, leading to enhancement of antigen-specific CD8 + T cell activation by 40%. Alisporivir, as well as a panel of other cyclophilin-inhibitors, induced an increase of MHC-I and beta-2 microglobulin on the surface of several cell lines. The drug neither enhanced MHC-I transcript or protein levels nor affected surface expression of other proteins or protein trafficking in general. Proteasome-inhibitors completely blocked the alisporivir-directed enhancement of surface MHC-I, suggesting an influence of the drug on peptide-availability. Conclusions: Alisporivir stimulates antigen presentation by inducing enhanced MHC-I surface expression, thereby promoting antigen-specific CD8 + T cell activation. This immunostimulatory function might further contribute to the antiviral activity of non-immunosuppressive cyclophilin-inhibitors. Ó 	body_text= Hepatitis C Virus (HCV) establishes persistence in a high rate of patients, resulting in more than 160 million chronic infections worldwide. In recent years, direct acting antivirals and host targeting antivirals have been identified, allowing new opportunities for antiviral therapy of patients chronically infected with HCV [1] . Characteristics of host targeting antivirals include their broad genotype coverage and their high barrier to resistance [1] . One such compound is the cyclophilin (Cyp)-inhibitor alisporivir, which targets the cellular protein and HCV host factor cyclophilin A (CypA), and has been in clinical development for HCV therapy [2] [3] [4] . Furthermore, due to involvement of cyclophilins in the lifecycle of a broad range of viruses like human immunodeficiency virus (HIV), Hepatitis B Virus (HBV) or several coronaviruses, alisporivir represents a promising candidate for pan-viral therapy [5] . The drug is an analog of the cyclic undecapeptide cyclosporine A (CsA), which blocks T cell activation by forming a ternary complex together with CypA and the phosphatase calcineurin [6] . In addition, CsA suppresses major histocompatibility complex-class I (MHC-I) dependent antigen presentation by a poorly defined mechanism [7] . The structure of alisporivir differs in two amino acid residues from the immunosuppressant CsA, abrogating the inhibitory effect on T cell activation [8] . However, its effect on antigen presentation has not been clarified so far. We recently established an immunological model to characterize antiviral responses of HCV-specific CD8 + T cells activated by hepatoma cells harboring a self-replicating HCV replicon [9] . While this assay served to analyze the antiviral efficiency of T cell responses, its use to gain insights into MHC-I antigen presentation was limited due to ectopic expression of MHC-I. We have now engineered liver cells endogenously expressing the epitope-restricting MHC-I allele and ectopically expressing an epitope-matched HCV protein and thereby adapted our initial model to quantify antigen presentation. Alisporivir and further non-immunosuppressive Cyp-inhibitors stimulated antigen presentation by upregulation of MHC-I surface expression, resulting in induction of enhanced antigen-specific CD8 + T cell responses. These findings point to additional therapeutic benefits for application of Cyp-inhibitors as antivirals mediated by enhanced immune responses. Cell lines of hepatic origin: Huh7_Lunet, HepG2, Huh6.1 and cell lines of nonhepatic origin: HEK 293T, HeLa, 5637, LS180, CaCo2 and A549 were cultured in Dulbeccó s modified essential medium (DMEM; Life Technologies, Darmstadt, Germany) supplemented with 2 mM L-glutamine, 0.1 mM nonessential amino acids, 100 U/ml of penicillin, 100 lg/ml of streptomycin (all Life Technologies) and 10% fetal calf serum (FCS). Huh7_Lunet cells have been described previously [10] . Huh6.1 cells are a sub-clone of the human hepatoma cell line Huh6 and were generated by curing Huh6 cells harboring a persistent GT1b Con-1 HCV replicon with CsA and daclatasvir [11, 12] . HepaRG cells were cultured in Williams E medium (Life Technologies) supplemented with 5 lg/ml of insulin, 50 lM hydrocortisone hemisuccinate (both Sigma Aldrich, Steinheim, Germany), 100 U/ml of penicillin, 100 U/ml of streptomycin and 10% FCS. NS5B 2594-2602 (NS5B EM )specific CD8 + T cell clones were generated by limited dilution after sorting of tet + CD8 + T cells as previously described [9] . All cells were incubated in a 37°C incubator at 95% humidity and 5% CO 2 . Alisporivir (Novartis, Basel, Switzerland) was dissolved to 20 mg/ml in DMSO. Detailed information on other inhibitors can be found in [13] and Supplementary materials and methods. A detailed description of the T cell co-cultivation assay has been published earlier [9] . In brief, 1 Â 10 5 T cells were co-cultured with 1 Â 10 5 Huh6.1 or HepG2 target cells in one well of a 96 well dish. Target cells were washed extensively before coculture to remove residual drug amounts. Handling of NS5B 2594 peptide (ALYDVVSKL; Biosynthan, Berlin, Germany) was described previously [14] . Upon 5 h of co-culture at an effector:target (E:T) ratio of 1:1, T cells were stained for intracellular IFN-c and CD8 surface expression and analyzed on a BD FACSCanto II (Becton Dickinson, New Jersey, USA). Data were analyzed using FlowJo Software (Tree Star, USA). For staining of surface proteins, 1 Â 10 5 cells were seeded for drug treatment and detached on day 3 after seeding (resulting in $5 Â 10 5 cells). Cells were resuspended in ice-cold PBS with 3% bovine serum albumin (PBS 3% BSA) containing a primary antibody at the individual working dilution. After 1 h incubation at 4°C, cells were washed twice and taken up in ice-cold PBS 3% BSA containing a goat-anti-mouse antibody labeled with phycoerythrin (PE) (Santa Cruz, Dallas, USA). After incubation for 1 h at 4°C in the dark, cells were washed twice, resuspended in ice-cold PBS and analyzed on a BD FACSCalibur (BD biosciences, New Jersey, USA). All washing steps were performed with icecold PBS. Data were analyzed using FlowJo Software (Tree Star, USA). Primary antibodies and respective concentrations used during this study were anti-IFN-cR1 (1 lg/ml; R&D systems, Minneapolis, Canada), anti-CD13 (1 lg/ml; abcam), anti-HLA-A ⁄ 02 (1 lg/ml; abcam, Cambridge, UK), anti-b2 microglobulin (1 lg/ml) and anti-HLA-A, B, C (W6/32; 4 lg/ml) (both Biolegend, London, UK). Isotype control antibodies were mouse IgG2bj (Biolegend) and mouse IgG1 (R&D systems, Wiesbaden, Germany). For each condition data of at least 10,000 cells were collected. Staining for intracellular IFN-c and surface CD8 was performed as described previously [9] . For statistical analysis two-tailed paired t test was performed using GraphPad Prism 5 software (GraphPad Software, Inc., La Jolla, CA). Further details on statistical tests are indicated in the figure legends; ⁄ p <0.05; ⁄⁄ p <0.01; ⁄⁄⁄ p <0.001. Transcriptome data are accessible via GSE68927. An in vitro immunological model to measure functional antigen presentation The first aim of our study was the establishment of a cell culture model allowing a quantitative assessment of functional antigen presentation by hepatocytes. To this end, we modified a previously established assay employing co-cultivation of HCVspecific HLA-A ⁄ 02-restricted CD8 + T cells and Huh7 hepatoma cells ectopically expressing the MHC-I allele HLA-A ⁄ 02 [9] . To generate a more physiological environment, we used hepatoma cell lines endogenously expressing HLA-A ⁄ 02 and stably expressing an epitope-matched HCV-NS5B protein (NS5B EM ; EM = epitope-matched) (Fig. 1A ). NS5B-derived peptides are presented on the cellular surface via HLA-A ⁄ 02, resulting in antigen-specific T cell activation during co-culture of HLA-A ⁄ 02 + NS5B EM transduced hepatoma cells with NS5B-specific CD8 + T cells and concomitant production of various effector molecules (Fig. 1A) . Importantly, this co-culture assay encompasses the whole antigen presentation process, from physiological regulation of MHC-I expression and processing of an endogenous antigen to surface presentation via MHC-I. Therefore, modifications at any step of this pathway significantly impacting on the efficiency of functional antigen presentation should be detectable as changes in CD8 + T cell activation. We tested two HLA-A ⁄ 02 + hepatoma cell lines, characterized by either high (HepG2) or low (Huh6.1) HLA-A ⁄ 02 expression levels ( Supplementary Fig. 1A ). More than 90% of CD8 + T cells were activated upon co-culture with HepG2_NS5B EM cells, which could not be further enhanced by pretreatment of HepG2 cells with IFN-c, a well-known stimulant of MHC-I antigen presentation ( Supplementary Fig. 1B ) [15] . Therefore, HepG2 cells were not suitable for our model, since any condition enhancing antigen presentation would be missed. In contrast, Huh6.1_NS5B EM cells gave rise to a significant portion of activated CD8 + T cells, which was lower than for HepG2 (Fig. 1B) , possibly due to lower MHC-I expression levels ( Supplementary Fig. 1A) . The basic CD8 + T cell activation rate in co-cultivation assays varied between 6% and 42% in different experiments (compare Figs. 1B, 2C, D), due to passage number and pre-activation status of CD8 + T cells. Still, modulatory effects of cytokines remained the same, independent from baseline activation levels (data not shown). Upon confirming cytokine receptor expression on Huh6 Con1 cells (Supplementary Table 1 ), the founder cell line of Huh6.1, we used several pro-inflammatory cytokines (IL-1b, IL-6, TNF-a), antiinflammatory cytokines (TGF-b and IL-4) and type I, II and III interferons (IFN-a, IFN-c, IFN-k) to evaluate physiological regulation of antigen presentation in our system and to define its dynamical range. Pretreatment of Huh6.1 cells with IL-1b, IL-6 and TNF-a led to an enhancement of CD8 + T cell activation, while IL-4 and TGF-b, as expected, did not have a stimulating effect (Fig. 1B) . However, type I and II IFNs, well-known stimulants of antigen presentation [15] as well as a type III IFN and IL-27, which was reported to have IFN-c-like effects on hepatocytes [16] stimulated functional antigen presentation in Huh6.1 cells even stronger (Fig. 1B) . Furthermore, alterations in HLA-A ⁄ 02 surface expression of Huh6.1 target cells upon treatment with different cytokines correlated with changes in their antigen presentation potency (Fig. 1C) . In conclusion, endogenously processed peptides were presented by Huh6.1 cells and gave rise to an HLA-A ⁄ 02-restricted antigen-specific CD8 + T cell activation. HLA-A ⁄ 02 surface expression of Huh6.1 cells allowed physiological modulation of functional antigen presentation by cytokines, likely reflecting low MHC-I expression levels in human liver [17] . Overall, the Huh6.1 model therefore appeared suitable to detect immunomodulatory effects on antigen presentation. While CsA and FK506 have been reported to block T cell activation by inhibition of calcineurin and to antagonize antigen presentation [6, 7] , the CsA analog alisporivir does not suppress T cell activation [8] . However, whether alisporivir impairs antigen presentation, has not been investigated ( Fig. 2A) . Therefore, we pretreated Huh6. [9] . Hepatoma target cells endogenously expressing HLA-A ⁄ 02 and stably expressing an epitope-matched HCV NS5B protein (NS5B EM ) were co-cultured with HLA-A ⁄ 02-restricted NS5B EM -specific CD8 + T cell clones. Antigen-specific CD8 + T cell activation induced by binding of the T cell receptor (TCR) to the NS5B EM -peptide-MHC-I complex stimulates IFN-cproduction which is measured as indirect readout for the antigen presentation capacity of target cells. (B) Various cytokines stimulate antigen presentation by Huh6.1 cells as measured by a co-cultivation assay. Huh6.1 cells expressing NS5B EM , were pretreated with the indicated cytokines and co-cultured with CD8 + T cells. CD8 + T cells were stained for intracellular IFN-c and surface CD8. The upper right quadrant includes the percentage of activated IFN-c-producing CD8 + T cells. Depicted are results of one representative experiment (n = 3). (C) Cytokines which stimulate antigen presentation enhance MHC-I surface expression. Huh6.1 cells were treated with the indicated cytokines and stained for HLA-A ⁄ 02. Data are presented as histograms. Shaded area, IgG2bj isotype control; Diagrams on the right present the mean fold change of geometric mean fluorescence intensities (GeoMFIs) of two independent experiments (n = 2). ER, endoplasmic reticulum; HLA, human leukocyte antigen; b2m, b-2 microglobulin; TAP, transporter associated with antigen processing; TCR, T cell receptor. antigen presentation. Before co-culture with HCV-specific CD8 + T cells, target cells were detached and washed extensively to avoid drug carryover into co-culture (Fig. 2B) . IFN-c pretreatment of Huh6.1 cells and preloading of T cells with a NS5B 2594 peptide served as positive controls [14, 15] (Fig. 2C ), both resulting in strong activation of CD8 + T cells. As expected, pretreatment with CsA abolished antigen presentation by Huh6.1 cells (Fig. 2C) . In striking contrast, alisporivir pretreatment of Huh6.1 target cells dose-dependently increased the percentage of activated CD8 + T cells, suggesting a stimulation of the antigen presentation capacity of Huh6.1 by alisporivir (Fig. 2C ). We used 2 or 4 lM alisporivir in all subsequent experiments since this concentration range was regarded as physiologically relevant, based on its efficacy to inhibit HCV replication in vitro and in vivo [18, 19] . Next, Huh6.1 cells were pretreated for different times with alisporivir to explore the kinetics of immunostimulation. A 24 h pretreatment only had minor effects, whereas 48 h after treatment, the increase of CD8 + T cell activation reached a plateau and remained stable for at least 6 days (Fig. 2D) , arguing for a slow and accumulating process increasing antigen presentation by alisporivir. The percentage of activated CD8 + T cells was enhanced by approximately 40% upon alisporivir treatment during all experiments (compare Fig. 2C, D) . In summary, we demonstrated that alisporivir has an immunostimulatory effect on antigen presentation by human hepatoma cells, possibly supporting its antiviral activity in vivo. Alisporivir enhances MHC-I surface expression on cell lines of hepatic and non-hepatic origin So far, we detected an immunostimulatory effect of alisporivir by using CD8 + T cell activation as an indirect readout for antigen presentation. Assuming that the amount of MHC-I/peptide on the cell surface is a limiting factor of antigen-specific T cell activation, we measured surface HLA-A ⁄ 02 on Huh6.1 cells after different times of alisporivir treatment (Fig. 3A, B) . Indeed, alisporivir slowly and continuously enhanced HLA-A ⁄ 02 and beta-2-microglobulin (b2m) surface expression over time (Fig. 3A, B) . To assess whether this effect was restricted to Huh6.1 cells we analyzed further liver-derived cell lines (Fig. 3C ). Since these cell lines, apart from HepG2, endogenously expressed HLA alleles differing from HLA-A ⁄ 02 we measured their b2m surface expression, which was indeed significantly upregulated in Huh6.1, Huh7_Lunet, HepG2 and HepaRG after 48 h of alisporivir treatment compared to DMSO controls ( Fig. 3C-E) . Analogous results were obtained with an MHC-I specific antibody (Supplementary Fig. 2A, B) . Furthermore, alisporivir treatment significantly enhanced MHC-I surface expression on lung epithelial (A549) cells and a similar trend was observed for further cell lines of non-hepatic origin ( Supplementary Fig. 2C, D) . In summary, upregulation of MHC-I surface expression by alisporivir was not restricted to a particular cell line, but found in cells of various origins. Alisporivir does not affect MHC-I mRNA or protein levels nor generally impact on protein secretion We next wanted to address, whether the alisporivir-induced enhancement of MHC-I surface expression was due to changes in MHC-I mRNA and protein expression levels. While IFN-c, as expected, increased mRNA expression of both HLA-B and b2m, no effect was detected for alisporivir ( Fig. 4A; Supplementary  Fig. 3A ). In line with these data, MHC-I protein levels in Huh6.1 or HepaRG cells were not altered by alisporivir, in contrast to IFN-c treatment (Fig. 4B ). In addition, we found a slight but arguing against a mechanism involving regulation of HLA gene expression (Fig. 4C ). An enhancement of MHC-I surface expression by alisporivir could also be caused by a general effect on protein surface expression, in line with a previous study reporting an upregulation of pathways associated with protein trafficking by CsA analog NIM-811 [20] . However, neither surface expression of IFN-c-receptor chain 1 (IFN-cR1) , a type I transmembrane protein like MHC-I a-chain nor CD13, which is like MHC-I cotranslationally glycosylated inside the endoplasmic reticulum and transported through the Golgi to the cell surface [21] , was affected by alisporivir treatment (Fig. 4D) . A general effect on cellular transport was further excluded, because alisporivir did not affect Gaussia-luciferase (GLuc) secretion, transferrin Fig. 4) . In summary, our data suggested a specific enhancement of MHC-I and b2m surface expression upon alisporivir treatment, which was neither caused by an induction of basal MHC-I expression nor by general effects on cellular protein trafficking. We also found no evidence for a strong impact of alisporivir on MHC-I internalization or its passage through the Golgi. We were wondering if alisporivir induced effects depended on peptide availability, since peptide amount is a limiting factor for MHC-I antigen presentation [22] . Indeed, we observed a complete inhibition of MHC-I upregulation if alisporivir was combined with proteasome-inhibitors (MG-132 or PS-341), whereas basal MHC-I surface expression was not affected under these conditions (Fig. 5A) . These results argued for enhanced surface expression primarily of peptide-loaded MHC-I and for a critical role of peptide-availability, suggesting peptide-generation as potential target of alisporivir. Interestingly, a recent study observed upregulation of surface MHC-I upon p53 activation mediated by enhanced expression of endoplasmic reticulum aminopeptidase 1 (ERAP1) and transporter associated with antigen processing 1 (TAP1) [23] . We indeed detected a minor but insignificant upregulation of ERAP1 mRNA by alisporivir (Fig. 5B ) but found no changes in ERAP1 protein levels or p53 activation status (Fig. 5C-E and data not shown) . In conclusion, alisporivir led to upregulation of peptideloaded MHC-I complexes, possibly due to enhanced peptideavailability by a yet to be defined mechanism. In search for the molecular target of alisporivir we first focused on CypA and CypB, which are the most common and best studied of the Cyp family [20] and like other members, show comparable expression in Huh6.1 and Huh7 cells (Supplementary Table 2 ). However, simultaneous knockdown of CypA and B did not significantly affect MHC-I surface expression of untreated or alisporivir treated Huh6.1 cells (Fig. 6A -C and data not shown), suggesting that these most abundant Cyps were not the main determinants mediating surface accumulation of MHC-I. Cyp-inhibitors cover a broad range of cellular targets, including adenosine triphosphate binding cassette (ABC)-transporters, Na + -taurocholate cotransporting polypeptide (SLC10A1, NTCP) or calcineurin [24, 25] . Therefore, we tested a panel of compounds with different inhibition profiles [13] for their effect on MHC-I surface expression, using non-cytotoxic concentrations ( Supplementary  Fig. 5C ). CsA and the structurally unrelated Cyp-inhibitor sanglifehrin A (SFA) stimulated a significant increase of HLA-A ⁄ 02 and MHC-I surface expression to an extent similar to alisporivir (Fig. 6D, E; Supplementary Fig. 5A, B) , arguing for Cypinhibition as underlying cause. In agreement with this assumption, ABC-transporter inhibitors reversan and piperine both did not strongly impact on the amount of surface HLA-A ⁄ 02 or MHC-I, nor did FK506, which inhibits a different group of peptidyl-prolyl cis trans isomerases (Fig. 6D, E; Supplementary  Fig. 5A, B) [5] . In contrast, the cyclic undecapeptides PSC833 and cyclosporine H (CsH), both with reduced affinity at least for the most common Cyps [20, 26] induced a significant upregulation of HLA-A ⁄ 02 and MHC-I surface expression to a similar degree as alisporivir (Fig. 6D, E; Supplementary Fig. 5A, B) . In summary, cyclophilins are most likely the targets responsible for upregulation of MHC-I surface expression. For our study of Cyp-inhibitor mediated effects on antigen presentation, we modified our previously developed immunological model [9] by using Huh6.1 cells, intrinsically expressing HLA-A ⁄ 02 and with constitutive ectopic expression of HCV-NS5B. ⁄ 02 low ) activated CD8 + T cells and provided a dynamic range for up and downregulatory effects. In addition, pro-inflammatory cytokines and IFNs enhanced MHC-I surface expression and stimulation of CD8 + T cells during co-culture, arguing for physiological regulation of functional antigen presentation in Huh6.1 cells. Using this model, we were able to validate the proposed IFN-c-like effect of IL-27 on hepatocytes [16] . CsA has been shown to inhibit antigen presentation [7] , however, we now found that the CsA analog alisporivir exerted an immunostimulatory effect, mediated by slow enhancement of MHC-I surface expression over time, accompanied by a higher percentage of activated HCV-specific CD8 + T cells. Interestingly, CsA also increased MHC-I surface expression, but still entirely blocked functional antigen presentation to T cells by a yet to be defined mechanism [7] . Upregulation of MHC-I surface expression was confirmed in a panel of cell lines of diverse origins and not due to changes in total MHC-I expression levels. Furthermore, alisporivir specifically upregulated MHC-I since neither expression of other surface proteins (CD13, IFN-cR1) nor overall protein secretion was modified. We also could not find evidence for an impact of alisporivir on MHC-I trafficking, endocytosis or degradation, although we cannot exclude that we missed subtle effects in MHC-I transport dynamics, due to limitations of available assays. Still, we favor a mechanism mediated by enhancing peptide amounts available for MHC-I loading, since the stimulatory effect of alisporivir, but not regular MHC-I surface expression, was completely abrogated by proteasome-inhibitors. We hypothesize, that inhibition of the chaperone-like functions of Cyps by alisporivir might enhance peptide pools available for presentation on MHC-I by increasing the amount of unfolded proteins [24] . Still, further studies are required to clarify the mechanism how alisporivir stimulates MHC-I surface expression. Cyps are a group of peptidyl-prolyl cis trans isomerases, including at least 17 family members in humans, most of which are poorly studied regarding their inhibition by Cyp-inhibitors and their molecular function [20, 27] . Most Cyp-inhibitors are not restricted to Cyps, but also act against additional targets, like ABC-transporters or Calcineurin [24] . An involvement of Cyps in MHC-I surface upregulation was supported by the fact that all Cyp-inhibitors tested increased MHC-I surface expression, in contrast to FK506, an inhibitor of a different class of peptidyl-prolyl cis trans isomerases [5] and two ABC-transporter inhibitors, reversan and piperine [13] . However, PSC833 and CsH, CsA analogs with reduced affinity for the best studied Cyps [20, 26] induced the same phenotype as observed for CsA, alisporivir and SFA, whereas siRNA-mediated knockdown of CypA and CypB, the most common Cyps, did not affect MHC-I surface expression, as well as individual knockdown of several other Cyps (data not shown). Altogether, our results either point to a potential role of less studied Cyps, which can still be blocked by all compounds [27] or to a redundant role of many Cyps. Due to the variety of potential targets, the possible involvement of several Cyps with analogous functions, we did not attempt to further define specific Cyps potentially involved in regulation of MHC-I surface expression. For the same reasons and due to high abundance of many Cyps, we neither carried out Cyp-overexpression. Alisporivir, as well as other non-immunosuppressive Cyp-inhibitors like SCY-635 have been in clinical development for antiviral therapy of chronically HCV-infected patients [2] [3] [4] and represent potential broad-spectrum antiviral drugs since many viruses depend on CypA as host factor [5, 28] . Based on our in vitro data, we speculate that alisporivir exerts an immunostimulatory effect on hepatocyte antigen presentation during therapy of chronically HCV-infected patients which might complement its antiviral properties by supporting an efficient adaptive immune response. Stimulation of the antigen presentation capacity of hepatocytes, which normally express only low levels of MHC-I [17] might promote recognition of infected cells by CD8 + T cells and finally contribute to their clearance. As previously reported, IFN-free therapy of HCV-infected patients with direct acting antivirals, but not IFN-based treatment, has the potential, to restore functional T cell responses presumably by lowering high antigen levels, which may support T cell exhaustion [29] . Along these lines, it seems plausible, that alisporivir, if applied as an IFNfree regimen, could restore T cell responses even more efficiently and additionally promote clearance of infected cells by CD8 + T cells via enhancing functional antigen presentation. Indeed, immunomodulatory activities on peripheral blood mononuclear cells of HCV-infected patients have been observed in clinical trials of the Cyp-inhibitor SCY-635 [30] . To gain insights into the contribution of these immunostimulatory effects of Cyp-inhibitors to the cure of HCV infections, future clinical studies should also monitor the development of T cell responses during therapy. Alternatively, testing the drug in suitable animal models allowing thorough quantification of T cell responses could further shed light on the impact of alisporivir on adaptive immune responses in various organs in vivo. In conclusion, we could demonstrate that non-immunosuppressive Cyp-inhibitors, in striking contrast to CsA, increase functional antigen presentation by upregulation of MHC-I surface expression. Importantly, the enhanced antigen presentation upon alisporivir treatment might also support clearance of other viruses which are sensitive to Cyp-inhibitors. This project was funded by grants from the Deutsche Forschungsgemeinschaft (VL: FOR1202, TP3, RT: FOR1202 TP2; CNH: FOR1202 TP8). 
paper_id= fff188c8609726e392fd79ae9f5bb37d6f7a09ca	title= COVID-19 Fatality Rate Classification using Synthetic Minority Oversampling Technique (SMOTE) for Imbalance Class	authors= Timothy  Oladunni;Justin  Stephan;Lala Aicha Coulibaly;	abstract= SARS-Cov-2 is not to be introduced anymore. The global pandemic that originated more than a year ago in Wuhan, China has claimed thousands of lives. Since the arrival of this plague, face mask has become part of our dressing code. The focus of this study is to design, develop and evaluate a COVID-19 fatality rate classifier at the county level. The proposed model predicts fatality rate as low, moderate, or high. This will help government and decision makers to improve mitigation strategy and provide measures to reduce the spread of the disease. Tourists and travelers will also find the work useful in planning of trips. Dataset used in the experiment contained imbalanced fatality levels. Therefore, class imbalance was offset using SMOTE. Evaluation of the proposed model was based on precision, F1 score, accuracy, and ROC curve. Five learning algorithms were trained and evaluated. Experimental results showed the Bagging model has the best performance. 	body_text= The world has witnessed pandemics such as Ebola [1] , Spanish flu [2] , yellow fever [3] , HIV-AIDS [4] , smallpox [5] etc. Towards the end of 2019, the world witnessed another pandemic that has infected millions of people. Thousands of lives have been lost. As of today, May 11, 2021 , the United States tops coronavirus cases, follow closely by India and Brazil with records of 32 751 021, 22 992 517 and 15 209 990, respectively. On mortality count, United States, Brazil, and India recorded 582 355, 423 229 and 249 992, respectively [6] . There is no consensus about the origin of this pandemic, however, most scientists believe that the virus originated from Wuhan China and was transferred from animals to humans [7] . It later found its way from China into other parts of the world. For more than a year, COVID-19 has interrupted civilization. Humanity was forced to face the reality of a new plague. Before the introduction of vaccines, various governments and decision makers introduced different measures and policies to reduce and combat the spread of the disease: Mask mandate was enforced. Social distancing was imposed. Washing of hands was encouraged. Large gathering was outlawed. Some businesses were forced to close. Travel ban was imposed. Apart from the government regulations, the fear of COVID-19 affected our daily lifestyles: Airports were deserted because planes refused to fly. Online learning became the new normal for students. Conference meetings were moved online. Religious worship became video conferencing. Political rallies were not welcomed. Parks and recreation centers were deserted. No one was not spared the wrath of COVID-19. Scientists have carried out different studies in combatting the spread of the disease and improve mitigation strategies. Some of the studies involved the exploration of artificial intelligence. For example, the capability of machine learning algorithms to learn from dataset for pattern recognition and knowledge discovery was employed. In [8] , the authors aimed to predict COVID-19 fatality rate in India using Machine Learning Models. Linear Regression and Polynomial Regression learning algorithms were trained and evaluated. Per the outcome of the study, the polynomial regression model gave more accurate results. Since the study was limited to India, in the future, the authors plan on extending the proposed models to be trained on datasets from other countries. In another study, authors [9] used big data analytics to track the spread of the coronavirus. Comparison of tools in big data using machine learning algorithm was a part of their study. Machine learning techniques such as Linear Regression, Polynomial Regression and SVM algorithm was employed. Experimental results were compared using python and Tableau for visualization. According to the study, Linear Regression produced the most accurate results. Meng et.al [10] used dataset of 366 critical Covid 19 cases including patients who died and those who survived within 14 days after being labelled as high-risk patients. Convolutional neural network was trained and evaluated to predict the probability of covid-19 patients belongings to high-risk and low-risk group. Area under the curve and other evaluation techniques were used to evaluate their models. Evaluation result demonstrated that the proposed model performed well in predicting high risk patient. While there are different studies on COVID-19 prediction, there seems to be a paucity in machine learning literatures on the algorithmic classification of fatality rate at the county level; specifically, to low, moderate, or high fatality rate. Therefore, the focus of this study is to design, develop and evaluate a county level classification of COVID-19 fatality rate using learning algorithms. The proposed model will classify fatality rate at the county level into low, moderate, or high. We believe that all stakeholders should have an answer to the question; what is the fatality rate class of this county? A satisfactory answer will help decision makers in crafting policies on effective mitigation strategies to combat and reduce the spread of the disease. It will also help in the prioritization of vaccine to the most affected counties. Travelers and tourists would have an idea about a COVID-19 implication of a county they want to visit. It will also help . CC-BY-NC 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review) The copyright holder for this preprint this version posted May 24, 2021. ; residents of each county to know the level of COVID-19 in their county. This paper is organized as follow: Section II describes the methodology for the study. Section III evaluates the result of our experiments, while section IV compares the results. Sections V, VI and VII discuss the conclusion, implication of the study and future work, respectively. We highlighted the limitation of the study in section VII and acknowledged our funding source in section IX. Data Dataset was collected from the John Hopkins University COVID-19 repository [6] . The data comprises of COVID-19 fatality rates information of 3 006 counties of the USA. 171 features were recorded. Information includes city name, state name, fatality rate, age ranges, unemployment rate, race, days etc. Redundant and irrelevant features were removed during the data cleaning phase. We aimed to classify Fatality rate using five learning algorithms. Experimental flowchart is shown in figure 1 . • Drop insignificant and redundant features. • Discretize. • Normalize. • Apply SMOTE. • Split data into train and test dataset. Fatality Rate is a continuous variable to be classified into low, moderate, and high fatalities. Therefore, a code was written to convert Fatality Rate to categorical variable for classification purposes. The 'FatalityRa' values were converted to classes using the following function: Where x is the fatality rate. Labels were created for each example in the dataset using the 'FatalityRa' column. Since this is a classification problem, each of the three labels become the classes: low, moderate, and high. Imbalance classes are unique cases in machine learning where classes are disproportionally represented [11] . Dividing our datasets into the three classes, Low: (1054,), moderate: (807,), and High: (1281,). The ratio of moderate is much lower than the high and low, suggesting that fatality rate in the USA during the pandemic was at both extremes; either low or high. It has been shown that class imbalance generally produces misleading classification accuracy [12] . Since the challenge of imbalance class is a major concern in most predictive modelling, some strategies have been developed to reduce its impact. Some of the strategies include 1) collection of more dataset to increase the minority class, 2) change performance metrics for evaluation, 3) apply resampling methodology to the dataset, 4) generate synthetic datasets to augment the minority classes, 5) use ensemble learning algorithms, 6) penalize the models and, 7) apply anomaly detection. Each of these strategies have advantages and disadvantages [13] . For this project, we will explore the generation of synthetic datasets methodology. Scientists and researchers have adopted the use of SMOTE to improve the performance of imbalanced classes. For example, Andrew et.al., implemented SMOTE to improve sentiment analysis of written and spoken language. The proposed model was to predict a person's expression, emotions, and attitudes [14] . Nuanwan et.al., proposed a Random Forest with SMOTE to improve the predictive . CC-BY-NC 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint this version posted May 24, 2021. ; capability of online comments on cooking video clips. Per the result of the experiment, the proposed model had a 95% FI score [15] . A SMOTE algorithm has the following sequence: i. Select a random sample of the m minority class examples. ii. Chose the k nearest neighbors of the sample in i. iii. Create synthetic examples from the randomly selected points. iv. Update the minority class with the result of iii. Dataset was normalized to reduce the impact of large features. It helps to create a level playing field such that each feature has an equal contribution to the model. Dataset are rescaled to a range of 0 to 1 or -1 to +1. A normalized data improves generalizability [16] . For this study a Min-Max Normalization (MMN) methodology was used. It is mathematically represented as: Where the current value V is transformed into the new value V' . maxV and minV are the maximum and minimum values of each column respectively. In this study, supervised machine learning approach was used to train and evaluate our models. In supervised learning, labels were shown to the learning algorithm during the training phase. The learning algorithm is expected to learn the mapping of patterns in the datasets from the explanatory variables to the labels. At the testing phase, the learning algorithms are to make predictions based on the learned patterns [17] . We experimented with the following learning algorithms: K-Nearest Neighbor (KNN), Random Forest, Support Vector Machine (SVM), Logistic Regression, Bagging and Boosting. Dataset was divided into 70:30 for training and testing, respectively. Behind the scene, KNN algorithm calculate distance between training data points and a testing datapoints. The process of classifying elements in a set goes as follows: In a set of elements S, the algorithm find the most similar K training status for a given set. Each status class will be given a value that represents similarity sum between these training elements and the testing elements. The statistical value of each class is obtained with K elements and the scores are sorted. Assuming k is the nearest number with n numbers of example set, KNN follows this steps: • Step 1: Chose a value of K (initialization) •  Where n is the number of observations. Random Forest is a supervised machine learning algorithm. It builds forest that are ensemble of decision trees. It generates multiple decision trees by randomly selecting samples and features. The classification results of Random Forest Algorithm of multiple trees are based on the idea that minority is subordinate to the majority. Random forest is most of the time more effective than single trees. An advantage of this algorithm is that it uses a combination of learning models to increase accuracy. Compared to a single tree, it has been shown that Random forest is more effective in reducing errors [18] . Its ability to decorrelate trees gives it an edge in reducing overfitting and handling of inbalanced classes. It also has the capability of determining features importance. This algorithm can be used for association, regression, as well as classification problem. Formula: It first perform entropy on the training and testing dataset.  Bagging is an aggregation or ensemble trees for reducing the variance of decision trees. Given n predictive variables Z1 to Zn, each with a variance of σ 2 , the variance of the mean is given as σ 2 /n. Suggesting an improvement in the predictive capability since the variance is reduced. The SVM is a learning algorithm that improves its predictive capability by enlarging its feature space. Its use of kernels enables it to accommodate a non-linear boundary. This arrangement makes it suitable as a non-linear classifier. Given two r vectors a and b. their inner product is defined as; . CC-BY-NC 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint this version posted May 24, 2021. ; https://doi.org/10.1101/2021.05.20.21257539 doi: medRxiv preprint 〈 , 〉 = ∑ =1 (6) For two of observations xi, and xi', their inner products is defined as: The support vector classifier can be expressed as: are the initial parameter and estimated parameters respectively for i to n number of training observations. Logistic Regression Algorithm is supervised machine learning algorithm that can be used for classification problem was on probability concept. The algorithm designate observation to discrete values of classes set. It uses the logistic sigmoid function. With logistic sigmoid the cost function only varies between 0 and 1. The steps for Logistic Regression Algorithm are process are: • Step 1: Assign value of 0 to each coefficient and calculate probability of the first training instance. • Step 2: Calculate new coefficient value using update equation. • Step 3: Repeat the process for all training instance. Logistic regression can be expressed as Where: s(z) = output between 0 and 1 z = input to the function e = base of natural log The performance of the learning algorithms was evaluated using class precision, F1 score, accuracy, and ROC curve. Precision is defined as the ratio of true positive to true positive plus the false negative. This can be represented mathematically as: B) F1 Score F1 provides s a metric to measure the balance between precision and recall. It is the harmonic mean of the two metrics. This harmonic mean is a constrain on extreme values. Mathematically, F1 score is represented as: Accuracy is the ratio of correct samples predictions to total number of all predictions and shows the general model's performance in terms of correct classification. It is represented mathematically as: The performance of the models was visualized using the ROC curve. The ROC curve shows the True Positive Rate (TPR) and False Positive Rate (FPR) plotted on the y-axis and x-axis respectively. TPR is defined as: FPR is defined as: IV. RESULTS Using the above metrics, the result of the experiment is shown in tables 1 to 3. CONCLUSION COVID-19 pandemic has caused major distraction to the society since its outbreak in 2019. Millions of people have been infected and thousands of lives have been lost. Therefore, in this study, we have designed, developed, and evaluated a COVID-19 fatality rate classifier. Dataset was obtained from the John Hopkins University repository. Classification was based on low, moderate, or high. Imbalanced classes were offset using SMOTE. After addressing the class imbalance problem, the results showed the effectiveness of our strategy. Implication of the Study This study has the following implications: a. The bagging model has the highest performance based on class precision, F1 score and accuracy. b. Combining SMOTE with ensemble learning algorithms proved to be the most effective strategy. As shown in our result, Bagging and Random Forest came first and second, respectively. c. The lowest performing models were SVM and logistic regression with the latter proving to be the worse of the two. d. Improving the performances of these models may require further adjustments of the hyperparameter values. e. The high accuracy and precision of the Bagging model suggests that it is has the capability of classifying COVID-19 fatality rates into low, moderate, or high. In the future we plan to do the following: a. Increase classes to 5: very low, low, moderate, high, and very high fatality levels. b. Experiment with neural networks with the hope of improving the classification accuracy of the model. c. Apply the proposed model to other epidemiological datasets. Dataset was based on 3 006 counties in the United States. Therefore, number of counties in other countries may have significant effect on the accuracy of the model. This work is funded by the National Science Foundation grant number 2032345. 
paper_id= fff1bd26773f9da44db7f424db14ca5957439e7d	title= Excluded futures: the continuity bias in scenario assessments	authors= Paul  Raskin;Rob  Swart;	abstract= Global scenario assessments in support of climate, biodiversity, energy and other international policy deliberations tend to focus on a narrow bandwidth of possibilities: futures that unfold gradually from current patterns and trends. This "continuity bias" downplays the real risks (and opportunities) of structural discontinuity in the evolution of the global social-ecological system. The inclination to focus on mathematically tractable representations and conventional futures preferred by decision-makers is understandable, but constrains the scientific imagination and the scope of policy guidance. Earlier studies spotlighted discontinuous global futures, thereby revealing a broader spectrum of possibilities and repertoire of actions than found in contemporary scenario analysis. The paper revisits three types of futures introduced 25 years ago; examines three truths they convey about the contemporary moment; and points to three courses of action they suggest. Contemporary assessments centre on incrementally changing Conventional Worlds, yet varieties of global disruption (Barbarization) and progressive transformation (Great Transition) remain plausible alternatives. Corresponding to this triad, three synergistic action prongs-reform (incremental policies), remediation (emergency preparedness and prevention), and redesign (deep cultural and institutional change)-come into focus. Recovering a comprehensive perspective on the global possible would reinvigorate debate on the kind of transformation needed, broaden the action agenda, and stimulate innovative research for illuminating our indeterminate future. The COVID-19 pandemic, a concrete illustration of historical discontinuity, underscores the critical importance of emphasizing nonconventional futures in policy assessments. 	body_text= The emergent interdependent global system stands as a key feature of our historical moment. Far-flung forcesenvironmental, economic, cultural, technological, political-are binding us together in a single community of fate. At the same time, climate change, social fissures, and other powerful stressors are eroding social-ecological resilience as we drift toward perilous thresholds of instability and discontinuity [1] [2] [3] . These unprecedented conditions demand foresight on the broad range of futures that might materialize. Yet, international policy negotiations, and the scientific assessments that support them, have relied on a narrow bandwidth of scenarios that unfold gradually from current patterns and trends. This "continuity bias" downplays the real possibility of structural discontinuity in the evolution of the global social-ecological system. The COVID-19 pandemic is vividly illustrating one type of discontinuity. The inclination for assessments to focus on mathematically tractable representations and conventional futures preferred by decision-makers is understandable, but constrains the scientific imagination and the scope of policy guidance. Correspondingly, policies to address urgent environmental and social problems, such as climate change, biodiversity loss, and income inequality, focus on incremental nudges to socio-economic and environmental patterns in more sustainable directions. Lessons drawn from a quarter century of visionary global scenario, paired with observations on how the world has actually unfolded, can enrich the discourse on ways to enable deep transformation and avoid collapse. Calls for systemic change have grown more urgent, but have a long history. The World Commission on Environment and Development [4] irrevocably etched the challenge of long-term sustainability onto the international policy agenda. In its wake, the "problem of the future" [5] drew the attention of analysts, visionaries, and activists to core existential questions: Where are we headed? Where do we want to go? How do we get there? Although the future cannot be predicted, alternative narrative and quantitative global scenarios-plausible stories about how world history might unfold in the coming decades-laid the foundation for addressing these questions. Responding to the challenge, a multidisciplinary, international team of natural and social scientists formed the Global Scenario Group (GSG) in 1995. The GSG organized a wide range of possible futures into three broad paths: Conventional Worlds, Barbarization, and Great Transitions [6] . These scenario categories reflect archetypal social visions-continuity, degradation, transformation-with deep roots in the history of ideas. Each scenario has two variants. Conventional Worlds assume the continuity and spread of dominant values and socioeconomic patterns, driven by neoliberal policies (Market Forces variation) or, alternatively, by sustainability policies (Policy Reform). In Barbarization scenarios, Conventional Worlds crises spiral out of control, leading to an authoritarian future (Fortress World) or outright collapse (Breakdown). By contrast, Great Transitions envision responses to systemic crises of Conventional Worlds that bring forth enriched socio-economic forms, such as the autarkic Eco-Communalism variation or as a revitalized global civilization (New Sustainability Paradigm). For short descriptions and visual impressions of each scenario, please see Fig. 1 . This framework has been adopted in scores of scenario-based research studies [7, 8] . However, mainstream policy assessments, e.g., in the context of climate change, biodiversity loss and energy futures, have downplayed the possibility of collapse or structural reorganization [9, 10] , thereby painting pictures of the future that generally remain within the Conventional Worlds range of possibilities,: the "continuity bias" (cf. Figure 1 ). Some recent analyses refer to transformational policies, but the scenarios themselves remain firmly within a Policy Reform framework [11, 12] . For climate change, high greenhouse gas emissions scenarios have been included in some analyses [13] , but not the implications for the stability of global economic and natural systems. Influential policy assessments, by failing to foreground discontinuous trajectories, lack scientific rigor and imagination. The consequences are to obscure real risks, policy opportunities, and unconventional interventions. The continuity bias may be due, at least in part, to political pressure on analysts to conform findings to the narrow outcomes acceptable to decision-makers [14] . The bias resides, as well, within the scientific discourse itself, where continuity is baked into prominent economic-environmental projection models calibrated to gradually unfolding historic trends and patterns [15] . The inadequacy of applying such mechanistic techniques to deeply uncertain global futures, akin to applying Newtonian physics to quantum phenomena, highlights the need for basic methodological innovation. For the sake of sound science, effectual policy, and better public understanding, the time is long overdue for overcoming political constraints and transcending modelling deficiencies in order to highlight the full spectrum of the global possible, from catastrophic collapse to civilizational shift. Which future are we living in? Now, a quarter of a century since they were conceived, which of the GSG's scenarios are we living in? A scan of the heterogeneous world scene reveals the answer: all of them. Global society today comprises a mosaic of Conventional Worlds, Barbarization, and Great Transition tendencies in proportions that vary across space and time [16] . Dogmatic neoliberal policy and faith in technological solutions (Market Forces) remain pervasive. At the same time, Policy Reform has emerged in widely varying degrees, e.g., in the United Nation's Sustainable Development Goals and the Paris Agreement, and in multifarious efforts to tame the unwelcome social and environmental consequences of unbridled markets at local, national, and regional levels. Meanwhile, multiple vectors of disruption-among them ecological disturbance, financial instability, socioeconomic disparity, pandemics, and technological change-diminish social-ecological resilience, heighten migration, and unleash reactive forces. Harbingers of Barbarization lurk in rising xenophobia, chauvinism, and fundamentalism. The contemporary wave of authoritarianism could be precursor to a Fortress World, while regional chaos, conflict, state failure, and environmental calamity might presage the apocalyptic vision of Breakdown. By contrast, the same factors driving the crisis also incubate a rising cosmopolitan and ecological consciousness, antecedents of a potential Great Transition. This shift flourishes now in myriad social and environmental movements, civil society campaigns, and small-scale social experiments, all promoting cooperative economies, humane and diverse cultures, and revitalized ecosystems. This rapidly changing melange of social forms is characteristic of a complex system approaching thresholds of systemic instability. Some observers, peering through narrow philosophical perspectives, reduce world complexity to simple truths. Celebrants of Conventional Worlds amass evidence of a world growing safer, healthier, richer [17] , trusting in technological and economic responses to adequately counter the emergent crises of the growth-oriented development paradigm. Doomsayers attuned to the instabilities and inequalities that herald Barbarization warn that the end is near [18] . Paradoxically, both extremes have a case: average wealth and life expectancy have indeed risen, but so have income disparity and social fissures; local environmental remediation has transpired but macro-instabilities deepen the risk of structural rupture in biospheric processes. Transcending these polarities, novel conditions-global interdependence, shared risk, new technology-could forge another truth: the advent of a diverse transnational cultural and social movement for a Great Transition to a liveable and just future [19] . The world today evolves as a complex mixture of Conventional Worlds, Barbarization, and Great Transition tendencies. These three truths suggest three concurrent action prongs expanding on the current focus on gradual policy change: reform (incremental policy), remediation (emergency management), and redesign (system transformation). The reform prong resonates with dominant policy paradigms seeking to ease social-ecological stress, such as cautious efforts to control greenhouse gas emissions. Unfortunately, conventional institutions, notably the state-centric international order and corporate dominated political economy, appear profoundly ill-equipped to meet the challenge of deep reform. The most promising efforts, such as the Paris Agreement on climate change and the United Nations Sustainable Development Goals, are steps in the right direction, but not the leap forward now needed. Still, civil society reform efforts can help mute dangerous trends, thereby countering Barbarization while buying time for a Great Transition mobilization. However, evidence mounts that incremental action alone is insufficient, especially as key government and corporate leaders continue to deny, ignore, or respond indecisively to threats. The second action prong-emergency managementcounters head-on the real risk of system collapse (Barbarization). This strategy evokes an existential "precautionary principle" proscribing policies that allow further drift toward conditions where science cannot rule out social-ecological tipping points. It would be timely to extend the environmental precautionary principle, embodied in Principle 15 of the 1992 United Nations Conference on Environment and Development Rio Declaration [20] to the system level. Additionally, redoubled cultural and educational efforts are needed to counter the politics of hate and polarization. In parallel, international emergency preparation for humane intervention into hotspots of chaos and conflict are essential, lest military containment becomes the rule. Finally, critical consideration of selected geoengineering options compatible with the precautionary principle, such as massive biomass sequestration in soils, rather than perilous solar radiation management, would be prudent. Since thus far the reform and emergency prongs have proved too little, too late, the third prong comes to the fore: actions to advance transformative cultural and institutional change. A robust strategy for deep change has many dimensions, including designing innovative economic and governance models attuned to contemporary challenges, debating alternative global visions, and nurturing a shift toward values of global solidarity, ecological sensibility, and lives of qualitative fulfilment over consumerism. Critical to this approach are new initiatives to foster connectivity across popular movements and civil society networks, thereby creating a path to an overarching movement of global citizens for a Great Transition. The three action prongs-reform, remediation, and redesign-are best pursued synergistically, rather than as independent strategies. Non-government actors and networks are critical to all dimensions: prodding governmental reform, prompting calamity control, and galvanizing transformative movements. In parallel, research can better support and guide these efforts by giving priority to the exploration of nonconventional futures and their links to near term choices. For example, for climate, integrated assessment models used to quantify greenhouse gas emissions underlying climate projections do neither incorporate the potentially disruptive feedbacks of climate impacts on economic and demographic drivers of emissions, nor are they equipped to deal with deep societal or economic transformation. Most immediately, assessments such as those of the IPCC need to be enhanced to incorporate disruptive change, whether the feedbacks of severe climate change on economic and demographic assumptions or the impacts of a deep shift in human values and institutions. Beyond the climate issue, the search for pathways to social-ecological sustainability requires integrated analysis across sectors, geographic scales, and time horizons. The research agenda now taking shape to address this challenge [21] would be well advised to highlight the exploration of system discontinuity and transformation as a critical dimension for deepening understanding, broadening policy, and engaging citizens. Facing a holistic challenge, we need a new transdisciplinary science that, in collaboration with artists, historians, innovators and social visionaries, can propel awareness and action by illuminating the landscape of the future, in all its dire peril and unique opportunity. This would better connect science, policy and society, and foster explorations of alternative paradigms for a civilization fit for the twentyfirst century. The COVID-19 pandemic has painfully demonstrated the real risk of historical discontinuity. A varied array of other social-ecological discontinuities can plausibly emerge in the coming decades. Going forward, scenario assessments with claims to relevance and rigor must emphasize nonconventional global futures. 
paper_id= fff1c38a4f5d2acc4473d2dce90150a59f98be6e	title= Addressing decontaminated respirators: Some methods appear to damage mask integrity and protective function	authors= Richard E Peltier;Jiayuan  Wang;Phd  ;Brian L Hollenbeck;Jennifer  Lanza;Ryan M Furtado;Jay  Cyr;Richard T Ellison;Kimiyoshi J Kobayashi;	abstract= Decontamination of N95 respirators is being used by clinicians in the face of a global shortage of these devices. Some treatments for decontamination, such as some vaporized hydrogen peroxide methods or ultraviolet methods, had no impact on respiratory performance, while other treatments resulted in substantial damage to masks. 	body_text= (Received 24 May 2020; accepted 3 July 2020) Frontline clinicians rely on the availability of personal protection equipment (PPE), such as N95 respirators, to reduce the risk of personal infection from exposure to respiratory droplets from infected individuals. The rise of a global coronavirus disease 2019 (COVID-19) pandemic has disrupted clinical supply chains that provide PPE, resulting in sustained shortages of N95 respirators, in part because of the widespread nature of the pandemic and the degree to which infected patients can present with varied symptoms. 1 In an attempt to ameliorate this shortage, the US Food and Drug Administration issued an Emergency Use Authorization that allowed decontamination techniques for N95 respirators. 2 This decision did not include mandate for further performance testing of respirators. Several institutions have proposed the use of decontamination techniques to allow reuse of N95 respirators. Battelle has deployed a series of custom-designed vaporized hydrogen peroxide (vHP) sterilizers across the country. 3 Others have proposed the use of different methods, such as gas plasma hydrogen peroxide (gpHP) or ultraviolet germicidal irradiation (UVGI) treatments. This methodology has eased the shortages of respirators, but an open question remains: Do respirators continue to protect a wearer after decontamination? To assess post-decontamination efficacy, some institutions employ tests such as quantitative fit testing. Respirator performance is more complex than maintaining fit, and there remains a risk that fundamental aspects of respirators are degraded in a way that limits their performance, even though they retain acceptable fit parameters. Notably, N95 2 is narrowly defined as a filtering material that is capable of filtering 95% of 300-nm particles and is largely agnostic to other particle types or sizes. Because respiratory droplets are expelled in a wide spectrum of sizes, typically ranging from 100 nm to 50 μm or more in diameter 4 and because they persist in the environment for several minutes, 5 respirators must be effective across a range of potential conditions to provide protection. Given the global N95 shortages, clinicians face a choice: wearing a used, and potentially contaminated respirator, or wearing a respirator that was decontaminated through a process that may affect the integrity of the respirator. An urgent need exists to understand the quantitative effects on respirator filtration with the use of these techniques so that wearers remain protected. N95 respirators were obtained from hospitals actively using various decontamination techniques, and respirators were donned on a mannequin that was covered in a layer of soft closed-cell foam. The mannequin was installed in a 0.1-m 3 exposure chamber and flooded with polydispersed combustion aerosol. For this study, almost all respirators were 3M 1860 or 1860S models (3M, St Paul, MN). Air was sampled through the mask at 85 L/min and alternated between chamber and mask-occluded sampling, consistent with a method in our prior work. 6 Aerosol samples were delivered to a scanning mobility particle sizer (model 3225/3080, TSI, Minneapolis, MN), which characterized particle size distribution from 16.8 nm to 650 nm and provides much more detailed respirator performance information than standard filtration efficiency testing. Incense was burned in a separate combustion chamber and diluted~50× prior to delivery to the exposure chamber. Using incense aerosol is a more protective assessment because respirators are less effective at capturing combustion aerosol 7 compared to sodium chloride. Respirator decontamination was performed off site using standard hospital processing protocols, and the quantity of masks was limited. The related data are summarized in Table 1 . Respirators were characterized by filtration efficiency, which is a ratio of particles that are immobilized by a respirator relative to concentrations in the chamber. For each mask, this ratio is calculated for each of 107 different measured particle sizes. Figure 1 shows filtration efficiency across measured particle sizes across a spectrum from 16.8 nm to 650 nm. Experimental error was estimated at 5%; thus, when a particular mask was observed to filter 90% or more particles at 300 nm, it was deemed consistent with performance as a N95 respirator (Fig. 1a) . Other respirators that do not meet this threshold were deemed inconsistent with this protection and are included in Figure 1b . Respirators that were treated with vHP, or shorter decontamination cycles of gpHP, retained their original filtration capabilities. These included Battelle Critical Care Decontamination System (CCDS) and Bioquell (both processed just once), or Sterrad 100NX Express (processed up to 5 times). Steris V-Max Pro produced similar results; respirators maintained their filtration efficiency up to 10 treatments. Most ultraviolet treatments had minimal impact on respirator performance. In contrast, gpHP treatment (by Sterrad 100S or Sterrad 100NX standard) degrades respirator filtration performance, with substantially decreased collection efficiencies across the entire size distribution. UVGI appears to degrade respirator performance after 9 repeated cycles. For comparison, a KN95 mask (Dongguan Huagang), and a 4-ply polyester bandana are included in Figure 1 . Neither were treated with any decontamination, with filtration efficiencies much worse than all masks in Figure 1a . We also evaluated performance of a N95 respirator that was immersed in a 0.5% bleach solution (Fig. 1b) , and performance was also degraded. Respirator performance can vary greatly. Respirators are surprisingly complex matrixes that can be deleteriously impacted by external forces, such as damaging interaction with strongly oxidizing environments. Although the intent of decontamination is to furnish a sanitized respirator for clinical reuse, some treatments result in respirators that offer less protection to wearers. Although the evidence presented here are limited, there are some generalizable conclusion that can be drawn. Treatments that involve hydrogen peroxide and gas plasma at high concentrations (Sterrad 100NX Standard, concentration,~90%), or long dwell times (Sterrad 100S) appear to induce damage to masks. Repeated UVGI processing appears to slowly diminish filtration efficiency, and this diminished efficacy reaches a level that warrants caution after~9 repeated treatments. However, the Sterrad 100NX express cycle, which uses a lower concentration of gpHP and shorter time, has limited impact on respirator performance for up to 5 cycles. Our results are consistent with recent findings. 8, 9 Ou et al 10 also tested UVGI with results consistent to those shown here, but their methods, which were quite similar to ours, provided more granular details on filtration performance than typical filtration efficiency testing. In general, vHP appears to have less of a deleterious impact on respirator performance. Treatments involving vHP include Steris V-Pro Max (tested up to 10 repeated decontaminations) and Bioquell and Battelle CCDS (tested after only 1 decontamination cycle). We cannot draw any conclusions about whether repeated Bioquell or Battelle CCDS treatments were acceptable because we were unable to source sufficient respirators. The results of this study do not address fit or general respirator integrity, which are also important for proper respirator function. For any respirator decontamination, respirator integrity should be assessed (including elastic function or corrosion on staples), and fit testing should be performed before use. These results suggest that respirators continue to perform as expected when decontaminated, mainly by vHP, UVGI, or brief exposures to gpHP. Future studies could explore whether use of decontaminated respirators is linked to healthcare worker outbreaks as another potential indicator of respirator performance. Decontamination treatments need to be carefully considered by clinical staff, especially for decontamination methods not yet well understood, in the search for creative ways to provide sufficient respirators to clinical staff. 
paper_id= fff1e7b356f0d6cf7b28b019974833200e38f843	title= The cientificWorldJOURNAL Research Article Clinical Manifestations Vary with Different Age Spectrums in Infants with Kawasaki Disease	authors= Hao-Chuan  Liu;Chiao-Wei  Lo;Betau  Hwang;Pi-Chang  Lee;	abstract= Background. Kawasaki disease (KD) is an acute systemic vasculitis with unknown etiology. The diagnosis of KD depends on clinical manifestations. The prevalence of coronary artery abnormality (CAA) is 11.0% and results in cardiac sequelae, such as myocardial infarction or coronary aneurysm, which are the most serious complications in KD. Methods. We divided KD's children into different age groups: ≤6 months old, 7 months to 1 year old, and >1 year old, respectively. Different parameters were compared in each group. Results. Infants ≤6 months old are less likely to fulfill KD's major diagnostic criteria within 10 days, are prone to develop incomplete KD with the lowest cholesterol level, and have the greatest chance to have CAA and the laboratory features associated with CAA, such as the longest time needed to confirm CA diagnosis, lower hemoglobin level, lower albumin level, and higher platelet count. Infants <1 year old develop higher percentage of leukocytosis and sterile pyuria. But this group has fewer patients with neck lymphadenopathy. 	body_text= Kawasaki disease (KD), or mucocutaneous lymph node syndrome, is an acute febrile illness of young children. About 80% of the cases aged between 6 month to 5 years [1] . Male is predominant with the male to female ratio about 1.5 : 1 [2] . The incidence of KD varies around the world. It is 20 times more common in North East Asia. The highest rate was reported in Japan as 218.6 per 100,000 children from 0 to 4 years of age in the year 2008, 69 per 100,000 children under 5 years old in Taiwan, 86.4 per 100,000 in Korea, 20.8 per 100,000 in the USA, and 8.39 per 100,000 in England. The annual survey of KD shows the incidence rate increases year by year [3] [4] [5] [6] [7] . The prevalence of coronary artery abnormality (CAA) during the KD's acute phase was 11.0% in the 20th nationwide survey [4] . Previous literatures indicated that, among patients of KD, young infants have increased risks of CAA. They usually suffer from prolonged fever without classic KD's presentation [8] [9] [10] [11] . This may make the diagnosis challenging and delay intravenous immunoglobulin treatment. Such delay causes higher probability of developing CAA [12] . The cutoff points of infant's age in previous studies were 6 months or 1 year old [8-11, 13, 14] . Both of the age spectrums revealed similar results. In our study, we further subdivide infant age into ≤6 months old and 7 months to 1 year old. With those older than 1 year old, we compared the differences of clinical manifestations, laboratory results, and echocardiography results between each group. A retrospective chart review for all children with the diagnosis of KD admitted to the Taipei Veterans General Hospital from January 1993 to March 2008 was conducted. All the patients were divided into three groups on the basis of the age of KD at diagnosis: group 1 (≤6 months old), 2 The Scientific World Journal group 2 (7∼12 months old), and group 3 (>1 year old). All the clinical manifestations, laboratory results, and echocardiography findings were recorded. The diagnosis of complete KD was based on fever for more than 5 days together with ≥4 of the 5 major clinical manifestations, which included change in extremities, polymorphous exanthema, bilateral painless bulbar conjunctiva injection without exudate, change in lip and oral cavity, and cervical lymphadenopathy larger than 1.5 cm in diameter. Patients who had fever with less than 4 characteristic manifestations were diagnosed as incomplete KD when coronary artery involvement was detected by two-dimensional echocardiography [2] . All the diagnoses of these patients were confirmed by more than 2 pediatric cardiologist, and other possible diseases were excluded. Two-dimensional and color flow mapping echocardiography were performed in every patient at admission. Coronary artery abnormalities and the valves regurgitation were recorded and reviewed. If coronary artery had perivascular brightness under echocardiography, coronary artery involvement was defined [15] . Coronary artery dilatation's definition was according to Japanese Ministry of Health's diagnostic criteria of coronary artery lesions in KD: the coronary artery internal diameter >3 mm in children <5 years old, the coronary artery internal diameter >4 mm in children ≥5 years old, and the internal diameter of a segment ≥1.5 times of the adjacent segment [16] . Coronary aneurysm was defined as coronary artery internal diameter ≥5 mm, regardless of age [17] . Continuous data were analyzed statistically by one-way ANOVA test followed by post-hoc Holm-Sidak test for pairwise comparison. Categorical data were compared with each other statistically by Chi-square analysis with Yates correction for continuity or Fisher's exact test while the sample size was small. Statistical significant was defined as P < 0.05. A total of 145 patients were identified. Group 1 (≤6 months old), group 2 (7-12 months old), and group 3 (>1 year old) included 30 (21%), 35 (24%), and 80 (55%) infants, respectively. The collected data were compared in Table 1 , which included days between disease onset to diagnosis, male to female ratio, diagnosis of complete KD or incomplete KD, fulfilling KD's major clinical criteria, and clinical manifestations. Infants younger than 6 months old took the longest time from the day of symptoms onset to diagnosis, and they had the smallest proportion of patients fulfilling complete diagnostic criteria of KD while at the same time the largest proportion fulfilling the criteria of incomplete KD. Infants younger than 12 months old were less common to develop neck lymphadenopathy but were more frequently to be documented with sterile pyuria. The ratio of female patients increased in children older than 1 year old, comparing to those less than 6 months old. While comparing with laboratory results, patients who were <1 year old had higher white blood cell (WBC) count. Patients who were ≤6 months old had statistically the lowest hemoglobin, the lowest cholesterol, and the lowest albumin level, but the highest platelet count. The liver enzyme amount, triglyceride level, high-density lipoprotein level, and C-reactive protein (CRP) level were similar among the 3 groups ( Table 2 ). The low-density lipoprotein (LDL) level was the highest in those older than 1 year old and was statistically significant comparing to those younger than 6 months old. Even within normal limit, there was a trend that LDL level grew higher with age. Echocardiography results showed similar coronary artery involvement, similar irregular coronary artery surface, and similar valve regurgitation between the 3 groups. Infants ≤6 months old were the most likely to develop coronary artery dilatations. Even not statistically significant between group 1 and 3, patients ≤6 months old had more coronary artery aneurysm (Table 3 ). Kawasaki disease is an acute febrile illness with systemic vasculitis that predominantly occurs in infant and young children. The etiology is still unknown but the epidemiology features suggest an infectious origin. Due to the absence of a specific diagnostic test, clinical criteria have been used to establish the diagnosis. Other clinical and laboratory findings such as leukocytosis, elevated CRP, diarrhea, cough, rhinorrhea, and perianal desquamation are also suggestive of the diagnosis [17] . The major complication of KD is CAA, which includes coronary artery dilatation and coronary aneurysm [17] . CAA may cause myocardial ischemia, myocardial infarction, or sudden death [18, 19] . Previous reports have shown that patients <1 year old are more likely to have incomplete KD and CAA [10, 11, 20] . In our retrospective study, we demonstrated that coronary artery dilatation and CAA-associated laboratory features are more common in infants younger than 6 months old. Those patients younger than 1 year old have more WBC count and more sterile pyuria, but less neck lymphadenopathy. Demographic and laboratory factors were surveyed extensively in previous studies for identifying risk factors of CAA in KD. Demographic factors include male gender [21] [22] [23] , race [22] , longer time from fever to treatment [23] [24] [25] , incomplete KD [26] , and younger age [9, 10, 13, [20] [21] [22] 27] . Laboratory factors include lower hemoglobin level [25, 28, 29] , higher platelet count [24, 30] , higher WBC count [28, 29] , lower serum albumin [31] , and higher alanine aminotransferase [29] . The three most commonly reported laboratory factors were lower hemoglobin level, higher platelet count, and lower albumin level [9] . Although infants younger than 4 months old experienced physiologic anemia, infants in this study showed that those ≤6 months old had not only the least favorable laboratory results, but also the highest rate of coronary artery dilatation. Even not statistically significant between group 1 and 3, infants ≤6 months old were also seen to have the trend of much coronary artery giant aneurysm comparing to other age spectrums. In addition to the unfavorable laboratory results, in this study, infants ≤6 months had longer days between the disease onset to diagnosis, more incomplete KD, and fewer patients fulfilling the diagnosis criteria within 10 days, which were also risk factors of CAA [24] [25] [26] 32] . In our study, we demonstrated that infants ≤6 months were more vulnerable of having CAA in comparison to those <1 year old [8, 10, 13, 22, 31] . Besides that the risk factors of CAA are more in KD's infants younger than 6 months old, the difference of coronary artery anatomy by age might exacerbate the possibility of CAA. The pathogenesis of the vasculitis in KD had been proposed by several literatures. Jennette [33] described that the earliest lesion of CAA is subendothelial accumulation of leukocytes, including monocytes, macrophages, neutrophils, and T-lymphocytes [34] . The following transmural infiltration of mononuclear lymphocytes results in transmural inflammation as well as smooth muscle cell edema and degeneration in the vascular media. With the disease progress, the infiltration extends from the vascular media into the adventitia. Finally, the cascade of events result in destructions of the vascular media and aneurysm formation [35] . In human coronary artery, Ikari et al. [36] performed serial coronary artery sections for babies from 17week gestation to 23 months after birth. They found that the coronary artery intima is rarely noted before 30-week gestation. Only 38% of the newborns have coronary artery intima in the first week of life with intima/media ratio nearly 0.1. Until three months after birth, vascular intima can be detected in all infants. This result gave us a hint that there might be a relationship between the delayed development of the vascular intima and the higher rate of CAA. The higher CAA incidence in infants ≤6 months old might be due to the loss of or a thin vascular intima in young infants. Large amount of the leukocytes may penetrate the endothelium and infiltrate into the media and the adventitia easier if there is no or a thin endothelium. Large amount of the infiltrations would cause more severe vascular inflammation with serious vascular medial destruction and aneurysm formation. Patients with KD usually have peripheral leukocytosis. An elevated WBC count usually indicates an infection, inflammation, allergic disorder, or some forms of malignancy. In our study, patients <1 year old have statistically higher WBC count comparing to those older than 1 year of age. Multiple reports have indicated that an infectious agent is highly possibly the etiology of KD. Burns et al. [37] had suggested an environmental trigger in KD due to pronounced seasonality and temporal clustering of cases in Japan. Esper et al. [38] had found "New Haven Coronavirus" in KD patients' respiratory secretion. Rowley et al. [39] had used IgA antibody and detected a specific antigen within the coronary arteries, bronchial epithelium, and inflamed KD tissues. These results have been supporting the hypothesis that infection is the etiology of KD. The leukocytosis in KD might be due to the systemic reaction toward infection as well as acute inflammation of the coronary arteries. Focal inflammation of the coronary artery, such as coronary artery disease (CAD) in adult, can also cause leukocytosis [22, 24, 25] . In patients with KD, with the medium-sized elastic 4 The Scientific World Journal arteries inflammation, coronary artery inflammation should at least be part of the reason of leukocytosis. Kawasaki disease is a male predominate disease [2] . The boys also have a higher risk of CAA in previous report [10] . In our study, we have found the trend of increasing incidence of female patients with age. However, the reason was unknown or might be due to our small sample size. Urinalysis revealed sterile pyuria in about 33% of the KD's patient. This was probably due to urethritis instead of bacterial urinary tract infection [17] . In our study, sterile pyuria is more frequent in patients <1 year than older children. That might indicate that infants younger than 1 year old suffer from more serious urethritis than children. We have also demonstrated that neck lymphadenopathy is the least common sign of the KD's five major clinical criteria in infants <1 year old. Therefore, echocardiography should be performed to evaluate coronary artery in infants who have prolonged fever with the suggestive KD's clinical signs and laboratory results. There are several limitations in the present study. The case numbers are not large enough to represent general population. Furthermore, we conduct a retrospective study; The Scientific World Journal 5 there are some missing data which make the case numbers even smaller to be analyzed. Also, we cannot exclude the possibility that other infectious diseases which had similar clinical signs and symptoms were diagnosed as incomplete Kawasaki disease. After subdividing the infant age, we find that infants younger than 6 months old take the longest time to diagnose, are the least to fulfill the clinical major criteria, and have the least favorable laboratory results which are the risk factors of developing CAA. They are also more likely to have incomplete KD and coronary artery dilatation. Coronary giant aneurysm, even not statistically significant, is also more frequently observed in younger infants. Instead, the WBC count, sterile pyuria, and neck lymphadenopathy are seen more often in those younger than 1 year old. 
paper_id= fff1fa74115444043f4aa0238ee9126ef4539878	title= Operationalizing Framing to Support Multiperspective Recommendations of Opinion Pieces	authors= Mats  Mulder;Oana  Inel;Jasper  Oosterman;Nava  Tintarev;	abstract= Diversity in personalized news recommender systems is often defined as dissimilarity, and based on topic diversity (e.g., corona versus farmers strike). Diversity in news media, however, is understood as multiperspectivity (e.g., different opinions on corona measures), and arguably a key responsibility of the press in a democratic society. While viewpoint diversity is often considered synonymous with source diversity in communication science domain, in this paper, we take a computational view. We operationalize the notion of framing, adopted from communication science. We apply this notion to a re-ranking of topic-relevant recommended lists, to form the basis of a novel viewpoint diversification method. Our offline evaluation indicates that the proposed method is capable of enhancing the viewpoint diversity of recommendation lists according to a diversity metric from literature. In an online study, on the Blendle platform, a Dutch news aggregator platform, with more than 2000 users, we found that users are willing to consume viewpoint diverse news recommendations. We also found that presentation characteristics significantly influence the reading behaviour of diverse recommendations. These results suggest that future research on presentation aspects of recommendations can be just as important as novel viewpoint diversification methods to truly achieve multiperspectivity in online news environments. 	body_text= In recent years, traditional news sources are increasingly using online news platforms to distribute their content. Digital-born news websites and news aggregators, which combine content from various sources in one service, are also gaining ground [29] . In 2015, 23% of survey respondents reported online media as their primary news source, and 44% considered digital and traditional sources equally relevant [29] . This change also induces a wide adoption of news recommender systems that automatically provide personalized news recommendations to users. Communication studies generally acknowledge two important roles of media in a democratic society [17] . The first role is to inform citizens about important societal and political issues. The second role is to foster a diverse public sphere. Both roles are then related to multiple social-cultural objectives of democracy, such as informed decision-making, cultural pluralism and citizens welfare [24, 33] . The role of news recommender systems in promoting these democratic values is under heavy discussion in academic debate. For example, the term filter bubbles received increasing awareness, suggesting that high levels of personalisation would lock people up people in bubbles of what they already know or think [30] . According to Helberger, the democratic role of news recommender systems mainly depends on the democratic theory that is being followed. In their conceptual framework, this role is being evaluated for the most common theories: the liberal, the participatory and the deliberative [17] . In particular in relation to the participatory and deliberative model, the development of viewpoint diversification methods can be motivated. However, current diversification methods [21, 42] do not address viewpoint diversity, but define diversity as dissimilarity and operationalize it through topic diversity (e.g., corona versus farmers strike). Therefore, current diversification methods are not applicable in the news domain, and novel viewpoint diversification methods are needed to maintain and assure multiperspectivity in online news environments. To truly enable multiperspectivity, users should be willing to consume viewpoint-diverse recommendations. Moreover, their behaviour should be studied in real, online scenarios. Thus, we investigate the following research questions: R1: How is reading behaviour affected by viewpoint diverse news recommendations? R2: How is reading behaviour affected by presentation characteristics of viewpoint diverse news recommendations? To answer these questions, we propose a re-ranking approach for lists of recommended articles based on aspects of news frames, a concept taken from communication studies. In particular, a news frame describes how to identify a view on an issue, in a given article [12] . Thus, by bridging aspects from the social and the computational domains, we aim to overcome the current gap between the definition of diversity in recommender systems and news media. During an offline evaluation, the proposed method increased the viewpoint diversity of recommended lists of news articles on several topics. Further, we measured the influence of the viewpoint diversification method on the reading behaviour of more than 2000 users, which are likely to interact with the recommended articles, in an online study on the Blendle platform, a Dutch news aggregator platform. We found that reading behaviour of users that received diverse recommendations was comparable with the reading behaviour of users that received news articles optimized only for relevance. However, we did find a positive influence of two presentation characteristics on the click-through rate of recommendations, i.e., news articles with thumbnails and news articles with more hearts are more often read. Therefore, we make the following contributions: • a novel method for viewpoint diversification using re-ranking of news recommendation lists, based on framing aspects; • an online evaluation with more than 2000 users, on the Blendle platform, to understand: (a) how viewpoint-diverse recommendations affect the reading behaviour of users; and (b) how article's presentation characteristics affect the reading behaviour of users. In this section, we first investigate how communication science understands diversity. Then, we review current approaches for diversity in recommender systems. These allow us to bridge the gap between the domains of communication and computer science, by operationalizing framing aspects in a diversification algorithm. In news media, diversity refers to multiperspectivity or a diversity of viewpoints [14] . In communication science, diversity is, in general, a key measure for news quality [9, 22, 31] , thus fostering multiple democratic aspects, such as informed decision-making, cultural pluralism and citizens welfare [26, 39] . Two main approaches for assessing diversity can be distinguished: source and content diversity [2, 6, 26] , with most studies focusing on source diversity [1, 2, 26, 39] . When measuring source diversity, most methods follow Bennett [5] 's indexing theory, which assumes that including non-official or non-elite sources corresponds to high levels of diversity [2] . Alternatively, Napoli [26] approaches the issue from a policymaker point of view and distinguishes three aspects of source diversity: content ownership or programming, ownership of media outlets, and the workforce within individual media outlets. Critics, however, state that multiple sources can still foster the same point of view and therefore, source diversity is not a direct measure for viewpoint diversity [39] . Multiple studies also indicate that power distributions in society, commercial pressure of news media and journalistic norms and practices, significantly influence which sources gain media access [2, 6] . Therefore, it is often argued that viewpoint diversity can only be achieved by fostering content diversity [3, 9, 14, 22, 26, 39] . Content diversity is defined in [36] as "heterogeneity of media content in terms of one or more specified characteristics". Baden and Springer [2] identified six common approaches to assess content diversity. The first three methods focus on the tone or political position represented in the news, i.e., the inclusion of non-official positions, the diversity of political tone or analysis of political slant. These methods, however, assume that political disagreement equals viewpoint diversity [2] . Another approach uses language diversity to evaluate content diversity. However, this is again no direct measure, since different language can describe the same perspective [2] . The final two approaches use the concept of frames to assess content diversity. Framing theory states that every communicative message selectively emphasizes certain aspects of the complex reality [2] . Thereby, frames enable different interpretations of the same issue [32] . Framing has been put forward by many scholars to enhance content diversity. For example, Porto [31] states that news environments need to be evaluated by their ability to provide diverse frames. Baden and Springer [2] describe three frames' aspects that are central to the role of viewpoint diversity in democratic media. First, frames create different interpretations of the same issue by selecting some aspects of the complex reality [13] . Second, frames are not neutral but suggest specific evaluations and courses of actions that serve some purpose better than other [12] . Third, frames are often strategically constructed to advocate particular political views and agendas. Framing, thus, can be a suitable conceptualization of viewpoint diversity. Traditionally, research on recommender systems focused on evaluating their performance in terms of accuracy metrics [42] . Such focus, however, induced a problem which is known as over-fitting, e.g., a model is fitted so strongly to a user that it is unable to detect any other interests [21] . Additionally, there is a need for a more user-centric evaluation of recommender systems. Thus, diversity has become one of the most prominent beyond-accuracy metrics for recommender systems [42] . In this context, diversity is generally defined as the opposite of similarity [21] , and it is often based on topic diversity (e.g., corona versus farmers strike). For example, Ziegler et al. [42] proposed a topic diversification method based in the intra-list diversity metric. Current diversification methods for recommender systems, thus, do not focus on viewpoint diversity and are not applicable in the news domain. To the best of our knowledge, only one study for viewpoint diversification has been proposed so far [35] . Tintarev et al. [35] propose a new distance measure for viewpoint diversity based on linguistic representations of news articles. This diversity measure was then applied in a post-processing re-ranking algorithm [8] to a list of news articles. These allowed optimizing for the balance between topic relevance and viewpoint diversity. In a small scale user study [35] , readers indicated a lower intent to consume diversified content, motivating the need to study behavioural measures for newsreaders on a larger scale. Thus, we argue that more research is required to understand the relationship between the metric and the influence on readers behaviour. In this work, we aim to bridge the current gap between the notion of framing in communication science and potential computational measure. Additionally, we aim to study how viewpoint diversification affects the behaviour of newsreaders in an applied setting. The next section justifies the operationalization of framing in the computational domain. Framing is an extensively researched concept in different domains, including psychology, communication and sociology, having its roots in the latter domain. Bateson [4] state that communication only gets meaning in its context and by the way the message is constructed. Later, frame theory gained increasing momentum and was generally understood as follows: every communicative message selectively emphasizes certain aspects of a complex reality [2] . Thus, every news article (unintentionally) comprises some form of framing [2] . Frames are often deliberately used to construct strategic, often political, views on a topic. Consequently, frames enable different interpretations of the same issue [2] . However, every frame inevitably deselects other, equally plausible and relevant frame [2] . When considering frames in news articles, multiple definitions exist [10, 13, 15] . However, the definition of Entman [12] is the most commonly adopted in the literature. It states that framing includes the selection of "some aspects of perceived reality and make the more salient in a communicating text, in such a way as to promote a particular definition of a problem, causal interpretation, moral evaluation and treatment recommendation for the item described". Within this definition, the problem describes four framing functions -for which we also provide a running example -, namely: (1) Problem Definition : "what a causal agent is doing with what costs and benefits"; e.g., a second Coronavirus wave is approaching; (2) Causal Attribution : "identifying the forces creating the problem"; e.g., (it is due to the) government policy response; (3) Moral Evaluation : "evaluate causal agents and their effects"; e.g., response to approaching second wave came too late (negative evaluation); (4) Treatment Recommendation : "offer and justify treatments for the problems and predict their likely effects"; e.g., there must be predefined measures to be deployed at a critical threshold of virus spread. Additionally, Entman [12] describes how to find frames at different levels of analysis, including single sentences, paragraphs or articles as a whole. Also, a frame may not necessarily include all the four functions. Most framing analysis approaches focus on manual analysis of articles [20, 23, 38] . Only recently, some computer-assisted methods gained interest [7, 16, 40] . As a result, the identification of frames often falls into a methodological black box [23] . Thereby, the main issue includes the ambiguity of "which elements should be present in an article or news story to signify the existence of a frame" [23] . To overcome this problem, some recent studies [2, 23, 38] propose a novel identification method based on the extraction of the four aforementioned framing aspects in the definition of Entman [12] . To guide the operationalization of the framing aspects, we started with a qualitative analysis. Through a small focus group, we aimed to gain insights into how the four framing functions of the main frame of an article manifest in its content and how we can identify them computationally. 3.1.1 Participants. We invited three experts in the field of news article and framing analysis. All experts had a background in journalism, communication, or news media. They all had multiple years of relevant work experience. 3.1.2 Materials. As a basis for discussion during the focus group, we used opinion pieces on the topic of Dutch farmers protests. Opinion pieces refer to news articles that reflect the authors opinion and thus, do not claim to be objective. An initial discussion with domain experts indicated that this type of news article is the most suitable to identify framing functions. 3.1.3 Procedure. The focus group procedure consisted of two steps. 1. Annotation session: First, the participants were asked to perform framing analysis on an opinion piece, using the four framing functions as described by Entman [12] . In particular, the participants had to individually highlight parts of the article, such as word clauses or sentences, that can be related to one of the four framing functions of the main frame of the news article. 2. Review session: Second, the results were discussed, together with some general questions on news article analysis and framing. For every highlighted part, we asked the participants to motivate why the highlighted part is related to one of the four framing functions. Besides, we used the results as input to a broader discussion on news article analysis and framing, such as: • What is the main heuristic that you used to analyze the article? • What procedure did you follow to analyze the framing functions of the article? • Can you derive any patterns in the way framing functions manifest in opinion pieces? During the review session, all experts indicated that they used the article structure as the main heuristic to find the framing functions regarding the main frame. They also pointed out that opinion pieces are still strongly shaped by journalistic values on how an article should be structured. We further analyzed this heuristic according to the four framing functions: (1) Problem Definition : In opinion pieces, the first part of the article often presents the main problem that the author addresses and includes the title, the lead, and the first x paragraphs. Work on manual frame analysis [20] supports this finding. The number of introductory paragraphs, x, can be different per source, author, or article. (2) Causal Attribution + Moral Evaluation : The body of an article is used to analyze the main problem and usually contains different factors that contribute to the problem under investigation and their evaluation. We can match this with: a) the causal attribution of a frame (forces creating the problem), and b) the moral judgements (evaluate the causal attribution and their effect) [12] . (3) Treatment Recommendation : Treatment recommendations can be seen as suggestions to improve or solve the issue described by the problem definition of the main frame. They normally appear in the concluding paragraphs, according to the focus group members. Note, however, that this structure is only a heuristic and it only applies to opinion pieces. Other types, such as interviews, are structured differently. The results of the annotation session also indicate that each framing function related to the main frame of an article can normally be found within one paragraph. Additionally, a paragraph can include multiple framing functions, but words, clauses, and sentences generally represent a single framing function. In this section, we describe the experimental dataset, which consists of opinion pieces, in Dutch. The choice of article type is motivated by the focus group session presented in Section 3, in which the structure of this article type is put forward as the primary heuristic to find framing aspects. We picked topics that we expected a) to be present on the Blendle platform at the time when we performed the online user study; b) to contain different viewpoints addressed in the news; and c) to balance issues that more current versus longstanding. The dataset consists of four ongoing topics: Black Lives Matter, Coronavirus, U.S. Elections -as more current topics, and the dominance and privacy issues around Big Tech -as a long-standing topic. We collected our dataset from an archive containing more than 5 million Dutch news articles. The archive is known to undergo checks for articles quality, to remove undesirable content, such as the weather or short actualities. For each topic, we used the search terms (queries) and restrictions shown in Table 1 . We provide the list of search terms in their original language, Dutch, because we do not want to add additional bias through translation. Additionally, since the proposed method heavily relies on the structure of the article, we set up a filter for the minimum number of words to 450 and a filter for the minimum number of paragraphs to 5. Table 2 provides an overview of the dataset, per topic. While the length of the articles varies across topics, they are usually far longer than the 450-word limit we chose. Four publishers are present for all topics: De Volkskrant, De Standaard, Trouw and Het Algemeen Dagblad. Furthermore, De Volkskrant is the most prominent publisher for all topics, except for the U.S. Elections topic. The inclusion of other, less frequent, publishers varies per topic. Overall, our dataset covers a set of 15 unique publishers. We also present some properties concerning the presentation characteristics of the articles on the news aggregator website. We observe that the ratio of articles that contains a thumbnail image depends on the topic. For the Black Lives Matter and Coronavirus topics, more than half of the articles have a thumbnail image, while the opposite holds for the other two topics. The number of custom titles from the editorial team and the average title length also differ considerable per topic. Only a few articles have an editorial title, and they usually appear for the Big Tech and U.S. Elections topics. We proposed a novel diversification method based on framing aspects, using the insights from the focus group. First, we describe the extraction pipeline, which supports the structure heuristic described in the results of the focus group session (Section 3). The pipeline forms the basis for the generation of recommendation lists that we use in the offline evaluation (Section 6) and the online study (Section 7). We implemented the pipeline using methods employed by the news aggregator platform and off-the-shelf natural language processing toolkits, such as IBM-Watson. We chose to use stateof-the-art and off-the-shelf methods used by the news aggregator platform to ensure output quality. Then we describe the distance function, which combines the metadata related to each framing aspect in a measure for viewpoint diversity for news articles. Finally, we present the re-ranking algorithm based on this viewpoint diversity measure. Our contribution, therefore, stands in the novelty of the overall diversification framework, rather than the implementation of specific components. Figure 1 shows an overview of the end-to-end pipeline.  For each framing aspect, as described in the definition of Entman, we implemented an extraction pipeline: Problem Definition . As described in Section 2, the problem definition can be understood as the central issue or topic under investigation [23] . Therefore, we decided to use a topic model as the main extraction method for this framing aspect. The model, provided by the research partner, included a 1000-topic latent Dirichlet allocation (LDA) model trained on 900k Dutch news articles. Based on the conclusions from the focus group described in Section 3, the title and the first x paragraphs are used to retrieve metadata related to this framing aspect. We also applied multiple pre-processing steps on the content, including cleaning, chunking, tokenization, lemmatization and stop-word removal. Causal Attribution + Moral Evaluation . According to Entman [12] , the causal attribution of a frame relates to the forces creating the problem, while the moral judgements evaluate the causal attribution and their effect. From the discussion of the focus group session, described in Section 3, we concluded that the body of an article usually elaborates on these aspects. Additionally, paragraph-level seems to be the most suitable level of analysis. Therefore, a text-classification algorithm was applied using the IBM Watson Natural Language Processing API. The service returns a category for each paragraph according to a predefined five-level taxonomy, from the most general category (e.g. level 1 -technology and computing), to the most specific one (e.g., level 5 -portable computer). To extract information related to the evaluation of these attributions, we also analyze the sentiment of these paragraphs, using the IBM Watson NLP API. Thereby, it would be able to identify if two articles evaluate the same aspects of a problem differently. The content of interest for this task includes all paragraphs except the introductory and concluding paragraphs. We optimize these variables during the offline evaluation. Treatment Recommendation . Following the definition of Entman [12] , a treatment recommendation suggests remedies for problems and predicts their likely effect. The research domain of suggestion mining, which involves the task of retrieving sentences that contain advice, tips, warnings and recommendations from opinionated texts [27] , was found to be highly relevant for this framing aspect [28] . However, the state-of-the-art models are topicspecific [28] , and can not be easily applicable to our domain. Thus, only the more naive rule-based approach could be applied for this study, being more generally applicable. In a crowdsourcing task with domain experts, we evaluated, and we optimized the generally applicable rules from the literature on the news article content. Afterwards, we implemented the method to extract sentences that contain suggestions from the article content. Then, to obtain comparable information between the suggestions of two articles, the suggestion sentences of each were classified using the same textclassification algorithm that was used for the causal attribution framing aspect. Corresponding to the conclusion of the focus group described in Section 3, the content of interest for this framing aspect includes the concluding paragraphs of an article. We optimize this variable in the offline evaluation. Having defined the extraction pipeline for each framing aspect, i.e., problem definition, causal attribution, moral evaluation and treatment recommendation [12] , we now define our distance function. We compare the extracted metadata for every pair of articles. Thus, we implement a distance function for each framing aspect. Problem Definition . The metadata regarding the problem definition framing aspect involves a probability distribution over 1000 topics. Thus, we need a statistical distance measure. We chose the Kullback-Leibler divergence because it is one of the most commonly used statistical distance measures for LDA-models, and it is used in the comparable work [35] on viewpoint diversification. Causal Attribution and Moral Evaluation . We compare the five-level taxonomy categories extracted from the pipeline described in the previous section, to obtain a distance measure for the causal attribution framing function of the primary frame. Thus, we use the weighted Jaccard index, which measures the similarity (or diversity) of two sets [18] . The index is calculated for each level of detail in the five-level taxonomy, such that we apply weight factors per taxonomy level. Thereby, overlap in higher levels of detail can contribute more to the overall similarity score. In the offline evaluation, we compare different weight factors per taxonomy-levels. For the moral evaluation framing aspect, we implement the distance function by multiplying the Jaccard distance and the absolute sentiment difference between each paragraph combination of two articles. Thus, paragraphs with no overlapping categories yield a value of zero, while highly similar paragraphs, with different sentiment scores, lead to high levels of diversity related to the moral evaluation framing aspect. Treatment Recommendation . For the treatment recommendation we used the five-level taxonomy classification, i.e., from the most general to the most specific category, as returned by IBM Watson Natural Language Processing API, and the Jaccard index. We implement the re-ranking of the input list of articles using the Maximal Marginal Relevance (MMR) algorithm [8] . In our case, the re-ranking consists of ranking news articles that are more diverse higher. First, we normalize the output of the distance functions related to each framing aspect using a min-max normalization, and then we combine them in a diversity score through a weighted sum. We optimize the weight factors during the offline evaluation. We note here that we re-rank news articles that are known to also be relevant for the given topic. Where most re-ranking algorithms for recommender systems order lists only on relevance, the MMR algorithm provides a linear combination between diversity, in our case viewpoint diversity, and relevance, set by the parameter . Thus, the re-ranking algorithm is defined as follows: Since this work proposes a measure for viewpoint diversity rather than a relevance measure, we decided to implement the relevance score using a simple frequency-inverse document frequency (TF-IDF) score. In this section, we describe the offline evaluation of our viewpoint diversity-driven approach for re-ranking lists of news articles. For our offline experiment, we used the news dataset introduced in Section 4, which covers 214 news articles on four topics. The experimental procedure consists of four main steps that we detail as follows. First, we process and enrich all the news articles in our dataset according to the four framing aspects as defined by Entman [12] : problem definition, causal attribution, moral evaluation, and treatment recommendations (for details see Section 5.1). Second, we generate the diversity matrix by comparing all combinations of two articles, based on the enrichment described in Section 5.1. Thus, using the distance function defined in Section 5.2 we measure the dissimilarity of two articles based on the framing aspects. Finally, since the MMR algorithm re-ranks a list of news articles based on a linear combination between diversity and relevance, we calculate the TF-IDF relevance matrix, including a relevance score for each two article combination. Third, we optimize the model variables and evaluate the performance using cross-validation. For each article in the dataset, we calculate a set of recommendations by re-ranking the remainder articles in the dataset. To prevent over-fitting, we use cross-validation. Thus, we split the dataset into distinct sets. We experimented with different values of = 5, 10, 20 and = 3, 6, 9. For every set, we take the following steps: (1) Grid search of model variables on training set: The training set contains the − 1 subsets of articles. We obtain the optimal combination of the model variables for the training set using a grid search. An overview of the model variables can be found in Table 3 and in Section 6.2.1. (2) Evaluation on test set: After the variables are trained on the − 1 subsets, the model is evaluated on the test set for different values of , between 0 and 1 with a step of 0.1. As described before, for each article in the test set, a set of recommendations is calculated by re-ranking the remaining articles in the dataset. And finally, we combined the results of all cross-validations. Table 3 shows the model variables that we optimize during the offline evaluation. We choose the variation of the weights for each framing aspect such that no single framing aspect can have the majority. Additionally, a step-size of 0.1 is assumed to bring enough variation. We consider two variations for the taxonomy level weights: equal weights for each taxonomy level or ascending weights. Finally, the number of introductory and concluding paragraphs can be either 1 or 2.  We assess the performance of the viewpoint diversification method using a metric from literature [35] . The metric is based on the Intra-List Diversity metric [35, 37, 41, 42] and it is defined as the average distance between all pairs of articles and , such that ≠ . Thereby, the distance between a pair is defined by the articles' channels (predefined taxonomy of 20 high-level topics) and the articles' LDA topic-distribution, as derived from the enrichment methods in Section 5: The channel distance is calculated using the cosine distance, whereas the LDA distance is computed using the Kullback-Leibler divergence. 6.3.1 Additional metrics. Besides the viewpoint diversity metric, we also measure the effectiveness of the diversification model on other properties, as follows: Relevance: We measure the TF-IDF relevance for the recommendation lists, such that we can measure the effectiveness of the viewpoint diversification method. Kendall's : We compute the Kendall's rank correlation coefficient [19] to measure the similarity between two ranks of recommended items. Average number of words: We compute the average number of words for the recommended article lists as a measure of quality (i.e., longer news articles can be considered to be higher quality). Publisher Ratio: We measure the publisher ratio for the recommendation lists because this could potentially provide insights on the effect of the content diversity on the source diversity. To assess if the proposed diversification method can increase the viewpoint diversity based on the presented metric, we compare it with a baseline, consisting of a full relevance MMR, where = 1, such that we rank the recommendations purely on the TF-IDF relevance. We chose this baseline because it has minimal effects on the recommendations in terms of viewpoint diversity. In Figure 2 , we show the performance of the model in terms of viewpoint diversity and relevance for different values of , and the optimal setting of the model variables. The red bars represent the results of the viewpoint diversity metric, while the blue bars represent the relevance scores. Variations of the cross-validation variable did not yield significant differences between the results, and thus, we fixed = 10. The list size did show to influence the number of publishers included in the recommended list, but the results were not significant. Thus, we fixed the list size to = 3, to better align the offline evaluation set up with the online evaluation set up, where only 3 recommended news articles can be shown at a time. Table 4 shows the optimal model variables values, per topic. Across all topics, the proposed diversification method is capable of increasing the viewpoint diversity of recommendation lists. According to the metric, the viewpoint diversity increases on average from 0.55 to 0.79 between = 1 and = 0. Additionally, the average relevance score decreases from 0.58 to 0.27. Kendall's . We computed the Kendall's rank correlation to assess whether the proposed diversification method is capable of providing different recommendation lists compared to the baseline. We computed the coefficient between the baseline ( = 1) and each other value of = [0.0, 0.1, ..., 0.9]. Overall, we observed that the re-ranking of the set of recommendations based on viewpoint diversity results in different recommendation lists compared to the baseline. The coefficient decreases for smaller values of , but it is bounded around = 0 for decreasing values of . Average number of words. We observe no consistent pattern in the average number of words for different values of across topics. For the Black Lives Matter and Big Tech topics, the average number of words increases for larger values of , for the U.S. Elections topic the average decreases and for Coronavirus the average is stable. Publisher ratio. Figure 3 shows the average number of articles in the recommended lists, normalized by the input ratio, for each value of . For every topic, the number of publishers increases for larger values of and the number of different publishers for the baseline recommendation list is larger than the one in the diverse recommendation list. Thus, we observe that the diversification method influences the publisher ratio. For small values of , some publishers get amplified, while others are excluded. We see this effect primarily for the topics of U.S. Elections and Big Tech. The topic of Corona Virus seems to be the only exception. We conducted a between-subjects online study on the Blendle platform to compare the reading behaviour of users who receive news articles optimized only for relevance, versus news articles that are also diverse on viewpoint. In the online study, we used the articles collected in Section 4. We selected 2076 active users of the news aggregator platform. These users were assumed to most likely see and use the recommendation functionality. We included only users who clicked at least four times on a recommended article below any article read, in the last 14 days before the study. Groups for baseline and diversified recommendations were created by randomly splitting the users. In the between-subjects user study we manipulated the following conditions, referring to the recommended list of news articles: • baseline recommendation: was implemented using a MMR that was based only on relevance ( = 1.0) • diversified recommendation: was implemented using a MMR that maximized viewpoint diversity ( = 0.0) During the two-week experiment, six days per week, we provided recommendations for two articles featured on the selected users' homepage. We provided sets of three recommendations below the content on the reading page of the original article. Every morning, we chose these two articles manually, to match any of the topics that we selected (Black Lives Matter, Big Tech, Coronavirus, and U.S. Elections). Afterwards, both the baseline and diversified recommendation sets were calculated for both articles and included in the news aggregator platform.  To analyze the reading behaviour of the two different user groups and answer RQ1, we measure specific events on the news aggregator platform (i.e., check whether the user opened the article and if the user finished reading the article). Based on these available events, we observe multiple implicit (click-through-rate per news article, click-through-rate per recommendation set and completion rate of recommendation) and explicit (heart ratio) measures of the reading behaviour. To answer RQ2, we look into presentation characteristics of the recommended articles (i.e., presence of editorial title, presence of thumbnail and counting number of hearts). 1. Click-through rate per article: The number of clicks on a news article is divided by the total number of users who finished one of the original news articles for which that article was recommended. The completion of an original news article is registered using a scroll-position. 2. Click-through rate per recommendation set: The total number of clicks on either of the three news articles in the recommendation set is divided by the number of users who finished the original news article (using scroll-position) for which the recommendation set was presented. 3. Completion rate of recommendation: Is implemented as the number of users that read the full recommended article (using scroll-position) divided by the number of users who opened the news article. The completion rate is assumed to be a measure for the user satisfaction with the recommendations. We can argue that short news articles are more likely to be completed than long news articles. Thus, we also analyze the completion rate of a news article in relation to the number of words in the news article. 4. Favourite ratio: The news aggregator platform allows users to mark an article as a favourite, illustrated by an icon of a heart. The users can click this icon at the end of the article content. We implemented the measure as the number of users of the user group (baseline or diverse) that clicked on the icon, divided by the number of users in the same group that completed the article. The metric is assumed to be a marker of user satisfaction with the article. We measured three additional properties of a recommended article during the experiment, which referred to the presentation characteristics of recommended news articles. First, the editorial team can replace the original title of a news article with a custom, editorial title. In general, these custom titles are longer and more explanatory than the original ones. Second, articles can be presented with or without a thumbnail image. Third, the number of users who selected the article as a favourite is visualised by a counting number of hearts in the left-upper corner of an article banner. All three properties are assumed to potentially influence the click-through rate and are, therefore, measured during the experiment. 6. Source diversity: Finally, we also measured the influence of the source diversity of the recommendation set on the click-through rate. As seen in Section 6, higher levels of viewpoint diversity showed to influence the number of times a publisher is included in the recommendation. The online study ran six days a week for two weeks. Thus, we provided recommendations below 24 articles. During the experiment, the topic of Coronavirus became extremely prominent, so we provided recommendations below 18 out of 24 news articles on this topic. In contrast, the Black Lives Matter topic lost all actuality, resulting in no recommendations for this topic. For the U.S. Elections topic, we provided recommendations below four articles, and for the Big Tech topic, below two news articles. Click-through rate per recommended article. The mean clickthrough rate per recommended article for the baseline recommendations was 0.11 (stderr. = 0.011) while for the diversified recommendations was 0.087 (stderr. = 0.0083) when looking at all topics. Furthermore, according to the Mann-Whitney U test (U=570, p-val>0.05), we did not find a significant difference between the two user groups in terms of click-through rate per recommended article. The same result holds per topic. Click-through rate per recommended set. The mean click-through rate per recommended set for the baseline recommendations was 0.31 (stderr. = 0.016) while for the diversified recommendations was 0.25 (stderr. = 0.016) when looking at all topics (Figure 4a ). According to the Mann-Whitney U test (U=2.9, p-val<0.05), we find a significant difference between the mean click-through rate per recommended sets for the two user groups. Per topic, we find such difference significant only for Coronavirus, shown in Figure  4b , with a click-through rate per recommended set of 0.32 (stderr. = 0.018) for the baseline recommendations and 0.25 (stderr. = 0.018) for the diversified recommendations (U=80.0, p-val<0.05). For the other topics, we found no significant difference between the two user groups. Completion rate. We found no significant difference in terms of completion rate for the two user groups. We also applied the Spearman's rank correlation to see whether the completion rate is correlated with the length of the articles. However, we did not find any correlation in either of the two conditions. Heart ratio. We found no significant difference, for all topics and across topics, in terms of heart ratio for the two user groups. This suggests that the quality of the recommendations was comparable between the two conditions. 7.6.1 Influence of presentation characteristics. We measured the influence of three factors, namely the presence of an editorial title, the presence of a thumbnail and the number of users that chose the article as a favourite on the click-through rate of an article. Editorial title. Regarding the influence of the inclusion of an editorial title on the click-through rate, no statistical significance was found for neither user groups. Thumbnail image. We found no significant influence of the inclusion of a thumbnail image on the click-through rate for baseline users. In contrast, recommendations with a thumbnail are 3.1% more times opened than recommendations without a thumbnail for diverse users, as seen in Figure 4c , a difference that is also statistically significant. Favorite articles. We applied the Spearman's rank correlation to see whether we find a correlation between the click-through rate and the number of hearts. Figure 4d shows the distribution of click-through rates and the number of hearts. We only found a moderate positive correlation of 0.57, also statistically significant (p-val<<0.05) for the diversified user group. 7.6.2 Source diversity. As seen in the offline evaluation, higher levels of viewpoint diversity turned out to have remarkable effects on the publisher ratio. Therefore, we also evaluated the effect of the source diversity of a recommendation set on the click-through rate. For each recommendation set, we computed the number of different publishers and we found recommendation sets in which all articles are from a different publisher and sets in which two articles are from the same publisher. Afterwards, the click-through was calculated for each category. The results for both the baseline users and diverse users show that no statistically significant difference can be found in the click-through rate between two or three different publishers in the recommendation set for neither baseline nor diversified users. We first discuss the results of the offline and online evaluation and then provide an overview of the limitations of our approach. The offline evaluation indicated that the proposed method is capable of increasing the viewpoint diversity of recommendation sets according to the metric defined in [35] . The average viewpoint diversity scores across all topics increased from 0.55 to 0.79 for an increasing level of diversity in the MMR algorithm. Simultaneously, the average relevance score decreased from 0.58 to 0.27. Remarkably, the diversity score of 0.41 in [35] is considerably smaller than the maximum average value of 0.79 found in this work. A possible factor could be the fact that in [35] the LDA topic model was excluded from the diversification method to prevent any interference with the evaluation metric, whereas the diversification method in this work still depends on an LDA topic-model. Therefore, the difference in viewpoint diversity scores between the methods can possibly appear due to the interference of metadata between the viewpoint diversity metric and diversification method in this work. A remarkable effect of the diversification algorithm that was found in the offline evaluation includes the decreasing publisher ratio for larger contributions of diversity in the MMR. After investigating the effect in more detail, it was found that the maximum frequency an article is included in the recommendation lists is around 2 to 4 times higher at = 0, compared to = 1. Thus, for larger contributions of diversity, the algorithm increasingly selects the same article for the recommendation lists. This could be a possible explanation of the decreasing publisher ratio, suggesting that some outliers in the dataset get amplified, thereby suppressing the inclusion of different sources. To be able to study this effect thoroughly, the offline evaluation could have benefited from a setup in which it was possible to assess the contribution of individual framing aspects to the global viewpoint diversity score per article. We can conduct a broader discussion about the viewpoint diversity metric used. Although approaches that use source diversity are more popular, scholars generally agree that viewpoint diversity can only be achieved by fostering content diversity, because, multiple sources can still refer to the same point of view [39] . Based on these findings, this study used a content-based approach. From the results of the offline evaluation, it became clear that increasing levels of content diversity exclude multiple publishers and thus, decreases source diversity. Moreover, some specific publishers got amplified remarkably for high levels of content diversity. Therefore, viewpoint diversification methods could benefit from considering both content and source diversity.  No major influence of viewpoint diversification on the reading behaviour was found, except for the click-through rate calculated per recommendation set, which indicated a statistically significant difference between baseline and diverse users of 6.5% (in favour for baseline recommendations). However, the results of the clickthrough rate calculated per recommendation indicated no significant difference between the two user groups. Likewise, the other two measurements of the reading behaviour, including the completion rate of recommendations and the ratio of users who selected a recommendation as a favourite, showed no significant difference between baseline and diverse users. In reflection on the motivation of this study, the proposed diversification for news media is capable of enhancing the viewpoint diversity of news recommendation, while maintaining comparable measures of the reading behaviour of users. The results thus suggest that recommender systems are capable of preserving the quality standards of multiperspectivity in online news environments. Thereby, situations of extreme low diversity, known as filter bubbles, could also be mitigated. These results are in contrast with the most comparable study, Tintarev et al. [35] , who found a negative effect on intent to read diversified news articles. The authors proposed a viewpoint diversification method based on the MMR-algorithm with linguistic features, such as gravity, complexity and emotional tone. During a user study, 15 participants were asked to make a forced choice between a recommendation from the diverse set and a recommendation from the baseline set, after reading an article on the same topic. It was found that 66% of the participants chose the baseline article, compared with 33% who chose the diverse article. However, in the current study, we observed the reading behaviour of both user groups without them being aware, and we argue that the present setup simulates the situation in a more realistic way. Additionally, the results shed light on the importance of how a recommendation is presented. Multiple presentation properties, such as the inclusion of a thumbnail image and the number of times an article is marked as favourite, were shown to have a significant influence on the click-through rate of recommendations. Future research, thus, should not only address the capability of a model to enhance viewpoint diversity according to an offline metric but also evaluate what presentation characteristics could impact the users' willingness to read multiperspectival news. Related research on viewpoint-aware interfaces, which aim to explain the recommendation choices to users, can be seen as very valuable [25, 34] . We further discuss the limitations of our approach. Choice of participants in the online study. Only users who frequently followed recommendations below articles were selected for the experiment. Thus, the click-through rates presented in this study are higher than for average news readers. Limited number of topics and articles. For both the online and offline evaluations, we used only opinion pieces. Furthermore, each evaluation had a limited number of topics, namely four, as well as a limited number of news articles. New topics could reveal additional results that hold across topics. Missing user perceptions. While we were able to study user behavior at a reasonable scale, a notable omission is users' qualitative judgement of viewpoint diversity in the resulting recommendations. We plan to continue collaborating with the news aggregator platform to refine the proposed framework, i.e., to improve the viewpoints extraction. Presentation characteristics. Some presentation characteristics, and in particular the heart ratio, could also be markers of quality. Further qualitative analysis is needed to e.g., understand how much of user behavior is directed by quality. We also saw that for some topics the presence of thumbnail was more common than for other topics, and it would be relevant to study whether this also interacted with user perceptions of relevance or quality. Relevance metric. The offline study could use a more sophisticated relevance measure between the recommendation and the original article. The relevance score was based on a simple TF-IDF score, limited to the terms in a handcrafted search query. Influence of . Given limited time for online testing, we only compared against a maximum viewpoint diversity score. Influence of publishers. In Figure 3 we see that, although 15 publishers are represented in the datasets, three publishers are predominant. Due to the limited number of articles and the unbalance in terms of publishers, the inclusion of a wide variety of perspectives on a topic can be challenged. In this paper, we proposed a novel method for enhancing the diversity of viewpoints in lists of news recommendations. Inspired by research in communication science, we identified frames as the most suitable conceptualization for news content diversity. We operationalized this concept as a computational measure, and we applied it in a re-ranking of topic relevant recommended lists, to form the basis of a novel viewpoint diversification method. In an offline evaluation, we found that the proposed method improved the diversity of the recommended items considerably, according to a viewpoint diversity metric from literature. We also conducted an online study with more than 2000 users, on the Blendle platform, a Dutch news aggregator. The reading behaviour of users receiving diversified recommendations was largely comparable to those in the baseline. Besides, the results suggest that presentation characteristics (thumbnail image, and the number of hearts) lead to significant differences in reading behaviour. These results suggest that research on presentation aspects for recommendations may be just as relevant as novel viewpoint diversification methods, to achieve multiperspectivity in automated online news environments. As future work, we plan to investigate further the presentation characteristics and how they influence user experience, in addition to behaviour. In more controlled settings, we will study the relative effects of actual (e.g., as judged by experts) versus perceived quality (e.g., number of hearts in the interface) of recommended news items. Future work will also focus on defining a better metric to measure viewpoint diversity, as opposed to topic diversity, c.f., [11] . Additionally, we learnt that contextual information, i.e., general knowledge about a topic (e.g., the current measures in place to stop the spread of coronavirus) can also be essential to reveal a specific frame. We hope that this work will encourage further research on how framing can be defined, conceptualized, and evaluated in the computational domain. 
paper_id= fff23c9301640d4753f79cdf65721a67ca05f91a	title= Highlights  COVID safe behavior includes mask use  Face touching is 1.5 more likely in non-mask wearers Face touching in the time of COVID-19 in Shiraz, Iran	authors= Ramin  Shiraly;Zahra  Shayan;Mary-Louise  Mclaws;	abstract= Background Iranian were advice to wear a mask and not touch their face during COVID-19 restrictions in Iran. Methods 1000 people were observed for 15-30 minutes in public places between 22 April and 9 May 2020. The average number of touches to the mucosal zone was calculated per hour and mask wearers (N=568) were compared with those not wearing a mask (N=432). Findings 92% were observed touching their face at least once an hour and averaged 10 (SD 6) touches per hour. Non-mask wearers touched their face significantly more often than mask wearers (11 vs 8 times per hour, P<0.001). Nonmask wearers were 1.5 (95%CI OR 1.2-2.0) times more likely to touch their mucosal zone than mask wearers (P<0.001). Conclusion Face touching is a common behavior and may have a role in COVID-19 transmission in the absence of hand hygiene. Mask use decrease the frequency of touching the mucosal zone. 	body_text= SARS-CoV-2 has spread rapidly affecting over 10.4 million global citizens and the easy of spread has been demonstrated by the high burden across the globe. 1 Two characteristics of the virus that offers an understanding of the effectiveness for transmission are direct transmission during the pre-symptomatic and symptomatic period and indirect transmission from viral particles on high touch-surfaces. 2, 3 The principal method of community transmission has been assigned to direct exposure to respiratory droplets followed by self-inoculation of the virus into the facial mucosa after touching contaminated surfaces and fomites. 4, 5 Face touching is practiced by humans for several hypothesized reasons that include self-regulation of emotions and to convey non-verbal messaging. 6 The frequency of face touching behavior in the public was evaluated in Australian medical students in their first year at university over a total of 240 minutes of lectures that identified face-touching occurred on average 23 times per hour of which 11 (44%) touches involved mucosal areas of the face; mouth (36%), nose (31%), eyes (27%). 7 Another study estimated the average face-touching was 16 times per hour in 10 students observed performing office work. 8 The public have been advised to protect themselves from COVID-19 with physical distancing, hand hygiene and refraining from face-touching. The aim of our study was to establish the frequency of face-touching behavior by the public after quarantine restrictions were lifted in Shiraz, Iran. The Iranian government commenced lifting quarantine restrictions from 20 April 2020 and observations of face touching in the general community was conducted between 22 April and 9 May 2020. The general community were observed where physical distancing was required including public parks, outpatient clinics, banks and bus stations in Shiraz, Southern Iran. A convenience sample of observations of only adult was used who could be observed for at least 15 minutes in public and who were not using a cellphone. School children were excluded because they were not expected to perceive the risk of COVID-19 pandemic. Every person was observed from a distance of several meters by a trained observer who made no contact with the participants. Observation time of each person ranged from 15-30 minutes. Observers made judgements of the observed persons age (young, middle aged, elders) and gender (male, female) and whether they wore a face mask with or without gloves or none. Face touching was categorized by zone, mucosal zone included eyes, nose, mouth and all other areas were categorized as non-mucosal. The observers used a checklist to record the number of times each observed person touched their facial zone, their mask and duration of observation. 7 The total number of touches for mucosal and non-mucosal zones were summed and divided by the duration of observations to obtain an average number per hour. The mean and standard deviations (SD) was calculated for persons wearing a mask, with or without gloves, and no mask and tested for a significant difference using ANOVA. The likelihood of touching the mucosal zone was assessed for age, gender, glasses wearing and mask wearing using a backward stepwise regression analysis to establish odds ratio with confidence intervals (OR, 95%CI OR) with significance level was set at 5%. SPSS version 19 (IBM, United States) was used for all statistical analysis. A total of 1000 people were observed in public parks (350/1000, 35%), banks (370/1000, 37%), outpatient clinics (260/1000, 26%) and bus stations (20/1000, 2%). Just over half (530/1000, 53%) were male and observers judged the proportion of observed persons to include 44% (440/1000) young, 37% (370/1000) middle-aged and 19% (190/1000) to be older adults. Just over half (57%, 568/1000) wore a mask. The majority (92%) of 1000 people observed made at least one face touch and made on averaged 10 (SD 6) touches per hour. Significantly more face touches regardless of zone were observed in persons without a mask (11 per hour, SD 6) compared with those wearing a mask (8 per hour, SD 5) (P<0.001). Non-mask wearers touched their mucosal zone more frequently than mask wearer (5.5 vs 1.9 times per hour, P<0.001). Nearly half (47%) of all touched by mask wearers were to the mask. Only, non-mask use was a significant predictor for touching the mucosal zone with non-mask wearers 1.5 times (95%CI OR 1.2-2.0, P<0.001) more likely to touch their mucosal zone compared with mask wearers. After Iran lifted restrictions our observation of face touching in 1000 Iranian support the suggestion that this behavior is common. However, the number of face touches per hour in Iranian observed was lower, 10 time per hour during direct observation, than frequencies reported in two student populations 7,8 prior to COVID-19 by 37% to 56% (6 to 13 less touches per hour respectively). In two USA studies that reported direct observations of face touching the average frequency was 9 and 62 times per hour 9,10 and from two studies from USA 11 and Japan 12 using video recordings the average were 16 and 18 times per hour respectively. These ranges may be impacted on the methods of observation. Without pre-COVID-19 observations of facetouching in Iran for comparisons it is not possible to suggest that the public health messages about the potential dangers of face-touching resulted in fewer facetouches. Certainly, we observed just of half of all Iranians wore a mask which was not a cultural norm prior to COVID-19. Mask use was associated with significantly fewer face touches and fewer touches to the mucosal zone than non-mask wearers. There are two studies of noteworthy power that have reported similar protective levels for Sudden Acute Respiratory Syndrome (SARS), OR 0.3 (95%CI OR 0.12 -0.73) 13 and OR 0.32 (95%CI OR 0.17 -0.61) 14 . Evidence for the protectiveness of cloth and non-medical grade masks worn by the public post SARS outbreak has been limited due to a range of design biases. Yet, public cloth mask use has become mandatory for some countries and bundled with other COVID-19 prevention strategies. [15] [16] The World Health Organization (WHO) updated mask guidelines that clarified the use of cloth and non-medical grade masks for protection of the public where physical distancing is unachievable, such as on public transport, and in the public and workplace. 17 The construction of cloth masks now required three layers to improve filtration efficacy. 18 Mask use may reduce transmission of serious viral infections as non-mask wearers were 1.5 times more likely to touch their mucosal zone. Mask touching was 47% of all touches made by wearers and this may be reduced with better fitting and improved breathable masks. Mask worn by health workers for more than 6 hours has been associated with mask contamination with respiratory viruses. 19 But concern for the public becoming contaminated with SARS-CoV-2 on the outer surface of masks may be unsubstantiated. Of the 90 PPE, including respirators, sampled for SARS-CoV-2 from 30 health workers caring for up to 10 minutes of positive patients, all samples tested negative. 20 As with any observations study there are limitations to our study. To reduce any Hawthorne effect participants were not engaged before being observed but we could not standardize the duration of observations and to compromise 15 minutes was chosen as the minimum period. This study was conducted in the first days after lifting quarantine restrictions in Iran. Although quarantine was not strictly enforced in Iran it was observed that there was a reluctance to attend public places. Therefore, the observed population may not be a true representation of Iranian society. We did not find any community data on face touching before the pandemic to compare our observation. We do not expect our results to be generalizable to the healthcare setting because of extended period of wear and type of mask required of health workers. Face touching is a complex behavior which is affected by many cognitive and emotional influences. 6 We could not evaluate the factors influencing facetouching but expect COVID-19 awareness could have reduced this practice. Masks can indirectly reduce the risk of COVID-19 through its mechanical action but also as a mechanism for preventing face touching especially two of the three mucosal areas of the face. Approved by Research Ethics Committee of Shiraz University of Medical Sciences 
paper_id= fff28c4948e827bb5efc14f075b97fa5ed7c1cf5	title= 	authors= Zengyun  Hu;Qianqian  Cui;Junmei  Han;Xia  Wang;Wei E I Sha;Zhidong  Teng;	abstract= In this study, an epidemic model was developed to simulate 	body_text= In the past more than fifty days, the Chinese government and all the people in China fought against the COVID-19 disease, and employed extremely and rigorously controlling measures to protect the people avoiding the infection of the COVID-19 virus, such as the lockdown of many cities in Hubei province (e.g. Wuhan city) and initiating a top-level emergency response to rein in the outbreak of the epidemic associated with COVID-19 in the other provinces of China. With these strong and effective strategic policies, the number of the daily new confirmed COVID-19 cases was significantly decreased from the largest value of 3887 at Feb 4, 2020 to the value of 648 at Feb 22, 2020 from the National Health Commission of the People's Republic of China (http://www.nhc.gov.cn/)(excluding more than 140,000 cases at Feb 12, 2020) . Recently, more and more researchers have been paid large attention on the COVID-19 variations in China, such as detecting the clinical characteristics (Guan et al., 2020) , estimating the spreading characteristics (Wu et al., 2020; Zhao et al., 2020a; 2020b) and exploring the effects of the control strategies (Chinazzi et al., 2020; Huang et al., 2020; Lin et al., 2020; Tang et al., 2020) .The individual behavioural reaction and governmental actions played a key role in controlling the spread of the COVID-19 outbreak for the public health in the world, e.g., holiday extension, travel restriction, hospitalisation and quarantine (Chinazzi et al., 2020; Lin et al., 2020) . Until now, there are only few researches about the effects of different population migration and quarantine strategies on the COVID-19 variations in China. Guangdong province has the largest gross domestic product (GDP) than the other provinces in China. Moreover, according to the present COVID-19 variations and control strategies, the Guangdong province adjusted the emergency response level of epidemic prevention and control from the first level response to the second level at Feb 24, 2020 (http://www.gd.gov.cn/). More and more workers will come back to Guangdong province from other provinces. Thereby, we choose Guangdong province as a case study to explore the effects of the population migration and quarantine strategies on the COVID-19 variations. Based on the present rigorous and extreme control measures in Hubei province, input population from Hubei province are not considered. In this study, we focus on the input population and quarantine strategies influencing on the disease variations, including the peak values of the cumulative confirmed cases, the daily new increased confirmed cases and the confirmed cases, and the corresponding times. The organization of this paper is as follows. In the next section, the establishment of SEIRQ model, data and methodology are illustrated. In Section 3, the input population and quarantine strategies at different scenarios are investigated which are our main results. A brief discussion is provided in Sections 4. In this study, according to the characteristics of the COVID-19 transmission, the whole population at time t is divided into seven compartments which include the susceptible individuals S(t), exposed individuals E(t), infectious individuals I(t), removed individuals R(t), quarantined susceptible individuals S q (t), quarantined exposed individuals E q (t) and quarantined infectious individuals I q (t). The COVID-19 disease is transmitted from I(t) to S(t) with the incidence rate of β, and from E(t) to S(t) with the incidence rate of σβ, respectively. The susceptible individuals S(t) is partly quarantined with the rate of q 1 (t). We assume that exposed individuals E(t) and quarantined exposed individuals E q (t) are transmitted to infectious individuals I(t) and quarantined infectious individuals I q (t) with the same transition rate of ν. The quarantined rates of exposed individuals E(t) and infectious individuals I(t) are q 1 (t) and q 3 . The death rate induced by the COVID-19 disease is α in both infectious individuals I(t) and quarantined infectious individuals I q (t) which removed to the removed individuals R(t). γ(t) is the recovery rate of quarantined infected individuals I q (t) which is the mainly part of removed individuals R(t). Moreover, based on the population migration, we assume that the input population and output population have constant numbers. Susceptible individuals S(t), exposed individuals E(t) and infectious individuals I(t) have their respective input individuals of p 1 (t)A(t), p 2 (t)A(t) and p 3 (t)A(t), and the parameters p i (t), i = 1, 2, 3 are the rates of susceptible individuals, exposed individuals, infectious individuals in the total input number of A(t) from other provinces. The output population are B 1 , B 2 and B 3 for the susceptible individuals S(t), exposed individuals E(t), infectious individuals I(t). The COVID-19 disease transmission and population migration are demonstrated by Figure 1 in details. Quantined Infective (I q ) Quantined Susceptible (S q ) The SEIRQ epidemic model can be described by the following system of ordinary differential equations where the prime ( ) denotes the differentiation with respect to time t. Here, parameters 0 < β, ν, γ(t), α < 1 and the quarantined rates 0 ≤ q 1 (t), q 2 (t), q 3 ≤ 1. All the initial values of different individual groups: S(0), E(0), I(0), R(0), S q (0), E q (0), I q (0) are non-negative. In this study, the COVID-19 cases of Guangdong province, Hubei province and mainland China are obtained from the Health Commission of Guangdong Province (http://wsjkw.gd.gov.cn/), the Health Commission of Hubei Province (http://wjw.hubei.gov.cn/), and the National Health Commission of the People's Republic of China (http://www.nhc.gov.cn/), respectively. The data are from Jan 20, 2020 to present which include the number of the cumulative confirmed cases, the number of the confirmed cases, the number of the cumulative cured cases and the number of cumulative death cases. The numbers of the total population of Guangdong Province, Hubei Province and mainland China are employed at the end of 2018 from the National Bureau of Statistics(http://www.stats.gov.cn/). The numbers of the input and output population from Hubei province and the other provinces of mainland China to Guangdong province are from the Baidu migration(http://qianxi.baidu.com/). These data are covering the period of Jan 1, 2020 to Feb 20, 2020 which are employed to display the population migration variations from other provinces to Guangdong province. Because the input population from Hubei province to Guangdong province is significantly decreased from 26.86% of the total input population at Jan 26, 2020 to the 6.84% at Jan 27, 2020, for the Guangdong province, the starting date of the COVID-19 disease data is from Jan 27, 2020. In this study, for the COVID-19 variations, we focus on the cumulative confirmed cases and confirmed cases. The largest value of the cumulative confirmed cases means the total number of the population infected by the COVID-19 disease. The disease extinction time is defined as the day with no confirmed case which is the time of I q (t) = 0. The initial values and parameters can be obtained from the Text methodology of the supplementary information. The baseline parameters noted as (A, B, p 1 , p 2 , p 3 , q 1 , q 2 , q 3 , α, β, ν, σ, γ) = (A * , B * , p 1 * , p 2 * , p 3 * , q 1 * , q 2 * , q 3 * , α * , β * , ν * , σ * , γ * ) is obtained from the simulation result of the cumulative confirmed cases, the daily new confirmed cases, the confirmed cases and the recovered cases. To compare with the baseline results, three aspects from the perspectives of the input population and quarantine strategies on the COVID-19 variations are analyzed: (1) aspect 1, effects of the input population at different scenarios; (2) aspect 2, effects of quarantine rates at different scenarios and (3) aspect 3, effects of both input population and quarantine rates at different scenarios. To evaluate the accuracy of our model, five statistical indices are applied, including the absolute error (AE), relative error (RE), mean absolute percentage error (MAPE), determinant coefficient R * 2 which is the square of correlation coefficient R * and distance between indices of simulation and observation (DISO) (Hu et al., 2016; . The details are displayed in Text methodology of the supplementary information. In this section, the variations of the COVID-19 in Guangdong province are simulated and predicted based on our SEIRQ model only considering the input population from the other provinces of China (excluding Hubei province). The simulated period are from Jan 27, 2019 to Feb 19, 2020. The parameter values and the initial values of our simulation and prediction are provided in Table 1 . The performance is evaluated by the data from Feb 20, 2020 to Feb 23, 2020, and R * 2 , AE,RE, RMSE, MPAE and DISO are employed to quantify the accuracy. The simulation and prediction results are displayed in Table 2 and Figure 2 . Our model has the ability to simulate and to predict the COVID-19 variations with the very high accuracy (Table 2 and Figure 2 ). Particularly, the determinant coefficients R * of the cumulative confirmed cases, confirmed cases and recovered cases are highly to 0.9973, 0.9898 and 0.9934, respectively ( Table 2 ). Very small estimations are obtained with the AE values of -5.33, -2.63 and -3.38 for the cumulative cases, confirmed cases and recovered cases. The comprehensive accuracies of our model are quantitatively measured by the DISO with the values of 0.06, 0.11 and 0.17 for the cumulative cases, confirmed cases and recovered cases. For the validation at Feb 20, Feb 21, Feb 22 and Feb 23, 2020, the very small RE values of the cumulative confirmed cases, confirmed cases and recovered cases indicate that our model also has very high accuracies and it can be employed to predict the future variations of the COVID-19 disease (Table 2) . Moreover, the largest number of cumulative confirmed cases is 1397 at May 7, 2020 which indicates that the COVID-19 disease will become extinction after 102 days in Guangdong province ( Figure 2A , STable 1). The peak value time of daily new confirmed cases is Feb 1, 2020 which is highly agrement with the reported time at Jan 31, 2020 ( Figure 2B ). For the confirmed cases, the peak value and the corresponding time are both obtained by our model with the simulated values of 1002 at Feb 10, 2020 and reported values of 1007 at Feb 9, 2020 ( Figure 2C ). The number of the recovered cases will reach about 1400 which is consist with the future changes of the cumulative confirmed cases ( Figure 2D ). The fraction of input population 0.9999927 Computed into susceptible class p 2 The fraction of input population 0.0000073 Computed into exposed class p 3 The fraction of input population 0 Assumed into infected class Initial values Definitions Esimated value Source N (0) Initial total population 113460000 GSY S(0) Initial suscetible population 113346174 Estimated E(0) Initial exposed population 31 Estimated Initial quarantined susceptible population 113460 Estimated E q (0) Initial quarantined exposed population 128 data I q (0) Initial quarantined infected population 184 data R(0 Initial recovered population 4 data Note. GSY: Guangdong Statistical Yearbook 2019 In order to further explore the forecasting accuracy of our model, we have been compared the forecasting result with the observed data prolonged 11 days from Feb 24, 2020 to Mar 4, 2020. The absolute values of RE (relative error) of the cumulative confirmed cases are smaller than 1% ( Table 3) . The corresponding figures also display that our model can capture the temporal variations in a relative longer period (see SFigure 1 in the supplementary information)  The input population variations include the percentage changes p 2 of the exposed individuals and the number changes A of the input population which impact the disease on the peak value of the cumulative confirmed cases and the disease extinction time (Figures 3, 4) . From the above analysis, it can be concluded that the increased numbers of the input population can mainly shorten the disease extinction days and the increased percentages of the exposed individuals of the input population increase the number of cumulative confirmed cases at a small percentage. Both the increased input population and the increased exposed individuals have no impacts on the peak values and peak value times of the confirmed cases.  In this section, the effects of quarantine rates at six scenarios on the COVID-19 variations are displayed in Figure 5 and Figure 6 . For the first time point t 1 = 10, Feb 6, 2020, Sce 1 (q 1 , q 2 ) = (0q 1 * , 0q 2 * ) has significantly negative impacts on the COVID-19 variations with the disease outbreak again which suggest the very high risks appear at the quarantine strategy of Sce 1 ( Figure 5A , Figure 6A ). Specifically, the confirmed cases reaches its first peak value as the baseline result at Feb 10, 2020, and then the number is decreased close to 97 at Mar 14, 2020. A sharp increase is detected to the second peak value of the confirmed cases with the number of 1016704 at 165 days ( Figure 6A ). The disease will become extinction after 361 days with the MVCCC dramatically reaching to more than 9 million ( Figure 5A and STable 2). Sce 2: (q 1 , q 2 ) = (0q 1 * , 0.5q 2 * ) and Sce 3: (q 1 , q 2 ) = (0q 1 * , q Moreover, we also explored that the second outbreak of the disease appears when both the values of q 1 and q 2 are nearly close to zero, such as (q 1 , q 2 ) = (0.01q 1 * , 0.01q 2 * ), (0q 1 * , 0.05q 2 * ) at t 1 = 10, and (q 1 , q 2 ) = (0q 1 * , 0q 2 * ) at t 1 = 11 (Figure 7 , STable 3). This suggests that no quarantine or very weak quarantine on the susceptible individuals and exposed individuals before the days of the peak values of the confirmed cases may lead to the disease outbreak again.  The impact results of both the input population and quarantine rates on the COVID-19 disease are displayed in Figure 8 , Figure 9 and STable 3. According to the results in Section 3.2 and Section 3.3, the second outbreak of the disease are obtained in the scenarios with no or very weak quarantine strategy. Therefore, Figure 8 and Figure 9 only provide the COVID-19 disease variations of the scenarios with second outbreak, and the disease variations in other scenarios are not provided. STable 4 provides the results of all the scenarios. For time point t 1 = 10, Sce 1: (p 2 , A, q 1 , q 2 ) = (1.5p 2 * , 1.5A * , 0q 1 * , 0q 2 * ), Sce 2: (p 2 , A, q 1 , q 2 ) = (1.5p 2 * , 2A * , 0q 1 * , 0q 2 * ), Sce 7: (p 2 , A, q 1 , q 2 ) = (2p 2 * , 1.5A * , 0q 1 * , 0q 2 * ) and Sce 8: (p 2 , A, q 1 , q 2 ) = (2p 2 * , 2A * , 0q 1 * , 0q 2 * ) have the MVCCC larger than 10 million at 328, 313, 327 and 312 days ( Figure 8A, STable 3) . In fact, they have the two outbreaks of the disease with the confirmed cases having the first peak value as the baseline result at Feb 10, 2020 and the second peak values larger than 1 million at 142 days, 132 days, 141 days and 130 days for Sce1, Sce 2, Sce 7 and Sce 8, respectively ( Figure 9A, STable 3) . The magnified figure in the period of Jan 27, 2020-Apr 26, 2020 clearly displays the second outbreak of this disease ( Figure 9A ). Moreover, the weak changes of the four scenarios in the quarantine rates or around the time point t 1 = 10, the second outbreak also resulted in the second outbreak of the disease. If the control measures employed as the four scenarios after the other three time points t 1 = 20, t 1 = 28, and t 1 = 38, the MVCCC are rapidly decreased with still larger than the baseline results, and the DDE are prolonged except the Sce 2 and Sce 8 of t 1 = 28, and t 1 = 38 (STable 4). For the other scenarios: Sce 3-Sce 6 and Sce 9-Sce 12 of the four time points, the DDE become smaller than the baseline result due to the larger input population and more exposed individuals. Moreover, the weaker quarantine rates together with the more input population resulted in the more infected individuals and increased the MVCCC (STable 4).  Since the COVID-19 disease reported in Wuhan city, Hubei province of China, the Chinese government and all the people have been fighting against the disease for more than two months. Now, the daily new confirmed cases have been continuously decreasing, and the latest value is 427 at Feb 28, 2020 from the National Health Commission of the People's Republic of China (http://www.nhc.gov.cn/). According to the present COVID-19 disease situation, some provinces have been adjusted the emergency response level of epidemic prevention and control from the first level response to the second level, such as Guangdong province. More and more workers are coming back to Guangdong province from other provinces. To address the effects of the input population on the disease variations, taking Guangdong province as a case study, the impacts of the input population and quarantine strategies are explored using a dynamical epidemic model at three aspects. They include aspect 1: effects of the input population at different scenarios; aspect 2: effects of quarantine rates at different scenarios and the last aspect (i.e. aspect 3): effects of both input population and quarantine rates at different scenarios. For the population flow, recent study (Tang et al., 2020) considered the data from the Baidu migration website in a stochastic discrete transmission dynamic model. Both our study and Tang et al (2020) obtained the risk of the secondary outbreak when the population flow are changed at a serious input population flow. In Tang et al (2020), with more data from the Health Commission of Shananxi Province, they estimated the daily new increased confirmed cases, and the daily new increased infectious individuals from the population flow by the Poisson distribution. In our study, constrained by the data policy of the Health Commission of Guangdong Province, the input population is defined as the deterministic and continuous input. Moreover, the ratio of the exposed individuals accounting for the input population is defined as the percentages of the exposed individuals in the total population of China excluding Guangdong and Hubei provinces which is derived from the daily new increased confirmed cases according to the 3-7 days latent periods. In the development of the COVID-19 model, Tang et al (2020) considered the quarantined susceptible individuals returned back to susceptible individuals after 14 days quarantine. While this condition is not included in our study The major reasons are displayed as follows. Under the present quarantine strategies in China, the susceptible individuals are quarantined in the forms of home quarantine, community quarantine. Although the quarantined susceptible individuals can be returned to susceptible individuals after 14 days, they will certainly employ very strict other controlling strategies against the COVID-19 virus, such as wearing the medical masks and washing their hands frequently, and which result in only very small part of the quarantined susceptible individuals back to the truth susceptible individuals. For the simulation and prediction abilities of our model, it displayed that our model can well capture the COVID-19 variations with high accuracy. In general, it is very hard to capture the disease variations with high accuracy by the dynamical models. We have been compared our forecasting with the observed data prolonged 11 days from Feb 24, 2020 to Mar 4, 2020. The absolute values of RE (relative error) of the cumulative confirmed cases are smaller than 1% ( Table 2) . The corresponding figures also display that our model can capture the temporal variations in a relative longer period (see SFigure 1 in the supplementary information). The weaker forecasting capabilities from Feb 24, 2020 to Mar 4, 2020 than these from Feb 20, 2020 to Feb 23, 2020 are resulted by the parameter estimation period of Jan 19, 2020 to Feb 19, 2020. At the same time, it inspired that if we want to obtain a high accuracy in a relative longer period the dataset used to estimate the parameters should be changed or prolonged with the time. Our result indicated that the increased numbers of the input population can mainly shorten the disease extinction days and the increased percentages of the exposed individuals of the input population increase the number of cumulative confirmed cases at a small percentage. Both the increased input population and the increased exposed individuals have no impacts on the peak values and peak value times of the confirmed cases. For the impacts of aspect 2, no quarantine or very weak quarantine on the susceptible individuals and exposed individuals before the days of the peak values of the confirmed cases may lead to the disease outbreak again. This proves the significant role of the quarantine strategy on the disease control. If we increase the input population and decrease the quarantine strategy together around the time point of the peak value of the confirmed cases, there will appear second outbreak of the disease exponentially. Moreover, the weaker quarantine rates together with the more input population resulted in the more infected individuals and increased the number of the cumulative confirmed cases. More information about our simulation and quarantine situation can be explored if more data can be obtained. In this study, to address the quarantine situation in Guangdong province, 108 scenarios are listed from the input population and quarantine strategies which may include the present quarantine strategies in Guangdong province. The other further analysis of the COVID-19 variations, such as the daily number of people under medical observation, will be explored when more new data are obtained in future. Based the above analysis, we have the major conclusions as follows. (1) The COVID-19 disease variations can be simulated by our models with very high accuracy, including the cumulative confirmed cases and confirmed cases. (2) Under the present daily input population and quarantine strategy, the COVID-19 disease will become extinction in May 11, 2020, with the cumulative confirmed cases number of 1397. (3) In Guangdong province, the adjustment of the emergency response level of epidemic prevention and control from the first level response to the second level at Feb 24, 2020 is reasonable which is also predicted by our model. (4) The disease will have a second outbreak risk when the input population is remarkably increased and the present quarantine strategy rapidly decreases to the values around zero. 
paper_id= fff3152c65deed9063be33352383f9f00db9197b	title= Smart testing and selective quarantine for the control of epidemics ✩	authors= Matthias  Pezzutto;Nicolás  Bono Rosselló;Luca  Schenato;Emanuele  Garone;	abstract= This paper is based on the observation that, during Covid-19 epidemic, the choice of which individuals should be tested has an important impact on the effectiveness of selective confinement measures. This decision problem is closely related to the problem of optimal sensor selection, which is a very active research subject in control engineering. The goal of this paper is to propose a policy to smartly select the individuals to be tested. The main idea is to model the epidemics as a stochastic dynamic system and to select the individual to be tested accordingly to some optimality criteria, e.g. to minimize the probability of undetected asymptomatic cases. Every day, the probability of infection of the different individuals is updated making use of the stochastic model of the phenomenon and of the information collected in the previous days. Simulations for a closed community of 10'000 individuals show that the proposed technique, coupled with a selective confinement policy, can reduce the spread of the disease while limiting the number of individuals confined if compared to the simple contact tracing of positive and to an off-line test selection strategy based on the number of contacts. Annual Reviews in Control xxx (xxxx) xxx 	body_text= During the Covid-19 epidemic, one of the limiting factors that affected the capability to handle the spread of the disease was the limited number of available tests. This lack of information has created major issues in several countries and promoted the idea that testing is essential in the control of an epidemic (Salath et al., 2020) . Recent research works support the importance of testing to effectively control an epidemic, see Brotherhood, Kircher, Santos, and Tertilt (2020) , Eichenbaum, Rebelo, and Trabandt (2020) and Wang (2020) . In this regard, the selection of the individuals to be tested has become a major concern in many countries. However, to the best of the authors' knowledge, research on how to define these testing policies is still at a very early stage (Nowzari, Preciado, & Pappas, 2016) . This observation is testified by the de facto policies applied by decision makers during the Covid-19 epidemic. Among the various policies we can mention the use of contact tracing of individuals exposed to positive cases (Cereda et al., 2020) , contact tracing combined with additional random testing (Shim, Tariq, Choi, Lee, & Chowell, 2020) , the use of exhaustive control of new arrivals in isolated communities (Wang, Ng, & Brook, 2020) , and the testing of people with high number of human interaction such as health care personnel (Padula, 2020) . It is worth to note that most of these strategies rely on the appearance of symptomatic cases and required the use of hard lockdown policies to be effective. Interestingly enough, the selection of individuals to test has important similarities with the problem of sensor selection for state estimation in the context of Wireless Sensor Networks. In both cases only a limited amount of information on a partially unknown process can be retrieved due to a limited amount of resources, i.e. the number of available tests or the channel bandwidth, respectively. The objective is to optimize where to collect the measurements based on the available information and on the model of the process. Sensor selection has been an active field in the last two decades: a method based on convex optimization is proposed by Joshi and Boyd (2008) , a stochastic policy is studied by Gupta, Chung, Hassibi, and Murray (2006) and the optimal periodic policy for two sensors is given by Shi, Cheng, and Chen (2011) . In the case of a general number of sensors the problem has been explored by Vitus, Zhang, Abate, Hu, and Tomlin (2012) over a finite horizon, by Mo, Garone, and Sinopoli (2014) over the infinite horizon, and for a general number of independent dynamical systems by Han, Wu, Zhang, and Shi (2017) . However most of available works on sensor selection focus on real-valued dynamical systems, while the case where the process state assumes values from a finite set is at the best of our knowledge still largely unexplored. The first step to propose an effective smart testing is the selection of an adequate model to monitor the epidemic. Compartmental epidemic models proved to provide accurate estimations of the dynamics of an https://doi.org/10.1016/j.arcontrol.2021.03.001 Received 24 July 2020; Received in revised form 19 December 2020; Accepted 2 March 2021 epidemic (Brauer, 2008) . These models can be divided in deterministic models, governed by differential equations (McCluskey, 2010) , or stochastic models, where the heterogeneity of small communities can be better represented (Bøjrnstad, Finkenstädt, & Grenfell, 2002; Lopez-Herrero & Amador, 2017) . New models tailored for the Covid-19 case have been developed seeking for more suitable approaches for the design of control strategies, e.g. Casella (2020) , Franco (2020) and Giordano et al. (2020) . However, the nature of compartmental models implies an homogeneously distributed population with random mixing between individuals, which does not inform about the granular distribution of the disease. To model the granularity of the spread of a disease, network diffusion models provide a better insight of the population's distribution and allow to identify the critical clusters of the spreading. The most common network diffusion models are based on Stochastic Cellular Automata (SCA), where the spread of the disease depends on the interaction between neighbouring cells (Mikler, Venkatachalam, & Abbas, 2005; White, del Rey, & Sánchez, 2007) . This idea has been lately extended to more complex network topologies (Keeling & Eames, 2005; Li, Tsai, & Yang, 2014) . In these complex network models the interactions between individuals are modelled as the edges of a graph. This representation makes it possible to also model time-varying interactions, as well as selective quarantine policies (e.g. by removing the connections of certain individual with the rest of the population). From the theoretical viewpoint it is possible to prove that any SCA model is equivalent to a Markov chain (Ruhi & Hassibi, 2015) . As we will discuss later on in this paper, this fact, although important from the theoretical viewpoint, is however not very useful in practice as the resulting Markov chain has a number of states that is exponential in the number of the states of SCA. It is important to mention that while the use of network models has been often overlooked due to the difficulty to monitor and define the interactions in real communities, in the authors' opinion the conception of more advanced tracking systems during the last pandemic leads naturally to this kind of approaches. The problem of estimating the state of partially observable dynamic networks has been object of only a few studies in the last few years. One of the most studied problems is the estimation of the source of an information spread in networks using only limited observations. Zhu and Ying (2014) , Zhu and Ying (2016) propose a sample path algorithm to estimate the location of a source of information or a disease. Alexandru and Dragotti (2019) extend this idea to the case where multiple rumours are spread and the time of the origin of the information is unknown. These works provide interesting idea that can be possibly adapted for the estimation of the evolution of an epidemic over a network. An alternative approach to the surveillance of epidemics within networks can be found on the use of a sentinel system to estimate the evolution of the epidemic as done by Braeye, Quoilin, and Hens (2019) . A sentinel system involves a limited network of selected reporting sites monitoring the disease in small portions of the population. The obtained data is used to estimate the behaviour of the entire network. Souty and Boëlle (2016) estimate the total number of cases of influenza based on the population density associated to each reporting site. Although this approach uses the density of population to improve the estimation of the state of the epidemic, the total population is still divided into clusters with homogeneous distribution and interactions. At the time of the writing of this paper, some early work presenting attempts to define smart testing and quarantine policies have been just published. In particular (Berger, Herkenhoff, & Mongey, 0000) propose a policy based on conditional quarantine and random testing. However, the model based on partial observations assumes that tested negatives are ''tagged'' and they remain observable after a single test. In another recent paper on the subject by Kasy and Teytelboym (2020) , the trade-off between quarantine and testing is regarded by defining a certain threshold based on the infection probability and related to the cost of testing or quarantining an individual. In this case, the partial information is inferred based on the social group of the individual rather than its interactions within the network. The main contribution of this paper is to propose a smart testing strategy to select the individuals to be tested based on the estimated probability of infection of each individual. As a first step we propose a method to make an approximated estimation of the current state of the epidemic which is computationally inexpensive. On the basis of this estimation, the testing policy is defined as a constrained optimization problem. This testing policy is coupled with a selective confinement policy which allows to only confine few individuals of the population based on the outcome of the tests. We compare the proposed strategy with the current best practice, namely contact tracing of positives, and a suitable topology-based strategy, where individuals to test are selected according to their number of contacts. Numerical simulations show the advantage of this approach both in terms of number of infected individuals and in terms of number of individuals put in quarantine at each time. In particular, on a population of 10'000 individuals, the total number of infected is 8 times less and the total amount of days spent in quarantine is 5 times less with respect to the current best practice, and the improvement with respect to the topology-based strategy is even more evident. These results also show that tracing of contacts is crucial to keep under control the epidemic but it can be largely improved by using the algorithms proposed in this paper. The proposed algorithms can be used in a centralized way (e.g. by a decision maker) but they are also suitable to work in a distributed privacy-aware fashion and to integrate with tracing devices. The remainder of the paper is organized as follows. In Section 2 the proposed model of the epidemic is presented. Sections 3 and 4 introduce the exact and the approximated estimations of the evolution of the epidemic. Section 5 defines the testing strategy and Section 6 the quarantine actions. In Section 7 several simulations demonstrate the performance of the proposed strategies. Section 8 provides conclusions and future works. Consider a population of individuals where a disease is spreading. Each individual can be susceptible, infected, or removed. The spreading of the epidemic is modelled according to the following assumption. Assumption 1. A susceptible individual can be infected by other infected individuals of the population with whom he had a direct contact. Once an individual is infected, the individual will eventually become removed and cannot be infected a second time. The exposure to an infected individual is a necessary but not-sufficient condition for a susceptible individual to become infected. Indeed, the contagion actually takes place if some events (e.g. exchange of body fluids for flu-like illnesses) have occurred and thus it is intrinsically stochastic. Motivated by these considerations, we model the transmission of the disease through random variables. Similarly, also recovery is modelled as a random variable to capture the uncertainty of the recovery process. Mathematically, each individual has at fixed time instants, say every day, a state ( ) ∈ { , , } that can take three logical states: • S -susceptible, the individual is healthy and was never infected before, so it is susceptible of being infected; • I -infected, the individual is infected and can infect others; • R -removed, the individual has been infected in the past and cannot be infected anymore (because immune or dead). We denote with ( ) ∈ {0, 1} the binary stochastic input representing the stochastic contagion event at time . The variable takes value ( ) = 1 if the th individual has been infected between time and time + 1, and ( ) = 0 otherwise. To characterize we introduce the transmission variables ( ) ∈ {0, 1} which takes value ( ) = 1 if the infection is transmitted from to between time and time + 1 given that the individual was infected and the individual was susceptible. The same way ( ) ∈ {0, 1} denotes the binary stochastic variable representing the stochastic recovery event at time . In particular, ( ) = 1 if the th individual becomes removed between time and time +1, and ( ) = 0 otherwise. Note that the recovery variable ( ) indicates the moment that individual is not infected anymore, due to immunity or death. Finally the state of each individual evolves according to the following equation The variable ( ) is modelled according to the following assumption, Assumption 3. The recovery ( ) is a Bernoulli random variable with mean constant over time. Moreover ( ) is independent of ( ) ∀ , ≠ , , of ( ) ∀ , , , and of the initial state (0) ∀ . In general the system is partially observable as symptoms only appear in a small percentage of the population. The appearance of symptoms is modelled by the random variable ( ) ∈ {0, 1} taking value ( ) = 1 if th individual is infected and shows symptoms between time and time +1, and 0 otherwise. We model it according to the following assumption. Assumption 4. The appearance of symptoms ( ) is a Bernoulli random variable with mean constant over time. Moreover ( ) is independent of ( ) and ( ) ∀ , ≠ , , of ( ) ∀ , , , and of the initial state (0) ∀ . In this paper we consider the case in which only a limited amount of tests are available at each time . We assume that when a test is performed on the th individual at time we obtain the information if ( ) = or not. No other information is provided by the test, so it is not possible to distinguish if an individual is susceptible or recovered. We can formally introduce the auxiliary state ( ), taking value ( ) = 1 if ( ) = , and ( ) = 0 otherwise, that represents the binary variable accessed by the test. For Covid-19 the value of ( ) can be retrieved by exploiting several different kind of tests. To date, even if false negatives are not completely avoided, PCR tests are widely considered to be very accurate (it is the only recommended method for the identification of Covid-19, see World Health Organization (2020a), and the reference standard for many medical studies, see e.g. Dinnes et al. (2020) ). Based on this consideration, the paper assumes that available tests are ideal, which is in line with an ample part of the literature dealing with testing strategies (Berger et al., 0000; Piguillem & Shi, 0000) . To model the testing phase, we introduce the selection variable ( ) taking value ( ) = 1 if individual is selected to be tested at time and ( ) = 0 otherwise. The variable ( ) is thus a controlled variable that can be managed to tackle the disease diffusion. Beside tested individuals, we also consider that additional information is provided by symptomatic individuals. In the context of this work, a symptomatic individual is assumed to be an infected person who spontaneously visits a medical centre with clear symptoms and it is diagnosed with the disease. Let ( ) = { 1 ( ), 2 ( ) , … , ( ) ( )} be the set containing the indices of the individuals who are tested at the time instant and of the individuals who show first symptoms at time . Note that the cardinality ( ) of the set is time-dependent since the number of symptomatic individuals is not constant. The observed output at time can be then expressed as while the set of the observed outputs up to time instant is and it represents the information available at time instant . Beside the testing phase, the system evolution is affected by the quarantine mechanism represented by the control variable ( ), taking value ( ) = 1 if individual is in quarantine at time and ( ) = 0 otherwise. The variable ( ) is the control variable that governments use to tackle the epidemic. The goal of this paper is the definition of a policy to select the individuals to be tested that, in conjunction with a selective quarantine policy, is able to reduce the spread of the disease while keeping a limited number of individuals in quarantine. To do so, we tackle the problem by proposing the closed-loop structure reported in Fig. 2 consisting of three stages: (1) estimation of the states ( ) using the information available 0∶ from the feedback of the outputs; (2) selection of the individuals to test by optimizing a reward function (⋅); and (3) based on the output ( ), execution of control actions through selective quarantine. The following sections focus on the derivation of a proper state estimation given the information available 0∶ and the definition of a suitable reward (⋅). The state of the whole population is defined by the vector Under Assumptions 1, 2, 3, at any time , the next state of the population ( + 1) depends only on the current state of the population ( ). Accordingly, in line of principle, the stochastic process describing M. Pezzutto et al. To model the dynamics of the Markov chain, we have to derive the transition matrix ∈ 3 × 3 whose entries are where , ∈ { , , } represent two possible states of the network, and z, v represent the indices of the transition matrix associated to them. Under Assumptions 1, 2, 3, the next states of two individuals are independent given the previous state of the population. It follows that allowing to compute the transition probabilities of the network as a derivation of the transition probabilities between the states of each single individual. Since only the transition from susceptible to infected depends on the state of other individuals, the following simplification holds where , ∈ { , , }. The state transition probability of any individual at time can be computed as while the probability of remaining in a given state is All other transitions have probability 0. A major difficulty in our setting is that the evolution of the system can be only observed by symptomatic individuals and selective tests on the population. Since ( ) < , the Markov model is hidden and can be only partially observed through the output. The complete characterization of the state given the available information is provided by the joint distribution ( ( ), 0∶ ). For a given 0∶ , the joint distribution can be represented by a vector of dimension 3 with entries ( , 0∶ ) =̂( ) ∀ ∈ { , , } . In the case of Hidden Markov Models, this joint distribution can be easily computed by means of the forward algorithm (see Blunsom (2004) and Rabiner (1989) ), providing the following expression The computation of ( ( )| ( ) = ) is then easy: ( ( )| ( ) = ) = 1 if the state ( ) = gives the output ( ), and 0 otherwise, namely if ( ) is not a possible output for the state . From the joint distribution, the conditional distribution is where we used the Bayes rule and the law of total probability. Recall now that the optimal estimate of a random variable corresponds to the expected value given the observations, see Anderson and Moore (2012) . Then the optimal estimate of the -individual̂( | ) iŝ where we used the definition of expected value for a binary variable and the law of total probability. This procedure allows to obtain the probability of each individual to be infected at time given the complete vector of observations 0∶ . However, in spite of allowing to compute the exact probability, this approach requires the computation of all the transition probabilities of the matrix and the use of a vector variable of size 3 , which is not computationally feasible even for small populations. Due to the prohibitive burden of an exact probability computation, in this paper we propose an approximated low-computational algorithm to estimate such probability. The proposed approximated estimation is based on the idea of temporal and spatial truncation of the updates and is also suitable for decentralized implementations More precisely, we propose to propagate the information from testing only to individuals that are the most correlated to the tested individuals, namely the ones that have direct contact with tested people, while for the remaining part of the population the update is performed based on previous estimates and the topology of the network representing the population. In the same way, only a limited amount of past state estimates are assumed to be affected by the new information. This approximation allows to retrieve the information regarding the individuals that are most affected by the result of each test while keeping a limited computational time. We define the estimate of the state of the individual aŝ where the local information set for the th individual  0∶ is defined as The local information set  0∶ consists of the local state estimates of direct contacts, updated at the time instant of the interaction, and the state estimates of tested individuals, updated just after the test. In order to keep the computations associated to the th individual limited, we define a local approximated estimation which can be retrieved based only a partial knowledge of the network. More precisely, the state of each individual is estimated under the assumption that only its contacts are known, and no information on the connections between any other individuals is assumed to be available. In this sense, we will focus only on individuals with whom the th individual was in contact: in the case of untested individuals we will use only the previous local estimateŝ ( − 1| − 1) while in the case of tested individuals we will use the current state ( ) ∈ ( ) and the updated estimation̂( | ), ≤ − 1 of past states. Denote by 0∶ the (random) vector collecting all the random variables ( ) ≤ , namely 0∶ = ( (0), … , ( ) ) ′ . With a little misuse of notation 0∶ = 0 denotes the case where all the past states ( ), ≤ , are equal to 0. We assume that the random variables ( ) ( ), ≠ , are conditional independent given 0∶ = 0. Similar assumptions are made by Boguná, Castellano, and Pastor-Satorras (2013) and Valdano, Ferreri, Poletto, and Colizza (2015) . The rationale is that, if the individual has always been healthy, the coupling between two of his neighbours and is negligible, as in the case when individual is the only connection between and or, even if ( ) ≠ 0, contacts of are enough different from contacts of . It follows that To further simplify the estimation algorithm we will simplify the stochastic recovery with a deterministic one based on the average recovery time . Then we havê We compute ( 0∶ = 0 |  0∶ ) and ( 0∶ − = 0 |  0∶ ) as where the last equality holds since ( ) ( ) ≠ are conditional independent given 0∶ −1 = 0. To obtain the numerical value of ( 0∶ = 0 |  0∶ ) would require to compute ( ( − 1) = 1 | 0∶ −1 = 0,  0∶ ) that in turn would require ( ( − 2) = 1 | 0∶ −1 = 0, 0∶ −1 = 0,  0∶ ) and so on. Since this propagation is very computationally expensive we make the approximation that The underlying assumption is that the state of an individual and those of its neighbours are independent. The accuracy of this assumption has been explored by Gleeson, Melnik, Ward, Porter, and Mucha (2012) where it has been shown that the dynamics are well approximated if the degrees of closest neighbours are high. At the same way, the assumption holds when the underlying network of contacts is timevarying, but the results can be less accurate if pairs of individuals have frequent interactions, and many contacts in common. Since this happens in real life (think of relatives and colleagues), we introduce a correction factor ( ( ), ( − 1), … , (0)) ∈ [0, 1], for simplicity denoted by ( ), that accounts for the coupling of individuals and due to the interactions before ( ( − 1) = 1| 0∶ −1 = 0,  0∶ ) = ( − 1) ( ( − 1) = 1 |  0∶ ). (20) In line of principle, ( ) is smaller when more interactions have occurred between and in the past. In fact, the probability that is infected given that has been healthy (namely the left hand side of Eq. (19)) is lower than the probability that is infected without any knowledge on the past states of (namely the right hand side of Eq. (19)). An efficient way to compute ( ) is defined in Section 7. We can conveniently incorporate the correction factor ( ) in the term ( ) as̄( ) = ( ) ( ). We finally obtain the following update rule ( 0∶ −1 = 0| 0∶ ). (21) To keep a limited number of computations, we also make the following approximation with initialization ( ( ) = 1 |  0∶ ) =̂( | ). Roughly speaking, if individual has direct contact with a tested individual and individual has direct contact with but not with , the state estimates of will be corrected based on the outcome while the state estimates of will use the old estimation of , as derived without the knowledge of the outcome. This means that we use the information regarding the outcome from the tests to only update the direct contacts of a tested individual. Since the update of each individual uses only knowledge from local connections, new information can be used differently for tested individuals, individuals with a direct contact with them, and the remaining of the population. Let denote the outcome of the test to the individual . Then we have  for ≤ − as no additional information on past states is given by a negative outcome. As ( ) = 0, ( ) may be equal to 1 only if a contagion occurs in the interval ( − , − ), therefore the infection probability is updated aŝ  for ≤ − . Formally the neighbours of a tested individual are defined by the set ( ) = { | ∃ ( ) ≠ 0, ∈ ( ), < } which represents the set of individuals that has been in contact at least once with at least a tested individual. According to the definition of local information set, the update of the estimation exploits also the updated estimate of the past states of tested individual. The probability relative to the initial time instant is not changed By using the information from the contacts that have been tested at time instant we can update the probabilities starting from (21) as using (22), where ,1 ( , ) = Note that the previous update takes advantage from the knowledge of the update estimate of the past state of tested individuals. The last equality holds only if individual has not been tested before, otherwise ( 0∶ −1 = 0 |  0∶ −1 ) would be different according to the update relative to a tested individual. In that case, the correction procedure starts from the instant where the individual was tested. The correction procedure works if more than one neighbour have been tested even in different time instants. Note that 2, ( , ) > 1 if ( ) = 0 and 2, ( , ) < 1 if ( ) = 1. Finally, the infection probability at time is computed aŝ For each individual not having direct contact with any tested individual, the open-loop estimate is computed as ( 0∶ = 0 |  0∶ −1 ) based on the previous estimates ( ( − 1) = 1 |  0∶ −1 ) provided by its contacts as using (21) and (22). Other required values are updated according to The state estimation scheme proposed above performs a hierarchical update of the infection probability. This update is structured around individuals that are tested at time , the neighbours of the tested individuals and the remaining of the population. At each time instant, the estimation is thus divided into 3 levels of update based on the derivations obtained in the previous subsection: This scheme is depicted in Fig. 3 . In line of principle, buffers of increasing length are needed to store past probabilities. In the spirit of a temporal truncation of the updates, since the current test outcomes bring little information on the oldest states except for positive tested individuals, we assume that for untested individuals past probabilities older than are not affected by the new outcomes, i.e. Under this approximation, in terms of information storage, the local update of the current state estimate requires the storage of the following two buffers of information for each individual, namely the Susceptibility buffer and, the Infection probability buffer It is worth to note that the complexity of the proposed estimator is much lower than the optimal estimation devised in the previous M. Pezzutto et al. (38). Since only effective contacts (namely those with ≠ 0) affect the product, the number of required multiplications drastically falls and scales with the node degree. The second level update requires, for each contact of a tested individual, the correction of the last values and involves simple multiplications of scalar quantities, see (34) . Similarly, at the first level, for each tested individual, the update of the last estimates requires only elementary operations using already available quantities (see (27) and (30)). An interesting feature of the proposed approach is that it is not only computationally efficient to be used in a centralized way for a given community, but that it can be also implemented in a decentralized manner. This is the case where each individual is equipped with a smart device (e.g. a smartphone) provided with small computational capability and able to communicate with other devices and with a central testing unit, see Fig. 5 . Contact tracing mechanisms have already been applied by many countries during the Covid-19 epidemic and software applications are already available in the market. With respect to them, our algorithm can be implemented based on the same hardware and with a larger amount of transmitted data. In particular, when individuals get in contact during the day, their previous estimatê( − 1| − 1) has to be exchanged. The outcomes of the tests ( ) are provided to tested individuals who compute and communicate the updated estimates of the previous stateŝ( | ), ∈ ( ) to the server. Then remote communications of those updated estimates are performed once per day from the main server to the population. An explanatory representation is given in Fig. 4 . Note that no information on interactions is communicated neither to the central unit nor to other individuals, so that privacy is preserved and vulnerability of a central data collector is avoided. Then, each individual transmits the updated estimateŝ( | ) to the server which decides who to test the next day and convokes them. Similarly to the literature on sensor selection, it is possible to formulate the test selection problem as a constrained optimization problem based on the state estimate. Formally, we introduce the binary control variable ( ) taking value ( ) = 1 if th individual is selected at time instant to be tested at the next time instant + 1, and ( ) = 0 otherwise, while we denote ( ) = ( ( ), … , ( )) ′ . Then the test selection problem can be formulated as where (⋅) is a suitable reward function. Several possibilities exist for the choice of the cost function. Differently from most of the works on sensor selection for remote estimation, we avoid to adopt the error covariance matrix because it is computationally infeasible for large . More suitable cost functions can be computed based on different metrics of the current state of the population or the topology and characteristics of the network. Namely, intuitive choices would focus on the expected number of detected people, the expected number of infections at the next time instant + 1 or targeting individuals with high number of contacts (e.g. first-line health workers). In this context, different cost functions may provide different results based on the time of application of their actions, the number of available tests or the applied quarantine actions. Based on good preliminary results, in this paper we propose to maximize the expected value of the number of detected positive individuals, that is This policy is equivalent to select the individuals whose probability of being infected̂( | ) is the highest. It should be noted that the proposed cost function is a primary attempt to define an efficient metric in line with the presented framework. Nonetheless, the selection of the optimal cost function is out of the scope of this paper and remains an open problem for further research. The outcomes of the tests are exploited to act on the population through a selective quarantine. Formally, we introduce the control variable ( ) such that ( ) = 1 if th individual is selected to be quarantined at time instant , and ( ) = 0 otherwise. In this paper for any positive we propose to quarantine the closest neighbours, i.e. the individuals with the highest transmission probability ( ). The parameter can be properly tuned to trade-off between the total number of quarantined for positive and the expected number of infected (but not detected) that are quarantined because they have a direct contact with a positive. We consider that individuals will leave quarantine after days. Note that in line of principle other quarantine strategies can be designed based on probabilities of infection of the neighbours of a positive tested, as well as preventive quarantine based only on the state estimate, and they will be the subject of future investigations. This section shows, through numerical simulations, the effectiveness of the proposed solution by comparing it to current approaches. The simulation setup considers a closed population of 10'000 individuals with the following parameters regarding the spread of the disease • 0 = 2. This value is equivalent to a virus with high spreading, e.g. the Covid-19, when no social distance measures are adopted (Giordano et al., 2020; Salath et al., 2020) . • 0.1% of the population is initially infected. • 20% of new infected present symptoms of the disease before the recovery, in agreement with Ing, Cocks, and Green (2020) and Lavezzo, Franchin, Ciavarella, Cuomo-Dannenburg, Barzon, Del Vecchio, et al. (2020) . • 0.5% of the population can be tested at each day, corresponding to = 50. This value is similar to the percentage of daily tests in South Korea or in USA at December 2020, see https: //covidtracking.com/data. • The = 5 closest individuals of each individual with positive test are put in quarantine for = 14 days. When in quarantine, all the transmission probability are reduced to 1∕100 of their normal value. The population distribution can be conveniently represented through a weighted undirected graph, where each node represents an individual, an edge between two nodes represents an interaction between two individuals, and the weight is set equal to the probability of transmission . The graph topology has been generated to emulate a small-world network. This kind of graphs are characterized by the presence of clusters, which are subgraphs that are (almost) complete, and of short paths connecting (almost) any pair of nodes. They have been introduced to capture the evidence of human connections and have been widely studied in the literature, see de Sola Pool and Kochen (1978) and Watts and Strogatz (1998) . In our case, each individual belongs to more clusters (at least 2, up to 6) to mimic families, offices, habitual relations and activities, etc. The dimensions of the clusters are uniformly distributed and the range depends on the kind of relationship that they capture: for example the dimensions of households randomly vary from 1 to 8, while dimensions of offices vary from 4 to 40. Random links are also added to the network. The resulting graph is then heterogeneous and possibly unbalanced. The average weights are set in a realistic way, e.g. the average weights in a household are four times the ones in a small office. For the sake of simplicity the graph is assumed to be time-invariant, except for the effects of quarantine actions. Initial conditions (0), i.e. which individuals are initially infected, are stochastically generated based on the initial probability of each node to be infected. To test the robustness of the proposed strategy, we assume that the probability distribution of the initial conditions is perturbed up to the 10%. It is also assumed that 10% of the arcs of the graph are unknown. The presented simulations compare three different scenarios: • Test and trace (T&T) . This policy traces the contacts (based on the knowledge of the network) of symptomatic and detected cases (see Dar, Lone, Zahoor, Khan, and Naaz (2020) and Ferretti et al. (2020) ). More formally we define the set of individuals that have been detected at a generic time as ( ) = { ∈ ( ) ∶ ( ) = 1}. Then for any individual ∈ ( − 1) we retrieve the set of recent contacts  ( ) = { ∶ ( ) > 0, − < < }, and we refine it by removing already detected individuals and individuals that have been recently tested. From the set  ( ) we select the individuals that have been more in contact with the individual ∈ ( − 1). Among the different possibilities for doing so, we choose  ( ) ⊆  ( ) such that where | ⋅ | denotes the cardinality of the set. Less rigorously, we can say that  ( ) contains the contacts of with which the last interaction has been the most dangerous, while the number of individuals in each set  ( ) is chosen such that tests are allocated as uniformly as possible among the sets  ( ) of contacts of detected positive. Please note that, since in the following simulations the graph is fixed, ( − 1) ≥ ( − 1) implies that ( ) ≥ ( ) for any ≤ −1 if , has not been in quarantined. Remaining tests are used to randomly explore other parts of the graph in order to model test selection policies that are not based on the probabilities of infection, as it is done in reality where tests are also partially allocated to the employs of interested companies. Test and trace strategy is a well-known policy which has provided good results in several countries and it has been considered the best practice by the medical community (World Health Organization, 2020b). • Topology-based testing. This scenario presents a policy where, based on the topology of the graph, i.e. the number of contacts, certain individuals are periodically tested. In particular we choose a period = 20 days and we solve the constrained optimization problem Individuals such that = 1 are then randomly sorted and tested accordingly. The periodical testing campaign is delayed on-line in order to allocate tests to the closets neighbours of a new detected. However we consider only a partial tracing, assuming that at most 8 contacts are provided. The application of topology-based centrality metrics for test selection is quite new but they have been studied to select the edges to remove (see Doostmohammadian, Rabiee, and Khan (2020) and references therein). • Smart testing (T&EST). This scenario follows the proposed control scheme where individuals are selected according to the probability of being infected, Eq. (47), and the outcomes of the tests are used to update the state estimate according to Section 4. The correction term is set as so | ( )| is the number of days in the last with a contact between individual and . We set parameter by fitting the time evolution of the mean probability computed by the proposed estimator to the incidence of cases obtained by simulating a graph of a similar topology in the case of no action on the system. Given the stochastic nature of the model, 100 simulations have been generated for each scenario. The spread of the infectious disease is monitored for a time span of 300 days.  As we can see in Fig. 6 , the proposed control mechanism (testing based on estimation and conditional quarantine) is effective in reducing Table 1 Summary of the averaged results for the 3 scenarios. Peak of active cases  Total infected  Work days lost   Test and trace  116  1472  2564  Topology-based strategy  365  3859  5847  Smart testing  20  169  444 the total number of infected people in a given temporal window both with respect to the topology-based strategy and test-and-trace. It is important to note that the test-and-trace strategy clearly outperforms the topology-based strategy even if the latter, when no new known positives are present, allocates tests on crucial points of the network instead of on random individuals. This result confirms how important is to trace contacts of positive individuals in order to keep the epidemic under control. On the other hand, the comparison between T&T and T&EST shows that the performances of pure tracing can be largely improved by using control methodologies. In particular, when many positive individuals have been detected, it is impossible to test all of their contacts due to the limited number of available tests. In that case, with the proposed strategy, the contacts that should be tested naturally come up among all the other contacts. When no new positive is detected, T&EST selects who to test using updated information on the state of the population. Thus, the proposed strategy takes into account both information on the graph topology (for individuals that have not been in contact with tested individuals, estimation is affected by the number of interactions) and the information from tests. The obtained results show that including dynamics provides better performances than simpler off-line strategies. The proposed strategy is effective also in mitigating the epidemic outbreak by avoiding any peak of active cases, as shown in Fig. 7 . This is an important result since it is fundamental to have a low number of active cases to avoid the health-care system maximum capacity to be reached. It is important to remark that the presented approach has shown to be very effective when applied to cases where the initial number of infected individuals is small compared to the total size of the population. In such a case, a fast identification of clusters of infection is essential, providing enough tracing to detected positive individuals but also exploring new areas of the network. This improvement in the performances can be better appreciated in Fig. 8 . The number of people in quarantine at each time instant is depicted in Fig. 9 . Although not intuitive, this plot shows lower numbers of people in quarantine for the smart testing policy, indicating that the improvement in performance does not require a greater number of people in quarantine but that actually can be achieved with less but better focused quarantines. In these simulations the number of people in quarantine for the T&EST is almost negligible, showing that an efficient testing policy can have a great impact also in the required control actions. This is a very promising result especially from an economic point of view since it would limit the social and economical impact of the measures. A synoptic overview of the numerical simulations is reported in Table 1 . The results show the clear improvement on the containment of the epidemic, in terms of both active cases and people in quarantine, by using a testing and quarantine policy based on the presented probability estimation algorithm. An important aspect of the presented strategy is the assumption of a good knowledge of the network topology. In this sense, Fig. 10 provides the variation in the performances of the presented strategy with respect to the number of individual interactions known. From this plot it can be seen that within the range of 80%-100% of knowledge of the network, the results are very similar and promising. For a percentage of unknown interactions superior to 20%, a threshold behaviour can be seen, where the performance is clearly worse and a more evident peak of infection can be seen. However, it must be noted that even if the performance is clearly poorer, the results are still much better than the other strategies. In this paper we presented a novel testing strategy to smartly select the individuals to be tested during an epidemic. This policy is based on a decentralized state estimation of the status of the epidemic obtained from the outcome of the tests. The testing policy is defined as an optimization problem based on the state estimation. The proposed estimation algorithm is computationally inexpensive and can even be implemented in a distributed fashion. The numerical results based on Monte Carlo simulations demonstrate that the use of the proposed scheme, testing and selective quarantine, significantly reduces the total number of infected people as well as the peak of active case and the number of people put in quarantine. Future works will focus on the link between the test selection objective functions and the quarantine policies. The case where the reliability of the tests is considered is another subject of study in future research. 
paper_id= fff31c0828a5c80ae6575df3f33f6c56b22b7978	title= ARTICLE TEMPLATE Federated Learning in Smart Cities: A Comprehensive Survey	authors= Zhaohua  Zheng;Yize  Zhou;Yilong  Sun;Zhang  Wang;Boyi  Liu;Keqiu  Li;	abstract= Federated learning plays an important role in the process of smart cities. With the development of big data and artificial intelligence, there is a problem of data privacy protection in this process. Federated learning is capable of solving this problem. This paper starts with the current developments of federated learning and its applications in various fields. We conduct a comprehensive investigation. This paper summarize the latest research on the application of federated learning in various fields of smart cities. In-depth understanding of the current development of federated learning from the Internet of Things, transportation, communications, finance, medical and other fields. Before that, we introduce the background, definition and key technologies of federated learning. Further more, we review the key technologies and the latest results. Finally, we discuss the future applications and research directions of federated learning in smart cities. Federated learning; Smart city; Internet of Things; to key issues such as the development of the Internet of Things[2][3],medical care[4][5], transportation[6],communications[7][8],etc. Sensors will generate a lot of data [9] in this large-scale information exchange process. These data are of great significance for improving the applications of the program and helping managers optimize decisions. However, a large part of the data is sensitive data. It involves user-generated personal privacy [10] . Firstly, we must evade users' private data in data processing in smart cities. In addition, we will face the problems of low data resource utilization and network congestion [11] in the process of data interaction. nodes[13], system simulation [14] and other computing implementations all have large network bottlenecks in practical applications of smart cities. And it still has the problem of low efficiency in using network resources. Therefore, it requires a distributed learning paradigm. It can reduce network bottlenecks in this case. At the same time, it have solved the problem of users' privacy while discovering valid data through the collaborative sharing model of IoT devices. Some examples of distributed organization computing are mentioned in the current smart city applications. But these practical applications (such as the realization of D2D communication [8] and the realization of multi-layer fog computing for large-scale IoT data analysis [15] ) have not solved the problem of user privacy well. Federated learning (FL) take advantage of solving the above-mentioned problems [16] . Under the framework of FL, users can make use of the data without obtaining other participants' data. The related data is stored locally [11] . Users only periodically share their local model gradients with the coordination server for a period of time. The server organizes training data and measures the contributions of all participants [17] . The server constructs a global model by averaging all gradients in the network [18] at the server level. After that, the coordination server will distribute the new model update to all clients [19] . Each client uploads its local model to the server. Users download new updated models and use cloud distribution to realize inference on the device. The coordination process of the entire server will continue until it stops [20] . This is a complete operating principle of the federated learning algorithm. Federated learning has been applied to smart cities. It has the advantages of distributed processing and effective privacy protection. Some common distributed communication devices (such as mobile phones and wearable mobile devices, etc.) have communication transfer problems. Federated learning proposes a federated domain adaptive method based on the domain transfer problem. This model solves the problem of data privacy and efficiency [21] . On the other hand, Some scholars implement a blockchain federated learning (BlockFL) architecture. This architecture can realize the exchange and verification of local learning model updates. And it can describe the best block generation rate by considering communication and consensus delay issues [22] . In addition, the vehicle network has low-latency communication (URLLC) and resource allocation (JPRA) issues. Some researchers propose a novel distributed method. It realizes the estimation of the tail distribution of the queue length [23] . Related research results of federated learning have carried out many practices in the fields of the Internet of Things, communications and public services. These practices promote the update and development of applications in smart cities. The organization of the rest in this article is as follows. The second part introduces the definition and key technologies of FL; The third part introduces the applications of federated learning in the IoT system of smart cities. The fourth part introduces the applications of federated learning in the transportation systems. The forth part introduces the applications of federated learning in the financial field of smart cities. The sixth part introduces the applications of federated learning in the medical field of smart cities. The seventh part introduces the communication of federated learning in smart cities field applications. The eighth part discusses the future developments and directions of federated learning in smart cities. Finally, we summarized. At present, the popularization of federated learning in smart cities has not been widely used. This prompted us to conduct a comprehensive investigation. This work is as following contributions. • This work introduce the background, definition and key technologies of federated 	body_text= The current urban internet architecture is complex. And the concept of "smart city" provides a new idea for the problems encountered in the process of urban development. Smart city refers to the use of different types of electronic Internet of Things (IoT) sensors to collect data [1] . Decision makers then use the insights gained from this data to effectively manage the urban area of assets, resources and services. The operating model of a smart city is to use IoT sensors to collect data. Furthermore, it can realize effective applications in a series of fields such as urban public services, resource allocation and communications. At the same time, smart cities have provided good solutions  The concept of federated learning is now widely proposed [24] [25] [26] . It has been implemented and applied in various fields. Most of the existing large-scale work has been applied to distributed learning in the development of big data [27] and cloud computing [28] [29] [30] . Data pushed directly to the server can compromise user pri-vacy. The core of federated learning is to build a machine learning model by using data distributed across multiple devices. This solve the problem of data privacy. At present, distributed computing agents are growing rapidly. FL has become an effective solution of protecting user privacy in the process of information and knowledge sharing [20] . For example, smart behaviors currently exist on mobile devices. Mobile phones and tablets use image classification to predict pictures previewed multiple times [31] . Federated learning is based on data and information processing to improve user experience. In addition, Many insurance companies have always had great concerns about protecting their data. They are unwilling to share with other entities [32] . In this case, they can use multi-party data in the FL framework. It solves the privacy problem in machine learning. Recent research improvements on federated learning are mainly aimed at statistical challenges in federated learning [20] [33] and security issues [34] [35] . At the same time, research work has made federated learning more personalized [36] . These related research work are concentrated on the federated learning of distributed equipment. This process involves factors such as data interaction among distributed mobile users, unbalanced data distribution, and communication costs in equipment reliability. It can inspire researchers to continuously overcome challenges in data privacy, computational constraints and communication costs. This work is innovative. The concept of federated learning is extended to cover other collaborative learning programs between organizations. Here we make a preliminary explanation on the extension of the original concept of '"federated learning" to other distributed collaborative machine learning [37] . In this article, we will further investigate the application of federated learning in smart cities. The work content will also include discussing its development status and future direction. In this section, we provide a more comprehensive overview of federated learning. This aspect considers the definition, privacy, training process and classification structure of federated learning. We define N data owners as F 1 , F 2 , ..., F N . All of them hope to train their own machine learning models by merging their respective data D 1 , D 2 , ..., D N . A conventional method that exists is to put all the data together. It uses D = D 1 ∪ D 2 ...D N to train and obtain a model M SU M . And federated learning is a systematic learning process. In this process, the owners of the data jointly train the model M F ED . No data owner F i will disclose his own data D i to others. In addition, the accuracy of M F ED expressed as V F ED should be very close to the performance of V SU M of M SU M . In terms of expression, let ε be a non-negative real number; if | V F ED − V SU M |< ε, We can think that the federated learning algorithm has a ε error accuracy. Privacy management is one of the core elements considered in federated learning. The realization of this requirement requires some security model analysis. In this section, we briefly describe the different privacy technologies currently used for federated learning. The SMC security model involves data from multiple parties. It provides safety certification under a known and clear simulation framework. This is a model that guarantees zero interaction of knowledge data. In this situation, apart from the input terminal and output terminal, neither user knows these information data. The '"zero-knowledge" model formed under this condition is highly expected. However, this desired property requires a complex calculation protocol. And it has situations that may not be implemented effectively. With security assurances, we can consider that the knowledge casn be partially disclosed. At present, researchs have shown that SMC can be used to establish a security model to improve computing efficiency under low security conditions [38] . In addition, the MPC protocol performs model training and verification. In this process, users do not need to disclose privacy-sensitive data [39] . This work has realized the sharing of participants' data among servers. Existing researchs use differential privacy [40] or k-anonymity [41] technology to achieve the protection of data privacy [42] [43] . The above methods complete the processing of the data. And they achieve the purpose of covering up certain privacysensitive attributes. This makes it impossible for third-party users to distinguish between users. This will make the data unrecoverable and realize the protection of user privacy. But the disadvantage of this method is that it needs to transfer the data to other places. This overrun will affect the accuracy of the data. Therefore, we need to make a trade-off between accuracy and privacy. At present, there are many applications implemented by this privacy processing method. Some researchers have proposed a differential privacy method for federated learning. They have realized of hiding customer contributions during the training process to protect client data [35] . At present, homomorphic encryption [44] has been widely used. Its operating model is the encryption mechanism in the machine learning process. It uses parameter exchange to protect the privacy of user data [45] [46] . The difference between it and differential privacy protection is that the data and models themselves will not be transmitted. Their data will also be encrypted without being discovered. Therefore, its advantage is that the probability of leakage in the original data is very small. In practice, the additive homomorphic encrypt model [47] is widely used. At present, polynomial approximation is used to evaluate nonlinear functions in machine learning algorithms [48] . In the FL training system, the owner of the data participants in the FL system. Together, they train a shared model in the aggregation server center. In this architecture, a basic premise is that the data owners are honest and the data they provided is true. It requires data users to use their real private data for training. After the training is completed, the relevant parameters of local model training is submitted to the FL server. Generally, the FL training process includes the following three training steps. We first define the local model as the model trained on each participating device. The global model refers to the model after the FL server is aggregated. • Step 1. Realize the initialization of the task. The server determines the training task. That is to determine the target application and corresponding data requirements. At the same time, the server specifies the global model and establishes the parameters in the training process, such as the learning rate. The global model parameter W 0 G will be initialized by the server. And the training tasks are assigned to the participating users to complete the task assignment. • Step 2. Realize the training and updating of the local model. The training is carried out on the basis of the global model W t G . t represents the current iteration index. Each participating client uses local data and equipment to update the local model parameter W t i . The ultimate goal of participant i in iteration t is to find the optimal parameter W t i that minimizes the loss function L(W t i ), namely W t * i = arg minW t i . • Step 3. Realize the aggregation and update of the global model. The server aggregates the local models of participating users. And it will send the updated global model parameter W t+1 G to the user who holds the data. The server continuously calculates the minimum global loss function Repeat steps 2-3 until the training global loss function converges or reaches the required training accuracy. In the smart city architecture in recent years, the development of the Internet of Things has provided technical support for the transformation and progress of smart cities. More and more intelligent products based on the Internet of Things appear in large numbers. The emergence of federated learning provides a good solution to the key problems in the development of the Internet of Things. There are a large number of data changes and information processing events in the Internet of Things. Many user privacy issues and information security issues have been exposed during this process. The framework model of "Federated Learning + Internet of Things" has solved many problems. The "smart city" architecture is therefore more complete. Federated learning builds a scalable production system for the field of mobile devices [49] . It improves the design of the system architecture. In addition, the combination of blockchain and federated learning constitutes a blockchain federated learning (BlockFL) architecture. It compares the performance between different terminals [50] . In the process of realizing these applications, the following factors need to be considered: • Privacy. One of the core goals of Federated Learning is to protect the private information of participating users. Some recent research work has shown that participants or FL servers may be malicious in participating in the FL process. This situation may cause privacy and security issues. At the same time, this may damage the generated global model. On the other hand, it will damage the privacy of participating users during model training. In addition, FL does not need to exchange data for collaborative model training. Its maliciously participating users can still infer sensitive information (such as gender, occupation and location) based on the sharing models of other participating users. We use the FaceScrub dataset to train a binary gender classifier. In this process, the accuracy of inferring whether a participant's input is included in the data set is as high as 90% by checking the shared model [51] . • Security. During the FL training process, participating users train the learning model locally and share the training parameters with other participants. It achieves the purpose of improving forecast accuracy. However, it is often vulner-able to various attacks in this process. For example, data and models are missing or even poisoned. In this attack mode, malicious users may send wrong parameters or corrupted models. Thus it will provide a false learning model during the global aggregation process. The global model will update incorrectly. And the entire learning system will be damaged.  Currently, under the framework of the Internet of Things, federated learning based on device data has been widely deployed. At present, some scholars have proposed a novel federated learning framework for efficient communication and privacy protection. It improves the performance of IoV. After that, it stabilized the data flow dynamics through TCP CUBIC flow on the WiFi network. In the end we got a good training model [52] . On the other hand, some applications provide accurate video recommendation services. The creation of a joint cloud video recommendation framework based on deep learning for mobile Internet of Things meets the needs of users. At the same time, it uses quantitative methods to reduce the uplink communication cost and network bandwidth [53] . In addition, federated learning enables resource-constrained edge computing devices (such as mobile phones and IoT devices) to learn shared predictive models [33] . The development of blockchain provides a new development direction for the Internet of Things. BlockFL architecture has completed the update of the local learning model well. It uses the consensus mechanism in the blockchain. Therefore, on-device machine learning does not require any centralized training data or coordination. In the end, it performed a good performance data analysis [22] . In the industrial internet of things, some researchers have designed a secure data sharing architecture authorized by blockchain. This process maintains data privacy well through the shared data model. It is compared with real-world data sets. The proposed data sharing scheme has good accuracy, high efficiency and safety [54] . The current widespread application of edge computing has resulted in faster network service response. At the same time, it meets the basic needs of the industry in realtime business, application intelligence, security and privacy protection. At present, federated learning is combined with edge computing. It achieves a good practical application. We know that the use of edge and terminal computing can meet the needs of cloud capacity and equipment at the edge of the network. This process speeds up content delivery and improves the quality of mobile devices. Under this condition, Federated Learning has realized the application of the 4G/5G-based interconnected vehicle edge computing platform. This model completes the edge collaborative learning of real data sets collected by large electric vehicle (EV) companies. This method has the advantages of driver personalization, privacy services, reduced delay (asynchronous execution) and security protection. In addition, the personalized federated learning of the application of the intelligent Internet of Things can well alleviate the negative impact of heterogeneity in different aspects [55] . At the same time, the framework design based on federated learning can utilize limited bandwidth resources. It effectively improves the communication efficiency and reduces the communication cost [56] . Currently, we need to combine deep learning techniques and federated learning frameworks with mobile edge systems at the same time [57] . This can accelerate the applications of mobile edge computing. The existing implementation mode of federated learning is to allow computing nodes to synchronize only the local training model in distributed training. And it does not synchronize the original data. This leads to the federated learning architecture relying on highly concentrated types and large server bandwidth. However, the network capacity distribution between nodes is highly uniform and smaller than that of a data center. In [57] , the author proposed a method. It can make full use of the node-tonode bandwidth to speed up communication. The first point is that it is staff selection through segmented gossip aggregation and bandwidth awareness of the network. The second point is that it makes full use of the bandwidth between nodes and between workers and workers. This ultimately speeds up the convergence speed and reduces the number of communication rounds involved. Federated learning allows workers to train models using distributed data. The general federation learning system uses a central parameter server to coordinate a large federation of participating workers. Workers use their data set to train a local model. And they are regularly updated to a centralized server for synchronization. It achieves the same synchronization effect as the parameter server. But it consumes a lot of bandwidth resources. The model updates of all nodes in the system are sent to all other nodes. Performance will be severely reduced and costs will increase. So we use the model split level synchronization mechanism. In the first aspect, we "divide" a model into a set of segments-subsets containing the same number of model parameters. These parameters do not overlap. In the second aspect, the workers aggregate the partial divisions with the corresponding divisions of k other workers. Then it performs segmentation level update. In the third aspect, we divide other workers. This process maximizes the bandwidth capacity between workers. It shares the communication cost and further accelerates the convergence speed. In addition, the author also proposed in [57] to combine deep reinforcement learning techniques and federated learning frameworks with mobile edge systems. This can be used to optimize mobile edge computing. In this process, the "In-Edge AI" framework was designed. It can intelligently use the cooperation between the device and the edge node to exchange learning parameters. It finally achieves dynamic system-level optimization and application-level enhancement. Based on this process, unnecessary system communication load will also be reduced. The key and difficulty is that computing offloading requires wireless data transmission. This may cause congestion of the wireless channel, thereby affecting the effectiveness of the decision. The optimization of the entire communication and computing integration system-how to jointly allocate communication resources and computing resources of edge nodes. The author uses progressive reinforcement learning to jointly manage communication and computing resources. At the same time, it also uses floating and edge cache calculations between mobile edge computing (MEC) systems. In addition, federated learning [58] [59] has also been introduced as a framework for training agents in a distributed manner. The effects of this method are as follows: 1) uploading through wireless channels greatly reduces the amount of data that should be used, 2) responding to the mobile communication environment and cellular network conditions, and 3) interacting with the actual cellular network Heterogeneous user equipment (UEs) adapt well, 4) protect personal data privacy.  Transportation is an integral part of building a smart city. With the further development of deep learning, self-driving cars have gradually developed [60] [61] . We can solve various problems in transportation systems through federated learning, such as communication delays, calculation data processing, and data privacy. The performance of emerging transportation applications largely depends on vehicleto-vehicle (V2V) communication [62] . Therefore, the URLLC in the vehicle network is [66] . The constraint of URLLC is characterized by extremum theory and modelled as a tail distribution of network scope queue length over a predefined threshold. It can effectively reduce delay and enhance reliability [67] [23] , and ensure reliable federated learning in mobile networks [68] . They proposed a distributed transmission power and VUEs' resource allocation process based on Lyapunov stability. It can reduce unnecessary overhead by enabling VUEs to distribute the queue's tail at the local learning network scope without sharing a sample of the actual queue length. The proposed solution is very accurate in estimating the parameters and dramatically reduces user vehicles' queue length compared with the centralized learning module. Nevertheless, it would be impractical to collect all the data to a centralized server due to delays in the vehicle-server network. Real-world applications, cloud-based learning methods are relatively slow. The author proposed a systematic IoT network design approach by [52] accelerating the learning process of data transmission protocols (such as TCP) that converts our vehicles into mobile data centers. This method performs federated learning to improve TCP performance, data transfer volume, and diversity of Internet performance. Their mathematical analysis based on equalization provides essential insights for the development of feasible federated learning algorithms for net-work control. The latest generation of aircraft has onboard computing power and links to data on the ground, different from vehicle systems. However, due to the large amount of data generated by the aviation system and the lack of computing resources, it cannot handle the aircraft's fault prediction. Moreover, deploying additional airborne resources is very complicated and expensive. Therefore, in [69] , the author proposed a method of combining active learning and federated learning. This method uses an active online decision tree based on confidence as the basic model of client learning [70] by sending it to the server. Finally, a single decision tree uses the integrated model from the server to mark the request. They can establish mechanisms for transmitting and identifying uncertain data under the communication budgets by classifying standard samples with minimum computational power. This method effectively achieves high communication efficiency and cheap calculation and is suitable for practical application cases. Although federated learning can solve insufficient computing resources, high communication costs, and privacy involved in the IoT, communication inefficiency is the bottleneck of federated learning. UAV is increasingly used as the relay between ground base terminal and network base station [71] to improve network connectivity, and [72] extend coverage area and communication efficiency. [73] A joint formation method between cooperative UAVs is proposed to motivate UAVs to participate in federated learning training. This method achieves a stable coalitional structure and maximizes the allocation of profit.  The financial sector includes banking, insurance, trust, securities and leasing. These criminal activities have been frequent in recent years. Some financial crimes can reach hundreds of millions of dollars, such as the mortgage crisis. These criminal activities have created a crisis for families and society as a whole. The financial industry also spends a lot of money every year to fight fraud, but not effectively. In recent years, it has been necessary to use machine learning to reduce losses to banks and consumers. In deep learning, the sample size needs to be sufficient to train a better model. A single bank can't provide enough information of a person's consumption and credit cards. And it is also difficult for one bank detect frauds. The proposal of federated learning gives the financial industry a new way to train models using deep learning [74] . The owner of each data can collaborate on the model without sharing the customer's private information. The financial industry faces a number of challenges in reviewing user qualifications and screening quality customers. The combination of federated learning and finance can effectively address this challenge, while also protecting customers' private information from disclosure. Federated learning can be very effective in these areas. For example, Banks can use cameras to identify suspicious transactions, prevent malicious multi-party lending, and so on. [75] , prevent malicious multi-party lending [76] , and so on. Anti-fraud field Other fields Distribute public key Information Source Collaboration group Application group Figure 5 . The processing framework of federated learning in the financial field. First, the collaborative group sends the key to the data source. Then encrypt the exchanged data to ensure data security. Finally, the learning model is updated in real time to perform model output for different application scenarios. With the advent of the digital age, many transnational financial crimes have emerged in the world. Common sub-categories of financial crime are financial theft, fraudulent loan and money laundering. Credit card fraud will bring large losses to both banks and consumers. Yang et al. proposed a framework for training fraud detection models using behavioral features and federated learning [77] . They build a shared Fraud Detection System (FDS) by aggregating locally calculated fraud detection model updates. The result shows that the area under the curve(AUC) based on the federated learning FDS reaches 95.5%, which is about 10% higher than the traditional FDS. Chuan et al. also give satisfactory answers to some questions (Model Aggregation, Data Poisoning, Scaling Up Issue) [78] . In addition to improving federated learning, some researches combine federated learning with other algorithms. For example, Toyotaro Suzumura et al. [79] use federated learning and traditional graph learning methods to build a more accurate machine learning model. It can capture complex global money laundering activities across multiple financial institutions. In recent years, privacy issues have gradually attracted attention [80] . For the rich, they value whether their private information in the bank can be effectively protected. Due to the development of machine learning, more and more bank user data is analyzed and trained into relevant bank marketing models. However, protecting the private information of financial users is an important research direction. Feng Yan et al. [81] proposed a bilateral privacy-protected federated learning scheme, which also protects the iterative parameters during the training process. This scheme further protects model parameters from being acquired by external attackers on the basis of traditional federated learning only considering the privacy of the client. The asymmetric vertical federated learning proposed by Yang Liu et al. [82] can effectively protect the privacy of different bank users. Building a data service platform for the insurance industry requires integrating financial, medical, and other data from multiple parties. If an insurance company wants to improve its risk management capability and business development level, it needs to consider the impact of multi-party data. How to effectively use data without infringing on personal privacy is also an important issue facing the insurance industry. Ma lgorzata et al. [83] proposed that the key technologies that promote the insurance industry's reform include federated learning and computable insurance contracts. Wang [84] explained the primary application of federated learning in the insurance industry and used Shapley values to explain the federated learning model. The results show that compared with the entire feature space results, the robust host features important results for part of the feature space. Yuan et al. [85] proposed a configurable FL benchmark suite FLBench. This kit can simulate various isolated data islands according to specific research requirements and covers areas such as insurance and securities. When different insurance companies and multi-party data providers implement federated learning, how to measure participant contributions is also a more realistic problem. Wang et al. [32] proposed that the service model can use related models to use data models to integrate information to obtain better feedback. Yan et al. [86] proposed an online evaluation method that is more sensitive to the quality and quantity of data and compared it with the results obtained by Shapley Value in game theory. Besides, federated learning has more realizations in the insurance industry [87] [88] [89] [90] . With the rapid increase of COVID-19 worldwide, the burden on medical staff has gradually increased. How to treat patients with effective methods is a major problem. Smart medicine is an area of future medical development, and this area is expected to benefit from the rise of federated learning technology. In the past, due to the independence of hospitals and the privacy of information, there was a lack of sufficient samples for machine learning. Federated learning can unite previously independent individual hospitals into a collective population, greatly increasing the sample size of model training. It can solve this problem well and improve the accuracy of the model [94] .  Direction Framework Contribution [77] 2019 Financial FL Detection framework FFD [79] 2019 Financial FL Federated Graph Learning [78] 2020 Privacy protection FL An intelligent aggregation method [80] 2020 Financial Secure multi-party learning The desired security properties [82] 2020 Privacy protection FL Asymmetrical federated model training [91] 2020 Bank FL Propose Open banking [92] 2020 Financial FL FL algorithms for vertically partitioned data [93] 2021 Privacy protection FL The key exchange technology Figure 6 . Comparison of training process between traditional machine learning on medical data and federated learning on medical data 6.1. The combination of federated learning and medical privacy protection With the Precision Medicine Initiative in the United States and the emergence of a large amount of personal health electronic information, patient data is usually protected in localized silos. People want to merge data sets from different medical systems it's difficult. Since the constraints of establishing a calibration model locally may limit the degree of improvement, Huang [95] proposed a safe multi-party calculation (SMC) method to establish a global isotonic regression calibration model. Fang et al. [96] proposed a method to reduce transmission bandwidth and protect privacy in distributed learning. And it is worth noting that Praneeth et al. [97] further demonstrated how to minimize the distance correlation between the original data and the intermediary representation. Thereby reducing the leakage of sensitive raw data patterns during client communication while maintaining the accuracy of the model. It reduces the leakage of communication payload and original data in medical data. And M. A. P. Chamikara et al. [98] proposed a privacy-protected FL framework for multi-site functional MRI analysis, and studied the use of brain function connections to classify the communication speed and privacy protection of autism spectrum disorders and health control problems. In terms of privacy protection, there are also related methods to improve accuracy, efficiency and scalability [98] [99][100]. FL has revolutionized leading fields such as health care technology. It has made outstanding achievements in many fields such as drug discovery. Artificial intelligence (AI) models usually require a large amount of high-quality training data, which is in sharp contrast to the small data currently faced by new drug discovery. The proposal of FL allows the drug development industry to use distributed data from different sources without leaking sensitive information of these data. This emerging decentralized machine learning paradigm is expected to greatly improve the success of artificial intelligence drug discovery [101] . Chen et al. [102] verified the feasibility of applying the horizontal federated learning (HFL). FL quantitative structure-activity relationship (FL-QSAR) under the HFL framework provides an effective way to break the barriers of pharmaceutical institutions in QSAR modeling. The solution promotes the development of collaboration and privacy-preserving drug discovery, and extends it to other privacy-related biomedical fields. Xiong et al. [101] demonstrated the application of FL in predicting drug-related properties. At the same time, they also emphasized its potential role in solving small data and biased data dilemmas in drug discovery. Machine learning has been proven to be an effective way to help the medical industry make decisions and predict diseases. FL can further expand the sample size and protect privacy. In the medical field, more accurate judgments are often required for disease prediction and decision-making. For example, in the detection of lung nodules, the lung nodes are often too small to be detected. In actual tests, lung nodules are often confused with blood vessels and other small underlying biological structures. It is often mistakenly confused as tuberculosis. If it is wrongly tested, it is likely to reduce the patient's chances of survival. Pragati et al. [103] proposed a method of using FL to effectively use medical and health records for disease prediction. This method helps to use FL to maintain health records for disease prediction and strengthens the maintenance of patient privacy. FL helps to produce enhanced prediction results and can protect data privacy and security. It can help FL to make greater progress. FL is trained through distributed machine learning. It allows distributed machines or users to cooperate in training machine learning models with the help of parameter servers. It also uses data sets from edge devices to train local models. At the same time, it regularly updates it to a centralized server to protect user privacy. But with the development of deep learning, in order to make machine learning and have better performance, we need a large number of data sets. In order to achieve the target accuracy, the participant and the server need several rounds of communication to achieve accuracy. This process may result in the need for millions of parameters [104] . Communication costs have become very expensive. In addition, in this process, there are problems such as the delay of the Internet of Things terminal equipment [105] , the instability of the communication link, and the lack of expensive data links and computing resources in aviation communications. The efficiency of this transportation system is also low and expensive. Therefore, it is necessary to improve communication efficiency, reduce delay, improve link availability, and reduce communication costs. At present, FL has made great breakthroughs in the field of multiple access channel communication. The views are as follows: • Multiple access channels: One of the challenges faced by FL due to its iterative nature and large model size is communication overhead. A new method to alleviate the bottleneck of FL communication is to allow simultaneous display of user traffic on multiple access channels. This may make better use of communication resources. Or another way is to explore the superposition characteristics of the wireless multiple access channel to calculate the required function of the distributed local calculation update (that is, the weighted average function). Previous work relieved the communication bottleneck by compressing the gradient before transmission. Two commonly used gradient compression methods are (A) quantization and (B) sparse gradient quantization. It follows the lossy compression idea of using a small number of bits to describe the gradient. These low-precision gradients are transmitted back to the parameter server (PS). However, these independent compression techniques have not been adjusted to the underlying communication channel exchanged between the user and the parameter server. Channel resources may not be fully utilized. Another study of FL through wireless channels is the more general multiple access channel. The stacked nature of the wireless channel allows gradients to be clustered together in the air and allows more effective training. These methods can be roughly classified into digital or analog solutions. It depends on how the gradient is transmitted through the channel. In the simulation scheme, the local gradient is scaled and transmitted directly through the wireless channel. In the digital scheme, the slave users are decoded separately, but the transmission still occurs on the multiple access channel. Although in terms of bandwidth, the analog solution is better than the digital solution [106] [107] . Digital solutions have the following advantages: • Backward compatibility, they can be easily implemented on existing digital systems. • They are not easy to slow down users. • They are more reliable because they various error control codes can be used. • Digital solutions do not require the tight synchronization required for analog transmission. Driven by the above discussion, they considered the learning of federated learning on multiple access channels. It focuses on the design of digital gradient transmission scheme. The gradient of each user is the first quality conversion. This process is transmitted through multiple access channels and decoded separately on the parameter server. The conditions are: a) the informality of the gradient of each user; b) the underlying channel conditions; they proposed a stochastic gradient quantization scheme to optimize the quantization parameters according to the capacity area of the multiple access channel. The results show that, especially when users experience different channel conditions or different degrees of information gradient, the channelaware quantization of federated learning is better than the non-perceptual channel quantization scheme (for example, uniform distribution). The difference between this scheme and the scheme in [108] is that it allows each user to have their own quantitative budget. First, a scheme for arbitrary user M is proposed, and the convergence speed of the scheme is analyzed. The algorithm puts forward the general optimization problem of quantitative budget allocation based on multiple access channel capacity. Then, they showed an example with M=2 users and found the best quantitative budget and communication rate. To this end, they researched and analyzed a channel-aware quantization scheme that is superior to uniform quantization and other existing digital schemes. Research on federated learning is still in early stages. It currently offers obvious opportunities from the edge to the core network. However, it has several key challenges in applying federated servers, as described in [109] . Security and privacy. It adopts a secure aggregation algorithm and does not need to transmit local sensitive data to the center. But the encrypted local model can reveal the local situation by analyzing the global model. In the case of federated learning, the model is trained through sensitive user data. The premise of these federated learning is to use users to effectively process data memory without revealing private information. Ultimately, this process can reduce the possibility of data disclosure in the event of an attack. At the same time, federated learning may be subject to reasoning attacks and confrontational attacks. The enemy embeds carefully designed samples into the data, effectively affecting the local training data set to manipulate the results of the model-polluting the federated learning process. Therefore, how federated learning can improve its own defense mechanism against these attacks should also be explored. Considerations such as the optimal number of local learners participating in the global update, the grouping of local learners, and the frequency of local updates and global aggregation that lead to a trade-off between model performance and resource protection, are all application dependent and worthwhile the study. In addition, the scale of FL networks update can be very large for low-power devices such as IoT nodes. Therefore, the method of sparse and compressed model parameters has higher computational efficiency and reduces resource consumption. FL has been continuously developed since it was proposed in 2016 [11] . In addition to the main issues discussed at this stage (asynchronous [110] , communication security [111] and privacy issues [19] ), there are still the following key open directions to be explored. Defense against attacks. Although FL can protect important information, if people deliberately launch a poisoning attack on distributed devices, it may also lead to the leakage of important information [112] . For example, due to the stochastic gradient descent (SGD) [113] [114] in the actual application process, and the leakage of these gradients may actually leak data information [115] . Chuan et al. also studied the potential privacy and security issues in FL [78] . Therefore, how to effectively defend against privacy and security issues in FL is still an open challenge. Algorithm efficiency. The rapid growth of network traffic has become the main technical bottleneck for the development of the IoT. Although FL can effectively connect distributed devices, optimization algorithms are also needed to better realize practical applications. For example, reduce time complexity [116] , FedAvg algorithm [117] is used for local calculation update and aggregation, and used for client-side differential privacy preservation federal optimization algorithm [35] . Due to the limitation of computing power, the related algorithms of FL still need to be optimized in face of massive data. Technology application. FL has broad prospects in smart cities. It can involve almost all aspects, especially in the fields of finance, medical care, transportation and et al. FL can perform model training on data associated with multiple standards. Taking smart healthcare as an example, FL can train models that cannot be directly aggregated by hospitals. However, FL can fuse sensitive information without revealing privacy and overcome the data island. Combining more data can greatly improve the accuracy of the model. The practical application of FL will also make cities smarter. The gradual development of FL has brought new vitality to all walks of life. This article introduces the application of FL in smart cities, including communications, life services and the IoT. It is expected that in the near future, FL will lead the further development of smart cities. FL will also be combined with all walks of life to form a good ecological community, so that everyone can benefit from it. FL has been widely used and developed in various fields. This paper investigates the development achievements of FL in the field of Internet of things, transportation, communication, medical care and finance. Meanwhile, we think about the future research direction of FL in other fields of smart city. It will face heterogeneous, communication security and privacy issues. We also think about proposing certain ideas and implementations in defense against attacks, potential privacy security, algorithm efficiency, and broader application scenarios. Moreover, we put forward our future technology development prospects. After that, we will continue to conduct in-depth research on key technologies. 
paper_id= fff3678cfe3ce7a9ccae1e7becf17d5d71d1b54a	title= Gaining Insights into the Codon Usage Patterns of TP53 Gene across Eight Mammalian Species	authors= Tarikul Huda Mazumder;Supriyo  Chakraborty;	abstract= TP53 gene is known as the "guardian of the genome" as it plays a vital role in regulating cell cycle, cell proliferation, DNA damage repair, initiation of programmed cell death and suppressing tumor growth. Non uniform usage of synonymous codons for a specific amino acid during translation of protein known as codon usage bias (CUB) is a unique property of the genome and shows species specific deviation. Analysis of codon usage bias with compositional dynamics of coding sequences has contributed to the better understanding of the molecular mechanism and the evolution of a particular gene. In this study, the complete nucleotide coding sequences of TP53 gene from eight different mammalian species were used for CUB analysis. Our results showed that the codon usage patterns in TP53 gene across different mammalian species has been influenced by GC bias particularly GC 3 and a moderate bias exists in the codon usage of TP53 gene. Moreover, we observed that nature has highly favored the most over represented codon CTG for leucine amino acid but selected against the ATA codon for isoleucine in TP53 gene across all mammalian species during the course of evolution. 	body_text= TP53 gene encodes tumor protein p53 which is known as the "guardian of the genome" as it plays a vital role in maintaining genomic stability by preventing mutation in the genome [1] . The p53 primarily acts as transcription factor and stands out as a key player in restricting tumor cell invasion that includes the ability to induce cell cycle arrest, DNA repair, senescence and apoptosis [2] . Mutation in p53 results in abnormal proliferation of cells that leads to the formation of tumor development and so TP53 gene is cataloged as tumor suppressor gene [3] . The nucleus of a cell is the main store house of tumor protein p53 where it binds to DNA. When any damage occurs in the DNA of a cell by some external agents like toxic chemicals, radiation, exposure to sun light or ultra violet rays, p53 plays the crucial role in activating other genes and inhibits cell cycle to repair the damage [4] . In case of failure of DNA repair, the tumor protein p53 prevents the cell from dividing and provokes signals to a wide variety of genes that contribute to TP53 mediated cell death i.e., apoptosis [5] . Unequal usage of synonymous codons that encode the same amino acid during translation of a gene into protein is known as codon usage bias (CUB). Some codons in a synonymous group are used more frequently whereas others less frequently in the genome of an organism [6, 7] . CUB is a unique property of the genome and it may vary between genes from the same genome or within a single gene [8, 9] . The advent of whole genome sequencing in different organisms and the easily accessible nucleotide database from NCBI (GenBank) have attracted much attention of the scientific community to study CUB in gaining clues for understanding the molecular evolution of genes and genome characterization. Previously, several studies were conducted on synonymous codon usage bias in a wide variety of organisms including prokaryotes and eukaryotes [10] [11] [12] [13] [14] [15] [16] , and till date in many organisms the codon usage patterns have been interpreted for diverse reasons. Many genomic factors such as gene length, GC-content, recombination rate, gene expression level, or modulation in the genetic code are associated with CUB in different organisms [17] [18] [19] [20] [21] . In general, compositional constraints under natural selection or mutation pressure are considered as major factors in the codon usage variation among different organisms [8, [22] [23] [24] [25] . Moreover, studies revealed that mutation pressure, natural or translational selection, secondary protein structure, replication and selective transcription, hydrophobicity and hydrophilicity of the protein and the external environment play a major role in the codon usage pattern of organisms [26] . In unicellular and multicellular organisms it was observed that, preferred synonymous codons/optimal codons with abundant tRNA gene copy number rise with gene expression level within the genome that supports selection on high codon bias confirmed by positive correlation between optimal codons and tRNA abundance [18, 22, 27] . Urrutia and Hurst (2003) reported weak correlation between gene expression level and codon usage bias within human genome though not related with tRNA abundance [19] . However, Comeron (2004) observed that in human genome, highly expressed genes have preference towards codon bias favoring codons with most abundant tRNA gene copy number compared to less highly expressed genes [28] . The study of codon usage bias acquires significance in biology not only in the context of understanding the process of evolution at molecular level but also in designing transgenes for increased expression, discovering new genes [29] based on nucleotide compositional dynamics, detecting lateral gene transfer and for analyzing the functional conservation of gene expression [30] . Codon usage bias may be superimposed on the effect of natural selection. The amount of protein produced from the mRNA transcript may vary significantly since the translational properties of alternate synonymous codons are not equivalent [31] . Several studies have further shown that codon usage bias is associated with highly expressed genes as some codons are used more often than others in the coding sequences [32] . Moreover, literature suggested that a gene can be epitomized not only by the sequence of its amino acid but also by its codon usage patterns shaped by the balance between mutational bias and natural selection [33] . As a consequence of selection pressure within a gene, differentiation in codon bias may arise between species of the same genus. The present study was undertaken in order to perform a comparative analysis of codon bias and compositional dynamics of codon usage patterns in TP53 gene across eight different mammalian species using nucleotide chemistry (GC contents) and several genetic indices namely effective number of codons (ENC), relative synonymous codon usage (RSCU), and relative codon usage bias (RCBS) etc. Our analysis has given a novel insight into the codon usage patterns of TP53 gene that would facilitate better understanding of the structural, functional as well as evolutionary significance of the gene among the mammalian species. Codon usage patterns in TP53 genes across mammalian species Correlation coefficient between codon usage and GC bias was analyzed using heat map ( Fig. 1 ) in order to find out the relationship between the codon usage variation and the GC constraints among the selected coding sequences of TP53 genes. In our analysis, nearly all codons ending with G/C base showed positive correlation with GC bias and nearly all A/T-ending codons showed negative correlation with GC bias. But, 8 G/C-ending codons (ATC, ACG, TAC, TTG, TCC, CAC, GTG, GGG) showed negative correlation with GC bias whereas 6 A/T-ending codons (AAT, ATT, TGT, CGA, GTA, GGA) showed positive correlation with AT bias although statistically not significant (p>0.05). Two G-ending codons i.e. TCG for serine and CTG for leucine amino acid showed strong positive correlation (p<0.01) with GC 3s , indicating that codon usage has been influenced by GC bias due to GC 3s . Interestingly, we observed that the codon ATA encoding isoleucine amino acid was not favored by natural selection in TP53 genes across mammalian species during the course of evolution. Thus, scanning the codon usage pattern provides the basis of the mechanism for synonymous codon usage bias and has both practical as well as theoretical significance in gaining clues of understanding molecular biology [34] . We analyzed the nucleotide composition of coding sequences from TP53 genes ( Table 1 ) which revealed that mean value of C (361.50) was the highest followed by G (306.75), A (270.88) and T (227.50) among all the selected mammals. The mean percentage of GC and AT compositions was 57.3% and 42.7% respectively. Thus, the overall nucleotide composition suggested that the nucleotide C and G occurred more frequently compared to A and T in the coding sequences of TP53 gene across the mammalian species. The nucleotide composition at the third position of codon (A 3 ,T 3 ,G 3 ,C 3 ) showed that the mean values of C 3 and G 3 were the highest followed by T 3 and A 3 . The GC 3 values (ranged from 58.7%-70.6%, mean = 65.1%, SD = 0.040) was compared with that of AT 3 values (ranged from 29.4%-41.3%, mean = 34.9%, SD = 0.040) in the coding sequences of TP53 genes. The average percentage of GC contents at the first and second codon positions (GC 12 ) was found in the range of 52.6% to 54.9% with a mean value of 53.4% and a standard deviation (SD) of 0.008. Therefore, nucleotide composition analysis suggested that GC-ending codons might be preferred over AT-ending codons in the coding sequences of TP53 genes across the selected mammalian species. Further, we calculated the occurrence of frequently used optimal codons (Fop) for each amino acid as suggested by Lavner and Kotler (2005) [14] . The frequency was allied with statistical analysis to find out the highest and lowest frequently used codon. Our results showed that the most frequently used codons were G/C-ending for the corresponding amino acid (Fig. 2) in TP53 genes across mammalian species. The relative synonymous codon usage values of 59 codons for TP53 gene across eight mammalian species were analyzed excluding the codons ATG (methionine) and TGG (tryptophan). In our calculation RSCU value greater than 1.0 represents that the particular codon is used more frequently and less than 1.0 represents the less frequently used codon for the corresponding amino acid. The RSCU value greater than 1.6 indicates over represented codon for the corresponding amino acid. The overall RSCU values in the selected coding sequences of TP53 gene revealed that 25 codons were most frequently used among the 59 codons and the most predominantly used codons were G/C-ending compared to A/T-ending (Table 2) . Besides, it was observed that C-ending codon was mostly favored compared to G-ending codon in the coding sequence of TP53 gene among the selected mammalian species. Our results showed marked similarities as reported by Dass et al., (2012) in serotonin receptor gene family from different mammalian species [35] . Further, clustering analysis of RSCU values (Fig. 3) depicted that the codon GCC, CGC (except Rattus norvegicus), ATC (except Tupaia chinensis), CTG, ACC (except Rattus norvegicus, Macaca mulatta), GTG (except Felis catus) were displayed as the over represented codons (RSCU>1.6). The highest RSCU value was found for the codon CTG for leucine amino acid in all TP53 genes across mammalian species. The codon ATA showed the RSCU value zero because natural selection has not favored this codon in TP53 gene across mammalian species. Codon usage patterns of TP53 gene correspond to phylogeny of mammalian species We have performed a neighbor joining tree analysis based on Kimura 2-parameter (K2P) distances of the coding sequences in TP53 gene across mammalian species (Fig. 4) . We observed that codon usage patterns in TP53 genes have significant similarities among the closely related mammalian species. The gene TP53 in H. sapiens showed resemblance to the TP53 gene in M. mulatta, Similarly, TP53 of F. catus resembled to that of C. lupus and M.unguiculatus with R. norvigicus. Generally, genes with similar functions exhibit similar patterns of codon usage frequency [36] . Our analysis further suggested that the coding sequence of TP53 gene share similar patterns of codon usage bias across eight mammalian species. The ENC values of the coding sequences ranged from 52 to 59 with a mean of 55.5±2.33 indicating relatively smaller variation in the codon usage of TP53 gene across eight mammalian species. However, the GC 3s values ranged from 0.59 to 0.71 with a mean value of 0.65±0.040. Significant negative correlation (Pearson r = -0.979, p<0.01) was observed between ENC and GC 3s . Moreover, a plot of ENC vs GC 3s revealed that the ENC values had negative correlation with the GC 3 content (Fig. 5 ) and comparatively lower ENC was linked to higher GC 3s values. All the selected coding sequences of TP53 gene across the selected mammalian species had Overall frequency of optimal and non optimal codon used in TP53 genes among mammals. Red color coding represents optimal used codons with corresponding amino acid. doi:10.1371/journal.pone.0121709.g002 a higher predominance of G/C-ending codons. It suggested that GC 3s values determined the codon usage pattern in the coding sequences of TP53 gene [33] . Nabiyouni et al., (2013) reported that eukaryotic organisms with very high GC-contents have high GC 3 -composition while organisms with low GC-content have low GC 3 -composition in the genome [37] . We also calculated GC 3 skew values which ranged from 0.000 to -0.094, indicating that GC 3 composition at the third position of codon might have played an important role in the codon usage bias [38] . Negative GC skew was observed in all the coding sequences of TP53 gene which revealed that the abundance of C over G [39] . In addition, lower values of the frequency of optimal codons (FOP) and the effective number of codons (ENC) along with higher GC contents suggested that a moderate bias exists in the usage of synonymous codons [33] for TP53 gene in different mammalian species. Predominant codon usage bias was observed in TP53 gene of M.unguiculatus compared to other mammalian species (Table 3) . RCBS value of a gene can be used as an effective measure of predicting gene expression and its value depends on the patterns of codon usage along with nucleotide compositional bias of a gene [20] . The distribution of RCBS values for TP53 gene across eight mammalian species is shown in figure below (Fig. 6) . The RCBS values ranged from 0.006 to 0.065 with a mean value of 0.039 and a standard deviation (SD) of 0.021. In our analysis, low mean RCBS value suggested that there exists a low codon bias for TP53 gene associated with low expression level [20] . In brief, our results showed that codon usage in TP53 gene in mammals has been influenced by GC bias, mainly due to GC 3s . The majority of frequently used codons were G/C ending in which C-ending codons were mostly favored compared to G-ending codons for the corresponding amino acid. The most over-represented codon was CTG encoding the amino acid leucine in the TP53 gene of all the selected mammalian species. We further observed that the codon ATA encoding isoleucine was selected against by nature in TP53 genes across the mammalian species under study during the course of evolution. The codon usage pattern for TP53 in H. sapiens showed resemblance to that of M. mulatta; similarly, F. catus to C. lupus and M. unguiculatus to R. norvigicus. Moderate codon bias was observed for the TP53 gene in different mammalian species. The codon usage patterns in the coding sequence of TP53 gene across different mammalian species showed significant similarities, suggesting that the evolutionary pattern might be similar. According to Yang and Nielsen (2008) , codon bias in mammals is mainly influenced by mutation bias and the selection on codon bias is weak for nearly neutral synonymous mutations [40] . From the outstanding work of Grantham et al., (1980 Grantham et al., ( -1981 on "genome hypothesis" it was evident that species specific genes share similar spectrum of codon usage frequency [41, 42] . The present study revealed that specific gene of closely related species with similar functions exhibit similar patterns of codon bias across different mammals as evident from the previous work of Dass et al., (2012) [35] . To the best of our knowledge, this is the first report on the codon usage pattern in TP53 gene across the mammalian species. Since our analysis has given better insights into the codon usage, it may have theoretical value in further understanding the molecular evolution of TP53gene. The complete nucleotide coding sequences (cds) for TP53 gene having perfect start and stop codon, devoid of any unknown bases (N) and perfect multiple of three bases, were retrieved from National Center for Biotechnology Information (NCBI) GenBank database (http://www. ncbi.nlm.nih.gov). Finally, we selected eight coding sequences for TP53 gene that fulfill the above mentioned criteria in different mammalian species and used in our CUB analysis (Table 4 ).  The occurrence of overall frequency of the nucleotide (G+C) at first (GC 1 ), second (GC 2 ) and third (GC 3 ) position of synonymous codons were calculated to quantify the extent of base composition bias. Moreover, we analyzed the skewness for AT, GC and GC 3s of each coding sequence to estimate the base composition bias particularly in relation to transcription processes. ENC is generally used to quantify the codon usage bias of a gene that is independent of the gene length and number of amino acids [43] . This measure was computed as per Wright (1990) to estimate the extent of CUB exhibited by the coding sequences of TP53 gene across the selected mammalian species: Where, F k ( k = 2, 3, 4 or 6) is the average of the F k values for k-fold degenerate amino acids. The F value denotes the probability that two randomly chosen codons for an amino acid with two codons are identical. The values of ENC ranged from 20 indicating strong codon bias in the gene using only one synonymous codon for the corresponding amino acid, to 61indicating no bias in the gene using all synonymous codons equally for the corresponding amino acid [43] . Fop is a measure of codon usage bias in a gene [44] . Fop values represent the ratio of the number of optimal codons used to the total number of synonymous codons [22] . The Fop value ranges from 0.36 for a gene showing uniform codon usage bias to 1 for a gene showing strong codon usage bias [45] . Fop value for each selected coding sequence was calculated using the formula given by Lavner and Kotler (2005) [14] . RSCU is defined as the observed frequency of a codon divided by the expected frequency if all codons are used equally for any particular amino acid [46] . RSCU values of codons for each of the selected coding sequence of TP53 gene was calculated as follows: Where, g ij is the observed number of the ith codon for the jth amino acid which has n i kinds of synonymous codons [26] . Gene expression was estimated through RCBS which can be defined as the overall score of a gene indicating the influence of relative codon bias (RCB) of each codon in a gene [20] . The RCBS value of each coding sequence of TP53 gene was calculated as follows: where, Oc is the observed number of counts of codon c of the query sequence and E[O c ] is the expected number of codon occurrences given the nucleotide distribution at three codon positions (b1b2b3) [20] . logw RCB c Þ À 1 [47] Where, O tot is the total number of codons The above mentioned genetic indices were estimated in a PERL program developed by SC (corresponding author) to measure the CUB on the selected coding sequences of TP53 genes in different mammalian species. All statistical analyses were carried out using the SPSS software. Cluster analysis (Heat map) of correlation coefficient of codons with GC3 and the RSCU values of codons among the eight mammalian species were clustered using a hierarchical clustering method implemented in NetWalker software [48] . 
paper_id= fff37ff9de7ac14189bacc386448ae96a624f19f	title= Specific elimination of coxsackievirus B3 infected cells with a protein engineered toxin-antitoxin system	authors= Jung-Ho  Park;Jin-Ho  Park;Wonho  Choi;Byung-Kwan  Lim;	abstract= Coxsackievirus B3 (CVB3) is a member of the family Picornaviridae, and along with polioviruses, belongs to the Enterovirus genus. The CVB3 genome is composed single-stranded rna encoding polyproteins, which are cleaved to individual functional proteins by 2a and 3C proteases proteins which have been targeted for drug development. Here, we showed that protease activity required to activate a toxic protein may be used to prevent viral infection. Methods: We modified the MazE-MazF antitoxin-toxin system of Escherichia coli to fuse a C-terminal fragment of MazE to the N-terminal end of toxin MazF with a linker having a specific protease cleavage site for CVB3. This fusion protein formed a stable dimer and was capable of inactivating the mrna interferase activity of MazF which cleaves the ACA sequence in mRNA substrates. Results: The incubation of 2a proteases with the fusion proteins induced cleavage between the MazE and MazF fragments from the fusion proteins; the subsequent release of MazF significantly inhibited virus replication. additionally, we note that, CVB3 infected HeLa cells quickly died through a MazF toxin mediated effect before virus protein expression. Conclusion: These findings suggest that the MazEF fusion protein has a strong potential to be developed as an anti-virus therapy following CVB3 infection. 	body_text= CoxsackievirusB3 (CVB3) is a cardiotropic virus that binds to the coxsackievirus adenovirus receptor to infect host cells [1] [2] [3] . CVB3 is a member of the family Picornaviridae, and along with polioviruses, it belongs to the Enterovirus genus. although most enterovirus infections are subclinical, acute myocardial inflammation triggered by these infections can induce severe arrhythmias and sudden cardiac death or may lead to the development of chronic myocarditis and dilated cardiomyopathy [4] [5] [6] [7] [8] [9] [10] . We previously reported several antiviral drugs developed from small chemical compounds. an inhibitor of enterovirus protease 3C (3CPi) showed a particularly strong antiviral effect in a murine viral myocarditis model. it significantly reduced viral replication in the heart and minimized myocardial damage 11 . The proteases encoded by rna viruses, such as Corona virus, hepatitis a virus (HaV), and CVB3, play an essential role in viral infection, as they are required for the processing of virus-encoded polyproteins 12 . Most bacteria have the toxin-antitoxin (Ta) system which is a mechanism to control the cell growth. Toxin induces cell growth arrest and even cause cell death without the co-expression their cognate antitoxins 13, 14 . Thus, virus proteases have been considered ideal drug targets. in the present report, we attempted to positively use the activity of the viral proteases to activate a latent toxin (MazF) of Escherichia coli from a toxin-antitoxin (MazF-MazE) fusion protein by cleaving off antitoxin MazE fragment. MazF thus released functions as an ACA-specific mrna interferase 15 to eliminate almost all cellular mrna as well as single-stranded viral rna in the vi-rus-infected cells. The CVB3 genome, consisting of a positive single strand rna 7400 bases, encodes three major polyproteins (P1, P2, P3) which require processing by the CVB3 protease, 2a. as these viral proteases cleave polyproteins at highly specific amino acid sequences (aXXQ:g), these specific protease cleavage sites for individual rna viruses may be incorporated into the linker between MazE antitoxin and MazF toxin of the MazE-MazF fusion proteins (MazEF) so that viral proteases induced upon infection cleave the linker to activate MazF as an ACA-specific mRNA interferase. MazF has been shown to be a potent toxin, effectively causing apoptotic cell death in mammalian cells 16 . in E. coli, two dimers of MazF form a stable heterohexamer complex with one MazE dimer in the center 17,18 ( Figure  1 ). in this study, we developed a new enterovirus antiviral system using bacteria to drive the anti-toxin and toxin system. The MazEF toxin significantly inhibited virus replication with viral protease 2a activating the bacterial MazEF toxin. Here, we also note that CVB3 infected Hela cells were quickly destroyed through a MazEF toxin effect before virus protein expression. These findings suggest that MazEF has the potential to be developed as an anti-viral drug for CVB3 infections. CVB3 was derived from an infectious cDna copy of the cardio tropic H3 variant of CVB3. Virus titers were determined using the plaque-forming assay in Hela cells as described previously. Hela cells were maintained in Dulbecco's Modified Eagle Medium (DMEM, Welgen inc, gyeongsan-si, Kor) supplemented with 10% fetal bovine serum. HeLa-UVM cells were obtained from Dr. eS Jeon (Samsung Medical Center, Seoul, Korea) 19 . We previously generated a MazEF fusion protein with a linker contain a specific cleavage site for HCV proteases ( Figure 1A ). We have modified the specific cleavage site between C-terminal region of MazE (MazEΔ1-41) and MazF genes from E. coli for CVB3 protease 2a and added four extra residues (gly-gly and gly-Ser at the n-terminal and the C-terminal ends, respectively) ( Figure 1B) 18 . We generated a MazEF working system into green fluorescence protein plasmid (pEGFP) for the in vitro cell survival assay. in brief, Hela cells were transfected with the MazEF/pEGFP plasmid for 24 hours followed by addition of CVB3 at multiplicity of infection (m.o.i.) of 1 (10 μL) and 2 (20 μL). After 12 hours of infection, we observed the impact on cell morphology and cytopathic effects using fluorescence microscope, and extracted protein for western blot analysis. Cells were lysed in riPa buffer (50 mM Tris-HCl, pH 8.0, 0.1% SDS, 1% nP40, 150 mM NaCl, 0.5% sodium deoxycholate). aliquots (10 μg) of total cell extracts were loaded onto 10% SDS-PAGE gels. Following gel electrophoresis, proteins were transferred to Hybond-eCl nitrocellulose membranes (amersham Biosciences, Piscataway, nJ, USa). The membranes were blocked in 5% non-fat dry milk solution in phosphate-buffered saline (PBS) containing 0.1% Tween-20 and incubated with anti-enterovirus VP1 (1 : 1000, mouse monoclonal; Novocastra, Lincolnshire, IL, USA), eIF4G1 (1 : 1000, rabbit polyclonal; Cell Signaling Technology, Danvers, MA, USA), and GAPDH antibodies (1 : 1000, rabbit polyclonal antibodies; Cell Signaling Technology, Danvers, MA, USA) 19 . The data are presented as the mean±SEM. Differences in measured parameters between control and target experimental groups were examined using the Mann-Whitney U t-test (GraphPad Prism 3.0 for Windows; graphPad Software, la Jolla, Ca, USa). Survival rates were analyzed using the Kaplan-Meier method. P<0.05 was considered significant. To confirm the antiviral effect of the MazEF toxin, we have overexpressed GFP conjugated MazEF (MazEF/ pEGFP) or MazEF only (MazEF/pcDNA3). After 12 hours, CVB3 was added into the cell culture plate and to confirm the cytopathic effect on Hela cells. as shown in Figure 2A , MazEF-GFP expressing HeLa cells indicate MazEF toxin expression and these cells experienced a greater cytopathic effect in a virus concentration dependent manner compare to GFP overexpressed cells. Western blot analysis revealed that MazEF toxin significantly decreased CVB3 replication in CVB3 infected cells, resulting in a reduction in de- tectable virus capsid protein VP1 compare with control cells ( Figure 2B ). The overexpression of MazEF toxin in HeLa cells resulted in rapid cell death and reduced expression of virus capsid protein VP1 in CVB3 infected cells. Translation initiation factor protein eIF4G1 was cleaved by viral protease 2a soon after CVB3 infection and inhibited host protein production. As shown in Figure 3 , cleaved eIF4G1 was detected in CVB3 infected cells. However, viral capsid protein VP1 was significantly decreased in MazEF toxin treated HeLa cells, because cell death is induced by the MazEF toxin before virus replication. These results demonstrated that CVB3 infected Hela cells were killed by direct overexpression of MazEF toxin before initiation of virus replication. amplification of viral rna amplification is essential for viral replication. However, MazEF toxin resulted in rapid cell death in virus infected cells. We confirmed the CVB3 capsid protein positive and negative strand rna replication using rT-PCr. in CVB3 infected Hela cells, CVB3 genome expression was significantly decreased in cells expressing the MazEF fusion protein compared with control cells (Figure 4 ). CVB3 pro-tease2A modified the structure of the MazE and MazF proteins. MazEF fusion resulted in the rapid death of CVB3 infected cells which subsequently affects virus genome amplification. Xiong et al. have previously demonstrated in vitro that the enteroviral protease 2a directly cleaves murine dystrophin in the hinge 3 region 20 in order to determine whether this cleavage plays an important role in the development of enterovirus-mediated myocarditis, sarcolemma membrane disruption, viral propagation and direct virus-mediated cytopathic effect in vivo 21 . in this study, we applied this concept to inhibit virus propagation and systemic side effects accompanying in CVB3 infection. The first stage of viral replication involves processing of the protease 2a into a polyprotein; this is a critical time point to inhibit virus amplification and propagation to the other cells. Since these viral proteases cleave polyproteins at highly specific amino acid sequences, these specific protease cleavage sites for individual rna viruses may be incorporated into a linker between the MazE antitoxin and MazF toxin of the MazE-MazF fusion protein so that viral proteases induced upon infection cleave the linker to activate MazF as an ACA-specific mRNA interferase. Activation of MazF in mammalian cells has been result in BaK-dependent apoptotic cell death 16 . We could regulate CVB3 infected cell death through MazEF fusion protein induction by CVB3 expressed protease 2a. in addition, the fusion protein may potentially be used as a tool for the prevention and/or treatment of rna virus infections, as cells carrying the fusion protein with a linker cleavable with a specific rna viral protease may attain resistance to a rna viral infection 18 . However, the MazEF fusion protein in vivo systemic effect is not clear and additional research is required to better elucidate this. These results suggest that the new application for CVB3 protease 2a induction of bacterial toxin protein, MazEF, is applicable to the treatment of many single-strand rna virus infections, (eg, CVB3 myocarditis, enterovirus71, and influenza virus). MazEF induction was confirmed in CVB3 infected Hela cells and it was strongly suppressed virus replication and genome amplification. Therefore, the MazEF toxin system could be beneficial in the treatment of cardiomyopathy associated with enterovirus infection. CVB3 is the main cause of viral myocarditis and meningitis in childhood. However, effective antiviral drugs have not yet been developed. our Ta system leveraging the MazEF fusion protein, is a potential targeted antiviral system. MazEF activation is regulated by target virus protease expression and may be a potential therapeutic target in human CVB3 infections. 
paper_id= fff4157b1a3a559f636dfe48c43d8c41d626690a	title= High incidence of stroke and mortality in pediatric critical care patients with COVID-19 in Peru	authors= Alvaro  Coronado Munoz;Jaime  Tasayco;Willy  Morales;Luis  Moreno;David  Zorrilla;Angie  Stapleton;Patricia  Pajuelo;Giuliana  Reyes;Matilde  Estupiñan;Ricardo  Seminario;Manuel  Ortiz;Jesús  Domínguez;	abstract= 	body_text= The mortality of pediatric patients with COVID-19 acute respiratory failure has been mainly associated with comorbidities. [1] [2] [3] Epidemiological studies from different countries report mortalities of 0.1-0.3% for all patients hospitalized with COVID-19. The novel presentation of multisystemic inflammatory syndrome in children (MIS-C) alerted the medical community about the pathogenesis of COVID-19 in pediatric patients. [4] [5] [6] [7] [8] The broad definition proposed by the CDC includes inflammatory processes in different systems, such as cardiac, renal, neurological, respiratory, and gastrointestinal systems. The severity of MIS-C has been mostly associated with cardiac complications ranging from severe heart failure and coronary aneurisms. 4, 6, [8] [9] [10] [11] However, the mortality in epidemiological studies of this syndrome is also low, reported as high as 2%, with most complications associated with cardiac failure and shock. The mortality and morbidity of COVID-19 in pediatric critical care patients has been described in different studies. [12] [13] [14] The mortality in those studies was also low, ranging from 2 to 6% for patients admitted to critical care units. One of the largest studies from pediatric critical care units (PICUs) reported a mortality of 5.7% from 530 patients from the United States and Canada databases. 15 Since the emergency state was declared for the COVID-19 pandemic, one of the measures adopted by the government in Peru was to create a dedicated COVID-19 hospital. Within that hospital, a six-bed pediatric critical care unit was implemented. When the capacity of that unit was reached with acute COVID-19 cases, other hospitals were able to admit patients with MIS-C. The mortality in pediatric critical care patients in Peru in the dedicated COVID-19 unit is >20%, significantly higher than that reported in studies with similar populations. 15 In the past 3 months, different pediatric critical care groups and pediatric emergency groups in Peru alerted an increasing number of pediatric patients presenting to emergency rooms with neurological symptoms and COVID-19. This acute presentation has not been described previously in other settings. We present the cases admitted to the COVID-19 PICU and the data of one additional center that hospitalized MIS-C patients. The purpose is to analyze the causes of mortality for pediatric critical care patients with COVID-19 in Peru. We performed a prospective case-control study for patients hospitalized from 26 March to 15 August 2020 in two centers in Lima, Peru. Patients were selected from two hospitals. The first hospital is a COVID-19 center, reporting patients from the pediatric critical care unit. The second hospital included patients admitted to the PICU with MIS-C diagnosis. This study was reviewed by the IRB office at each center: "Comité de Ética en Investigación para COVID-19, ESSALUD" and "Coordinadora de la Unidad de Docencia e Investigación, HEVES". Both IRB committees deemed the study exempted due to the COVID-19 emergency and authorized the completion of the study. We included patients 0-17 years old with positive testing for COVID-19 (PCR, IgM, or IgG) with SARS-CoV-2 or MIS-C admitted to the pediatric critical care unit. We excluded patients with a positive test for COVID-19 but that the admission diagnosis was non-COVID-19 related (e.g., trauma). The definition of MIS-C followed the criteria defined by the United States CDC. 5 We divided our patients into three groups by clinical presentation: acute COVID-19, MIS-C, and neurological presentation. Patients with acute COVID-19 were considered patients with respiratory failure requiring mechanical ventilator support. Patients with MIS-C met the United States CDC definition. Patients with neurological presentation were patients with neurological symptoms at admission. The primary outcome was mortality during hospitalization. The variables included were demographics (age and sex) and referring center. For therapies, we collected information on treatments considered to be directed COVID-19 treatment in Peru: ivermectin and hydroxychloroquine. We also included the use of steroids, vasoactive medications, and intravenous immunoglobulins. For steroids, we included dexamethasone and methylprednisolone, low dose, and pulse steroid dose. Vasoactive medications included dopamine, dobutamine, epinephrine, and norepinephrine. For anticoagulation therapies, we included antiplatelet medication (aspirin), heparin, and enoxaparin. We included severe respiratory distress syndrome utilizing the Berlin definition of a PaO 2 /FiO 2 less than 100 (ref. 16 ). Laboratory values were collected for all the patients, including in the analysis the worst value (peak value) of the first 96 h of admission. Brain imaging results were obtained for all the patients with neurological complications and a description of the patients presenting with cerebrovascular accident (CVA), ischemic stroke, or hemorrhagic stroke. Echocardiograms data were also collected. Treatments used in all the patients were also collected. Statistical analysis was performed with frequentist analysis. Continuous variables were analyzed with the Mann-Whitney U test, and categorical variables were analyzed with Fisher's exact test. Statistical analysis was performed with the IBM SPSS 26.0 software package. During the study period, a total of 47 patients were admitted; 39 patients were from the COVID-19 unit and 8 were from the second institution. A total of 15 (31.9%) patients were admitted with SARS-CoV-2, 21 (44.7%) were admitted with MIS-C, and 11 (23.4%) patients presented neurological complications. Most patients included were male patients, 30 (63.8%). The age ranged from 1 month to 16 years. Only four patients with neurological presentation had severe or moderate acute lung injury concomitantly. Only five with neurological presentation met MIS-C criteria. Comorbidities were present in 15 (31.9%) of the patients, with most within the Acute COVID-19 group (12 (80%), p < 0.01). The three groups also differed in steroid use: acute COVID-19 11 (73.3%), MIS-C 8 (38.1%), and neurological 9 (81.8%), p 0.03. A detailed description of the patients included is presented in Table 1 . The mortality of our study was 21.3%, and 10 patients died during the study period. The mortality of patients with neurological presentation was 45.5%, which was significantly higher than the mortality of acute COVID-19 (26.7%) and MIS-C (4.8%), p 0.18. Other risk factors of mortality vs survival in our cohort were comorbidities 7 (70%) vs 8 (21.6%), p 0.007, strokes 4 (40%) vs 2 (5.4%), p 0.014 and higher white blood cell count, median, and interquartile ranges for mortality 21 (14; 31) vs 13 (9; 16) for survival, p 0.01. Therapies and mode of admission were not associated with mortality. Severe acute lung injury, defined by a PaO 2 /FiO 2 value of less than 100 at 24 h of admission, was not associated with mortality. The CRP level was not associated with higher mortality. We present these variables analyzed for mortality in Table 2 . Cerebrovascular events occurred in six patients (12.8%) in our cohort, and four of them died. Most patients had hemorrhagic stroke, and only one patient had an ischemic stroke. One patient with hemorrhagic stroke and one patient with ischemic stroke survived but had significant sequelae. The presentation of these patients varied from seizures to vomiting. There was no previous use of anticoagulation therapies or trauma in any of those patients. One patient with hemorrhagic stroke had a comorbidity, and a diagnosis of leukemia was made 2 days after admission. A detailed description of neurological cases can be seen in the Supplementary Material. Brain tomography images on admission are shown in Fig. 1 . Comorbidities in our study were associated with mortality. For the group presenting with acute COVID-19, all mortalities were associated with comorbidities: one patient had acute lymphocytic leukemia, another patient had cerebral palsy and hypoxic ischemic encephalopathy, the third patient had trisomy 21 and an unrepaired ventricular septal defect, and finally, the last patient had undiagnosed Epstein anomaly. The patient with MIS-C who died during the period of study had a history of nephrotic syndrome and dialysis-dependent kidney failure. For the patients in the neurological group, one of them had pulmonary tuberculosis and developed tuberculosis meningitis, initially thought to be COVID-19 encephalitis, and the other patient was the undiagnosed patient with leukemia who presented with a hemorrhagic stroke. Differences between the three groups associated with morbidity and management can be found in Table 1 ; laboratory results by admission diagnosis can be found in Fig. 2 . Our study presents risk factors for mortality in pediatric critical care patients with COVID-19 in Peru. The mortality rate in our cohort was 21%. Mortality was higher in patients with comorbidities, strokes, and higher white blood cell count. One significant finding is the association of mortality of patients with neurological presentation. The presentation on admission of hemorrhagic strokes in pediatric patients with COVID-19 has not been described in other populations. Recognizing this presentation is extremely important for the medical community working with pediatric patients to rapidly diagnose and manage strokes. Neurological cases associated with COVID-19 in pediatric patients have ranged from encephalitis to Guillain Barre and acute disseminated encephalomyelitis). 3, 6, 17 There is one report of a 12-year-old patient with ischemic stroke who presented with seizures. 18 In the series from Riphagen, one patient developed ischemic stroke while on extracorporeal membrane oxygenation (ECMO) support for shock. 8 The unexpected number of cases that present with hemorrhagic stroke in Peru opens the possibility of describing a new presentation of COVID-19 in pediatric patients. It will be important to recollect data from countries with similar populations to identify risk factors and differentiate if it is a new phenotype or changes in the pathogenesis of the virus. Another important point is the need to have pediatric medical teams ready to manage this unusual presentation. Hemorrhagic and ischemic strokes in non-COVID pediatric patients have higher risks of morbidity and mortality if centers are not prepared with an appropriate neurocritical care infrastructure and team. The neurological complications of COVID-19 have been described in detail in the adult literature. [19] [20] [21] [22] Studies examining patients who had stroke in adult populations described the event occurring in patients with severe presentation of SARS-CoV-2, and most of those patients had that event during their hospitalization. 21 In adults, the occurrence of this type of event has been described in different critical conditions, including viral infections. 23 The physiopathology proposed for CVAs secondary to COVID-19 is the hyperinflammatory process, associated vasculitis, and coagulopathic states. 22, 24 It is unclear whether the same pathophysiology explains these events in pediatric patients. COVID-19 has been associated with inflammatory processes in pediatric patients, alterations of coagulation, and vasculitis, clearly seen in MIS-C. However, from the data presented in our study, the patients who had neurological manifestations had fewer comorbidities or preexisting conditions than the patients with respiratory failure and had similar or less severe inflammatory processes than the MIS-C patients, as noticed by the inflammatory markers. Additionally, not all the patients with neurological presentation in our cohort met the MIS-C criteria. Recognizing hemorrhagic and ischemic strokes as a presentation in pediatric patients with COVID-19 is also especially important to make therapeutic decisions regarding anticoagulation in patients hospitalized with MIS-C. As demonstrated by larger studies, comorbidities are associated with mortality in pediatric patients. 1, 3 The cohort we present shows this correlation. The comorbidities that our study presents are common diagnoses managed in pediatric critical care units, including congenital heart disease, leukemia, and cerebral palsy. An important detail about our study is that the group that had the most comorbidities was the acute COVID-19 group. All the patients who died from that group had a comorbidity and similar conditions to those described in a large series. 1, 3, 12 Of the patients with neurological presentation, two patients had significant comorbidities, and both died during their admission. One of those conditions was undiagnosed leukemia that presented with hemorrhagic stroke and establishing causality with COVID-19 is difficult. The caveat to this is that there were five other patients with similar cerebrovascular events without comorbidities. The other patient with comorbidities from the neurological group was a patient with pulmonary tuberculosis who presented with meningitis and was thought to have COVID-19 on admission. Further studies to establish SARS-CoV-2 immunosuppression that would facilitate the pathogenesis of other infections are necessary. The government of Peru has approved ivermectin and hydroxychloroquine as therapies approved for COVID-19. We did not find a difference in the utilization of those therapies across the three groups, and there was no correlation with mortality. Interestingly, steroids were more utilized in the acute and neurological groups than in the MIS-C group and did not correlate with mortality. There was no difference in vasoactive medication utilization or mortality outcomes. We did not find a difference in the laboratory results (peak value) in our patients. However, patients with higher WBC counts had higher mortality. Since the management of pediatric critical care patients was centralized in one center, the patients included in our study were the sickest patients who were hospitalized in Peru, especially during the first two months of the pandemic. This setup had significant limitations: the patients who presented in emergency departments (EDs) that required critical care stayed in an observation area in that ED until a bed was available in the COVID-19 unit, and the transfer of the patient was possible. There is no clear information about the waiting time to have a bed available in the COVID-19 unit, which explains the high mortality of patients who were not able to be hospitalized in a PICU. By September 2020, according to the Peru Health Department (MINSA), 78 patients had died from COVID-19 in the group 0-9 years old, and 62 patients had died in the group 10-19 years old. A larger pediatric epidemiological study is necessary to understand the morbidity and mortality of patients who did not die in a pediatric critical care unit. We examined whether the mode of admission (local ED vs referring ED) and origin of reference (Lima vs other cities) had an impact on mortality. We did not find a difference between those variables. Having the waiting times could help establish if the model of a single center with six beds had an association with mortality. For a population of 10 million people younger than 18 years old, the number of beds designated for the PICU is likely not enough. The justification for a single center was to minimize the overutilization of protective equipment. The presentation of patients with strokes is challenging in any clinical setting, and it is difficult to argue that the outcomes we present would have been different in other countries. Our study presents some limitations. Our number of patients is small compared to larger epidemiological studies. The acute presentation with neurological compromise and high mortality are higher than other studies in critical care. This study only captures the patients who were able to be hospitalized to the COVID-19 pediatric critical care unit and does not include a larger denominator. A larger national epidemiological study is necessary to determine if more cases of hemorrhagic strokes are present. The mortality of pediatric patients seems to be similar to that of patients in other countries, but the patients who were not included in our cohort died without accessing critical care. This is one of the challenges of the management of this pandemic in countries with developing health systems. Studies investigating viral characteristics in Peru are necessary. Cerebrovascular events associated with COVID-19 in pediatric patients, including infants, must be recognized as one of the more severe presentations of this infection in pediatric patients. The management of these patients without critical care interventions can be fatal, and patients who survive can have severe sequelae. Recognizing this presentation in pediatric patients might help plan anticoagulation therapies for critical care patients with COVID-19. 
paper_id= fff48ee1d77323393dde3de4a7b3a5821be290a5	title= EVALUATION OF THE FIRST PHARMACY-LED WEIGHT MANAGEMENT PROGRAMME IN GREECE	authors=  A Peletidi;R  Kayyali;	abstract= interventions or recommendations were made, averaging 3.3 per resident. Table 1 shows that the highest proportion (30.4%) of these were public health related, whilst changing and stopping medicines accounted for 17.9% and 12.8% respectively. The majority (63%) of interventions made by community pharmacists were public health related, whilst those made by the mental health specialist pharmacist most frequently concerned changing medicines (25%), stopping medicines (18%), and blood monitoring (13%). Conclusion: The study findings indicate a high level of polypharmacy among the ID residents and a high number of interventions / recommendations were needed to improve care, in line with national priorities. 1,2 The small scale of the study is acknowledged, and further research is warranted. However, the findings suggest that this service model may be an effective use of the respective skill sets of the pharmacists involved and suitable for wider adoption, with community pharmacists focusing on holistic care and specialist mental health making specialist medicines interventions. 	body_text= Introduction: In my thesis I am exploring the role of pharmaceutical packaging design in relation to the user. This topic is becoming increasingly relevant as the number of issued prescriptions in Slovenia is rising every year, treatment with prescription medicine is experienced by almost everyone. Medicine packaging must therefore provide essential information effectively and efficiently. Aim: The purpose of this thesis is to improve current heterogeneous conditions by developing a standardized design system for all prescription drugs by taking into account users' needs at each stage of the process. The final goal is a simpler and more effective use of products for everyone involved. Methods: Research was conducted in three stages. In the first stage, the existing condition in packaging design was analysed: information hierarchy/arrangement on 8 significant manufacturers' products considering 1 -the type of information and 2 -different user groups. Second stage consisted of conducting surveys with 2 focus groups representing two main user groups who use the packaging differently -medicine consumers and healthcare professionals.(1) Consumer focus group consisted of 81 participants, recruited randomly from various age groups (age 10 to 89). They were asked closed-ended questions. Healthcare professionals focus group consisted of 5 pharmacists with extensive experience. They were asked open-ended questions. The collected data from both research stages was statistically and qualitatively analysed in order to define the main problems with medicine packaging design and use. Identified problems were then addressed through the design process. The third stage included development of a standardised design system in accordance with information design theory and cognitive psychology findings.(2) These helped establish the system building blocks/rules: information hierarchy and organization, use of colour, shape and typography. Results: Analysis of existing conditions clearly exposed the heterogeneity and unsuitability of the majority of medicine packaging design. These caused similar problems to both user groups: trouble finding information (73 %), lack/ redundancy of information (47 %/17 %), illegible, unreadable typography (39 %), distracting visual elements (26 %), unclear distinction between medicines (17 %). These lead to various consequences: incorrect route of administration (39 %), consuming/prescribing expired (30 %) or incorrect product (8 %), time loss (8 %). Each of these problems was addressed through establishment of new, highly precise rules in packaging design: regulating hierarchy and typography, introducing visual categorization through symbols and illustrations (information category, pharmaceutical form, ATC group) and color-coding medicine strength. The rules form a standardised system which provides unity, consistency and quality regulation, improving the everyday experience of many people. Conclusion: The research was carried out as a part of a BA thesis. The execution of the project would require a change in the legislation on state level. It therefore serves as a speculative proposal, aiming to raise questions that are currently not being addressed properly within the industry. The possibility of implementation could be recognized through gradual transformation of individual rules/building blocks of the system into new state regulations or guidelines. Discussion with the industry and the profession has not yet been carried out due to the Covid-19 crisis. 
paper_id= fff4aa00c890f3503803154a13de3d93d86f9260	title= 	authors= Aleš  Tichopád;Ladislav  Pecen;	abstract= 23 	body_text= The global pandemic of the COVID-19 disease caused by the SARS-CoV-2 virus will certainly be tracked 46 among the most significant global events in the recent history with a death toll nearing 2.5 Million as of 47 the end of January 2021 [1] . Its humanistic, economic, and political impact has not only shaped the 48 ongoing efforts to contain its spread but also relationships between countries, their citizens, and 110 has been however published to date. We want to respond to speculation that the virus could be in Italy before the outbreak in China, which Italy, which we believe describes the best the epidemic progression within a naive population with 119 minimum imposed measures affecting its intrinsic dynamics. It was repeatedly shown that epidemic 120 curve in its early portion can be well fitted by simple phenomenological models as intrinsic growth is the 121 determining variable [13, 14] . Considering the interval from January through April only, we identified the 122 most linear early portion of new cases projected day-by-day on log-linear plot, using a semiautomatic 123 process. In SAS v. 9.4 for MS Windows, we stepwise rolled a 10-day window over the data and  In addition, we looked at 95% and 99% confidence interval for the d 0 estimates in both countries to 152 judge the probability that Italy hosted the virus before China. Performing the log-linear analysis on the Italian data, we identified the interval from 28 February through  As a sensitivity analysis we trimmed the early data from the Chinese curve which departs from the log-  We made an attempt to revise the speculation that Italy could host SARS-CoV-2 prior China has 194 confirmed the outbreak. However may this seem to be built on a weak foundation to anybody who  The work by Russo et al. suggests 14 January as the day-zero in Italy, which is eight days earlier than 211 our estimate and within our 99% confidence interval (11 January, 3 February). The difference could be 212 explained by the delay between the onset of the disease and the date of diagnostics. In our model we 213 did not adjust our estimated day-zero for this delay as, for the early period of the outbreak, we found no 214 reliable data to support it. The Italian outbreak was initially concentrated in the north of the country and 215 hence the data we used for fitting come predominantly from this regional outbreak and does not 216 compound several parallel outbreaks. It was first later in March and April when the epidemics spread 217 over the entire country and abroad [16] . This further justifies the use of a simple model over a  Much has been written and said about specificity of various SARS-CoV-2 antibody-based assays and it 237 is out of scope of this paper and competence of its authors to deep dive into this topic. One of several 238 possible explanations of the surprising finding by Apolone et al. could consist in the use of a cross-239 reaction of the used antibodies [19] . The strength of our study is its robustness consisting in the use of adequate model on a consistent data 241 with minimum assumptions needed. The main limitations of our study is the unknown proportion of 242 undetected infected individuals and the unknown time between the infection and the test date in the 243 early days of the pandemic. Assuming that both would have a substantial effect, it would shift the day-244 zero by days. We believe that this error would rather constantly affect both countries and hence more 245 or less preserve the order. In our work no medical or research intervention was imposed on humans or animals. We did not 248 specifically address individual human subjects. Authors  
paper_id= fff4c031e501e9ac1e56f466607f640d3725fc39	title= Membrane cofactor protein (MCP; CD46): deficiency states and pathogen connections	authors= Kathryn  Liszewski;John P Atkinson;	abstract= 	body_text= Membrane cofactor protein (MCP; CD46), a ubiquitously expressed complement regulatory protein, serves as a cofactor for serine protease factor I to cleave and inactivate C3b and C4b deposited on host cells. However, CD46 also plays roles in human reproduction, autophagy, modulating T cell activation and effector functions and is a member of the newly identified intracellular complement system (complosome). CD46 also is a receptor for 11 pathogens ('pathogen magnet'). While CD46 deficiencies contribute to inflammatory disorders, its overexpression in cancers and role as a receptor for some adenoviruses has led to its targeting by oncolytic agents and adenoviral-based therapeutic vectors, including coronavirus disease of 2019 vaccines. This review focuses on recent advances in identifying disease-causing CD46 variants and its pathogen connections. As one of the most ancient components of innate immunity, the complement system traces its origins to more than a billion years ago as it evolved to protect against pathogens and to engage in cellular processes [1, 2] . Interestingly, 'living fossils' such as coral, sea urchin, sponge and horseshoe crab have complement activating and regulatory components similar to present day humans. The contemporary primate complement system consists of at least 60 proteins and activation products and serves as an effector arm for the adaptive immune response. It features three activation cascades (alternative, classical and lectin) and a common terminal cytolytic pathway (reviewed in Refs. [3] [4] [5] ). Within five minutes, millions of complement activation fragments can covalently swarm onto bacterial or viral pathogens to a) elicit opsonization and lysis (i.e. membrane perturbation phenomena) and b) promote the inflammatory response (e.g. the anaphylatoxins). Such a powerful surveillance and membrane-modifying system requires strict control in order to avoid excessive host damage. Membrane cofactor protein (MCP; CD46) is a type one transmembrane complement regulatory protein expressed by almost every cell type (with the noteworthy exception of erythrocytes) (reviewed in Refs. [6] [7] [8] ). It binds the two key activation fragments, C3b and C4b, that covalently deposit on self-tissue. Subsequently, CD46 serves as a cofactor for serine protease factor I (FI)-mediated cleavage of these two fragments to prevent their further engagement by the activation pathways. CD46 is rather unique among complement proteins in that most cells coexpress four isoforms that arise by alternative splicing (see Figure 1 ). The MCP gene is located in the Regulators of Complement Activation (RCA) gene cluster located on the long arm of chromosome one (reviewed in Ref. [7] ). CD46 is particularly potent against the alternative pathway (AP) [9] , although BC isoforms provide enhanced protection (relative to C isoforms) against the classical pathway [10] . CD46 also has other key capabilities. First, CD46 impacts reproduction as it is expressed as a hypoglycosylated isoform (C isoform) on the inner acrosomal membrane of human spermatozoa where it participates in the interaction between spermatozoa and oocyte during fertilization (reviewed in Refs. [7, 8] ). Second, because of its overexpression on a variety of human tumors, CD46 is emerging as a key player in both malignant transformation and in cancer therapeutics (reviewed in Refs. [11] [12] [13] ). Third, CD46 signaling via motifs in its tails may critically impact cell behavior. For example, CD46-mediated intracellular signaling: a) enhances macrophage activity and survival, including cytokine and nitric oxide production and antigen presentation [14, 15] ; b) regulates autophagy of epithelial cells during pathogen invasion [16] or oxidative stress [17] ; and c) modulates T cell activation by providing costimulatory signals during TCR engagement [18, 19 ] and for optimal CD8 + T cell effector functions [20] . CD46's signaling capabilities have been best studied in CD4 + T cells (reviewed in Refs. [21, 22] ). Indeed, a more detailed inspection of CD46 during T cell activation led to the discovery of an intracellular complement system, or complosome, which assists in immune defense via key interactions including modulating nutrient uptake and cellular metabolism [19 ,23-25] . Of note, wild-type mice (and most other rodents) express MCP only on the inner acrosomal membrane of spermatozoa and in parts of the eye. Thus, murine models may rely on MCP transgenic animals. In rodents, the cellular complement regulator, Crry, replaces CD46 activity on most cells (reviewed in Refs. [7, 8, 26] ) Crry is not expressed by other mammals, including primates. This review focuses on recent advances in disease-causing CD46 variants and in its pathogen connections. To meet editorial guidelines, we often rely on reviews rather than original articles. Linkage analyses, genome-wide association studies and next generation sequencing have identified more than 80 disease-associated mutations in MCP ( [27, 28, 29, 30] , reviewed Ref. [6] and as of 3/1/2021 unpublished tabulation by MK Liszewski, JP Atkinson and MK Herlin) ( Figure 2 ). Most of these mutations have been linked to a rare thrombotic microangiopathy (TMA) called atypical hemolytic uremic syndrome (aHUS) ( [31 ] and see National Organization for Rare Disorders, https:// rarediseases.org/rare-diseases/ atypical-hemolytic-uremic-syndrome/). However, putative associations also have been suggested for other diseases including systemic lupus erythematosus [32] , rheumatoid arthritis [33] , asthma [17] , multiple sclerosis [34, 35] , glomerulonephritides (reviewed in Ref. [6] ), Alzheimer disease [36] , bullous pemphigoid [37] and pregnancy-related disorders (see below and Ref. [38 ] ). Typical features of HUS include the triad of microangiopathic hemolytic anemia, thrombocytopenia (i.e. low platelet count) and acute renal injury. Rare MCP mutations most commonly occur in the CCP domains and predispose to aHUS (also called complement-mediated HUS, C-HUS). Currently, they account for $10À15% of aHUS cases. Penetrance is $50%, suggesting the need for a secondary trigger ( [39] and reviewed in Refs. [31 ,40] ). Most mutations are missense although nonsense and splice-site variants have been recognized (reviewed in Ref. [6] ). In $75% of aHUS cases, the mutant protein is not expressed on the cell membrane. Thus, a majority of mutations result in haploinsufficiency. However, a smaller portion of individuals are homozygous [39, 41, 42] . Additionally, a specific MCP SNP block in the promoter region, termed the MCPggaac risk haplotype, may be associated with decreased transcriptional activity. This has been linked to aHUS, but only if associated with a causative variant in another complement regulator or AP component ( [43, 44] , and reviewed in Ref. [40] ). Intronic mutations also have been described, as in the case of CD46 splicing variant IVS2 + 2T > G (also known as c.286 + 2T > G, rs769742294). Two studies of this splicing variant pointed out that it can produce two different mRNA transcripts. In one case, the variant caused deletion of 155 base pairs at the 3 0 of exon 2 (deleting 48 amino acids in CCP1) [42] , while another study found it produced an mRNA causing a frame-shift mutation resulting in CD46 truncation in CCP2 (E97Kfs*33) [45] . The IVS2 + 2T > G variant was the most prevalent mutation (and a 'hot spot') in a cohort of Schematic of CD46's protein structure. The amino-terminus consists of four contiguous complement control protein (CCP) modules. Each CCP bears $60 amino acids consisting of four invariant cysteines (forming two disulfide bonds) and 10-18 highly conserved amino acids. CCPs 1, 2 and 4 bear N-glycans. Next is an alternatively spliced domain that is enriched in serines, threonines and prolines (STP region, site of O-glycosylation). While the MCP gene contains three STP exons (termed A, B and C), the commonly expressed isoforms contain B + C or C alone. This region is followed by a common, juxtamembraneous segment of 12 amino acids of undefined function. The carboxyl-terminus includes a transmembrane domain and one of two nonhomologous, alternatively spliced cytoplasmic tails; namely, CYT-1 with 16 amino acids or CYT-2 with 23 amino acids. Thus, isoforms are termed BC1 (343 amino acids), BC2 (350 amino acids), C1 (328 amino acids) or C2 (335 amino acids) to reflect splicing in the STP and cytoplasmic tail domains. The M r varies: C isoforms range from $51-58 kDa while BC isoforms range from $59-68 kDa. UND, undefined domain; TM, transmembrane domain; CK-2 casein kinase 2; PKC, protein kinase C. aHUS-afflicted Indian children [46] . Further, it was also the most prevalent mutation (13/485) in an international aHUS cohort analyzed by Piras et al. [41] . These studies highlight the potential variable outcome of intronic mutations and the importance of their rigorous analyses. The 'typical' or post-infection form of HUS represents $90% of cases. Patients, primarily children, develop diarrhea secondary to infection, most commonly by Escherichia coli serotype O157:H7, that produces a Shiga-like toxin (STEC). However, recent studies have determined that some cases originally diagnosed as STEC-HUS were actually aHUS triggered by STEC infection in the setting of a complement deficiency. Two patients with a clinical history of STEC-HUS that progressed to end stage renal disease (ESRD) [47] had heterozygous complement gene rare variants, one for factor I and the other for the previously described MCP splice-site mutation (IVS2 + 2T > G) associated with aHUS. Additionally, a retrospective study assessed the frequency of complement gene rare variants in a French national cohort of children with STEC-HUS [48] . Next generation sequencing for six complement genes associated with aHUS identified rare variants in one or two 128 Host pathogen Disease-associated CD46 mutations. A schematic depicting CD46 protein, genomic organization, and disease-associated amino acid mutations. CD46 has a 34-amino acid signal peptide (SP). The gene consists of 14 exons (numbered in black) and 13 introns (numbered in red) for a minimum length of 43 kb. A majority of the mutations for aHUS and other diseases are located primarily in the four CCPs. Note also that a risk haplotype, MCPggaac (boxed), has been suggested to lie within the promoter region (see text). Black, aHUS mutations; blue, aHUS and other diseases; green, non-aHUS disease (see text). # indicates the mutation has not yet been published. Note that there is inconsistency in the literature for CD46 mutant numbering. Some published mutations do not count the SP or all STP exons. In this review, we follow the recommendations of the Human Genome Variation Society and include the SP and all exons. For the sake of uniformity, older published mutant numbers may have been updated. For original mutation citations, see Ref. [6] and as indicated in text. genes in Shiga-toxin positive patients, including one in CD46: N170Kfs*7. The authors concluded that genetic screening should be pursued in patients with post-diarrheal HUS who progress to end-stage renal disease. Pregnancy-related and other disorders CD46 mutated proteins have also been implicated in the pathophysiology of other disorders. For example, studies have evaluated the MCP gene in the pregnancy-related disorder: pre-eclampsia (PE), especially the HELLP (hemolysis, elevated liver enzymes, and low platelet count) syndrome (reviewed by Burwick and Feinberg [38 ] and Salmon et al. [49] ). PE is a devastating multisystem disorder that occurs in 3-5% of pregnancies accounting for significant neonatal morbidity and mortality. HELLP is the most severe form of this disorder, accounting for 1% of all pregnancies. MCP mutations were identified in $8% of cases, although the range varied between 0-12% (summarized in Ref. [38 ] ). Similar to HUS, the precise etiology is unknown but likely relates (at least in part) to endothelial cell dysfunction secondary to excessive complement activation. Since pregnancy in women with SLE and/or anti-phospholipid syndrome (APLS) is associated with PE or miscarriage, Salmon et al. sequenced MCP and other complement regulatory genes in a large cohort of patients [49] . They found that 18% of patients had heterozygous mutations (including in MCP), thus identifying the first genetic defects associated with PE in SLE or APLS. Additionally, rare CD46 variants have also been associated with miscarriage [50] , systemic sclerosis, glomerulonephritis and thrombotic thrombocytopenic purpura (reviewed in Ref. [6] ). Such studies have involved a small number of patients and require further investigation. What is clear from the above studies is that diseases featuring a failure to adequately control the complement system, in particular its powerful AP, can lead to tissue destruction, organ failure and death [51, 52] . Determining which MCP mutations drive disease versus those that are simply rare variants is a current challenge. Additionally, since aHUS is successfully treated with eculizumab (a mAb to C5), decisions relative to treatment length may be assisted by genetic screening to identify if variants are known to be benign, pathogenic or, more commonly, catalogued as a 'variant of uncertain significance' (VUS) [53] . Alterations in complement proteins identified as a VUS may require functional analyses including quantification of CD46 on peripheral blood cells via flow cytometry as well as characterization and functional analyses of recombinantly produced protein [12] . Widespread expression, complement regulatory activities, immune-modulating signaling functions and internalization mechanisms make CD46 an appealing candidate for exploitation by a diverse group of 11 pathogens (reviewed in Ref. [6] ). CD46 has been called a 'pathogens' magnet' [54] . This group includes: five viruses -multiple species of adenoviruses (AdV) B and D, measles virus, herpesvirus 6A (reviewed in Ref. [6] ), cytomegalovirus [55 ] ; and six bacteria -Streptococcus pyogenes, Neisseria gonorrhea, Neisseria meningitides, E. coli, Klebsiella pneumoniae [56] and Fusobacterium nucleatum (see Figures 3 and 4) . Additionally, bovine CD46 (CCP1) serves as a receptor for the bovine viral diarrheal virus (reviewed in Refs. [6, 57] ), porcine CD46 serves as a receptor for the classical swine fever virus [58] and teleost CD46 may act as a receptor for Edwardsiella tarda and Pseudomonas fluorescens [59] . Pathogens target different CD46 domains for attachment and dissemination. Following engagement, CD46 can be shed or internalized via clathrin-coated pits or macropinocytosis. For example, the human herpesvirus 6 (HHV-6) envelope glycoprotein complex binds to CCPs 2 and 3. This is followed by internalization via clathrin-coated endocytosis and subsequent entry into the nucleus for viral nucleic acid replication (reviewed in Ref. [6] ). Charvet et al. proposed a link between HHV-6A binding to CD46 and activation of a human endogenous retrovirus (HERV) element [34] . Specifically, they found that the binding of HHV-6A to CD46 CCPs 3 and 4 induced expression (via CYT-1) of a multiple-sclerosis-associated Current Opinion in Immunology CD46 is a receptor for at least 11 pathogens. For seven, the attachment site has been identified (as indicated). A recent report established that 16/17 randomly selected adenovirus-D types use CD46 as cellular receptor [61 ] . Bovine, swine and teleost CD46 serve as pathogenic viral receptors. Bovine CCP1 is the binding site of bovine viral diarrhea virus. retrovirus envelope protein, MSRV-Env [34] . The authors hypothesized that CD46 not only might serve as a transactivator of retroviral envelope genes, but also that this could impact the pathogenesis of inflammatory disorders such as multiple sclerosis. A major virulence factor of S. pyogenes, M protein, binds CD46 via CCPs 3 and 4, a property that facilitates its adhesion and infectivity. This interaction leads to CD46 shedding, induction of apoptosis and cell death (reviewed in Ref. [6] ). Further, M protein engagement of CD46 on T cells promotes an immunosuppressive/regulatory phenotype in T cells (reviewed in Ref. [6] ) CD46 is also a receptor for N. meningitides (NM) and Neisseria gonorrhoeae (NG). The Type IV pilus of Neisseria mediates the initial attachment to epithelial cells by binding to CCP 3 as well as to the STP domain (reviewed in Refs. [6, 8] ). Infection by Neisseria stimulates the phosphorylation of CD46/CYT-2 by c-YES, a member of the Src family of protein tyrosine kinases. Further, Neisserial binding to CD46 triggers a cytoskeletal rearrangement and proteolytic cleavage of both CD46 tails. Additionally, during early infection NG binding to isoforms with CYT-1 induces autophagy in epithelial cells by CD46 interaction with the scaffold protein, GOPC, and the autophagosome formation complex, Beclin1/VPS34 [60 ] . However, later in infection, NG downregulates CD46/CYT-1 and disrupts lysosomes. This dual interference with the autophagy pathway promotes NG intracellular survival [60 ] . Further, in a CD46 transgenic mouse model, NM engagement of CD46 accelerated the initiation of sepsis by modulating inflammation and survival of macrophages [15] . What is clear from these studies is that NM utilizes multiple strategies to overcome CD46-host mediated cytoprotection. Measles virus (MV) and certain species of adenoviruses also target CD46. MV hemagglutinin as well as the AdV fiber knob protein in species of AdV types B and D attach to CD46 through CCPs 1 and 2. Intriguingly, the species of AdV-D bind CD46 through a noncanonical entry mechanism, the adenovirus hexon capsid protein [61 ] . Indeed, 16 out of 17 randomly selected AdV-D types were shown to engage CD46 as a receptor in this manner [61 ] . information regarding CD46 responses in monocytes, dendritic cells and macrophages (reviewed in Ref. [8] ). For example, binding by measles virus elicits internalization, alters intracellular processing and reduces antigen presentation (reviewed in Ref. [8] ). What is currently lacking in the field is a unified hypothesis that can identify whether CD46 will be either shed and/or internalized. Examples provided above illustrate the complexity of the issue. Likely, both processes are related. Further, the phenotypic expression differences of the four commonly coexpressed isoforms of CD46 probably complicate matters; that is, there is an expression polymorphism in the population in that 65% of individuals predominantly express the more heavily glycosylated BC isoforms, 29% express equivalent levels of BC + C, and 6% have C isoform predominance (reviewed in Ref. [62] ). Also, our unpublished work suggests that BC isoforms shed more efficiently than C isoforms (Liszewski and Atkinson, unpublished) . Further, the presence of two distinct CD46 cytoplasmic tails with independent signaling motifs may impact shedding or internalization. Thus, whether cell-specific, isoform-specific or condition-specific directed cell surface loss, much remains to be determined relative to the effect on CD46 engagement by its ligands and pathogens. Pathogenic microbes also produce CD46-like proteins to subvert host defense (reviewed in Refs. [6, 63] ). For example, poxviruses express a protein that is $35% homologous to CD46. Such complement regulatory inhibitors are called PICES (poxviral inhibitors of complement enzymes). Thus, proteins from variola (the causative agent of smallpox) and monkeypox are termed SPICE and MOPICE, respectively. The complement regulator from vaccinia, the vaccine strain, was named earlier as VCP (vaccinia complement protein, also called VICE). These inhibitors consist of three or four CCPs that structurally and functionally mimic CD46. They possess cofactor activity against C3b and C4b. However, they also have decay accelerating activity similar to fellow RCA regulator, decay accelerating factor (DAF; CD55). Further, these virulence proteins possess heparin-binding properties that allow them to attach to cell surface glycosaminoglycans in order to down-regulate complement activation. CD46 is emerging as a therapeutic oncologic target (reviewed in Ref. [11] ). While CD46 expression level on peripheral blood mononuclear cells and granulocytes is $10 000/cell, tumor-derived cells and cell lines range from 100 000-250 000/cell (reviewed in Refs. [13, 64] ). As a result of its overexpression in multiple cancers, a macropinocytosing CD46-antibody drug conjugate has been developed that is currently undergoing clinical trials for several oncologic applications [65] . The most remarkable example of overexpression may be in relapsed multiple myeloma in which CD46 expression is increased up to 14fold in patients who have the region on chromosome 1q carrying CD46 genomically amplified (reviewed in Ref. [11] ). Additionally, species of AdV and MV (targeting CD46) are being exploited as engineered, modified vectors for wide-ranging therapeutic interventions. This includes a CD46-targeted AdV26-based vaccine against the spike protein of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) being developed by Janssen and as part of a vaccine 'Sputnik V' by the Gamaleya Research Institute (Ad5/Ad26) (reviewed in Ref. [66] ). Additional therapeutic trials targeting CD46 include those for oncology [11, 12] , HIV, Ebola virus and Zika virus ( [11] and reviewed in Refs. [61 ,67,68] ). Relative to MV, clinical trials are underway that employ an oncolytic MV encoding the thyroidal sodium iodide symporter (NIS) (facilitates viral gene expression and a tool for radiovirotherapy) [69, 70] . Modified MV is also being developed as a vaccine against SARS-CoV-2 (reviewed in Ref. [71] ). For example, the Pasteur Institute engineered MV to express spike protein (TMV-083) as a vaccine candidate. The same platform is being utilized against Chikungunya, Lassa, Zika and Middle East respiratory syndrome coronavirus. Further, CD46-targeted oncolytic adenoviral-based vectors are being developed as an alternative to AdV type C (e.g. AdV5) that binds to the coxsackie-adenovirus receptor, since the latter has low level expression [72] . Of considerable interest, Persson et al. suggest that vaccine vectors that are CD46-engaging (e.g. AdV-D) may more efficiently transduce antigen-presenting cells than AdV targeting receptors that are not expressed on such cells [61 ] . Since 80% of adults have neutralizing antibodies against CAR-targeting AdV5, most frequently used in oncolytic viral therapy, Ono et al. developed a novel oncolytic adenovirus that recognizes CD46 (AdV35) and efficiently lysed tumor cells [73] . Additionally, a chimeric AdV therapy engineering two group B adenoviruses (Ad11/Ad3), called Enadenotucirev, is undergoing multiple trials for several types of cancer [69, 74] . Further, a modified fiber knob protein of group B AdV35 is being used as a combination therapy with rituximab to treat patients with rituximab-refractory B-cell non-Hodgkin's lymphoma [75, 76] . Pre-clinical studies in mice and non-human primates demonstrated that pre-treatment with Ad35K++, a high affinity, solubilized recombinant CD46-binding fiber knob, resulted in transient removal of CD46 from tumor cells. Because CD46 can be overexpressed by an order of magnitude on such cells (blocking complement-dependent cytotoxicity), the pretreatment resulted in enhanced tumor killing by rituximab. These studies create the basis for the use of Ad35K ++ as a combination therapy with rituximab in clinical trials to treat B-cell malignancies. Since the discovery of CD46 as a membrane complement regulator more than 30 years ago, new knowledge has emerged not only about its structure and function as a complement regulator, but also its key interactions as a driver of cellular metabolism and component of the intracellular complement system [7] . Highlighted in this review were the disease-causing loss-of-function rare variants and the multiple connections of CD46 with a diverse group of pathogens. Importantly, new therapeutic applications targeting CD46 range from treatment of cancer to SARS-CoV-2 (COVID-19) vaccines. Undoubtedly, other surprises are yet in store as more knowledge is gained about this multi-functional protein. MKL wrote and edited the manuscript; JPA conceived, outlined and edited the manuscript. MKL has no competing interest; JPA reports serving as a consultant for Achillon Pharmaceuticals, Celldex Therapeutics, Clinical Pharmacy Services, Compliment Corporation, Gemini Therapeutics and Kypha; stock or equity options for AdMiRx, Inc, Compliment Corporation, Gemini Therapeutics, Kypha. 
paper_id= fff555f3e8c02422ce1e1f07ed94b87b0514b988	title= Clinical performance of the H. PYLORI QUIK CHEK™ and H. PYLORI CHEK™ assays, novel stool antigen tests for diagnosis of Helicobacter pylori	authors= Magnus  Halland;Rashidul  Haque;Jost  Langhorst;James H Boone;William A Petri;	abstract= Infection with Helicobacter pylori is a global health issue, and rapid and accurate testing is a key to diagnosis. We aimed to assess the performance of two novel enzyme immunoassays (EIA), the H. PYLORI QUIK CHEK™ and the H. PYLORI CHEK™ assays, for the detection of H. pylori antigen in stool. Patients from five geographically diverse sites across the USA, Germany, and in Bangladesh were tested for infection with Helicobacter pylori with the two novel stool antigen tests and two commercially available stool antigen assays. All patients provided a stool sample and underwent esophagogastroduodenoscopy for biopsy. Results were compared to a clinical diagnosis using a composite reference method consisting of histological analysis and rapid urease testing of the biopsy. A total of 271 patients, 68.2% female and mean age of 46 years, were included. The overall prevalence of H. pylori infection was 24.1%. The sensitivity of the H. PYLORI QUIK CHEK™ and H. PYLORI CHEK™ was 92% and 91%, respectively. The specificity of H. PYLORI QUIK CHEK™ and H. PYLORI CHEK™ was 91% and 100%, respectively. No significant cross-reactivity against other gut pathogens was observed. The H. PYLORI QUIK CHEK™ and H. PYLORI CHEK™ assays demonstrate excellent clinical performance compared the composite reference method. 	body_text= It is estimated that half of the global population is infected with Helicobacter pylori (H. pylori), a gram-negative, spiralshaped microaerophilic bacterium [1] . The bacterium has several unique chemical and physical properties which allow for successful infection in the very hostile environment of the gastric lumen [2] . Specifically, 4-6 polar flagella which allow for motility in the mucus layer of the stomach and production of urease which hydrolyzes gastric acid to ammonia are important pathogenic factors [3] . Chronic infection with H. pylori is a potent risk factor for gastritis, peptic ulcer disease, and gastric malignancies such as gastric adenocarcinoma and lymphoma [3] . For example, it is estimated that 36-47% of gastric adenocarcinomas worldwide are solely attributable to chronic infection with H. pylori [4] . The infection is typically acquired in childhood [5] , and while the exact method of transmission is unclear, poor sanitary conditions and high density family households are known risk factors [6] . Highly effective therapy for H. pylori infection is available for most patients in whom a diagnosis has been made [7] . Accurate diagnosis and timely treatment of the infection improves outcomes for patients infected with H. pylori [8] . For example, eradication of H. pylori has been shown to reduce the risk of subsequent peptic ulcer disease and gastric adenocarcinoma, both of which can lead to significant morbidity and mortality [8, 9] . H. pylori infection can be diagnosed on biopsies obtained from esophagogastroduodenoscopy (EGD) using either special stains on histology [10] , rapid urease testing [11] , culture [12] , or polymerase chain reaction testing [12] , but performing an EGD in order to diagnose H. pylori infection is expensive, invasive, and not universally available. Certain noninvasive tests, such as the urea-breath test and stool antigen assays, are also limited by high cost and/or a long turnaround time to diagnose the infection [12] . In addition, most patients with H. pylori infection require more than one test during their diagnostic and therapeutic journey as confirmation of eradication after treatment for H. pylori is recommended [7] . The aim of this study was to determine the performance characteristics of two new stool-based enzyme-linked assays (EIA) tests designed to rapidly detect presence of H. pylori antigen. We prospectively recruited patients at five geographically diverse clinical sites from August 2017 to May 2018 in order to determine the diagnostic performance of a novel stool antigen tests in patients in the initial diagnosis of H. pylori infection. Three of the study sites were in North America (Minnesota, VA, USA), one in Europe (Essen, Germany), and one in Southeast Asia (Dhaka, Bangladesh). For the initial diagnosis claim, we included patients with symptoms of dyspepsia, gast r i t i s , o r p e p t i c u l c e r . A l l p a t i e n t s u n d e r w e n t esophagogastroduodenoscopy where at least six gastric biopsies were obtained for histological analysis. One additional gastric biopsy was obtained for rapid urease testing. All patients provided a stool sample. Exclusion criteria included asymptomatic patients, patients in whom the presence/ absence of H. pylori was already known. Patients had to have refrained from antibiotics and bismuth compounds (e.g., Pepto-Bismol TM ) for 2 weeks prior to submitting a fecal sample. Proton-pump inhibitor (PPI) use was recorded if within 2 weeks of providing the fecal sample. All study sites collected stool specimens from patients within 48 h before or after the reference endoscopy procedure. Stool specimens were subsequently stored at 2-8°C for up to 14 days then frozen if not tested. All stool specimens were analyzed at the central reference laboratory. Stool samples underwent testing with H. PYLORI QUIK CHEK™ test, a rapid membrane EIA (rapid EIA), and the H. PYLORI CHEK™ test, a microwell EIA (EIA) according to the manufacturer's instructions (TechLab Inc., Blacksburg, VA). An optical density (OD) of ≥ 0.120 at single wavelength (450 nm) or ≥ 0.080 at dual wavelength (450/620 nm) were considered positive for the H. PYLORI CHEK™ test and visual results were used to determine a positive result for the H. PYLORI QUIK CHEK™ test. In addition, all stool samples were also tested using two commercially available enzyme immunoassays following the manufacturer's instructions (Premier Platinum HpSA Plus; Meridian Bioscience (microwell) and ImmunoCard STAT! HpSA, Cincinnati, OH (rapid test)). Gastric biopsies were stained with hematoxylin and eosin or modified Giemsa stain for routine histological analysis to determine the presence or absence of H. pylori. A single gastric biopsy was placed in the gel of the CLOtest* Rapid Urease Test (Kimberly-Clark*) and then stored at ambient temperature. A positive test was defined as a change to the reference color within 24 h based on the manufacturer's instructions. H. pylori culture was not routinely performed at all study sites. However, results were included in a sub-analysis for completeness. Diagnostic performance of the stool antigen tests was determined by comparing results to a composite reference method (CRM, gold standard for H. pylori infections) that includes 3 possible tests performed on the biopsy: culture, histology, and rapid urease test results, where a positive diagnosis is made if 2 out of 3 tests are positive [13] . For the purposes of this study, the results from rapid urease testing and histology were utilized for case definition as these two tests were performed in all patients across all study sites (Table 1 ). In addition, an  Patients with a single positive urease test at baseline may be more appropriately considered infected overall analysis which included indeterminate cases based on the CRM definition as well as data from sites which performed H. pylori culture based was also performed. A subgroup analysis which excluded patients in whom PPI exposure occurred within 2 weeks was performed (Table 1) . The specificity of the H. PYLORI QUIK CHEK™ and H. PYLORI CHEK™ assays was challenged by examining the reactivity of a wide range of common intestinal organisms and viruses (Tables 2 and 3) . For the analysis, the bacteria were grown to early stationary phase (> 10 8 CFU/mL); McFarland Standard #4 and stock cultures of viruses were purchased. The cultures were diluted 1:10 in (i) fecal matrix that was negative for H. pylori (negative fecal pool) or (ii) fecal sample matrix that was spiked with H. pylori antigen (ATCC strain 43526) at 2-3 times the amount to produce a positive results (C 95 ; positive fecal pool). The preparations were assayed in both assays and qualitative results were reported. All patients provided informed consent and the study protocol received internal review board approval at each individual study site. The demographic results were compiled and descriptive analysis, including counts and percentages were performed using SAS Software (JMP Pro 14.1.0). The clinical sensitivity and specificity for the H. pylori antigen assays were determined by a comparison to the composite reference method by crossclassifying each case as clinically present or absent with 95% confidence interval [14, 15] . The analysis included continuity correction. Patient demographics and clinical data are presented in Table 4 . Overall, 271 patients participated in the study across five distinct geographic locations with 223 (82%) of the collected stool samples being tested following a single freezethaw on the assays.  In the N = 261 study population, the Premier Platinum HpSA Plus assay had a sensitivity of 87% and specificity of 87%. The Immunocard STAT had a sensitivity of 92% and a specificity of 97% (Table 5) . Among patients in whom no PPI exposure occurred (N = 182), the H. PYLORI CHEK™ assay had a sensitivity of 95% and specificity of 90% for the detection of H. pylori infection compared to the CRM. The H. PYLORI QUIK CHEK™ assay had a sensitivity of 93% and specificity of 99% for the detection of H. pylori infection compared to the CRM (Table 6 ). Detailed comparison of histopathological, culture, and rapid urease results with stool assays Table 7 shows detailed results of comparison of the endoscopic-based tests with the stool-based assays in the entire study cohort (N = 271). A total of 38 common intestinal bacteria ( In addition to aiding initial diagnosis, these rapid assays may offer a useful tool to assess response to therapy and predict noninvasively those patients requiring retreatment. H. pylori antigen in stool may be present for weeks following treatment and current guidelines recommend that stool antigen testing not be done until at least 4 weeks after completion of antibiotic therapy to confirm eradication [7] . The strengths of this study include the robust performance of the stool antigen test across multiple diverse geographic sites. Furthermore, the diagnostic performance of the new stool assays was compared with a rigorous CRM consisting of gastric biopsies in combination with histology and rapid urease testing. We also compared the new stool assays to other commercially available immunoassays to assess performance. Specificity of the new assays was challenged using gut bacteria and viruses. In addition, a subgroup analysis showed that the overall performance of this assay did not appear to be significantly impacted by recent PPI exposure, although further work is needed to determine whether the current recommendation to hold PPI use prior to testing for H. pylori is needed. Limitations of this study include the high, but not unexpected, variability in H. pylori positivity between the different study sites and the female predominant study population. We did not collect data on additional pathological findings on biopsies beyond H. pylori positivity. In addition, H. pylori culture was not performed at all study sites. Finally, our study criteria of avoiding bismouth compounds and antibiotics for 2 weeks prior to testing could theoretically have led to some falsenegative tests; however, the impact of this is unlikely to change the overall conclusions about the diagnostic performance of the assays. In conclusion, the H. PYLORI QUIK CHEK™ and H. PYLORI CHEK™ assays demonstrate excellent clinical performance compared the composite reference method. Potential advantages of these new assays include the high accuracy and rapid availability of results for initial diagnosis and assessing response to therapy. Further studies need to examine the impact of same day results on initiation of therapy and eradication rates in the treatment of Helicobacter pylori. Author contributions 1. Halland-data collection, analysis, drafting, and review of manuscript 2. Haque-data collection, analysis, and critical review of manuscript 3. Langhorst-data collection, analysis, and critical review of manuscript 4. Boone-data collection, analysis, drafting, and review of manuscript 5. Petri-data collection, analysis, and critical review of manuscript Funding TechLab provided a research grant for patient recruitment and sample collection. Data availability All authors had access to the data and study materials. Conflict of interest J Boone is a Senior Scientist at TechLab. No other author reports any conflict of interest. Ethics approval The study was approved by IRB at each institution. Consent to participate All participants gave informed consent for participation in the study. Code availability Not applicable. 
paper_id= fff58f3a8b0ae796a901faba3ca65cde902c603e	title= The mediating role of narcissism in the effects of regulatory mode on positivity	authors= Daniela Di Santo;Calogero  Lo;Destro  ;&amp;  Conrad Baldner;Alessandra  Talamo;Cristina  Cabras;Antonio  Pierro;	abstract= Positivity (i.e., the individual tendency to positively approach life experiences) has proven to be an effective construct applied in positive psychology. However, individuals' self-regulation may have contrasting effects on positivity. We specifically examined whether positivity could be partially explained through two aspects of motivation concerned with self-regulation: locomotion (i.e., a motivational orientation concerned with movement) and assessment (i.e., a motivational orientation concerned with comparison and evaluation). Furthermore, based on previous literature that found a link between these aspects and narcissism, we examined whether "adaptive" and "maladaptive" dimensions of narcissism could mediate the effects of locomotion and assessment on increased or decreased positivity. Narcissism was defined by previous research as adaptive or maladaptive insofar as it leads or does not lead to increased psychological well-being. We estimated a mediation model with multiple independent variables and multiple mediators in a cross-sectional study with self-reported data from 190 university students. We found that both locomotion and assessment were associated with adaptive narcissism, which in turn was positively associated with positivity. However, assessment was also associated with maladaptive narcissism, which in turn was negatively associated with positivity. Relationships between aspects of self-regulation, narcissism, and positivity can have significant implications which will be discussed. 	body_text= The tendency of individuals to view and address life and experience with a positive outlook (i.e., "positivity"; Caprara et al., 2012 ) is a crucial variable in psychological adjustment with positive benefits for one's life. Positivity is defined as "as an individual propensity to positively evaluate, or to be positively oriented toward various life domains including oneself, and one's future and past experiences" (Caprara et al., 2009, p. 277) . This positive mindset therefore implies seeing life events through a positive lens and having the skills and personal resources to cope with adversity, loss, and failure throughout life (Caprara et al., 2012) . For example, and of current interest, positivity has recently been shown to be a psychological resource that can potentially protect people from the negative mental effects of Covid-19 (Yıldırım & Güler, 2021) . Despite the growing interest in this construct, we still do not know much about the factors-such as characteristics, predispositions, and beliefs-that might promote it. Yet, it is worth learning more, given the important biological and social functions performed by positivity (Caprara et al., 2016; Tisak, 2019) . Personality traits and beliefs of self-efficacy have been examined so far by the research (see Caprara et al., 2019, for a review) . Given the conflicting research surrounding narcissism (e.g., Morf & Rhodewalt, 2001) , we investigated whether narcissism could be associated with both higher and lower positivity, dependent on the individual's active self-regulation goals. It is suggested that self-regulation, or the process by which people evaluate, direct and control their means and actions towards goals, is linked to how people cope with difficulties (Aspinwall, 2001) , but little is known about the effects of selfregulatory orientations on positive outlook. We refer to two self-regulatory aspects, both required for the successful goal pursuit according to the Regulatory Mode Theory (RMT; Higgins et al., 2003; Kruglanski et al., 2000) , namely locomotion and assessment regulatory modes: the first mode is concerned for managing movement from state to state, such that the change of state takes place; the second mode is concerned for comparison and critical evaluation of alternative goals and means options, such that the right (or best) goal and the right (or best) means to pursue it are chosen . The first research question is whether and how regulatory modes have any effect on positivity. Relying on previous literature and findings on the regulatory modes (e.g., Higgins et al., 2003; Kruglanski et al., 2000) that a high mode of "acting" and "moving" leads to positive feelings as opposed to "chronically comparing", positivity could be stimulated by locomotion regulatory mode and hampered by assessment regulatory mode. However, consistent with the idea that self-regulation is central to narcissism (Morf & Rhodewalt, 2001) , research has shown that regulatory modes may also underlie different narcissistic feelings (Boldero et al., 2015) . In turn, narcissism affects various individuals' well-being outcomes (Clarke et al., 2015) . It is thus possible that narcissism could mediate the effect of the regulatory modes on positivity. In other words, depending on the type of narcissism experienced (see also Boldero et al., 2015) by locomotors (i.e., individuals who have a high locomotion orientation) and assessors (i.e., individuals who have a high assessment orientation), the effects of regulatory modes on positivity may vary. Accordingly, given its multidimensional nature, narcissism is presumed to have opposite effects on wellbeing outcomes, depending on its "adaptive" or "maladaptive" nature (e.g., Clarke et al., 2015) . An aim of the present work was to examine different dimensions of narcissism in mediating the effects of regulatory modes on positivity. For this purpose, we theorize a distinct mediational model involving both regulatory modes (locomotion and assessment), narcissism, and positivity. We tested this integrated model in a non-clinical university student sample. In the next sections, we will present the relevant literature on the variables of interest and our hypotheses. After this, we will present the method, analyses, and results of the study. Next, a general discussion will be implemented which also involves limitations and implications for future research. The work will be ended with general conclusions. Assessment and locomotion modes reflect two different aspects of self-regulation, the first being comparative in nature, the second being concerned with the movement from state to state (Kruglanski et al., 2000) . Since a focus on self-evaluation can highlight discrepancies between one's actual self as a state and the desired self as an alternate state (Higgins, 1987) , assessment mode, which is involved in perennial self-evaluation, ruminates on this discrepancy Kruglanski et al., 2000) . Accordingly, strong assessors tend to generally experience negative affect, lower optimism and self-esteem (Kruglanski et al., 2000) , and higher work stress (Lo Destro et al., 2017) . On the other hand, locomotion mode reflects the self-regulatory aspect concerned with initiating and maintaining action that will reduce the discrepancy between one's current state and the desired end state ; see also Kruglanski et al., 2016) ; consistently, high locomotors generally experience positive affect, optimism, and self-esteem (Kruglanski et al., 2000) , low work stress (Lo Destro et al., 2017; Lo Destro et al., 2018) , low hopelessness, and high subjective well-being Di Santo et al., 2020) . Consistent with this literature, locomotion mode, rather than assessment mode, should have a general positive attitude towards self and life, operationalized as a higher-order factor that captures both satisfaction with living conditions, positive expectations about the future (e.g., optimism), and self-esteem (Caprara et al., 2012) . Such general positivity factor is seen by researcher as a determinant of one's health and well-being (Caprara et al., 2016; Caprara et al., 2019) . In examining their relationship to other factors, research has found that certain personality traits (extroversion and neuroticism) predict positivity, which, in turn, mediates their relationship with subjective happiness (Lauriola & Iani, 2015) ; positive thinking has also been shown to be promoted by affective and social selfefficacy beliefs, or people's beliefs in their perceived ability to successfully manage affectivity and interpersonal relationships (see Caprara et al., 2019 , for a review). We assume that positivity could be stimulated in the opposite way by the two regulatory modes, i.e., positively having a high locomotion and negatively having a high assessment. However, given the associations previously found between regulatory modes and narcissism (Boldero et al., 2015; Hanke et al., 2019) , the content of narcissism experienced by locomotors and assessors could also partially explain the level of positivity they feel, insofar as narcissism and well-being are related in a multifaceted way. This will be briefly discussed in the next section. Literature in clinical and personality psychology has widely supported a multidimensional conceptualization of narcissism (e.g., Miller et al., 2011; Pincus et al., 2009; Wink, 1991) . Accordingly, Clarke et al. (2015) identified eight dimensions that reflected adaptive or maladaptive content of narcissism, corresponding to: (1) Leadership/Authority, (2) Superiority, (3) Grandiose Exhibitionism, (4) Contingent Self-Esteem, (5) Devaluing the Self, (6) Grandiose Fantasy, (7) Manipulative, and (8) Entitlement. Of such dimensions, Clarke et al. (2015) suggests that Leadership/Authority and Superiority would reflect adaptive narcissism, as they were positively correlated with selfesteem, and negatively correlated with neuroticism. Furthermore, although in some cases exhibitionism was suggested to be associated with poor adjustment (Raskin & Terry, 1988) , in the study by Clarke et al. (2015) Grandiose Exhibitionism was positively associated with self-esteem and not significantly associated with psychological distress. Contingent Self-Esteem and Devaluing the Self would reflect maladaptive narcissism, as they positively predicted depression and stress, and negatively predicted selfesteem (Clarke et al., 2015) . We mainly relied on these opposite effects on psychological well-being outcomes (Clarke et al., 2015 ; see also Cai & Luo, 2018) to help define the research hypotheses. Although we explore each of the dimensions of narcissism identified by Clarke et al. (2015) , we therefore focus our hypotheses on those more of adaptive and maladaptive narcissism in predicting that they are positively and negatively associated with positivity, respectively. The distinction of maladaptive and adaptive narcissism is in accordance with past literature (Raskin & Terry, 1988) suggesting that only certain aspects of narcissism are associated with maladjustment. Accordingly, facets of adaptive narcissism (i.e., narcissistic superiority) were found to predict life satisfaction (Miller et al., 2019) . A link was observed, instead, between more maladaptive narcissistic traits and depressive traits (Tritt et al., 2010) . Individuals may also present a specific vulnerability to pathological and maladaptive forms, as research (Engel-Yeger et al., 2016) that has found stable extreme sensory processing patterns as unique sensory profiles in individuals with depression and mood disorders. Moreover, consistent with previous findings that see locomotion and assessment as self-regulatory underpinnings of grandiose and vulnerable narcissism (Boldero et al., 2015) , we hypothesize that both modes can develop narcissistic factors. Specifically, our hypothesis on locomotion mode is that the concern with "making something happen" could develop factors of adaptive narcissism that facilitate the movement goals of people characterized by a locomotion mode; this would, in turn, increase positivity. This is consistent with a view supported by empirical findings (Kruglanski et al., 2016) , that locomotion mode generally promotes positive health outcomes. We thus expect that the positive relation between locomotion and positivity will be mediated by adaptive dimensions of narcissism (H1). The hypothesis on assessment mode is more complex. People characterized by an assessment mode need to consider different options in order to make the best choice . This mode has clear benefits, as people can perceive more assurance in their choices; however, they can also subject themselves and their choices to increased criticism. These two possibilities reflect two sides of narcissism: increased selfassurance reflects aspects of adaptive narcissism whereas increased self-criticism reflects maladaptive narcissism. As argued by Boldero et al. (2015) , the successes and failures encountered in trying to be "right" can change the nature of assessors' feelings. Accordingly, assessment could have a double relationship with narcissism, i.e., assessors could be both adaptive and maladaptive narcissists, and this ambivalence can be reflected in the positivity they experience as well. We could therefore expect that the concern with "making the right or best choice", which reflects greater assurance in one's choices, may develop adaptive narcissism factors; this would, in turn, increase positivity. Our hypothesis is that the positive relation between assessment and positivity will be mediated by adaptive dimensions of narcissism (H2). The assumption that both locomotion and assessment modes can be associated with adaptive narcissism is consistent with previous findings on the positive effects of the regulatory mode conjunction (i.e., the presence of both modes; Pierro et al., 2018) . However, we might at the same time expect that concern with doing the right thing reflecting an ongoing evaluation of the discrepancies between their current and desired state (Kruglanski et al., 2000) can develop maladaptive narcissistic factors and, in turn, decrease positivity. Our third hypothesis is that the negative relationship between assessment and positivity will be mediated by maladaptive dimensions of narcissism (H3). In sum, we expect that locomotion mode should have a positive relationship with adaptive narcissism and, therefore, also with positivity. The assessment effect should be ambivalent, in that it can predict both increased and decreased positivity, dependent on the dimension of narcissism-adaptive or maladaptive-that mediates the relationship. These assumptions are tested in the study described below. One hundred ninety students in the master's degree program in Psychology (150 females, 40 males; M age = 24.93, SD age = 4.396) at a large public Italian university participated in this research on a voluntary basis. A total of 190 students were invited in the study and replied our survey (response rate of 100%). The study complied with the Declaration of Helsinki and was approved by the departmental Ethical Committee under protocol 1249, titled "Analysis of the relationship between Regulatory Modes and Positivity." Participants completed a paper-and-pencil questionnaire comprising the set of measures described below and an explanatory letter. All participants completed the Locomotion and Assessment scales followed by the Narcissism scale. They then completed a measure designed to assess their positivity. Informed consent was appropriately obtained from the participants. The measures that were used in this study are described in detail below. Regulatory Mode Participants completed the Italian version of the Regulatory Mode Questionnaire (Kruglanski et al., 2000) , composed by two separate 12-item self-report scales designed to measure individual differences in locomotion (e.g., "I enjoy actively doing things, more than just watching and observing"; α = .76) and assessment (e.g., "I often compare myself with other people"; α = .77) on a six-point Likert scale ranging from '1' (Strongly disagree) to '6' (Strongly agree). The psychometric properties, internal consistency, temporal stability, and convergent and discriminant validity, of both locomotion and assessment scales, were previously demonstrated in an extensive research program which included Italian samples (Kruglanski et al., 2000) . Narcissism To measure participants' narcissism, we used the scale developed by Clarke et al. (2015) based on selected (N = 80) items from the Narcissistic Pathological Inventory (Raskin & Terry, 1988 ) and the Pathological Narcissism Inventory (Pincus et al., 2009 ). Participants of our study responded to the statements on a six-point Likert scale ranging from '1' (Strongly disagree) to '6' (Strongly agree) aimed at measuring the following 8 dimensions of narcissism: Leadership/ Authority (e.g., "I see myself as a good leader"; α = .89), Grandiose Exhibitionism (e.g., "I like to be the center of attention"; α = .88), Manipulative (e.g., "I can read people like a book"; α = .86), Superiority (e.g., "If I ruled the world it would be a much better place"; α = .82), Contingent Self-Esteem (e.g., "It irritates me when people don't notice how good a person I am"; α = .91), Grandiose Fantasy (e.g., "I often fantasize about being admired and respected"; α = .88), Devaluing the Self (e.g., "I often hide my needs for fear that others will see me as needy and dependent"; α = .87), and Entitlement (e.g., "I can get pretty angry when others disagree with me"; α = .85). The psychometric properties were previously examined by Clarke et al. (2015) . The multidimensionality of the narcissism scale was corroborated by explorative and confirmatory factor analyses (Clarke et al., 2015) . Positivity Participants' positivity was measured through the 8item (e.g., "I generally feel confident in myself"; α = .85) self-report Positivity Scale (Caprara et al., 2012) , with responses on a six-point Likert scale ranging from '1' (Strongly disagree) to '6' (Strongly agree). The psychometric properties of the Positivity Scale were previously examined (see Caprara et al., 2012, for details) . The unidimensionality of the Positivity Scale was corroborated by confirmatory factor analysis (Caprara et al., 2012) . Preliminary Analysis: Validity of Locomotion and Assessment Regulatory Modes, Narcissism Dimensions, and Positivity Measures To assess the convergent and discriminant validity of regulatory modes, narcissism, and positivity measures, a confirmatory factor analysis (CFA) by means of LISREL 8.5 (Jöreskog & Sörbom, 1996) was conducted with eleven (correlated) latent factors (the locomotion and assessment regulatory modes, the eight narcissistic dimensions, and positivity). The measurement model tested was specified as a partial disaggregation model (Bagozzi & Heatherton, 1994 ) that use aggregates of items to form two or more indicators per construct. Partial disaggregation models reduce the number of observed variables and the number of parameters being estimated in the models, which accommodates modeling with smaller sample sizes and reduces the likelihood of computational problems. Moreover, the aggregation procedure reduces measurement error in the observed indicators (Bagozzi, 1993; Bentler, 1989 ). In the present study, for each of the latent construct we computed two manifest indicators using the split-half procedure: each indicator is thus formed from half of the items included in the scales or sub-scales aimed at measuring the constructs considered. Goodness of fit of the model was evaluated via several different indexes: Chi-square, Comparative Fit Index (CFI), and standardized Root Mean Square Residual (RMSR), as recommended by various sources (cf. Bollen, 1989; Tanaka, 1993) . The covariance matrix was used as input. CFA results show that the model fit was satisfactory, χ 2 (154, N = 190) = 313.62, p = .00; CFI = .97; RMSR = .05. The factor loading values were all significant and above .52, thus demonstrating convergent validity for the constructs. The correlations between latent factors were all statistically less than 1.00 (ranging between .02 and .67), and therefore achieved statistical discriminant validity (Bagozzi, 1994) . It is notable that the correlations between latent factors are correlations corrected for attenuation and are expected to be higher than raw coefficients (see Table 1 ). To further prove discriminant validity of the constructs and control the common method/ source biases (due to the cross-sectional design, and, specially, to the all self-report measures used in the present study), we compared the estimated eleven-factor model with two alternative models: one with three latent factors (a unique regulatory mode factor, a unique narcissism factor, and one positivity factor) and one with one latent factor (see Podsakoff et al., 2003 , for review on Common Method Biases in Behavioral Research and Recommended Remedies). Results show that the eleven-factor model fits the data better than either the three-factor model (χ 2 (206, N = 190) = 1932.79, p = .00; CFI = .69; RMSR = .18), or the one-factor model (χ 2 (209, N = 190) = 2175.09, p = .00; CFI = .65; RMSR = .18), thus supporting the distinction between the different constructs used in the present study. In order to test the mediation of narcissism in the relationship of regulatory mode with positivity, we estimated a model that includes multiple mediators (the eight dimensions of narcissism) and multiple predictor variables (locomotion and assessment). The analysis was performed using the PROCESS macro for SPSS (Model 4) that can be used to estimate the coefficients in a multiple mediation model with multiple independent variables (Hayes, 2018) . We employed Preacher and Hayes's (2008) procedure to extrapolate estimates of direct and indirect effects. Ninety-five percent CIs were employed and 1000 bootstrapping resamples were run. Descriptive statistics and correlations between variables are presented in Table 1 . The table shows that positivity was significantly and positively correlated with Leadership/Authority, Grandiose Exhibitionism, Manipulative, Superiority, and significantly and negatively correlated with Contingent Self-Esteem and Devaluing the Self, whereas it was unrelated with Grandiose Fantasy and Entitlement dimensions. Locomotion was positively correlated with Leadership/Authority, Grandiose Exhibitionism, Manipulative, Superiority, Grandiose Fantasy, and unrelated with Contingent Self-Esteem, Devaluing the Self, and Entitlement dimensions. Locomotion was also positively correlated with positivity. Assessment was positively correlated with each dimension of narcissism, and negatively correlated with positivity. As anticipated, we tested the hypothesized mediating role of narcissism in the relationship between regulatory mode and positivity. Results are summarized in Table 2 and Fig. 1 . As expected and consistent with previous results (Clarke et al., 2015) , the dimensions of narcissism most representative of the adaptive form were significantly and positively associ- Locomotion was significantly and positively associated with the dimensions of adaptive narcissism, such as , respectively. Therefore, the positive relation between locomotion and positivity appeared to be mediated by adaptive narcissism of leadership, superiority, and grandiose exhibitionism, providing support for H1. On the other hand, assessment was significantly and positively associated with each of the dimensions of narcissism ( Fig. 1 . Therefore, the positive relation between assessment and positivity appeared to be mediated by adaptive narcissism of leadership, superiority, and grandiose exhibitionism, providing support for H2. Furthermore, there was a significant negative indirect effect of assessment on positivity through Contingent Self-Esteem (Indirect effect = −.17, SE = .06, 95% CI [−.2967, −.0504]) and Devaluing the Self (Indirect effect = −.10, SE = .04, 95% CI [−.1877, −.0285]). Therefore, the negative relation between assessment and positivity appeared to be mediated by maladaptive narcissism of contingent selfesteem and devaluing the self, providing support for H3. No significant indirect effects were found through Manipulative, Entitlement and Grandiose Fantasy. The model estimated was overall highly significant, F(10, 179) = 11.54, p < .001, with R 2 = .39. Consistent with our predictions, we found that content of narcissism (i.e., adaptive or maladaptive) has mediated the relationship between regulatory mode and positivity. Specifically, both locomotion and assessment orientations were positively associated with positivity through adaptive narcissism. At the same time, assessment was also negatively associated with positivity through maladaptive narcissism: this particular result on assessment is quite consistent with previous results (Boldero et al., 2015) that concluded that the aim of assessment to "do the right thing" can imply different feelings of narcissism in case of success and failure. Therefore, in our case, roles of authority, feelings of superiority and the desire to be the center of attention (i.e., adaptive narcissism), have increased assessors' positivity; at the same time, the need to be admired by others and the propensity to devalue themselves when they feel unappreciated (i.e., maladaptive narcissism), has reduced their positivity. Expanding previous research findings (Boldero et al., 2015) , the results of the present study confirm the susceptibility of locomotors and assessors to various facets of narcissism, but also outlined how this can affect their ability to see life with a positive outlook. Consistent with recent research on the positive effect of locomotion on well-being (e.g., Di Santo et al., 2020) , we see that locomotion promoted positivity through adaptive narcissism. However, assessment also, but not exclusively, promoted positivity through adaptive narcissism. These findings were consistent with previous results on regulatory mode conjunction positive effects (Pierro, Giacomantonio, et al., 2012a; Pierro et al., 2018) , that is, the beneficial co-presence of the two modes. Successful selfregulation involves both assessment mode, through which the individual thinks deeply about the right course of action, and locomotion mode, through which the individual implements the action; hence, high locomotion, pushed "to go", makes use of guidance and evaluation control to go "in the right direction" ; see also Pierro et al., 2006; Pierro, Pica, et al., 2012b) . Moving on to the present research, both regulatory modes were found to promote adaptive narcissism and positivity; on the other hand, the simultaneous experience of adaptive and maladaptive narcissism in the assessment mode (Boldero et al., 2015) has been shown to have significant repercussions on positivity. This work has several limitations that should be noted. We used a sample of university students; thus, we must be particularly cautious in generalizing the results to other populations. Furthermore, our data was collected from the same source and using the same method: all data derived from self-report measures and cross-sectional design. Thus, on the one hand, data and findings may be subject to common method/source biases, that may inflate relationships between variables, and, on the other hand, do not even allow to delineate the causality of the relationships found. Although we controlled the common method/source bias comparing alternative models via CFA, and confirming the discriminant validity of the measures used, further research should also profitably use multitrait-multimethod matrix (MTMM; Fiske & Campbell, 1992) to better address this issue. Furthermore, future research should provide confirmation for our hypothesis using longitudinal or experimental designs with manipulation of variables (e.g., Avnet & Higgins, 2003) . However, we can be cautiously confident in our results since previous studies found consistent relationships between regulatory mode and narcissism (Boldero et al., 2015) , and positive affects (Kruglanski et al., 2000) , as well as between narcissism and self-worth (Clarke et al., 2015) ; we thus determined the proper order of the variables in our proposed mediation model upon the relationships previously found. Beyond these limitations, these results can have theoretical implication linked to our attempt to connect previous conclusions (Boldero et al., 2015; Clarke et al., 2015) and examine a model that sees self-regulation, narcissism, and positive thinking as potentially linked processes. Certainly, future research can examine these relationships further. Or, given that narcissism was only a partial mediator, future studies should continue exploring other potential mediators. Practical implications are related to the possibility of stimulating (e.g., Avnet & Higgins, 2003) self-regulatory factors that increase individuals' susceptibility to different forms of narcissism. Of course, we must duly emphasize that narcissism is still at the center of a large debate in the clinical and social literature (see, for example, Di Pierro & Madeddu, 2018) . The debate stems from the high complexity of the narcissistic syndrome, and there is still widespread disagreement on its definition and measurement, as well as the identification of its central characteristics, so we find an abundance of conflicting results and theoretical perspectives in the literature, as was also argued by Morf and Rhodewalt (2001) . However, previous research (e.g., Cai & Luo, 2018; Clarke et al., 2015) has shown that narcissism can be "adaptive" if it brings positive psychological benefits to the individual. Therefore, by considering locomotion and assessment as possible self-regulatory bases of narcissistic forms, we may have in mind that stimulating both can lead to adaptive forms of narcissism. However, assessment has tendencies towards threatening and continuous evaluation with various behavioral consequences (e.g., Livi et al., 2014) , which can also lead to experience maladaptive outcomes, appearing the self-regulatory factor that stimulates different presentations of narcissism in individuals (see also the argument of Boldero et al., 2015) . It cannot be excluded that other factors may intervene to favor maladaptive outcomes, linked to the personal history of the individual; for example, child maltreatment has been shown to increase the risk of negative psychological outcomes and maladaptive responses (Pompili et al., 2014) . Thus, in the study of the development of narcissistic self-regulation and its effects, personal factors of the individual should be considered, for example relating to childhood experiences (e.g., Otway & Vignoles, 2006) . Turning to the current research, a distinction was confirmed in the positive effects of adaptive versus maladaptive narcissism. Therefore, this study offers some insight into how having (or possibly stimulating) locomotion mode, assessment mode, or both, and understanding the forms of narcissism linked with them, can help individuals live their lives with greater positivity. Results of this study show that regulatory modes were associated with increased or decreased positivity through narcissism. Specifically, locomotion was associated with greater positivity through adaptive narcissism. On the other hand, assessment was associated with increased and decreased positivity through adaptive and maladaptive narcissism, respectively. These findings raise important questions about the role of self-regulation in predicting types of narcissism and related well-being. In conclusion, what emerges from our findings is that regulatory modes and adaptive narcissism can help people approach their life positively. Code Availability Not applicable. Authors' Contributions All authors contributed to the study conception and design. Material preparation, data collection and analysis were performed by Antonio Pierro, Daniela Di Santo, Calogero Lo Destro and Conrad Baldner. The first draft of the manuscript was written by Daniela Di Santo and all authors commented on previous versions of the manuscript. All authors read and approved the final manuscript. Funding Open access funding provided by Università degli Studi di Roma La Sapienza within the CRUI-CARE Agreement. Data Availability The datasets generated during and analysed during the current study are available from the corresponding author on reasonable request. Conflict of Interest On behalf of all authors, the corresponding author states that there is no conflict of interest. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/. 
paper_id= fff5ef42f08b25d724bfca377d6f1c88ab33664e	title= The Immune Epitope Database and Analysis Resource	authors= Sette A ;Bui  Hh;Sidney J ;Kubo R ;Ponomarenko  Jv;Sathiamurthy M ;Stewart S ;Way S ;Wilson  Ss;Peters B ;	abstract= Epitopes are defined as the molecular structures interacting with specific receptors of the immune system such as antibodies, MHC, and T cell receptor molecules. The Immune Epitope Database and Analysis Resource (IEDB, http://www.immuneepitope.org) is a database specifically devoted to immune epitope data. The database is populated with intrinsic and contextdependent epitope data curated from the scientific literature by immunologists, biochemists, and microbiologists. An analysis resource is linked to the database which hosts various bioinformatics tools to analyze epitope data as well as to predict de novo epitopes. The availability of the IEDB will facilitate the exploration of immunity to infectious diseases, allergies, autoimmune diseases, and cancer. The utility of the IEDB was recently demonstrated through a comprehensive analysis of all current information regarding antibody and T cell epitopes derived from influenza A and determining possible cross-reactivity among H5N1 avian flu and human flu viruses. 	body_text= Epitopes are defined as the molecular structures interacting with specific receptors of the immune system such as antibodies, MHC, and T cell receptor molecules. Knowledge of the epitopes involved in the immune response is critical to detect, monitor, and design therapies to fight infectious diseases as well as allergies, autoimmunity and cancer. A vast amount of epitope-related information is available, ranging from epitope binding affinities for their receptors, to cellular and humoral responses, to data analyzing correlates of protection or immune pathology. We have developed a central resource that captures this information, allowing users to connect realms of knowledge currently separated and difficult to access. This new initiative, "The Immune Epitope Database and Analysis Resource", became available to the public in a beta version on 15 February 2006 (http://www.immuneepitope.org) [1, 2] . The priorities for inclusion in the database are epitopes from category A-C pathogens such as influenza, SARS and poxviruses and emerging/re-emerging infectious diseases. However, we anticipate that all immune epitope data will eventually be curated. B and T cell epitopes recognized in humans, non-human primates and laboratory animals are all considered within the scope of the project. Accordingly, we estimate that about 100,000 different literature records to be relevant. In addition, we expect to host a large volume of direct data submissions from various NIH-sponsored research contracts. Curation and query strategies have been developed to enable effective handling of this large amount of highly context dependent information. An analysis resource is linked to the database that hosts various bioinformatic tools to analyze epitope data (including, for example, population coverage and sequence conservation), as well as tools to predict epitope cellular processing, binding to MHC, and recognition by T cell receptors and antibody molecules. In this context we observed that a large number of predictive tools related to epitopes exist in the literature, and new ones are continuously being developed. Evaluating the performance of existing and newly developed tools will be an on-going effort for the IEDB team. The IEDB has been developed as a web-accessible database using an industry standard software design. The IEDB application is a Model View Controller (MVC) (http://java.sun.com/blueprints/guidelines/designing_enterprise_applications_2e/webtier/web-tier5.html) style Enterprise Java (J2EE) application with a relational database management system (Oracle 10g) data repository. The system architecture is divided into two websites, three application tiers, and two physical servers (See Figure 1 ). The application was constructed using existing Java frameworks and commercial products to create the infrastructure allowing the development team to concentrate on the novel functionality that was required. The enterprise architecture is flexible, extensible, scalable, and proven. Each epitope is associated with intrinsic and extrinsic features. Intrinsic features are those associated with its sequence and structure, while extrinsic features are context-dependent attributes dependent upon the specific experimental or natural environment context. Contextual information includes the species, health, and genetic makeup of the host, the immunization route and dose, and the presence of adjuvants. In order to describe an immune response associated with a specific epitope, both the intrinsic and extrinsic (context-dependent) features need to be taken into account. This immunological perspective has been a guiding principle in organizing the data in the IEDB [1] [2] [3] . The hierarchal nature of the data has been captured in an epitope-centric, ontologylike data structure [4] , developed in a top-down process where each class in a domain and its properties were defined before building the hierarchy. The primary classes of the IEDB consist of Reference, Epitope Structure, Epitope Source, MHC Binding, MHC Ligand Elution, T Cell Response, and B Cell Response (See Figure 2) . The Epitope class encapsulates all the individual concepts identified. In turn, the  To identify and extract relevant data from the scientific literature in an efficient and accurate manner, novel formalized curation strategies were developed, enabling the processing of a large volume of context-dependent data. This process is multi-step, involving an automated PubMed query, a manual abstract scan to select potentially relevant references, followed by methodical analysis of the selected references, and finally the manual curation of papers deemed relevant to the scope of the database by a team of dedicated curators with expertise in the areas of biochemistry, microbiology and immunology. Once the manual curation of a reference is complete, the curation is reviewed by an independent group of immunologists and structural biologists, thus, integrating experts and data curators to optimize quality, consistency and uniformity. To facilitate accurate translation of the information contained in the literature into the structured format of the database, we developed a Curation Manual and Data Ontology. These documents are designed to provide a consistent set of rules, definitions, and guidelines regarding the strategies and procedures for capturing, annotating and introducing data from the literature into the IEDB. Additionally, feedback from external experts in the fields of immunology and infectious diseases has been sought on an ongoing basis in order to improve both the database structure and curation practices. In this way, complex experimental data are captured in a consistent and accurate manner. Management of the curation of a large number of references by a team of curators and reviewers required the development of a formal tracking system. All transactions and comments pertaining to each reference are tracked to provide details of the progress of each curated paper from selection of the manuscript to final incorporation of the data into the IEDB. As of May 2006, over 1900 references have been manually curated. An analysis resource is linked to the database allowing users to analyze epitope data as well as to predict de novo epitopes. For example, one tool predicts the population coverage for a user-prescribed set of T-cell epitopes [5] , i.e. the fraction of an ethnic population likely to respond to one or more epitopes in the set. This is done by relating the known MHC restrictions of the epitopes to frequencies of the MHC alleles in different populations, and calculating the total population covered assuming linkage equilibrium between MHC loci. An illustration of the utility of this tool is that it allows a user to detect if a set of epitopes, which may be intended for use as a diagnostic tool or vaccine, has ethnically skewed or balanced population coverage. Another tool calculates the protein sequence conservation of epitopes. For a given starting sequence and threshold of sequence identity, the tool calculates the fraction of proteins containing the epitope. Focusing on epitopes that are conserved at a high level of sequence identity is specifically important for RNA viruses, which show a large degree of sequence variability between different isolates or strains. To predict the presence of antibody epitopes in protein sequences, a number of previously existing amino acid scale-based tools have been implemented from the literature. Although these particular tools have been shown to underperform [6] , they represent the current state-of-the-art in antibody epitope prediction and do provide a benchmark for future tool development. Our intent is to devote significant efforts towards the development of improved tools for the prediction of antibody epitopes. The most extensively tested predictions at present are those describing peptide binding to MHC class I molecules. The ability of a peptide to bind an MHC molecule is a necessary requirement for it to be recognized by T-cells. As MHC molecules are very specific in their binding preference, these predictions provide a powerful means to scan entire pathogens for T-cell epitope candidates. Three separate prediction methods were implemented, two of them based on scoring matrices (ARB [7] and SMM [8] ) and one based on an artificial neural network [9, 10] . The three methods were compared using five-fold cross validation on a large dataset comprising nearly 50,000 data points, in which the neural network based predictions outperformed the other two [11] . The complete benchmark dataset used in this evaluation is available at http://mhcbindingpredictions.immuneepitope.org/, and we encourage developers to use these data in training and testing their own tools. In addition to the tools described above, tools for predicting MHC class II epitopes [7] , proteasomal cleavage [12] and TAP transport [13] have also been implemented and will be evaluated in a similar manner to the MHC class I binding predictions. Also, for epitopes with a 3D structure available in the PDB, an epitope viewer has been developed displaying the epitope structure and its immune receptor interactions. As pointed out in a recent Nature editorial, the fight against flu is undermined by "the lack of an accessible store of information" [14] . Besides outbreaks and sequencing data, information is also lacking regarding influenza epitopes. This knowledge is crucial to predict potential cross-reactive immunity and coverage of new strains by vaccines and diagnostic candidates. To demonstrate the features of IEDB and in response to the global spread of highly virulent H5N1 influenza viruses, we have performed an analysis of influenza A epitope information to: 1) compile all current information regarding antibody and T cell epitopes derived from influenza A and 2) determine possible cross-reactivity among H5N1 avian flu and human flu virus. To compile all information available in the literature relating to influenza epitopes, we inspected over 2000 references, and more than 400 were added to the IEDB after detailed curation. An assessment of these curated records revealed that approximately 600 different epitopes, derived from 58 strains, recognized in 8 different hosts and derived from all flu proteins have been identified and reported in the literature, including several conserved epitopes and a small number of protective ones. The latter are of particular interest as they may confer cross-reactive protection against influenza strains of the avian H5N1 subtype. Significantly, however, this analysis made apparent the fact that: 1) few protective antibody and T cell epitopes are reported in the literature; 2) there is a paucity of antibody epitopes in comparison to T cell epitopes; 3) the number of animal hosts from which the epitopes were defined is limited; 4) the number of epitopes reported for avian influenza strains/subtypes is limited, 5) the number of epitopes reported from proteins other than hemagglutinin (HA) and nucleoprotein (NP) is limited. In summary, this analysis provides a unique resource to evaluate existing data and to aid efforts in guarding against seasonal and pandemic flu outbreaks. The IEDB is an initiative focused on creating large volumes of complex contextdependent immunological data paired with relevant analytical tools. The project should facilitate basic research, as well as the development of new vaccines and diagnostics. The experience gained in the process of developing and operating the IEDB will be of value in the development and integration of other biological databases capturing clinical, immunological, genomic and cellular biology knowledge. 
paper_id= fff600839441a60bb883d5f4328aa3227e127d1d	title= Effects of a porcine reproductive and respiratory syndrome virus infection on the development of the immune response against pseudorabies virus 4 2 7 ( 0 0 ) 0 0 2 0 8 -7	authors= M G M De Bruin;J N Samsom;J J M Voermans;E M A Van Rooij;Y E De Visser;A T J Bianchi;	abstract= The aim of this study was to investigate the effects of a porcine reproductive and respiratory syndrome virus (PRRSV) infection on the development of the immune response after pseudorabies virus (PRV) vaccination in pigs. Pigs were intranasally inoculated with the European PRRSV strain, Lelystad virus ter Huurne, and were vaccinated intramuscularly with PRV 2 weeks later (LV-PRV group). Control pigs were vaccinated with PRV only (PRV group). Eight weeks after PRV vaccination, pigs from both groups were challenged intranasally with wild-type PRV. We measured the lymphoproliferative, and the cytolytic responses to PRV of peripheral blood mononuclear cells (PBMC), isolated from blood samples. In addition, serum samples were examined for antibodies against PRV and LV. One week after PRV vaccination, PBMC proliferated abundantly to PRV in both groups. However, in the LV-PRV group the lymphoproliferative response declined after 1 week, whereas, in the PRV group, the lymphoproliferative response was high for 3 weeks and declined thereafter (P<0.05). After challenge, the lymphoproliferative response was 1 week earlier and was consistently and signi®cantly higher in the PRV group than in the LV-PRV group. The PRV-speci®c killing was higher at 3 weeks after PRV vaccination and 5 weeks after PRV challenge 19AE3 and 24AE6%, respectively, in the PRV group, compared to 7AE4 and 6AE9%, respectively, in the LV-PRV group (P<0.05). However, later after vaccination and challenge the cytolytic response was identical in both groups. The antibody titre against PRV developed equally in both groups. After challenge, no PRV virus was isolated from both groups. From these results we conclude that, although PRRSV infection did cause changes in the time course of the T-lymphocyte response after PRV vaccination, PRRSV infection did not inhibit the development of vaccine-induced protection after PRV. # 	body_text= Porcine reproductive and respiratory syndrome virus (PRRSV) is a positive stranded RNA virus that causes reproductive failure in late term gestation sows and respiratory disease in pigs of all ages. The virus primarily infects and destroys alveolar macrophages, which are important in the host defence in the lungs. Therefore, PRRSV-infected pigs may be more susceptible to secondary infections. Since PRRSV occurs endemically in many countries and may in¯uence the ef®cacy of host defence mechanisms, it is possible that a PRRSV infection interferes with the vaccination ef®cacy against PRV. Previous studies on the effect of PRRSV infection on secondary infections under experimental conditions are contradictory. Galina et al. (1994) described more clinical signs, meningitis and bacteraemia after a Streptococcus suis infection only when pigs were pre-inoculated with PRRSV. Thacker et al. (1999) described more severe and persistent pneumonic lesions and clinical respiratory disease when a Mycoplasma hyopneumoniae infection was followed by a PRRSV infection. van Reeth et al. (1994 Reeth et al. ( , 1996 described more severe disease after a dual infection with PRRSV and porcine respiratory coronavirus or in¯uenza virus compared with the single infections. In contrast, others described no in¯uence of a PRRSV infection on the clinical signs of disease after infection with Pasteurella multocida, Haemophilus parasuis, S. suis or Salmonella cholerasuis (Cooper et al., 1995) . In addition, Albina et al. (1998) could not demonstrate an immunosuppressive effect of PRRSV on a pseudorabies virus (PRV) glycoprotein immunisation and described an enhanced antibody response after challenge with PRV. More illness and a more severe disease outcome after infection with other microorganisms due to an earlier PRRSV infection have been described. These studies focussed on increased illness and pathological alterations by the secondary infection in PRRSVinfected pigs. However, the effect of PRRSV on the cellular immune response against secondary infections is not clear. Therefore, we examined the effects of a PRRSV infection on the development of the cellular immune response against a PRV infection in a well-de®ned PRV vaccination-challenge model. PRV is an alphaherpesvirus that causes Aujeszky's disease in pigs. Both humoral and cellular immunity appear to be involved in the development of a protective immune response to PRV. Previous reports have described the time course of the lymphoproliferative and cytolytic responses after a PRV infection and challenge in detail (Kimman et al., 1995b; De Bruin et al., 1998) . Therefore, we used this PRV model to investigate the immunosuppressive effect of PRRSV on a PRV vaccination and challenge in pigs. We investigated the development of the humoral, lymphoproliferative and cytolytic responses against PRV in pigs that were inoculated with PRRSV prior to PRV vaccination. PRRSV stocks for inoculation were prepared on alveolar macrophages as described by Wensvoort et al. (1991) . Pigs were inoculated with PRRSV strain Lelystad virus ter Huurne (LV-TH). PRV stocks for vaccination and challenge were prepared on SK-6 cells (Kasza et al., 1971 ) and secondary porcine kidney cells, respectively, as described by Kimman et al. (1995b) . Pigs were vaccinated with an avirulent mutant strain M141 (gE À ) of PRV (De Wind et al., 1990; Kimman et al., 1992) . The pigs were then challenged with wild-type PRV strain NIA-3 (McFerran and Dow, 1975) . This strain was also used to infect target cells for the cytolytic assay, as described by Kimman et al. (1995a) . Minnesota miniature pigs from our institute, which are inbred for swine±leukocyte± antigen complex (SLA) (haplotype d/d) (Sachs et al., 1976) , were kept under speci®c pathogen-free conditions. The pigs were born from unvaccinated sows and had no antibodies against PRV or PRRSV. Pigs were randomly allocated to two groups and housed separately. Six pigs were intranasally inoculated with 0.5 ml of PRRSV strain LV-TH at a concentration of 10 5 TCID 50 /ml per nostril. Five control SPF pigs were left uninoculated. After 2 weeks both groups were vaccinated intramuscularly with 10 5 plaque-forming units (PFU)/ml PRV strain M141. These pigs were subsequently challenged intranasally with 10 5 PFU/ml NIA-3 virus, 8 weeks after PRV vaccination. Blood samples were taken weekly, starting 1 week before and then 2 weeks after the LV inoculation. Rectal temperatures were recorded daily for 25 days after LV inoculation and 25 days after PRV challenge. Body weights were recorded at weekly intervals for 3 months. Pigs were considered to have fever when body temperatures were above or equal to 408C. Growth performance was assessed by calculating the mean relative daily gain (MRDG) in body weight according to Stellman et al. (1989) . The ethical committee for animal experiments of ID-Lelystad approved the experiments (Table 1) .  PBMC isolation and culturing was done as previously described by Kimman et al. (1995b) . Brie¯y, PBMC were stimulated in vitro for 4 days with negative cell-lysate of SK-6 cells, or infectious NIA-3 (PRV) virus. For cytolytic assays, PBMC were used directly (direct killing) or they were stimulated for 6 days in vitro with NIA-3 (indirect killing) as described by De Bruin et al. (1998) . Proliferation of PBMC was measured by 3 H-thymidine incorporation as described by De Bruin et al. (1998) and expressed as delta counts, i.e. the number of counts of virusstimulated PBMC minus the number of counts of negative cell-lysate-stimulated PBMC. Proliferation was investigated starting 1 week before LV inoculation and then at weekly intervals from 2 weeks after LV inoculation and was measured after subsequent PRV inoculations. The following target cells were used in the cytolytic assays: PRV-infected and uninfected L14 cells (a retrovirus immortalised B-lymphoblastoid cell line of the haplotype d/d SLA) (Kaeffer et al., 1990) and K562 cells (a cell line from human erythroleukemia cells) (ATCC, Rockville, MD). The K562 cells were used to assess the killing by porcine NK cells (Pescovitz et al., 1988) . Infected L14 cells were obtained by infecting the cells, 24 h before the start of the cytolytic assay, with NIA-3 virus at a multiplicity of infection of 10. The cells were labelled by incubation of various numbers of cells in a volume of 50 ml serum-free medium which contained 400 mCi 51 Cr (Amersham CJS4, Den Bosch, The Netherlands) for 2 h at 378C on a Rock'n Roller (Labinco, Breda, The Netherlands). After being labelled, the cells were washed three times in RPMI complete medium. Volumes of 50 ml medium containing 10 4 cells were added to the wells of 96-well V-bottomed microtitre plates (Nunc, Life Technology, Breda, The Netherlands). The cytolytic activity of the effector cells was measured by 51 Cr release. Volumes of 50 ml medium containing effector cells and 50 ml medium containing 10 4 51 Cr-labelled target cells were mixed in order to obtain effector:target ratios of 50:1 to 6:1. The plates were then centrifuged for 5 min at 200Âg. Maximal release of 51 Cr was determined by adding 50 ml 20% Triton X-100. Spontaneous release was determined in wells that did not contain effector cells. Killing of the target cells was determined by measuring the release of 51 Cr in the supernatant after an incubation period of 5 h. Volumes of 50 ml supernatant were mixed with 100 ml optiphase supermix liquid scintillation¯uid (EG&G Instruments, Nieuwegein, The Netherlands). Radioactivity was then measured in a Wallac Microbetaplus 1450 (EG&G Instruments, Nieuwegein, The Netherlands). The percentage of speci®c 51 Cr release was calculated as cpm experimental release À cpm spontaneous release cpm maximal release À cpm spontaneous release Â 100 Direct killing was investigated at 10 and 18 days after PRV vaccination. Indirect killing was investigated 1 week before the LV inoculation and then 2 weeks after LV inoculation and was measured at weekly intervals after subsequent PRV inoculations. Blood samples were collected from all pigs 1 day before LV inoculation and at weekly intervals after PRV inoculations. Serum samples were stored at À208C. Sera were tested for the presence of virus neutralising antibodies directed against PRV in a virus neutralisation test (VN titres) as described by Kimman et al. (1992) . Sera were tested for the presence of virus antibodies directed against PRRSV in an immunoperoxidase monolayer assay (IPMA) and for the presence of PRRSV by virus isolation as described by Wensvoort et al. (1991) . PRV virus excretion was monitored by collecting swab specimens of oropharyngeal uid (OPF) from the day before challenge until day 10 after challenge. Swab specimens were extracted with 4 ml of Dulbecco's minimal essential medium supplemented with 2% foetal bovine serum and antibiotics. To determine the virus content per gram OPF, we measured the weight of the collected¯uid after centrifugation of the swabs (Kimman et al., 1992) . The amount of PRV excretion was quantitated by titration of the virus on SK-6 monolayers as described by Kimman et al. (1992) . Differences in lymphoproliferative responses, virus-speci®c killing activity, VN antibody titres and MRDG were tested for statistical signi®cance by analysis of variance (ANOVA) after testing for homogeneity of variances within groups (Barletts' test). The signi®cance level was set at 5%. LV speci®c antibodies were detected from 1 week after LV inoculation in IPMA. This response peaked on 7 weeks after LV inoculation (titre log 4.6) and declined thereafter. After 15 weeks post-LV inoculation, antibodies against LV (titre log 3.4) were still detected. PRV VN antibodies were detected from 1 week after PRV vaccination with M141 (gE À PRV-strain), in the LV-PRV and the PRV group, with a virus neutralisation assay (titres log 2.7À3.3). After challenge with PRV, no increase of the antibody-titres was observed and no difference was observed in the development of the humoral response between both groups. In contrast to the antibody-titres against LV, the antibody-titres after PRV vaccination and challenge did not decrease during the experiment (15 weeks). One week after PRV vaccination, PBMC from PRV-infected pigs proliferated abundantly in vitro in both groups on stimulation with PRV (Fig. 1) . The proliferation of PBMC tended to be higher (but not statistically signi®cantly) in the PRV group (50,587AE36,565 CPM) than in the LV-PRV group (18,042AE19,888 CPM) (P<0.09). The response peaked 2 weeks after PRV vaccination in both groups. In the LV-PRV group the lymphoproliferative response declined 1 week later, whereas in the PRV group the lymphoproliferative response remained high for 3 weeks and declined thereafter. The proliferation was signi®cantly higher (P<0.05) in the PRV group than in the LV-PRV group in weeks 4 and 5 after PRV vaccination. In contrast to the VN antibodies, the lymphoproliferative response increased after challenge with wild-type NIA-3 virus ( Fig. 1) in both groups. This secondary lymphoproliferative response started 1 week after challenge in the PRV group and 2 weeks after challenge in the LV-PRV group (P<0.05). After PRV challenge, the speci®c proliferation of PBMC to PRV stimulation was consistently and signi®cantly lower, in the LV-PRV group than in the PRV group (P<0.05) (Fig. 1). 3.3. Time course of cytolytic activity 3.3.1. Indirect killing after LV and PRV inoculations: When PBMC from the PRV and LV-PRV group were stimulated in vitro with NIA-3 virus, PRV-speci®c killing (i.e. killing of PRV-infected L14 cells and killing of uninfected L14 cells) increased between weeks 3 and 6 after PRV vaccination (up to 30% of PRVinfected target cells were killed compared with up to 15% of uninfected target cells). In both groups, PRV-speci®c killing by these cells was detected for a period of at least 6 weeks after challenge; (up to 35% of PRV-infected target cells were killed compared with up to 15% of uninfected target cells). K562 cells were only poorly killed (10% after PRV vaccination). The PRV-speci®c killing was higher at 3 weeks after PRV vaccination and 5 weeks after PRV challenge 19AE3 and 24AE6%, respectively, in the PRV group, compared to 7AE4 and 6AE9%, respectively, in the LV-PRV group (P<0.05) (Fig. 2) . However, later after PRV vaccination the killing was comparable in both groups. K562 cells were poorly killed in both groups (up to 20% in the PRV and LV-PRV group 1 week after challenge). On day 10 after PRV vaccination, directly used PBMC from the PRV group killed the PRV-infected L14 cells more ef®ciently (30AE5%) than PBMC from the LV-PRV group (18AE7%) (P<0.05). In contrast, on day 18 after PRV vaccination, PBMC from the PRV and LV-PRV group killed the PRV-infected L14 cells equally well (14AE5%) and (11AE4%). No killing of K562 cells was detected. Pigs from the LV-PRV group had fever after inoculation with LV from days 3 to 6 after LV inoculation and again after PRV vaccination on days 17 and 18 after LV infection. In contrast, pigs from the PRV group had no fever after vaccination (Fig. 3) . Pigs from both groups had no fever after challenge. LV was isolated from sera from 4 pigs (n6). LV was isolated on day 3 (2 pigs), day 10 (1 pig) and day 24 (1 pig) after LV inoculation. No PRV was isolated from OPF of both groups after challenge with PRV. The mean daily weight gain did not differ after LV and after both PRV inoculations between both groups. Pigs had no clinical signs of PRRSV disease after LV inoculation (except fever) and no clinical signs of Aujeszky's disease after PRV inoculations. Results showed that a PRRSV infection prior to a PRV vaccination caused differences in the level and time course of the cellular immune response against PRV. The lymphoproliferative response tended to be quicker (not signi®cantly) and longer (2 weeks) (P<0.05) in the PRV group than in the LV-PRV group after PRV vaccination. This lymphoproliferative response was also quicker (1 week) and consistently higher in the PRV group than in the LV-PRV group after PRV challenge (P<0.05). Furthermore, when we used in vitro PRV-stimulated PBMC, the cytolytic response against PRV reached a maximum in week 3 after PRV vaccination and week 5 after PRV challenge in the PRVgroup, whereas the LV-PRV group did not (P<0.05). Later after vaccination and challenge, the cytolytic response was comparable in both groups. The same kinetics of the cytolytic response against PRV vaccination with M141 was described in an other study (De Bruin et al., 1998) , which indicates that the LV-PRV group in this study did not reach a maximum level in the cytolytic response, whereas the PRV group did. When PBMC were used directly, we also observed a signi®cantly higher cytolytic response against PRV on day 10 after PRV vaccination in the PRV group than in the LV-PRV group (P<0.05). These results indicate a mild immunosuppressive effect by the PRRSV infection on the development of a cellular immune response after PRV vaccination and challenge. An explanation for this lower T-lymphocyte response may be that PRRSV infects monocytes and macrophages, which may hamper ef®cient antigen-presentation and thereby suppressing the ability of PBMC to proliferate. Although PRRSV clearly affected the time course of the cellular immune response against a PRV infection, no effect on the humoral response against PRV was detected after vaccination and challenge in the LV-PRV group. This ®nding is in contrast to Brun et al. (1994) , who observed an enhanced secondary humoral response after an in¯uenza challenge, and Albina et al. (1998) and Molitor et al. (1992) , who also observed an enhanced humoral response after challenge with PRV, after an earlier PRRSV infection. It is possible that PRRSV infection stimulates the humoral response. However, an explanation for this discrepancy is that we used a very potent vaccine strain, inducing protection and no increase of antibody-titres (Kimman et al., 1992 (Kimman et al., , 1994 De Bruin et al., 1998) . We could not detect differences in protection against a PRV challenge between both groups, but a negative effect of a PRRSV infection on protection after PRV vaccination can not be excluded when less potent vaccine strains are used. However, the M141 vaccine strain was used, because this strain induced a high cellular immune response (De Bruin et al., 1998) and eventually negative effects of a PRRSV infection on the development of the cellular immune response could be measured very well. No weight loss or differences in mean daily weight gain were observed between both groups. Furthermore, no PRV was excreted after challenge indicating optimal protection. The only observed clinical signs were fever for 4 days after LV inoculation and again 2 days after the PRV vaccination in LV-PRV group. van Reeth et al. (1994) also described fever and more clinical signs after dual infections of PRRSV with porcine respiratory coronavirus and swine in¯uenza virus. Although we measured fever after LV inoculation and PRV vaccination, we could not detect clinical signs of Aujeszky's disease after vaccination with M141 (a PRV mutant strain that causes mild clinical signs (Kimman et al., 1994; De Bruin et al., 1998) ). Obviously, even though pigs were recovering from the earlier PRRSV infection, the PRV mutant strain M141 could not cause Aujeszky's disease. Thus, although T-lymphocyte responses were diminished against PRV, and pigs had some fever in the LV-PRV group, pigs were clinically protected against a PRV challenge. In conclusion, we demonstrated that a PRRSV infection prior to a PRV vaccination mildly affects the level and time course of the T-lymphocyte response against PRV. Despite this difference, the PRRSV infection did not in¯uence the capacity of the gE À PRV strain to induce protection against a wild-type PRV challenge. 
paper_id= fff60988428b82658458d381a6c27583209a686e	title= Development of A Loop-Mediated Isothermal Amplification (LAMP) for the Detection of F5 Fimbriae Gene in Enterotoxigenic Escherichia coli (ETEC)	authors= Kuiyu  Jiang;Ying  Zhu;Wenxin  Liu;Yufei  Feng;Lili  He;Weikun  Guan;Wenxia  Hu;Dongfang  Shi;	abstract= The objective of this study was to establish a loop-mediated isothermal amplification (LAMP) method for the detection of F5 fimbriae gene in Enterotoxigenic Escherichia coli. A set of four primers were designed based on the conservative sequence of coding F5 fimbriae. Temperature and time condition, specificity test, and sensitivity test were performed with the DNA of Escherichia coli (F5?). The results showed that the optimal reaction condition for LAMP was achieved at 61°C for 45 min in a water bath. Ladder-like products were produced with those F5-positive samples by LAMP, while no product was generated with other negative samples. The assay of LAMP had a detection limit equivalent to 72 cfu/tube, which was more sensitive than PCR (7.2 9 10 2 cfu/tube). The agreement rate between LAMP and PCR was 100 % in detecting simulation samples. Thus, the LAMP assay may be a new method for rapid detection of F5 fimbriae gene of ETEC. 	body_text= Adhesion fimbria play an important role in causing young animal diarrhea by Enterotoxigenic Escherichia coli (ETEC), it enables ETEC to colonize the small intestinal cell surface, and ETEC produce toxins to cause the host diarrhea [1] . The fimbrial adhesions most frequently diagnosed in ETEC strains isolated from piglets with diarrhea are F4(K88), F5(K99), F6(987P), and F41 [14] , which have different antigenicity. Thus, vaccines developed from one specific fimbria could not provide protection against an ETEC strain expressing a different fimbria. Therefore, it is necessary to detect fimbriae's type of ETEC-isolated strains and to select an appropriate vaccine prepared with right fimbriae antigen. The type of fimbriae is determined by the genes encoding fimbriae protein, thus detection of the genes encoding fimbriae protein can determine the type that the fimbriae has expressed. F5 fimbriae is one of the most common fimbriae antigen, it was established as a transmissible K antigen with adhesive properties in 1975 [17] and encoded on a 58-megadalton plasmid [8] . Hybridization technique and PCR have been used for detecting F5 fimbriae gene as nucleic acid detection technologies [3, 9, 16] , but the concentration of bacteria and purity may make an effect in accuracy and sensitivity with hybridization in practical applications, PCR requires special instruments and equipments, thus extensive application of these technologies are restricted. However, the invention of Loop-mediated isothermal amplification (LAMP) provides new ideas and technologies for establishing a rapid detection method of F5 fimbriae gene. LAMP is a new technology of nucleic acid amplification reported first in 2000 by Notomi. This method focus on an autocycling strand displacement DNA synthesis performed by the Bst DNA polymerase large fragment, which is different from PCR in that four or six primers perform the amplification of the target gene, it is sensitive, specific, and rapid [10] . At present, LAMP has been successfully used to detect many pathogens, such as human malaria parasites, mycobacterium tuberculosis, and Shiga Toxin-Producing Escherichia coli, detection of the HCLV against classical swine fever, and so on [6, 11, 18, 22] . The objective of this study was to develop a LAMP assay for detecting of F5 fimbriae gene more rapidly, accurately, and easily. E. coli C83529 (F5?), C83903 (F4ab?), C83915 (F6?), C83684 (F18ab?), and C83920 (F5? F41?) were obtained from China Institute of Veterinary Drugs Control. Recombinant E. coli TG1 (pMD18T-F41)-contained F41 fimbriae gene was structured in our laboratory; seven simulation samples of E. coli strains were prepared by a different Lab in our school, two of the seven strains were F5 fimbria positive and the other five were negative. Bst DNA polymerase was purchased from NEB. Betaine was purchased from Sigma, SYBR Green I was purchased from Xiamen Baiweixin Company (China), and MgCl 2 and dNTPs were purchased from Takara. Double-distilled water was used in all experiments. All other reagents were analytical grade. Four primers targeting F5 fimbriae protein genes (GenBank Number: M35282) were designed by means of the Primer Explorer V4 (http://primerexplorer.jp/e), including external primers F3 and B3, and internal primers FIP and BIP. A pair of primers P1 and P2 was used for PCR of F5 fimbriae gene [3] . Information regarding the primer names and sequences is provided in Table 1 . E.coli C83529 (F5?), C83903 (F4ab?), C83915 (F6?), C83684 (F18ab?), and TG1 (pMD18T-F41) were cultured using LB medium at 37°C overnight. One milliliter of each E. coli culture were transferred into 1.5-ml Eppendorf tube and centrifuged at 12,0009g. The supernatants were discarded, the cell pellets were suspended in 500 lL TE buffer (10 mM Tris, pH 8.0; 1 mM EDTA; Sigma-Aldrich), and heated at 95°C for 10 min in a dry heating block. After centrifugation at 12,0009g for 2 min, the supernatants were stored at -20°C until use as the DNA template for LAMP and PCR. According to the literature [5] , the LAMP assay was set up in a total volume of 25 lL consisting of 5 lL of 10 lM FIP and BIP primers, 0.5lL of 10 lM F3 and B3 primers, 2.5 lL of 10 mM dNTPs, 2 lL of 25 mM MgCl 2 , 2.5 lL of 5 M Betaine, 2.5 lL of 109 Thermo Buffer, 1.5 lL of Bst DNA polymerase, and 2 lL of DNA (C83529 (F5?)), and the total volume was made up to 25 lL with deionized water. The reaction was performed in a water bath for 1 h at 61°C by stopping at 80°C for 2 min. Then, 3 lL of the LAMP products were separated on a 1.5 % agarose gel. In addition, one microliter of SYBR Green I was added to the reaction tube and the reaction was visualized directly under daylight or under UV light (wavelength: 365 nm). According to the volume in the literature [5] , the LAMP reaction mixtures were incubated at 61, 63, or 65°C to determine the optimal reaction temperature. When reaction was finished, 3 lL LAMP products were separated on a 1.5 % agarose gel to obtain the desired reaction temperature. Then, the LAMP was performed at the desired reaction temperature for 15, 30, 45, 60, 75, and 90 min to determine the optimal reaction time. The reactions were terminated by heat inactivation at 80°C for 2 min. The amplified DNA products from the LAMP assays were visualized by 1.5 % agarose gel electrophoresis as above. The specificity of the LAMP assay was tested with the DNA of C83529 (F5?) strain, C83903 (F4ab?) strain, C83915 (F6?) strain, C83684 (F18ab?) strain, and TG1 (pMD18T-F41) strain. The LAMP system was done as described above and the reaction was performed at 63°C for 45 min. Three microlitre LAMP products were separated on a 1.5 % agarose gel, one microliter of SYBR Green I was added to the reaction tube, and the reaction was monitored directly under daylight or under UV light (wavelength: 365 nm). Sensitivity Analysis of the LAMP Template DNA from C83529 (F5?) (3.6 9 10 8 CFU/ml) was prepared as ''DNA Extraction'' section described and [3] were used to detect limits of LAMP and PCR. When reaction was finished, 3 lL of the LAMP products were separated on a 1.5 % agarose gel, one microliter of SYBR Green I was added to the reaction tube and the reaction was visualized directly under daylight or under UV light (wavelength: 365 nm) as above. Detecting seven simulation samples containing different fimbriae were provided by other staff of this Lab by LAMP and PCR methods [3] respectively. When reaction finished, 3 lL of the LAMP products were separated on a 1.5 % agarose gel, one microliter of SYBR Green I was added to the reaction tube, and the reaction was visualized directly under daylight or under UV light (wavelength: 365 nm) as above. Specificity test, sensitivity test, and detection of simulation samples were conducted in triplicate under the optimized conditions to make sure the development of LAMP is stable. The F5 DNA and specific primers targeting F5 fimbriae genes were included in a LAMP performed at 61°C in a water bath for 45 min. Product of positive reaction with LAMP showed ladder-like pattern on gel electrophoresis, while negative control did not show the characteristic bands (Fig. 1a) . The results showed that the primers were effective and specific. After the addition of SYBR Green I, the product under daylight showed green (Fig. 1b) . In contrast, the negative control shows pale orange; else, the product showed green fluorescent under UV light and the negative did not change (Fig. 1c)  The optimal reaction temperature and time of the LAMP were investigated. No significant difference was observed at different temperatures; however, the intensity of DNA at 61°C showed strongest among all the test temperatures (Fig. 2a) . Reactions were then performed at 15, 30, 45, 60, 75, and 90 min at 61°C. The subsequent results indicated that the DNA products showed the ladder-like pattern literally, when the reaction was performed for 45 min. The optimal reaction condition of the LAMP for F5 fimbriae gene was 61°C for 45 min (Fig. 2b) . The result of detecting by established LAMP were positive only for F5 fimbriae gene, and no positive DNA products of the LAMP assay were observed when these control strains (F4ab, F6, F41, and F18ab) were used as templates (Fig. 3) . The results showed that the detection limitation of LAMP was 72 CFU/tube (Fig. 4a-c) ; in contrast, the PCR has a detection limit of 7.2 9 10 2 CFU/tube (Fig. 4d) . The sensitivity of the LAMP was more sensitive than PCR. Seven simulation samples were tested by LAMP and PCR assay [2] , No. 2 and No. 6 were LAMP-positive samples, as same as PCR results. Test results were consistent with the simulated samples (Fig 5a-d) . Simulation samples' strain names and results were shown in Table 2 . Specificity test, sensitivity test, and detection of simulation samples were conducted in triplicate under the optimized conditions. It is suggested that the LAMP assay established has good repeatability and stability. LAMP is a highly specific, sensitive, rapid, and reproducible gene amplification assay. It is easy to perform, can get a lot of specific amplification products at a constant temperature without the need for complex equipment, and the amplification reaction result can be visually determined by turbidity or by adding dye [10] . LAMP has also been used widely for the detection of pathogens [2, 7, 20, 21] , but determining results by turbidity requires expensive equipment; thus it is not suitable for small-scale laboratory. In our lab, we added SYBR Green I in the reaction product, E. coli has a large-scale bacterial genome, about 4 million base pairs, and some sequences are not published in GenBank. Therefore, LAMP primers designed with known genome may result in false positive if they bind on some unknown genome. To guarantee primers' specificity, we appointed upper PCR primer of F5 fimbriae to F2 in FIP of LAMP primers [3] , then designed following primers online, thus it reduced workload of detecting primers' specificity. In a LAMP reaction, inner primer F2 in FIP hybridizes to F2c in the target DNA and initiates complementary strand synthesis at first. Outer primer F3 slowly hybridizes to F3c in the target DNA and initiates strand displacement DNA synthesis, releasing a FIP-linked complementary strand, which can form a looped out structure at one end. This single-stranded DNA serves as template for BIP-initiated DNA synthesis and subsequent B3-primed strand displacement DNA synthesis, leading to the production of a dumb-bell form DNA, which is quickly converted to a stem-loop DNA by self-primed DNA synthesis [10] . So, F2 in FIP and B2 in BIP first combined with target DNA in a reaction, their specificity have a direct effect on the reaction. Adhesion fimbria is one of the causative agents of ETEC. It mediates ETEC to colonize the small intestinal cell surface and then ETEC produce toxins to cause the host diarrhea; thus, many vaccines about this used fimbriae as the immunogen. Crouch, vaccinated cows with a combined vaccine against rotavirus, coronavirus and E. coli F5 (K99). There was a significant increase in the mean specific antibody titer against all three antigens in the serum of the vaccinated animals which was accompanied by increased levels of protective antibodies to rotavirus, coronavirus, and E. coli F5 (K99) in their colostrums and milk [4] . Rising [13] experiments showed that sows vaccinated with a vaccine against neonatal E. coli diarrhea in piglets containing purified F4ab, F4ac, F5, and F6 fimbriae and detoxified heat-labile toxin (LT) can make an effective protection of piglets against neonatal E. coli diarrhea. Rapid detection and identification of fimbriae's type of ETEC strains isolated from animals with diarrhea contribute to select an appropriate vaccine prepared with right fimbriae antigen for improving the immune protective effect, also contribute for epidemiological survey more convenient. So, we can identify the fimbriae type of ETEC that caused young animals' diarrhea, and we can make prevention and treatment against ETEC diarrhea in the more targeted region [9] . ETEC also causes diarrhea in humans, and the major virulence factors of ETEC strains in humans include colonization factor antigens (CFA) and toxins. Some studies suggest that up to 70 % of strains that cause diarrhea in humans express CFA/I, CFA/II, or CFA/ IV [12, 15, 19] , detection of the fimbriae type is also significant to choose or prepare an appropriate fimbriae vaccine. This study indicates that the LAMP is sensitive, specific, and rapid for detecting F5 fimbriae gene, and the LAMP assay have avoided low specificity, cumbersome operation, special and expensive instruments, and other shortcomings of conventional methods in detecting fimbriae gene. CFA of the ETEC strains in humans may also be detected by LAMP assay. 
paper_id= fff61512ae9f210c73ba42daff77d746dc8c600c	title= Delirium is common in patients hospitalized with COVID-19	authors= Otto  Leiv;  Watne;Kristian  Tonby;·  Aleksander;Rygh  Holten;Theresa  Mariero Olasveengen;·  Luis;Georg  Romundstad;·  Bjørn;Erik  Neerland;	abstract= Publisher's Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations. 	body_text= Delirium is a clinical syndrome with acute disturbances in attention, awareness and cognition, and is a common and severe complication to somatic illness. Several aspects of COVID-19 are known risk factors of delirium, such as hypoxia, inflammation, heavy sedation and mechanical ventilation and it was from the start of the outbreak hypothesized that COVID-19 patients could be at increased risk of delirium [1] . Early case reports informed that delirium could be the first sign of COVID-19 [2] and later published studies report that up to 12-25% of COVID-19 patients can have delirium already at hospital admission [3, 4] . An Italian study of 57 patients with dementia showed that delirium was the first sign of COVID-19 in 37% of the patients. Fever, dyspnea and other typical symptoms of COVID-19 appeared 24-96 h after delirium onset, and the authors suggest that delirium could be a prodromal sign of COVID-19 [5] . Delirium in COVID-19 is associated with increased mortality [4, [6] [7] [8] and the small amount of existing available data also suggest functional impairments after hospital discharge [9] . There are now several studies published on delirium prevalence in COVID-19 and it varies from 11% [8] up to 84 [10] , depending on study population. As expected is delirium most common in severe cases of COVID 19 [10] and in patients with co-morbidities, especially dementia [8] . In this cohort study, we report the occurrence of delirium among all COVID-19 patients admitted to Oslo University Hospital (OUH), Norway over an 8-week period from the start of the pandemic. All patients above 18 years hospitalized at OUH with confirmed COVID-19 until May 1 2020 were consecutively included. They were enrolled in a COVID-19 quality registry ("COVID-19 OUH"), approved by the data protection officer at OUH (Ref 20/08,822). Informed consent was waived in accordance with the data protection officer. Delirium was diagnosed according to the DSM-5 criteria by two experienced delirium researchers (LOW and BEN), using all available data, including results of delirium screening tools and after scrutinizing all sections of the case notes. This included information on previous medical history, dementia status and medication use. All sections of the case notes were scrutinized for key words suggestive of delirium, i.e., inattention, hallucinations, agitation, novel prescriptions of antipsychotics and, most importantly, a description of an acute change in mental status. Each of the two experts first classified the patients independently, and in cases of initial disagreement, they discussed the patient until a consensus was reached. Both hyper-and hypoactive episodes of delirium were recorded. Data were analyzed using SPSS version 26. Patient and Public involvement: the quality registry was established when the first patient was admitted, and lack of time made it impossible to involve patients in the planning. 186 patients above 18 years were admitted to Oslo University Hospital with confirmed COVID-19 between March 6 and May 1 2020. 18 patients were excluded from the study (14 patients were admitted with conditions non related to COVID-19 (e.g., child delivery or elective procedures) and in four patients were delirium status impossible to determine due to continuous sedation and mechanical ventilation until death or final date of follow-up (May 6 2020)). Of the 168 patients included in this study, 10% were delirious upon admission and an additional 19% developed delirium during the hospital stay. Patients with delirium were older and had more polypharmacy and co-morbidities. At admission, they had significantly lower oxygen saturation and a trend towards lower systolic blood pressure and higher body temperature. Significantly more patients with delirium had a quick Sequential Organ Failure Assessment (qSOFA) score ≥ 2 and a Glasgow Coma Scale (GCS) < 15. National Early Warning Score 2 (NEWS2) was also higher ( Table 1) . 41 patients were treated in the Intensive Care Unit (ICU) and of these, 30 (73%) developed delirium. All 25 patients requiring mechanical ventilation developed delirium. In 14 (56%) of these, extubation was postponed because of delirium and four patients (16%) had to be re-intubated. Ten out of 13 patients (77%) never on mechanical ventilation that died, experienced delirium in the terminal phase of palliative care. This study reports prevalence of delirium at a University Hospital over an 8 weeks period at the onset of the pandemic. Our results show that delirium is common, in particular in the ICU. All patients who were mechanically ventilated developed delirium. Long-term sedation using high dose opioids and benzodiazepines is likely to have been a contributing factor. The patients admitted to our hospital were rather young (median age 58 years) and none were admitted from nursing homes. Age, frailty and especially dementia are important risk factors for delirium, and we expect delirium rates to be even higher in a more frail population, i.e., in nursing homes. The findings from our study adds to the existing evidence that delirium is a highly relevant condition in COVID-19 patients for several reasons. (1) Delirium can be an early symptom of COVID-19, as also reported by others [2, 3, 5, 6] . (2) Delirium can make it difficult to deliver optimal care for patients with COVID-19, and we found that extubation had to be postponed in many delirious patients. People with delirium often misinterpret or misunderstand information about necessary procedures. Many are terrified and some have paranoid delusions, and understandably does not always co-operate to treatment, such as ventilatory support. Our findings are in line with a recently published French study of 150 ICU-patients with COVID-19 showing that agitation led to prolonged sedation in 70% of the patients and 8% of the patients (all delirious) experienced accidental extubation [10] . (3) Reports from delirium survivors confirm that delirium can be an extremely distressful experience [11, 12] . This is particularly worrisome since delirium is common in the terminal phase, as was seen in our study. There are no effective pharmacological treatment options, and non-pharmacological interventions are highly recommended. However, such interventions can be especially hard to implement in COVID-19 patients, and mandatory infection control measures (isolation, use of facemask and protective clothes, restriction of visits of family members) may trigger delirium [13] . A recently published study of risk factors for delirium in ICU-patients with COVID-19 found that involvement of family members was the only measure that significantly protected against delirium [14] . This finding highlights that, despite obvious challenges, health care workers should prioritize non-pharmacological interventions also in COVID-19 patients. A limitation of this study was that for many patients, the delirium diagnosis was chart-based. Such a method is known to have reduced sensitivity compared with bedside delirium assessments and some episodes of delirium might therefore have been missed [15] . On the other hand, there is also a risk that a chart-based evaluation inflates delirium rates since all notions of abnormal behavior ("inattentive", "confused", "disoriented") could potentially be interpreted as delirium. The latter is most likely to occur in ICU patients where we found delirium to be very common. However, the French ICU study, with prospective bed side evaluation of delirium, reported even higher delirium prevalence in the ICU than ours [10] . Our study shows that delirium is very common in COVID-19 patients, especially in the ICU. As delirium is associated with long-term cognitive impairment after critical illness we strongly recommend that cognitive assessments are included in the follow-up of COVID-19 survivors. Funding Watne and Neerland were funded by grants from South-Eastern Norway Regional Health Authorities. The authors declare that they have no conflict of interest. Human and animal rights statement The study was approved by the Data Protection Officer at Oslo University Hospital (ref OUS 20/07119). This was an observational study based on routine data and no experimental intervention was performed. All data were de-identified when transferred into the database. Informed consent Informed consent was waived in accordance with the Data Protection Officer. 
paper_id= fff6206f716e0c9c5d06f374217d98d54a76107c	title= 	authors= Daniel  García-Rodríguez;Paloma Remior Pérez;Eusebio  García-Izquierdo;Jorge  Toquero;Ramos  Víctor;Castro  Urda;Ignacio Fernández Lozano;Jorge Toquero Ramos;Víctor  Castro Urda;( D García;Rodríguez ) Bibliografía;	abstract= 	body_text= La pandemia de COVID-19 ha supuesto un reto epidemiológico y para la toma de decisiones diagnósticas y terapéuticas. Los fármacos empleados en pacientes afectados por SARS-CoV-2 conllevan un riesgo arritmogénico asociado con la prolongación del intervalo QT, incluso para los pacientes con un QT previo normal 1 . Registros recientes han confirmado que el tratamiento con hidroxicloroquina (HCQ), en monoterapia o combinada con azitromicina, se asocia con una prolongación significativa del intervalo QT en estos pacientes [1] [2] [3] . Sin embargo, salvo algunos casos comunicados, no está clara su asociación con la mortalidad de causa arrítmica, y algunos estudios apuntan un efecto neto neutro en la mortalidad hospitalaria en tratamiento de la neumonía por COVID-19 4 , lo que hace más necesarios los estudios sobre el riesgo arritmogénico de los tratamientos empleados. Nuestro objetivo es conocer la evolución del intervalo QT al inicio del ingreso y su relación con las combinaciones de fármacos empleadas en el tratamiento de la neumonía por COVID-19. Se analiza la supervivencia en el primer mes en función del grado de prolongación del intervalo QT en las primeras 48 h de ingreso. Se incluyó retrospectivamente a todos los pacientes ingresados en nuestro centro por neumonía por COVID-19 al inicio de la pandemia (marzo de 2020) con electrocardiograma (ECG) basal y a las 48 h tras el inicio del tratamiento (realizados por protocolo en nuestro centro). Todos los ECG se almacenaron en formato digital. El intervalo QT corregido por Bazett (QTc) se midió automáticamente (DXL ECG Algorithm, TMV, Philips, Países Bajos) y lo confirmaron 2 cardiólogos independientes en caso de duda. Se definió como grupo de riesgo a los pacientes con QTc largo > 460 ms o con un incremento (iQTc) > 60 ms. Aunque otros trabajos han considerado como grupo de mayor riesgo el de QTc > 500 ms y han recomendado precaución con QTc > 460-480 ms 4 . La elección de un punto de corte menos restrictivo permitió un seguimiento más estrecho del grupo de riesgo hasta conocer el comportamiento arrítmico en pacientes con COVID-19. Se incluyó a 226 pacientes entre el 1 y el 20 de marzo de 2020. El reclutamiento se detuvo cuando la cantidad diaria de ingresos impidió un seguimiento exhaustivo. Se excluyó a 65 pacientes por no disponerse del ECG basal o el ECG a las 48 h digitalizado, por lo que el número final de pacientes para el análisis estadístico fue de 161. El régimen terapéutico específico más utilizado fue el tratamiento doble con HCQ y lopinavir-ritonavir (LPV-r) (n = 111; 68,9%), seguido del tratamiento triple con HCQ, LPV/r y azitromicina o una quinolona (n = 30; 18,6%). La monoterapia con azitromicina o LPV/r fue la estrategia menos empleada (n = 12; 7,5%). Un total de 8 pacientes (5,0%) recibieron otras combinaciones alternativas. Las dosis de fármacos empleadas son similares a las utilizadas en otros centros: de HCQ se administraron 400 mg/12 h en las primeras 24 h y posteriormente 200 mg/12 h. La dosis de LPV-r fue de 400+100 mg/12 h y la de azitromicina, 500 mg/24 h. Se compararon las variables cualitativas mediante la prueba de la χ 2 y las cuantitativas, mediante la prueba de la t de Student. El análisis de supervivencia fue un análisis actuarial mediante la prueba de Wilcoxon-Gehan. En general, el intervalo QTc a las 48 h fue significativamente mayor que al ingreso (443 ± 30 frente a 435 ± 25 ms; p = 0,001) con un iQTc medio de 8 ± 28 ms. No se encontraron diferencias significativas en el iQTc ni en el QTc a las 48 h entre las distintas combinaciones de fármacos. La utilización de fármacos no específicos para SARS-CoV2 aumentó significativamente del 22,4% al ingreso al 37,9% a las 48 h (p = 0,003), y los añadidos más frecuentemente fueron metoclopramida, ondansetrón, loperamida y levofloxacino. La utilización de estos se asoció con mayor del iQTc comparado con los pacientes que no los tomaron, rozando la significación estadística (13 ± 28 frente a 4 ± 28 ms; p = 0,066). En total, 37 pacientes (23%) fueron clasificados como «grupo con QT de riesgo». La tabla 1 muestra las características basales de este grupo, que fueron similares a las del resto de los pacientes. Un total de 19 pacientes (12%) fallecieron durante el ingreso, ninguno por causa arrítmica. Solo 7 pacientes presentaron QTc > 500 ms a las 48 h y su supervivencia al mes tampoco fue significativamente distinta (86%; p = 0,187). Al mes de seguimiento (figura 1), la supervivencia fue similar entre los pacientes de mayor riesgo y aquellos con intervalo QT no prolongado (el 88% en ambos grupos; p = 0,882). No hubo diferencias en la necesidad de ingreso en la unidad de cuidados intensivos (23 pacientes en el grupo de QT normal y 7 en el grupo de riesgo; p = 0,920). Esta serie concuerda con otras previas y muestra una prolongación del intervalo QT en pacientes tratados con HCQ y azitromicina por neumonía por COVID-19. A diferencia de otras series, en esta la mayoría de los pacientes recibieron LPV-r. No se han detectado diferencias significativas en la prolongación del intervalo QT entre las diferentes combinaciones de fármacos específicos. Es fundamental prestar atención no solo a los fármacos para el tratamiento de la infección, sino también a otros fármacos que se administran durante el ingreso y pueden prolongar el intervalo QT de manera importante. La prolongación del intervalo QT no se asoció con mayor mortalidad durante el primer mes de seguimiento, y no se ha observado mortalidad de causa arrítmica. Características basales de la población del estudio  
paper_id= fff6589edf2f4bc3493ee5bdea0dedcbd4419527	title= Journal Pre-proof Two sorts of microthrombi in a COVID-19 patient with lung cancer. Two sorts of microthrombi in a COVID-19 patient with lung cancer. Corresponding Author	authors= Fiorella  Calabrese;Francesco  Fortarezza;Chiara  Giraudo;Federica  Pezzuto;Eleonora  Faccioli;Federico  Rea;Demetrio  Pittarello;Christelle  Correale;Paolo  Navalesi;	abstract= 	body_text= COVID-19; squamous cell carcinoma; microthrombi; autopsy. A 77-year-old non-smoker male was admitted to the intensive care unit (ICU) of Padova University Hospital for pneumonia with severe respiratory failure. He had been complaining of cough, dyspnea and shortness of breath for 19 days and empirical antibiotics had been previously administered. The medical history reported a kidney transplantation, type-2 diabetes, hypertension and ischemic heart disease. A nasopharyngeal swab was positive for SARS-CoV-2. Chest CT-scan, performed before ICU admission, showed a bilateral pleural effusion, patchy areas of ground glass, reticular thickening of interstitium and right hilar lymphadenopathy ( Fig. 1) . Laboratory findings reported lymphopenia and slightly increased D-dimer (500 µg/L). Despite 15L/min of supplemental oxygen through facial mask with reservoir bag, the patient accused severe hypoxemia (PaO 2 < 55 mmHg) and dyspnea (respiratory rate > 30/min). The patient promptly underwent non-invasive mechanical ventilation through helmet-interface. Pressure support ventilation was instituted with an initial fraction of inspired oxygen (FiO2) of 100% and both inspiratory pressure support and (Fig. 2a, 2c) . Hilar lymph-node metastasis (1 out of 3 examined) was also evident. The remaining parenchyma of both lungs showed diffuse chronic lymphocytic infiltration in alveolar wall. Foci of acute lung injury with diffuse alveolar damage/hyaline membranes, reactive pneumocytes and organizing pneumonia were detected only in left lower lobe. Mild to moderate chronic cell infiltration was detected in small and large airways. The vascular bed showed several intriguing changes: different grades of capillary/small vessel inflammation, in more severe form with endothelialitis and microthrombi (Fig. 2a,   2b ). Thus, in addition to neoplastic microthrombi other types were detected with features of fibrin clots in small and medium size vessels. The ultrastructural examination of nonneoplastic lung parenchyma highlighted the findings of severe endothelial injury with diffuse reduplication of basement membrane. The endothelial cells appeared swollen with numerous instances of cytoplasmic vacuolization (Fig. 2d) . Aggregates of viral-like particles were evident in both type II pneumocytes and endothelial cells (Fig. 2e) . The examination of the other organs was unremarkable for any kind of acute virus-associated pathological processes. The final diagnosis was non-keratinizing squamous cell carcinoma, pT2 N1a, with neoplastic thrombi associated to pneumonia with SARS-CoV-2 related vessel injury. Coronavirus  
paper_id= fff69e4894df7b4134bb2ddc830764459ac3edbe	title= INTERACTION BETWEEN THE SPIKE PROTEIN OF HUMAN CORONAVIRUS NL63 AND ITS CELLULAR RECEPTOR ACE2	authors= Stefan  Pöhlmann;Thomas  Gramberg;Anja  Wegele;Krzysztof  Pyrc;Lia  Van Der Hoek;Ben  Berkhout;Heike  Hofmann;	abstract= 	body_text= Coronavirus (CoV) infection of humans has so far not been associated with severe disease. However, the discovery of the severe acute respiratory syndrome (SARS) CoV revealed that highly pathogenic human CoVs (hCoVs) can evolve. As the characterization of new hCoVs is therefore an important task, we studied the cellular entry of hCoV-NL63, which was recently isolated from patients with lower respiratory tract illness. 1 Entry of CoVs into target cells is determined by the major viral envelope glycoprotein termed "spike" (S), which provides the virions with their characteristic corona-like shape. 2, 3 The main function of the S-protein in CoV entry is the binding to a host cell receptor followed by fusion of viral and cellular membranes. 4, 5 The domains in S that are required for membrane fusion locate to the C-terminal half of the protein (S2 subunit). Receptor engagement is conferred by the N-terminal S1 subunit; consequently, 6 animal and human CoVs exhibit the same functional organization, particularly the S1 subunits differ in amino acid sequence, resulting in interaction with specific cellular receptors. Within group I CoVs, hCoV-NL63 is phylogenetically highly linked to hCoV-229E, and especially the S-proteins of both viruses share a high sequence homology. The S-protein of hCoV-229E is known to employ the human aminopeptidase N (hAPN, also called CD13, herein referred to as hAPN/CD13) for infection of target cells 7 ; therefore, it was speculated that NL63 might use the same receptor for cellular entry. 1 The S-protein is sufficient to mediate CoV entry into receptor-positive target cells and can be incorporated into heterologous viral particles. Thus, CoV S-proteins can be expressed in the envelope of lentiviral particles, and these pseudotyped viruses ("pseudotypes") proved to be a useful experimental system to analyze SARS-CoV-S the S-protein of a given CoV can determine its cell tropism. Although the S-proteins of mediated cellular entry. [8] [9] [10] [11] For production of pseudotypes, envelope-defective lentiviral genomes encoding a reporter gene and a plasmid encoding a CoV S-protein are expressed in cells, which then secrete lentiviral particles harbouring the S-protein in their envelope. These particles infect susceptible cells in a CoV-S dependent manner, and entry efficiency can be quantified by determination of the reporter gene activity. In order to investigate whether NL63-S directly contacts ACE2, we employed a FACS-based binding assay. For this, the NL63-S1 region was fused to the constant chain of human immunoglobulin (IgG), expressed in 293T cells and concentrated from the culture supernatant. Then, NL63-S-IgG was incubated with cells expressing either hAPN/CD13 or ACE2 followed by staining with specific antibodies directed against the respective receptor protein to quantify receptor expression levels and an anti-human Fc antibody to detect the bound IgG fusion protein. Hereby, we demonstrated that NL63-S in contrast to 229E-S does not react with hAPN/CD13, but like SARS-CoV-S directly binds to ACE2. Finally, we analyzed the NL63-S interaction with ACE2 by employing ACE1 or ACE2 specific polyclonal antisera. Only the ACE2 serum interfered with infection by NL63-S-bearing lentiviral pseudotypes and replication-competent hCoV-NL63, thus confirming that ACE2 is a receptor for hCoV-NL63. Replication competent hCoV-NL63 has been cultured on tertiary monkey kidney cells, 1,12 but permissive human cell lines had not been identified so far. Using lentiviral pseudotypes, we therefore analyzed a panel of human cell lines for susceptibility to hCoV-NL63 S-mediated entry (chapter 4.2). Interestingly, the cell tropism measured for NL63-S was congruent with that observed for SARS-CoV, 8,10,11 but differed significantly from that measured for 229E-S, suggesting that both glycoproteins might interact with different receptors despite their high amino acid identity. Furthermore, we were able to show that hCoV-NL63 does not engage feline aminopeptidase N (fAPN) in contrast to all CoVs of class I investigated so far. 13 However, cells expressing the SARS-CoV receptor protein ACE2 14, 15 were susceptible to NL63-S driven infection. This was unexpected as NL63-S has no striking homology to either the whole S1 subunit of SARS-CoV or the already identified ACE2 interaction domain in SARS-CoV-S, 16 suggesting that both proteins either form a common three-dimensional structure that allows ACE2 engagement in a similar fashion or that both S-proteins evolved different strategies to target ACE2. The interaction between NL63-S and ACE2 was specific, as the closely related ACE1 protein did not react with NL63-S, and on the other hand, ACE2 was not able to confer 229E-S-mediated infection, suggesting that ACE2 is not a functional equivalent of fAPN in class I CoV entry. As mentioned above, the S-proteins of hCoV-229E and hCoV-NL63 share an overall amino acid identity of more than 50%. However, hCoV-NL63 S harbors a 178 amino acid N-terminal extension that is not present in 229E-S or any other known protein and that is therefore designated "unique domain". 1 In order to investigate if the unique region is involved in ACE2 binding, we first analyzed binding of the isolated domain to ACE2 expressing cells in a FACS based binding assay. However, the unique domain alone did not show any interaction with ACE2, indicating that it does not serve as an independent receptor binding domain. Because it is possible that the unique domain might confer ACE2-binding together with other sequences in the S1-domain of hCoV-NL63 and possibly hCoV-229E, we constructed a chimeric mutant comprising the N-terminal 178 amino acids of NL63-S fused to the S1 subunit of 229E. This mutant, however, showed no ACE2-interaction, but bound to hAPN/CD13 as efficient as the wildtype protein, indicating that the unique domain does not interfere with hAPN/CD13 recognition and does not allow binding to ACE2. When the unique domain was removed from NL63-S, the remaining protein still bound ACE2 and showed no affinity for hAPN/CD13, confirming that the unique region is dispensable for ACE2 binding. In summary, these observations indicate that amino acids in the highly conserved S1 regions of NL63-and 229E-S confer specificity for the interaction with ACE2 and hAPN/CD13, respectively. In order to map which region in the NL63 S1-protein is responsible for targeting ACE2, we analyzed a panel of N-terminal S1-deletion mutants. By this, we were able to narrow down the ACE2 interaction domain in NL63-S to amino acids 232 to 741. In parallel, we constructed chimeric S1-proteins comprising defined regions of NL63-S fused to complementary domains within 229E-S and analyzed them for hAPN/CD13 and ACE2 interactions. We found that several sequence elements in the center and Cterminus of the proteins can impact receptor binding, suggesting that some of these mutations might interfere with the integrity of the possibly complex three-dimensional structure of the proteins and thus with their capacity to recognize receptors. Our observations are in agreement with a model suggesting that the central region in the hCoV229E-S and possibly NL63-S proteins might determine the correct folding or orientation of a C-terminal receptor binding domain, as has been suggested previously for hCoV-229E-S. 17, 18 Taken together, a detailed point mutagenesis of NL63-S1 will be required to identify residues with a critical function in ACE2 interaction. We were able to show that the SARS-CoV receptor ACE2 is used by the recently identified hCoV-NL63 for entry into target cells. Simultaneously, the same observation of NL63-S with ACE2 was unexpected, as NL63-S and SARS-CoV-S share no significant amino acid homology. In contrast, NL63-S is highly related to the glycoprotein of hCoV-229E, which binds hAPN/CD13, and the hAPN/CD13 interaction domain is well conserved in NL63-S. The most striking difference between the S-proteins of hCoV-NL63 and -229E is a 178 amino acid extension that is exclusively present in NL63-S. This unique domain, however, is dispensable for ACE2-interaction; thus, amino acids in the highly conserved central portions and C-termini within the S-proteins of hCoV-229E and hCoV-NL63 determine the recognition of their respective receptors. Therefore, detailed point mutagenesis in combination with the determination of the threedimensional structure of both S-proteins is required to identify amino acids that mediate receptor binding. These data in turn will help to develop specific small molecule inhibitors against NL63-S-mediated infection. 
paper_id= fff6a09dd235dd353ff13bc7f4843f2b6a06a278	title= Criticality and Higher Education	authors= 	abstract= In this chapter, we are interested in how a critical-social educational theory can most appropriately advance human existence. To this end, we draw on Seyla Benhabib's (Critique, norm and utopia: a study of the foundations of critical theory. Columbia University Press, New York, 1986) re-articulation of Jürgen Habermas's notion of critique to ascertain how the practice of academic activism ought to be amended. According to Benhabib (Critique, norm and utopia: a study of the foundations of critical theory. Columbia University Press, New York, 1986: 279), Habermas's theory of communicative action is a justifiable form of critique as it re-establishes the relation between self-reflection and autonomy and it explains autonomy in communicative terms, that is, autonomy is not synonymous with selflegislation or self-actualisation or mimesis, but rather 'the cognitive competence to adopt to a universalist standpoint and the interactive competence to act on such a basis' (Benhabib, Critique, norm and utopia: a study of the foundations of critical theory. Columbia University Press, New York, 1986: 282). In relation to the notions of self-reflective autonomy and communicative autonomy, we examine what a philosophy of higher education looks like and what the implications of such a form of critique hold for academic activism. 	body_text= In the previous chapter, we conceived of academic activism as forms of reasoned human agency, actions or encounters -intent upon shaping and producing better humans and humane engagement and focused upon social justice. It seems apt to follow this conception with how a theory of critique -more specifically criticalsocial educational theory -can most appropriately advance human existence. To us, the matter of critique is a crucial one, especially so when it is seemingly omitted from the cultures and discourses of (South African) universities. While much attention has been given to the necessity of deliberative engagement and the cultivation of decolonised pedagogical knowledge spaces, very little, if any, such concern has centred on critique, bringing into question notions of a thinking university. Remaining with the interpretivist paradigm, established in Chap. 1, in this chapter, we turn to Seyla Benhabib's theory of critique. Standing in the traditions of Kant and Habermas, Benhabib (1986) articulates an understanding of critique that makes a necessary connection between rationality and autonomy. That is, any form of analysis of the social world is an act of philosophy (of higher education) and that such an analysis is dependent upon 'its commitment to the dignity and autonomy of the rational subject' (Benhabib, 1986: 15) . According to Benhabib (1986: 279) , Habermas's theory of communicative action is a justifiable form of critique as it reestablishes the relation between self-reflection and autonomy. It also explains autonomy in communicative terms -that is, autonomy is not synonymous with selflegislation or self-actualisation or mimesis, but rather 'the cognitive competence to adopt to a universalist standpoint and the interactive competence to act on such a basis' (Benhabib, 1986: 282) . In relation to the notions of self-reflective autonomy and communicative autonomy, we examine what a philosophy of higher education looks like and what the implications of such a form of critique hold for academic activism. Benhabib (1986) articulates an understanding of critique that stands in the traditions of Kant and Habermas: firstly, by making a necessary connection between rationality and autonomy in the sense that any form of analysis of the social world is an act of philosophy (of higher education) and secondly, that such an analysis is dependent upon 'its commitment to the dignity and autonomy of the rational subject' (Benhabib, 1986: 15) . The mode in which social life appears, contends Benhabib (1986: 4) , 'is an indication of the extent to which individuals are alienated from their social praxis'. Certain groups of students at South African universities, for example, might have gained external access to institutions from which they have been historically excluded, but the manner (or mode) in which they experience these institutions might place them at a distance, rather than evoking a sense of belonging. Their direct experiences of their encounters with the space and discourse of the institution serve only to remind them that their physical presence is not an indicator of internal inclusion and recognition. That they continue to stand on the peripheries of their university experience -whether in lecture theatres or the social spaces of student residences -confirms that their historically reduced identities have yet to find resonance and a place of belonging within their educational spaces. When they protest against this displacement and alienation -as a manifestation of critique -their actions, following Benhabib (1986: 4) , do not 'merely disclose the dependence of thought upon social being, of consciousness upon material praxis'. Rather, their actions also criticise 'this dependence from the standpoint of the struggle for the future'. The students recognise that their silence will serve not only as the further entrenchment of their exclusion but will legitimise these practices as a permissible norm. Too often, students and academics alike resort to a language of 'this is how things have always been' as a justification for questionable behaviour. Consider, for example, the widespread practices of initiation, often defined and accepted as the bedrock of universities residences. In 2001, the Minister of Education requested an inquiry into cultural initiations, following a number of deaths at schools and university. One of these cases involved that of a second-year student, who had been forced to participate in an initiation ritual at the university where we are based. After having his hair shaven off, being stripped naked and having his body painted, the student was dropped off outside of town and forced to walk back to the university hostel. While walking back, he was hit and killed by a car (SAHRC, 2001) . The inquiry by the South African Human Rights Commission (2001: 16) found that although university administrators argued in support of the maintenance of initiation practices, 'as part of the tradition and culture of the university', such views were not articulated by student groupings and some staff members who spoke to the SAHRC outside of the meetings with the administration (SAHRC, 2001: 16) . These student groupings and staff members favoured the abolishment of initiation practices. They described these processes as 'human rights abuses and in conflict with a democratic culture, and as having more negative aspects to them than positive' (SAHRC, 2001: 16) . At another South African university, an initiation ritual involved female and male students from residences devising a song and 'serenading' each other. While the initial intention was to enable students to get to each other, 'song lyrics became increasingly suggestive, and sexy dance moves crept in, along with questionable dress requirements' (De Klerk, 2013: 91) . In a letter to the Dean of Students and the Vice-Chancellor, a student described 'serenading' as follows: [L]ong compulsory evening practices demanding increasing levels of frenzy and excitement from performers; being sent out in pyjamas to watch boys dressed in T-shirts and boxers sing to them, accompanied by 'pelvis rolling and crass lyrics'; being instructed to serenade a men's res in return ('we roll our butts like strippers, the boys cheer. We push our breasts out, shake our hips, and gyrate our pelvis. All in accordance with the routine we have been taught. And the boys yell, 'Yeah!' as their eyes pop'). … an 'icebreaker' session involving women's room keys being anonymously handed to the boys…. (De Klerk, 2013: 91) What initiation rituals serve to do is to clarify and establish the 'rules' of a particular residence or university; it serves to dictate the conditions for student acceptance, inclusion and belonging. What it entrenches is a divide between those students who participate, and are, therefore, included, and those who refuse, thereby confirming their 'outsider' status. Not only is it a highly exclusionary practice, but it sets into motion certain norms regarding how students are to be treated going into the future. When students, parents as well as academics and administrators protest against initiation practices, which, in many instances, are long-standing and considered as part of the traditions of the university, the resistance is not only against a present-day dilemma but against what the implications are for future generations of students, higher education as well as social and societal norms. Firstly, to Benhabib (1986) , critique aims to grasp the present as a contradictory totality in which different normative ideals lend ideological justification to a social objectivity that oppresses human beings and frustrates their human potential; the goal of critique is to further the autonomy of the subject. The autonomy of the subject cannot be furthered or extended without 'the philosophical task of clarifying and reconstructing the norms to which criticism appeals' (Benhabib, Butler, Cornell & Fraser, 1995: 64) . As such, for social critique to be valid, and for relativism to be avoided, critique must appeal to some normative criteria that exist outside the critic's own context. Critique, therefore, allows for the individuals to enact their autonomy and agency by bringing into question certain norms, certain taken-for-granted practices -such as institutional traditions or modes of engagement, as encountered in initiation rituals, for example. This is not necessarily an antagonistic process, as the purpose of bringing into question can sometimes be limited to simply making the other aware of a presumed practice, which has not been subjected to scrutiny before. The student who wrote to the Dean of Students and the Vice-Chancellor to raise her concerns about 'serenading' did not enter into any disagreement or conflict with other students or the university administration. Instead, her point 'was made forcefully' by making the institution aware that 'by turning a blind eye to sexist behavior which is heterosexually normative, the institution could be accused of condonement, if not encouragement, of such behavior' (De Klerk, 2013: 92) . As Marriott (2018) clarifies, '[c]ritique legislates the judicious use of reason by separating it from any metaphysical or dogmatic origin, so that any risk of being carried away by the fictitious or merely pleasing is curtailed by the rule of philosophical judgement'. It is only when philosophy becomes critique, he continues, that it is then able properly to articulate reason and what is essential to it. In this sense, critique is constituted as a defence against or a victory won over that of unreason: a victory that, conversely, shows critique to be always shadowed or at risk from the various lapses that would founder it. For example, public schools in South Africa are given leeway in terms of defining their admission criteria for learners. Despite constitutional guidelines prohibiting any form or grounds for exclusion, a number of historically advantaged schools continue to employ criteria, which include granting preference to learners, who have historical ties with the respective school. In other words, applicants are explicitly asked as to whether previous generations of their family had attended the school. On the face of it, this seems like a rather mundane question, but in the context of South Africa's segregated schooling history, this question has direct implications for learners, whose parents and grandparents would have been disallowed from attending these schools. When one critiques what the school might couch as one of its traditions, it is often startling to realise how the schools do not necessarily see this criterion as a potential means of exclusion. Instead, there is a vociferous defence of long-held traditions and customs, even when these are blatantly discriminatory and socially unjust. Attempts to question and reframe schools as cohesive and inclusive spaces are interpreted as disruptive actions or as 'take-over' measures to nullify historical codes and ways of being. Similar expressions of critique can be levelled at initiation practices of certain universities, which are consciously tied to the preservation of traditions, which might hold unintended consequences of alienation or estrangement among students, who enter these spaces from different historical and social backgrounds. Only once the practice is explained from the perspective of those who do not have generational ties does the school or the university possibly begin to see the harmful effects. Of course, being made aware of something does not necessarily result in that practice being reconsidered or ceased, but what critique facilitates and brings to the fore is not only different perspectives and perceptions but the confirmation of multiple truths and lived experiences. In this regard, critique can be used and understood as a form of awareness, as a means of getting individuals to engage from the perspective of the other -hopefully with a purpose of detachment so that new forms of understanding might emanate. Critique, when understood in this way, adopts an activist premise in that it is aimed at questioning for the sake of awareness and reconsideration; it is an active process not concerned with labelling something as right or wrong, but in bringing something into contestation for the purpose of reconceptualisation. Following the above, we would venture that the goal of critique is not only to further the autonomy of the subject, as contended by Benhabib (1986) , but also to extend the perspectives and meaning-making of contexts -thereby broadening the scope for both participation and disagreement. When individuals or groups critique an ideology, or an institution and its practices, it invites or forces the institution to critically reflect on that which is being critiqued -with the possibility that new outcomes might be attained and that more voices might be taken into account. Secondly, critique also aligns itself with 'the struggles of those [people] for whom the hope of a better future provides the courage to live in the present' (Benhabib, 1986: 15) . In this way, critique is concerned with re-establishing the link between autonomous rationality and emancipation, that is, 'of making good the unfulfilled promise of justice and freedom' (Benhabib, 1986: 329) . Her description provides an apt background to how those who are oppressed or discriminated against persevere -only because of a hope that their condition will change. There is a certain resignation of tolerating a particular struggle, but with the belief that the future will be different. By implication, what critique brings to a philosophy of higher education is a moral imperative that accompanies human interactions such as those grounded in a sense of community where human responsibility, care and solidarity are no longer just confined to the private sphere of intimacy but extended to the public sphere of needs and solidarity. In the recent #FeesMustFall protests, a number of students expressed their resistance and opposition to exorbitant university fees by not attending classes, not submitting assignments and not writing tests, as well as examinations. They knew that their actions would hinder their academic progress and, in some cases, compromise their registration in particular programmes. Yet, they persisted in their protests, recognising that unless the issue of fees was adequately addressed by the state and universities, future generations of students would be subjected to the same or greater financial burdens. These students understood that when students are turned away or drop out of universities because of an incapacity to pay fees, they are facing similar kinds of exclusion experienced during apartheid. Although framed differently -as in not being tied to race and rather to finances -the effect and implications of exclusion are the same. The same category of students, who were historically excluded because they were 'black', are also being excluded in a democracy because they do not have the economic capital to access or to remain in higher education. Yet, the reason for a lack of economic capital is directly connected to apartheid's dispensation of racial inequality and inequity. It is for this reason that students experience anger and frustration -they see the democratic government as discounting the debilitating and residual effects of apartheid. These students are prepared to place their own studies at risk -not only for themselves but for all others, who might stand to benefit from a reduction in university fees. To them, the cause is greater than individual needs, and they see their opposition as a continuing narrative against the kinds of injustices perpetuated during apartheid. What this viewpoint confirms is that inasmuch as activism can be tied to self-interests, it can just as easily represent actions on behalf of others. There are broader social, political and moral issues at play. Thirdly, for Benhabib (1986: 342) , critique brings a communicative ethic to human relations based on 'the belief that the exercise of human reason is essential to the attainment of moral autonomy and fulfillment, public justice and progress' (Benhabib, 1986: 344) . As she sums up her advocacy for critique, the ideal community of communication allows 'the unfolding of the relation to the concrete other on the basis of autonomous action. Only then can we say that justice without solidarity is blind, and freedom that is incompatible with happiness, empty' (Benhabib, 1986: 342) . Put differently, critique is concerned with the creation of a 'new selfand other-relations that lie beyond the logic of capital, and of administrativebureaucratic and technical rationality' (Benhabib, 1986: 343) . By implication, critique relooks at emancipation in such a way that it is no longer just concerned with the 'democratization of administrative decision-making processes but [also] the formation of communities of need and solidarity in the interstices of our societies' (Benhabib, 1986: 353) . In short, critique is concerned with both the cultivation of a self-reflective and communicative autonomy -as is evident in the actions of students who oppose exorbitant university fees and who are intent upon realising and living in a socially just democracy. Mainstream conceptions of autonomy, explains Friedman (2003: 87) , share a common core understanding: first, reflection of some sort on relevant aspects of the self's own motivational structure and available choices and second, procedural requirements having to do with the nature and quality of that reflection (as in being sufficiently rational and uncoerced). While Benhabib centralises the concept of autonomy in her writings, she emphasises that the subject is constituted through social relations, as such, is embedded in a concrete lifeworld. Autonomy is not autocracy, she asserts, 'but rather the ability to distance oneself from one's social roles, traditions, history, and even deepest commitments and to take a universalistic attitude of hypothetical questioning toward them' (Benhabib, 1999: 353-354) . To Benhabib (1985: 91) , 'autonomy is not only self-determination in accordance with just norms but the capacity to assume the standpoint of the concrete other as well'. She maintains that an individual's autonomy does not simply exist. Rather, 'the subjects of reason are finite, embodied, and fragile creatures, not disembodied cogitos or abstract "unities of transcendental apperception" ' (1994: 174) . To Benhabib (1994: 174) , it is only by learning to interact in a human community that the human infant becomes a self, a being capable of speech and action. The identity of the self, explains Benhabib (1994: 174) , is constituted by a narrative unity that integrates what 'I' can do, have done and will accomplish with what you expect of 'me'. Autonomy, therefore, following Benhabib, emerges from and takes shape through a dialogical engagement with others. The human infant, she explains, 'becomes a person through contingent processes of socialization, acquires language and reason, develops a sense of justice and autonomy, and becomes capable of projecting a narrative of which she is not only the author but the actor as well' (Benhabib, 1994: 174) . Students, for example, do not simply embark on protests; there is a particular context of socialisation, of recognition and, more often, of misrecognition, which pushes them towards resistance. What the context is or how students arrive at a decision to voice their resistance is not necessarily a shared understanding. In fact, very often, that or those being protested against do not necessarily understand the reasons for the resistance due to a myriad of reasons. Sometimes, there is simply a schism between the lived experiences of the individual and the concrete other. While the #FeesMustFall and #RhodesMustFall campaigns took place at all universities in South Africa, the intensity of these campaigns varied in relation to the particular historical identities, as well as the presence of a critical base of 'black' students. The majority of 'white' students could not relate to the demands for the removal of the statue of Cecil John Rhodes from the University of Cape Town. In turn, many privileged students have little understanding of food insecurity experienced by a significant number of students. The general perception is that when students receive financial support, they have enough funding to cover all their needs. Funding, however, has specific designations and limitations, and students are most vulnerable at the commencement of the academic year when they are waiting for funds to be released and at the end of the academic year, as funds are depleted. At the time of writing, students at a few universities demanded the delay of the online academic programme, which has been forced upon universities due to the COVID-19 national lockdown. At first glance, and if one is unaware of the concrete others, one might think that any attempt to further delay an academic year, which is already at risk of not being completed, is an uninformed and frivolous call. But, upon careful consideration, the demand might not be that misplaced in light of the high number of students, who have access to neither digital devices nor the Internet. How, therefore, can teaching and learning proceed when the educational field is so unequal? Other times it has to do with a deliberate oppression of the other, where there is a genuine belief of superiority, based on race, religion, class or culture, and hence a deliberate refusal of acknowledgement or equal recognition. Then, of course, there are indeed actions, which might not be considered as reasonable or justifiable, such as students embarking on wanton practices of violence and vandalism, as forms of protest. However distorted and imperfect human reasons may be, such reasons constitute the basis of self-autonomous actions. In a way, reasonable analyses are not just literary criticism, aphorism or poetry but are enlightened or practical reasons proffered in justifying the legitimacy of social actions (Benhabib, 1986: 329) . In other words, reasons have been thought through concerning social and historical contexts in which they manifest. In the case of students' resort to violence, justifications stem from similar actions used to overthrow the apartheid government. And while the ideological context might have changed to that of a democracy, the experiences of apartheid remain present in the continuing unequal and, at times, hopeless lives of the majority of black people. Those who engage in the philosophical activity of claiming reasons on the basis of discourses of argumentation, disputation, debate and adjudication have actually used their self-autonomous agency to do so. Following her take on such a Habermasian notion of self-reflective autonomy, Benhabib (1986: 305) contends: Every agent capable of speech and action can participate in discourses … Everyone may problematize any assertion … Everyone may introduce every assertion into a discourse … Everyone may express his [her] attitudes, wishes, and needs … No one may be prevented from enjoying her above-outlined rights in virtue of constraints that may dominate within or without discourses. Individuals, therefore, firstly, must have an equal chance to initiate and to continue communication; secondly, they must have an equal chance to make assertions, recommendations and explanations and to challenge justifications; thirdly, they must have equal chances as actors to express their wishes, feelings and intentions; and fourthly, they must act as if in contexts of action there is an equal distribution of chances to order and resist orders (Benhabib, 1985: 87) . She explains that while the symmetry stipulation -captured in the first two actions -of the ideal speech situation 'refers to speech acts alone and to conditions governing their employment, the reciprocity condition refers to existing social interactions and requires a suspension of situations of untruthfulness and duplicity on the one hand, and of inequality and subordination on the other' (Benhabib, 1985: 87) . In the case of a university engaging with students regarding gender-based violence, for example, the primary concern of the university cannot be about protecting the reputation of the institution, or about reformulating policies, when the issue at hand is the actual safety and security of students. Moreover, it does not help to engage with students in this regard as if the university has more information or has a better direct experience of the debilitating impacts of gender-based violence. If the scourge of gender-based violence is to be addressed, then there has to be an institutional preparedness to reconceive it as a social justice issue, which impacts on the rights of individuals (not limited to women), to engage freely and without fear in the public sphere. In our opinion, what this understanding implies is that questions need to be asked about how the university's public sphere and institutional culture are used to perpetuate sexist or exclusionary dogmas. What needs to be disrupted, therefore, are not only acts and perpetuations of gender-based violence but the systemic structures that allow gender-based violence to unfold, in the first instance. Benhabib's (1985: 87) third and fourth conditions of the ideal speech situation, namely, that individuals must have equal chances as actors to express their wishes, feelings and intentions and they must act as if in contexts of action there is an equal distribution of chances to order and resist orders, are especially pertinent for academic activism as a manifestation of academic freedom. Not only does her assertion that all individuals ought to be included, or afforded the opportunity for speech, but that this chance be availed equally. Despite its foundational centrality within universities, academic freedom remains a highly contested matter -with disagreements easily descending into disturbing patterns of 'disinvitation' of speakers or academics, or forms of disruption, involving heckling and ridiculing, which ought to have no place in a university. Moreover, it is often the case, by virtue of institutional hegemonies, that some individuals have more rights and, hence, more voice than others. These hegemonies are as prevalent in the public sphere of a university, as they are in lecture theatres, or within the supervision process between an academic and his or her student. Students from minority groups often struggle to assert themselves among their peers, or they fear asking questions in case the question might be deemed as stupid. In turn, we are aware that 'black' students, and in particular 'black' students at our institution, who come from other parts of the African continent, might not enjoy the same levels of support as other students. Their presence at the university is not only constitutive of a minority group but is heavily laden with historical and contemporary politics, which make their navigation on campus and through the academic programme especially cumbersome. A number of African students are not only at our institution because they wish to pursue a degree at a South African university, but they are also there because of civil unrest, ethnic conflict and political upheaval, which make it near to impossible for them to consider higher education in their own countries. What Benhabib (1985) , however, is proposing in terms of countering any inherent inequalities is a presumption of equality -specifically phrased by her as: 'the speakers must act as if in contexts of action there is an equal distribution of chances'. It is an argument similarly propagated by Rancière (1999: 19) when he describes equality as 'empty freedom' that everyone possesses. Rancière (2002) conceives of equality as a quality of the individual, rather than society. As such, the individual has the political agency and autonomy not only to counter inequality (without having to wait on society) but also to lay claim to equality, when this right is being deprived. As such, it should not matter that students come from different backgrounds, which historically might have been constructed along racial and racist categories, or that black African students might be treated differently to other foreign students. What matters is that individuals conceive of themselves as equal to all others and presume that the context in which they find themselves will see and engage with them as equals. Integrating an ethics of critique, academic activism embraces a 'politics of empowerment' that seeks to combine 'the logic of justice with that of friendship' (Benhabib, 1986: 352) . Here, we specifically think of doing 'academic' things together -reading, writing and talking -for the sake of not only producing justly spirited scholarly works, for instance, a co-authored text or edited collections, but also creating anew a community of friendship and solidarity that would allow such a newly found community to fulfil its emancipatory potential more prudently. Of course, academic activism does not assume only a politics of intersubjectivity as if anything and everything should be done in the interest of the community. Rather, such activism also assumes what Benhabib (1986: 351) refers to as 'the politics of collective singularity'. In this sense, critique endorses both intersubjective and combined singular human actions that can empower and emancipate humans. Academic activism seen in such a light confirms not only people's humanity but also their human individualities. When academic activists look at the claims of the 'concrete other', they 'view each and every rational being as an individual with a concrete history, identity, and affective-emotional constitution … [through which] we abstract from what constitutes our commonality and seek to understand the distinctiveness of the other … [and] seek to comprehend the needs of the other, their motivations, what they search for, and what they desire' (Benhabib, 1986: 341) . Often we find ourselves working with multiple contributors to a book, and through our activist stances, we seek to support contributors through which others feel recognised and confirmed as concrete individual beings with specific needs, talents and capacities. In a way, our differences in joint academic work complement rather than exclude one another. The norms of our engagement are constituted by solidarity, friendship, love and care. In turn, much of our work revolves around our encounters with our students -whether in lecture theatres in a teaching-learning scenario or as supervisors of master's or doctoral studies. While teaching and learning involve its own set of processes and dialogical encounters, the supervision process has its own rhythm. While separate and distinct in terms of qualification and socialisation, both processes (teaching and supervision) speak to the emancipation of students. Both teaching and supervision are aimed at the evolution of something and someone new. In the case of the Postgraduate Certificate of Education (PGCE) students completing the philosophy of education module, the intention is to invite them into a space of reflection, deliberation and hopefully disruption, as they detach from seeing the world in a singular fashion. Our activism as teachers is made visible to the extent to which students feel empowered through engaging in their learning; it lives in pedagogies of simultaneous disruption and comfort as students feel safe in engaging with that which is unfamiliar and unknown. We have found that even in large classes, it is possible to create spaces of deliberation, friendship and emancipation, not only between ourselves and students. We find that when we venture into that which is less topical or makes students uncomfortable (such as the prevalence of racism, homophobia or xenophobia), students who ordinarily would not engage with each other often end up in debates. In engaging with and from different perspectives, they inadvertently emancipate themselves from particular perspectives, which might have narrowed their worlds, rather than broadening them. To Hytten (2017: 386) , classrooms are activist spaces -it is where knowledge is contested, habits are developed and communities are created -even if teachers are not critically conscious of the impacts of their pedagogical choices, efforts and relationships. Hytten (2017: 386) raises concerns, however, about the risks of indoctrinationwhen teaching becomes so partisan that it slips into the propagation of particular views and beliefs as if these are the only truths. When does activism cease and become indoctrination? While it might be good for academics to feel strongly about this or that issue, and while they might want their students to develop similar views and stances about these issues, there is a legitimate risk of an infringement of a particular worldview onto students. Indoctrination, states Merry (2018: 165) , works contrary to education -'not only because it entails the inculcation of unshakable beliefs or commitments, but also because it involves the inculcation of unwarranted beliefs and commitments -that is, those unsupported by reasons and evidence'. Quite differently, however, academic activism should not infer the inculcation of unwarranted beliefs. As forms of reasoned human agency, actions or encounters, it is continuously aimed at cultivating autonomous agents. Autonomy, as Callan (1997: 118) argues, 'enables us to choose intrinsically good lives; autonomy confers that ability without creating a bias against any particular ways of life that might have intrinsic value'. The challenge for activist teachers, as Hytten (2017: 386) asserts, is to adopt moral stances, to frame curricula and to design pedagogical activities around social justice values and commitments, but without stifling students' genuine inquiry. Students should feel neither compelled to buy into the particular views of an academic nor intimidated or afraid to disagree with an academic. In other words, academic activism should not fulfil the purpose of swaying students to this or that position. Instead, academic activism should make students alert to broad possibilities and perspectives and should be unconstrained and unregulated in terms of the positions or opinions they wish to adopt. Depending on the kind of relationship between a supervisor and student, the supervision process can allow for more intimacy in terms of contact and time and, hence, a better sense of the other. The one-on-one construction of supervision lends itself to the potential formation of a particular kind of relationship, which can exceed the mere advising and promotion of this or that thesis. Of course, it is entirely possible for this encounter to be wholly mechanical -dependent on designated consultation slots, drafts, comments, revisions and more drafts. A supervision encounter, as a form of academic activism, foregrounds the relationship between the supervisor and the student. On one level, the encounter suggests an implicit relationality, which necessarily hinges on responsibility and responsiveness from both the supervisor and the student. The student's thesis writing requires supervision. As such, he or she is in need of a supervisor who will act responsibly in relation to the student by responding to the student's writing. In turn, in order to supervise, the supervisor is in need of a student who will act responsibly by first producing the text, and then responding to the feedback and guidance, as provided by the supervisor. The relationship does not only centre on getting to know each other through the exchange of texts, but developing a friendship in the Derridian sense. To Derrida (1997: 204) , friendship is 'a sort of trust without contract'; it consists of loving, rather than being loved or expecting to be loved (Derrida, 1997: 235) . Friendship is constituted by extending oneself to another, without any expectation of reciprocity and without being befriended in return. To Derrida (1997: 62) , friendship demands a 'certain esteem of the other without genuine intimacy'; genuine friends are independent, autonomous and selfsufficient -that is, they 'keep an infinite distance' (Derrida, 1997: 63) . In sum, friendship is neither relational nor mutually contingent. In extending friendship to the student, the supervisor actively works towards not only supporting and guiding the academic success of the student but also inviting and mentoring the student into an academic discourse and scholarship. Accompanying this mentoring is a willingness to engage with students on matters beyond the academic project, to support in ways that speak to the personal circumstances of students, which make the supervision process more complex, but also potentially more rewarding. When academics or supervisors endeavour towards cultivating relationships with students, they do so on the basis of trust and a professional commitment to eliciting a relationship, or friendship, which will ensure the best options for students -whether in terms of academic achievement or human worth. In the interest of our academic efforts of the community for the sake of need and solidarity, we do not encourage one another to abandon our self-autonomy to question the interpretive assumptions of work. Through reflexive questioning, we not only articulate our needs as individuals but also engage with others about their needs. In this way, our self-reflection (autonomy) does not take us away from looking critically at our own work but rather invites us to communicate and engage in deliberation with other authors about their work. In this regard, Benhabib (1986: 333) posits '[t]he linguistic access to inner nature [of our academic work] is both a distancing and coming together … that we can name and drives what motivates us, we are closer to freeing ourselves of its power over us; and in the very process of being able to say what we mean, we come one step closer to the harmony of friendship of the soul with itself' (Benhabib, 1986: 333-334) . Thus, for us, academic activism in scholarly writing is at play when our communication of that which interests us is open and reflexive in relation to our needs and cultural persuasions so that our autonomy as persons is for once influenced also by the standpoints of the concrete others in our academic projects. As Benhabib (1986: 335) argues, '[a]lthough all finite agents describe their own well-being, each defines this well-being in a different way'. This implies that academic activism not only takes different forms and expressions, but the expectation is always there that these different perspectives of academic activism might come into disagreement and conflict. In this sense, while one academic or student might feel strongly about academic freedom as an unconstrained tenet of a university, others might feel equally strongly that this tenet always is conditioned and curtailed by the potential harm of such speech. In a recent example, it became apparent that an invited speaker at a university gathering intended to speak on the harmful effects of academic freedom. An activist group -comprising students and academics -wrote to the invited speaker, insisting that he reconsider the argument of his presentation. Oddly enough, the activist group did not conceive of their request as an infringement of academic freedom or as a constraint of the speaker's autonomy. Ironically, in writing the letter out of concern for what the activist group interpreted as the speaker's undermining of academic freedom, the group slipped into their own argument by insisting that the speaker aligns his argument with their views. The moment an individual's autonomy is narrowed or limited, the concrete other is suppressed, which means that new ideas and arguments can neither unfold nor be challenged. Such an understanding or action is as contrary to academic freedom as it is to academic activism. 
paper_id= fff6bbe9c980a69ad4cca02b70831470d265f6fd	title= Prediction of remdesivir resistance in COVID-19 illness: Need for development of clinical laboratory test	authors= Deepak  Kumar;Sukhpreet  Singh;Deepak  Yadav;•  Sumanpreet Kaur;• Monu Kumari;Samiksha  Jain;•  Rajasri Bhattacharyya;• Dibyajyoti Banerjee;	abstract= 	body_text= Remdesivir is thought to be effective for certain groups of patients suffering from COVID-19 illness. Currently, FDA has approved it for the purpose and WHO has issued a conditional recommendation for its use. Some evidence has been gathered in favor of its use, but data from large scale clinical trials are expected to give more insight into the matter [1] . At the present moment, molecular understanding regarding the use of remdesivir in SARS-CoV2 infected patients (COVID-19 disease) is scanty. Remdesivir is a nucleotide analogue that inhibits RNA dependent RNA polymerase (RdRp) of the virus. In case the binding site of the drug is mutated, remdesivir may not fit in the active site for the desired effect. It is in this context, we have docked remdesivir at the binding site of wild type RdRp bound template primer RNA complex (please see supplimentary). The remdesivir was observed to fitted there with binding energy -8.68 kcal/mol. The reported interacting residues (R555, V557, D618, T680, N691, D760 and D761) are in contact with remdesivir ring and are shown in Fig. 1a [2] . The nucleoside homolog ring faces towards the primertemplate implying inhibition of elongation of viral RNA. The most negative binding energy obtained in mutant cases (R555, V557, D618, T680, N691, D760, D761) were -8.93, -8.42, -6.55, -7.65, -7.42, -8.70, -8.26, respectively. In the mutant cases, remdesivir binding is shifted from the actual elongation site. However, in R555A and D760A mutant cases, the binding energy was observed to be more negative ( 8.93 and 8.70 kcal/mol, respectively). In Fig. 1b , remdesivir binding with D760 mutant RdRp with template primer is shown. We have observed that if any of the above-mentioned interacting residues are mutated, then remdesivir binding position is changed. So, in case of mutated RdRp, remdesivir may not stop the viral RNA elongation and the virus may acquire drug resistance. In SARS-COV2 virus, mutation near the binding sites (putative docking site of remdesivir in RdRp) is found [3] . However, it causes remdesivir resistance or not is currently unknown [4] . There are views that the remdesivir resistant strain cannot survive [5] . We believe that at the current moment it is impossible to comment with certainty that remdesivir resistant strain will cause a fatal COVID-19 disease or not and more focussed research should be dedicated in the concerned field. For stepping towards such direction, we must amplify the critical viral regions and sequence it for understanding the emergence of remdesivir resistant SARS-CoV2 strain. The online version contains supplementary material available at https://doi.org/10.1007/s12291-021-00987-w. Publisher's Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.  
paper_id= fff6fe12beb51ee2641ddb5381378ff3560d8103	title= Health shocks and their long-lasting impact on health behaviors: Evidence from the 2009 H1N1 pandemic in Mexico	authors= Jorge M Agüero;Trinidad  Beleche;	abstract= Worldwide, the leading causes of death could be avoided with health behaviors that are low-cost but also difficult to adopt. We show that exogenous health shocks could facilitate the adoption of these behaviors and provide long-lasting effects on health outcomes. Specifically, we exploit the spatial and temporal variation of the 2009 H1N1 influenza pandemic in Mexico and show that areas with a higher incidence of H1N1 experienced larger reductions in diarrhea-related cases among young children. These reductions continue even three years after the shock ended. Health improvements and evidence of information seeking via Google searches were consistent with changes in hand washing behaviors. Several robustness checks validate our findings and mechanism. Published by Elsevier B.V. 	body_text= Worldwide, the adoption of low-cost technologies could improve health outcomes and save lives. For example, regular physical activity reduces the risk of diabetes; condoms help reduce the spread of sexually transmitted diseases, including HIV; reductions in cigarette consumption help avoid pulmonary cancer; and hand washing with soap prevents gastrointestinal diseases. However, despite their effectiveness, the take up rates of these products or behaviors is very low (Dupas, 2011) . For instance, while the role of hand washing as an effective way to reduce gastrointestinal diseases has been known for more than a century (Koplik, 1902) , 2 in developing countries, only 30% of household members wash their hands before preparing food or after defecation. In some countries the rate is as low as zero (World Bank, 2005; Chase and Do, 2010) . Moreover, intensive small-scale interventions show significant reductions in diarrhea (e.g., Ejemot-Nwadiaro et al., 2008; Luby et al., 2004; Luby et al., 2005) , 3 but scaling up similar inter-2 For example, Koplik (1902) summarizes the research of the previous decade and recommends the "scrupulously" cleaning of hands and nails ("with brush and file") after changing a diaper for nurses handling a newborn. Mothers, he added, "should carefully cleanse their hands before feeding the baby" (p. 321). The rationale for this hygiene practice was also well understood "since this way contamination of the infant's food with fecal bacteria is avoided" (p. 46). 3 Common interventions to reduce diarrhea have focused on providing infrastructure or information. Infrastructure projects focus on providing safe water supplies and sanitation (e.g., Pattanayak et al., 2009; Kremer et al., 2011) , micronutrients or vaccinations (Dowling and Yap 2014 . Information-based interventions focus on hygiene education and community-led total sanitation (CLTS) (Dowling and Yap 2014 . Also, recent papers have discussed the external validity of these studies. See Hammer and Spears (2016) for a discussion on whether the sites where village sanitation projects take place might be likely to exhibit large effects. http://dx.doi.org/10.1016/j.jhealeco.2017.03.008 0167-6296/Published by Elsevier B.V. ventions does not (e.g., Meredith et al., 2013; Galiani et al., 2012) . 4 Understanding the barriers for the adoption of these preventive behaviors and products is an urgent issue in health and development economics. A growing literature in economics is studying the role that information plays on health behaviors as a salient input in the production of health outcomes (Cawley and Ruhm 2011). 5 In particular, an emerging conclusion suggests that the mere provision of information might not matter (e.g., smokers do know that smoking is harmful and may even overestimate the risk of smoking, Viscusi, 1990) . Rather, the emphasis seems to be on when information alters the decision-making process (Luoto and Carman 2014) . Our paper contributes to this literature by showing that health shocks such as disease outbreaks can operate as "natural nudges" that facilitate changes in health behaviors that lead to improvements in health outcomes, and that these types of shock-induced improvements can have long-lasting effects. In particular, we illustrate how in Mexico, a middle-income country with near universal access to water and sanitation but where intestinal infections are the second main cause of child death and 11 percent of children under five suffer from acute diarrhea, the onset of the 2009 H1N1 influenza (swine flu) led to a large, robust and long-lasting decline in diarrhea cases of children. This is done with a difference-in-difference framework using a balanced panel of annual state-level data aggregated from hospital discharges related to diarrhea and the total number of laboratory confirmed cases of the swine flu in Mexico derived from Mexico's Ministry of Health (Secretaria de Salud). Relying on administrative data −where trained medical professionals register their diagnoses-and several robustness checks, we rule out the possibility that our findings are driven by misdiagnosis of either H1N1 or diarrhea cases. Additional placebo tests and robustness checks further support our results. For example, we find no association between diarrhea-related cases before the H1N1 outbreak (2006) (2007) (2008) and the number of confirmed swine flu cases observed in 2009. There is also no evidence of people avoiding hospitals altogether due to the H1N1 pandemic. Furthermore, the negative effect on diarrhea is also observed in morbidity cases beyond hospitalizations, as captured by Mexico's Annual Morbidity Statistics. Unlike the small (and anticipated) nature of the seasonal flu, it quickly became clear that the new H1N1 strain was easily transmittable and that existing vaccines did not prevent contracting the swine flu. Thus, people experienced an environment where the new virus was affecting a large number of individuals, and could, in some cases, be fatal. This provides an important context to test theoretical models of health behavior where a decision to engage in preventive behavior is triggered only when the (contagious) disease crosses a high threshold (e.g., Philipson 2000) . Consistent with such models, we find a null effect for the seasonal flu on diarrhea outcomes that contrasts with our negative and large findings from the H1N1. Using Google searches originated in Mexico we identify an increase in the demand to learn about preventive behaviors. Specifically, we find that searches for the word "hand sanitizer" spiked during the peaks of the pandemic. Furthermore, we show that an 4 Other examples include Kremer and Miguel (2007) 's finding that information had no effect on Kenyans' investing in deworming treatments, and Ashraf, Berry and Shapiro (2010)'s report that information had no effect on chlorine water purification in Zambia. 5 Other strategies include, but are not limited to, taxation, cash incentives and restrictions on use or purchase of preventive products. See Cawley and Ruhm (2011) for an extensive discussion of policy options and theoretical models related to risky behaviors in advanced economies. For developing countries, a growing literature explores the role of subsidies on the adoption of health products and behaviors. See Dupas (2011 Dupas ( , 2014 ) for a review. increase in the incidence of the H1N1 is associated with more searches for this preventive behavior. Other mechanisms, such as the role of government expenditure and health infrastructure, do not seem to play a major role. Further incidence of the H1N1 after 2009 allows us to test whether such "reminders" continue to have an impact on health outcomes beyond 2009. Using an event study we show that the contemporaneous negative effect is observed in 2009 and 2010 but disappears by 2012. However, when testing for the persistence of the 2009 shock, we find that this effect dominates the contemporaneous impact suggesting long-lasting consequences of the larger shock. This is an important result because, as we will show later, the evidence on whether information campaigns have long-lasting effects is scant (e.g., Cairncross et al., 2005) . Taken together, our results provide empirical support to recent behavioral economics models where large health shocks alter the risk perceptions of individuals and affect the production of health outcomes. In that regard, our findings are consistent with previous studies that have shown, in the case of the United States, that smokers are more likely to quit when they experience more severe health shocks (Sloan et al., 2003; Margolis et al., 2014) . It is also consistent with the findings from Philipson and Posner (1994) , who document a rapid reduction in gonorrhea for the male homosexual community in San Francisco as a result of a change towards safer sex practices soon after the onset of the HIV pandemic. The rest of the paper is divided into six additional sections. We start by briefly describing the H1N1 outbreak in Mexico in section two. Section three describes the data sources and our econometric model. The main results, including our robustness checks, are presented in section four. Section five describes the possible mechanisms while section six examines the persistence of the effects. Section seven summarizes our findings, discusses policy implications as well as the way our paper expands our understanding of the production of health outcomes and concludes. In March and early April 2009, Mexico experienced an outbreak of respiratory illnesses which was later confirmed to have been caused by the novel influenza A(H1N1) 2009 virus or swine flu. 6 The H1N1 is a contagious virus transmitted via droplets from coughs and sneezes or by interacting with infected people. This influenza virus can survive on environmental surfaces such as kitchen counters and door knobs for up to eight hours. H1N1 shares many of the symptoms of the seasonal flu: fever, cough, aches. However, while it is rare to have gastrointestinal symptoms from the seasonal flu, some cases of the swine flu, around 13 percent, exhibited nausea, vomiting or diarrhea (SSA, 2011) . The World Health Organization (WHO) declared the 2009 H1N1 outbreak as the first flu pandemic in 41 years. As of June 2011, Mexico's Ministry of Health reported that there were more than 70,000 confirmed cases of swine flu in 2009, including more than one thousand deaths and around 2400 hospitalizations. Most of the confirmed H1N1 cases in Mexico involved a relatively younger cohort, aged 10-39, compared to the population typically affected by the seasonal influenza. The incidence of these cases was highest in May, June, and September of 2009 (Appendix Fig. 1 in Supplementary material). All states in Mexico were affected by the swine flu outbreak, but there was variation in the distribution of cases across states (Fig. 1, Panel A) . This distribution does not coincide with the spatial pattern observed for diarrhea cases prior to 2009 (Panel B of Fig. 1 ). The announcement of the first case of H1N1 on April 23, 2009, was followed by a series of other actions coordinated by the Mexican government, including the Ministry of Health, and other national and international organizations. These actions included enactment of the National Pandemic Preparedness and Response Plan which coordinated and implemented risk communication strategies to promote respiratory hygiene and to maintain the public informed about the transmission of influenza. TV and radio ads, a dedicated hot line, alert text messaging, and social media messaging were launched on April 25. The goal of the campaign was to educate the public about frequent and proper hand washing techniques, covering sneezes or coughs, using face masks and hand sanitizers, seeking care if ill, and discouraging self-medication. The Ministry of Health and the Office of the President coordinated with media outlets to provide daily updates on the number of confirmed cases, which was reflected on substantial coverage on the number of confirmed cases in each state as well as key messaging on how to prevent transmission. On April 26, the World Bank lent $25 million for immediate aid and $180 million in long-term assistance to address the outbreak. The announcement of the World Bank aid was followed by the closure of schools nationwide on April 27. On April 30, the Mexican government declared that all "non-essential" activities be suspended and implemented social distancing measures that closed restaurants, entertainment venues, and cancelled large public gatherings nationwide. Throughout the development of the outbreak, the World Health Organization actively updated their assessment from "event of international concern" on April 24 to pandemic phase 4 (sustained community transmission) on April 27, to phase 5 (imminent pandemic underway) on April 29, and then to phase 6 (pandemic) on June 11. Although the pandemic was still underway, activities began to return to normal in Mexico. By May 11, most schools had reopened nationwide. Parents and volunteers coordinated with school and health authorities to sanitize the schools with cleaning supplies paid for by the federal government before the schools reopened on May 11. In addition, parent-volunteers screened students in elementary schools to identify and send home students reporting or showing influenza-like symptoms. It is unclear how long the screeners were in place, but there are reports that screeners in schools were only present for a few days (SSA, 2009a) . Additional collaboration at the state, local, and international level also occurred to disseminate information in the workplace, public transportation, and local communities, although the details are too varied for us to summarize here. There were other waves of cases in June and September, but they were reported to be under control. Appendix Table 1 in Supplementary material provides further details about measures taken by the Mexican government and other organizations in the first months following the first confirmed cases of H1N1. Thus, while the swine flu remains in Mexico today, none of the following years had the same level of intensity, awareness and possible "panic" as observed in 2009. In the next section we describe our data and how we exploit the variation across time and space to identify the causal impact of an exogenous shock that induced changes in behavior and ultimately reduction in child diarrhea. We use several data sources for this paper, all but one, collected by Mexico's Ministry of Health (Secretaria de Salud) to create a stateyear balanced panel. First, we use hospital discharge data from all public hospitals. For the purpose of this study we use data for the 2006-2012 period and we aggregate the discharges at the year and state level to match the source of the time and spatial variation in the H1N1 data. 7 Common to many developing countries, the public hospital system covers most of the population and in the case of Mexico, 85 percent of all hospital visits are covered by these hospitals. 8 This large coverage strengthens the external validity of our findings. 9 A key advantage of this dataset is that the coding for the primary diagnosis of the discharge follows the International Statistical Classification of Diseases and Related Health Problems 10th Revision or ICD-10, created by the World Health Organization (WHO, 2010) , where the treating physician determines the diagnosis. Relying on the report of a trained professional represents a significant improvement in the literature as it helps reduce the recall bias and other measurement errors that plague self-reported data obtained from household surveys (Heady 2016) and improves the accuracy of the diagnosis. The coding included in the database comes from the actual diagnosis regardless of the initial reason that led the patient to the hospital. 10 Furthermore, the use of hospital discharge data implies that we are focusing on the extreme and treated cases of diarrhea (i.e., those severe enough that resulted in a hospital visit and even death) as opposed to those cases that were either treated at local health centers, at home or went untreated. Following Mexico's Ministry of Health's definition of "acute infectious diarrhea" we consider cases where the primary diagnosis was an intestinal infection as classified by ICD-10 codes A00 through A09X (SSA, 2012) . The details of the codes are presented in Appendix Table 2 in Supplementary material. In Mexico, diarrheal cases in children under five represent 51 percent of all hospital discharges where the primary diagnosis was diarrhea. 11 For this reason and following the literature of early childhood development we restrict our analysis mainly to this age group. 12 The hospital discharge data are complemented with morbidity information collected in the Anuarios de Morbilidad 13 (Annual Morbidity Statistics). The Anuario is a yearly report produced by the Mexican Health System that collects information from all the public and private health centers nationwide to create an Epidemiological Surveillance Bulletin for the National System of Epidemiological Surveillance or SINAVE, similar to the CDC's National Notifiable Dis-7 Data covering some public hospitals are available from 2002. Data for all public hospitals are available beginning 2004, but we do not incorporate it into this analysis because the age variable was not consistently coded for that year. For 2005, we do not have information of the limited rollout of the rotavirus vaccination campaign and therefore restrict the sample to 2006 onwards when the rotavirus program was nationally implemented. 8 This dataset does not record month of admission. This variable is only included in a dataset that, unfortunately, has a heavily restricted coverage (<50% of the national discharges) and does not represent a random sample of all discharges. These limitations prevent us from using the month-recording dataset in our main analysis. 9 Mexico's Ministry of Health reports that in 2009 there were 91.6 million users of the public hospital system. However, the report does not indicate whether the reported 91.6 million users included repeat users. (Sistema Nacional de Información en Salud (SINAIS). Población usuaria por entidad federativa según institución, 2009, Boletín de Información Estadística, Vol. III, Servicios Otorgados y Programas Sustantivos, Numero 29, Año 2009. http://www.sinais.salud.gob.mx/publicaciones/ index.html, last accessed November 24, 2013. 10 The primary diagnosis is determined based on the condition that was investigated and treated during the hospitalization. It is defined as, "The diagnosed condition at the end of the event that led to the primary cause of treatment for the patient. . .if there is more than one (condition), the one that is considered to be responsible for the most use of resources must be selected. If no diagnosis is made, the primary diagnosis is the main symptom, abnormal finding or problem" (SSA, 2010a, p.78). 11 There were 5.8 million hospital discharges in Mexico in 2011 with a rate of 456.3 discharges per 10,000 population and 129,000 cases were related to diarrhea. 12 Older children (5-14) represent 15 percent of all the hospital discharges diagnosed as diarrhea while people 45 and older constitute 20 percent of the cases. 13 Available at http://www.epidemiologia.salud.gob.mx/anuario/html/anuarios. html eases Surveillance System in the United States. The Anuarios de Morbilidad concentrates on the leading causes of morbidity rather than collecting all possible diseases. It includes ambulatory and physician office visits. This facilitates a weekly report of the incidence of selected diseases, where acute respiratory infections as well as gastrointestinal diseases have been consistently covered in the data throughout the period of analysis. As in the case of the hospital discharge data, diagnoses recorded in the Anuarios de Morbilidad follow the ICD-10 classification and have been determined by a medical professional. In addition, there is a quality assurance process that includes a multi-layer system of checks until the data are released to the public months after the end of the calendar year. This dataset complements our hospitalization records and is available at the state level and not at the individual case level. The third data source also comes from SINAVE and provides us with the swine flu cases −coded as J09-at the state level only, and yearly from 2009 onwards. 14 A key advantage of this source is the quality of the report. The swine flu cases included in the dataset have been confirmed by a laboratory as true cases of the H1N1 (SSA, 2010b). In an effort to improve and standardize influenza surveillance, the Ministry of Health required all Mexican medical institutions to confirm suspected cases of the H1N1 with laboratory tests −a practice that continues to this day (SSA, 2010b (SSA, , 2013 (SSA, , 2014 . This heavily limits the possibility of misclassification with other diseases that could share symptoms with the swine flu. 15 Additional data on health expenditures by the government (state and Federal), distribution of oral rehydration salts, distribution of vaccines, and hospital infrastructure as measured by the number of hospital beds come from Mexico's Ministry of Health SINAIS (National System of Health Information) data system. 16 We utilized two other data sources. Child mortality due to diarrhea is obtained from Mexico's Vital Statistics. This dataset has 100 percent coverage and includes deaths that occur at home and in hospitals. 17 Also, we use Google Trends data as they provide an index that captures the popularity of a given Internet search across time and states. There is a growing number of papers using data from Google searches (available as Google Trends: http:// www.google.com/trends/) to uncover economic issues. For example, these data have been used to predict economic indicators in the United States and Germany (Choi and Varian, 2012; Askitas and Zimmermann, 2009; D'Amuri and Marcucci, 2010) as well as discrimination and voting behavior (Stephens-Davidowitz, 2014) . 18 In our case, we are interested in searches of the Spanish word for hand sanitizers: "gel" or "gel antibacterial." We downloaded the information in pairs of states keeping constant the state with the highest value of searches between 2008 and 2009 because Google Trends only releases it as index. In that way, our data provide the 14 Multiple symptoms can be associated with the swine flu. The top two symptoms associated with potential cases of the swine flu were cough (90 percent) and fever (86 percent), with diarrhea, nausea or votiming being present in 13 percent of the cases (SSA, 2011) . 15 An alternative to laboratory-confirmed cases could be the use of the actual number of people infected with H1N1. However, this number is unknown, as infections do not always create influenza-like symptoms and different approaches are used to compute estimates for the aggregate number of infections (Shrestha et al., 2011) . 16 We also explored data on news coverage from a targeted search for H1N1related articles during the period 2006-2012 in LexisNexis that included 14 major newspapers with national coverage in Mexico. However, there is no spatial variation in this variable and cannot be used in the analysis. 17 Mathers et al. (2005) classify Mexico's mortality records as of the highest quality globally, with 100% coverage, extensive usage of ICD codes and with only 5% illdefined codes. The United States has the same numbers except that it has 7% of ill-defined codes. Germany's record for this measure is 14%. 18 Google even has a site dedicated to predict the incidence of the seasonal flu based on the results from a paper published in Nature (Ginsberg et al., 2009 ). same base, where the highest (lowest) volume is set to 100 (zero). 19 Note that the index is a measure of "relative popularity" and so it already takes into account the size of the search volume in each state and period. Table 1 presents summary statistics of the key variables used in our analysis for periods 2006-2008, 2009, and 2010-2012 . The last column of Table 1 shows the p-values of the difference in means between the pre-2009 and post-2009 periods. Given that H1N1 did not exist prior to 2009, the total number of confirmed cases of H1N1 is only available from 2009 onwards. Diarrhea-related hospitalizations for children under the age of 5 were lower in the post-2009, compared to the pre-2009 period, and the difference is statistically significant (p-value = 0.000). On the other hand, federal expenditures in health were higher in post-2009 than pre-2008 (pvalue = 0.020). Except for these two variables, the test of difference in means suggests there is no difference between the pre-2009 and post-2009 periods as captured by most key variables. We exploit the temporal (the onset of the swine flu in 2009) and cross-sectional variation (by state) of the swine flu to examine its effect on diarrhea cases, that is, diseases that may be prevented with improved hygiene behavior that followed the onset of the H1N1 pandemic in Mexico. Using a balanced panel at the state and year-level, our difference-in-difference identification strategy is formally presented in Eq. (1), where y st is the number of hospital discharges with a primary diagnosis of intestinal infections (henceforth referred to as diarrhea) for state s in year t. Variable H1N1 st represents the cross-sectional and temporal variation in the number of laboratory-confirmed swine flu cases reported in each of the states and year (with values equal to zero before 2009). We also investigate other treatment and baseline periods, which are discussed in Section 4.3. We use H1N1 counts rather than rates because we believe that part of the mechanism through which individuals' perceptions changed was rooted in the perceived magnitude of the problem as reported on news media channels. The news media reported the total number of cases that had been confirmed at the national level as well as the states where the highest number of cases had been confirmed. 20 Thus, ␤ is our parameter of interest as it captures the differencein-difference impact. Robust standard errors clustered at 32 states with a correction with a correction for small clusters are used and are complemented with p-values computed using a wild bootstrap method (Cameron et al., 2008) . Eq. (1) includes controls for year and state fixed effects ( t and s , respectively). The year fixed effects allow us to control for nationwide trends in diarrheal diseases while the state fixed effects account for time-invariant unobserved characteristics during the period of analysis at the state level (e.g., culture, geography, institutional settings). In our main specification, we compare the changes in diarrhea cases between 2008 and 2009 as we expect several state characteristics, including health infrastructure (e.g., stocks of hospitals and clinics), to remain constant over such a short period, thereby reducing the possibility of a bias in the parameter of interest. Thus, if the H1N1 pandemic induced changes in hygiene behavior, we would expect to observe a larger decline in the incidence of diarrheal dis- eases in states where the swine flu was more prevalent. In other words, we would expect ␤ to be negative and statistically significant. In the next section, we show the results of estimating Eq. (1) with the data described above. Table 2 presents the results from running the specification described in Eq. (1) for 2009 and 2008, with state and year fixed effects . As shown in Panel A, column 1, our estimate for effect of the 2009 H1N1 on diarrhea cases for children under five is negative, −0.105, and statistically significant at the five percent level, marginally, using the wild-bootstrap p-values (0.060). That is, there were fewer hospital discharges related to diarrhea in states with more swine flu cases, even after controlling for time and state fixedeffects. This coefficient implies that for every 1000 cases of the swine flu, there were 105 fewer cases of diarrhea in children under five. Given the average number of diarrhea-related hospitalizations for this group (1117 in the period 2008-2009), the estimated association indicates that for every 1000 cases of the swine flu we observe a 9.4 percent decline in diarrhea-related hospitalizations (-0.105*1000/1117). That is, 3404 cases of H1N1 (or 4.9 percent of all confirmed cases) would have the same effect in the reduction of diarrhea (32 percent) as the estimated average effect from the costly interventions reviewed by Ejemot-Nwadiaro et al. (2008) . This finding is reinforced when using the morbidity data (Panel B) that covers cases recorded in all public and private health centers, except for hospital inpatients. We find that 1000 cases of the H1N1 are linked to a 3.5 percent reduction in diarrhea cases among young children (p-value = 0.051). This impact is smaller than the 9.4 percent found in the inpatient setting, suggesting that the effect is more pronounced among severe cases, that is, those that required hospitalization. In this section we examine the possibility that the documented reductions in diarrhea cases were purely "mechanical" and driven by a misclassification of diagnoses created by the onset of the swine flu. For example, with the arrival of the H1N1, cases that should have been identified as diarrhea (belonging to ICD codes A00-A09X) could have been incorrectly classified as swine flu. After all, up to 13 percent of the diarrhea swine flu cases exhibited vomit, nausea or diarrhea as an additional symptom. We argue that this misclassification is an unlikely event for the following reasons. First, as explained in the data section above, the H1N1 cases used in our study were confirmed cases using laboratory tests. So even if some of them included a diarrhea symptom, these cases were coded as J09 in our data because a strain of the novel type of influenza was found in these patients. Second, the recorded disease is made by a trained medical professional. So if parents suspect their children have H1N1, when the actually have diarrhea, what is recorded is the actual assessment of the physician and not what the patient (or his parents) suspected. 21 Third, we expect medical professionals to be less likely to misdiagnose a diarrhea case not only because of their medical training but because Mexico has identified intestinal infections as a public health issue based on the magnitude and prevalence of the disease (with at least 11% of children under five being affected by this problem each year). These professionals are also more likely to be aware of the population at risk for different diseases. As shown in Fig. 2 , which presents the age distribution of these two diseases for hospitalizations, 22 the H1N1 affected disproportionally school-going children and adults while diarrhea affects primarily children under 5. This difference further reduces the possibility of misclassification. Fourth, even if one assumes that doctors might not be fully aware of these epidemiological differences but rather have a flat prior with respect to the risk by age groups, then we should expect the misclassification to take place at all ages. However, when we run our main specification for other age groups and not just for children under five, we do not find evidence suggesting there is a mechanical misclassification of diarrhea to H1N1 (Table 3 ). The parameter for the H1N1 is negative and statistically significant for those under five (column 2). However, it is positive, small and not statistically different from zero for all other age groups (columns 1, 3-5), including older patients who are the second most at-risk age group of diarrhea (as shown in Fig. 2, Panel B) . This pattern −negative effect for children under five and positive but close to zero for all other groups-is found using the hospitalization (Panel A) data as well as the overall morbidity data (Panel B). Moreover, we explore other outcomes that could be affected by better hygiene practices. In Table 4 we include cases of conjunc-21 It is worth noticing that the official guidelines sent by the Mexican government to parents and schools rarely include diarrhea as one the key symptoms to identify a possible H1N1 case. See SSA (2009b) (4) and (5) tivitis (ICD-10 codes B30 and H10) for children under 5 (column 2). This outcome comes from the morbidity dataset obtained from the Anuarios de Morbilidad. We find a negative association between H1N1 and conjunctivitis cases. However, conjunctivitis affects only a fraction of children under five compared to diarrhea. In 2008, for instance, there were 1.5 million cases of diarrhea nationwide but only 89,103 cases of conjunctivitis. This could explain the lack of statistical significance for this estimate. When we add hepatitis A (ICD-10 code B15, also from the Anuarios de Morbilidad) and conjunctivitis together as a new outcome we continue to observe a negative association with H1N1, alas unable to reject the null hypothesis (results available upon request). Again, this could be explained by the few cases of hepatitis A among this age group (only 4348 in 2008) relative to the diarrhea cases. We think this evidence, although not as strong as desired, adds to the support that the negative, robust and statistically significant effect of H1N1 on diarrhea is less likely to come from a mechanical classification problem. Finally, we take advantage of the fact that diarrhea cases are classified in different ICD codes depending on the cause or disease etiology, where for most cases the cause of diarrhea is not known. If during the H1N1 outbreak doctors misclassified diarrhea cases, these cases are more likely to come from those where the cause is unknown (ICD-10 code A09X) and much less so, if at all, from those where the cause is known (ICD-10 codes A00-A08) (see Appendix Table 2 in Supplementary material for details about specific ICD-10 codes). In Table 4 we test for this hypothesis by using the ratio between known cases of diarrhea and the unknown sources as our outcome variable. Misclassification implies that more cases of H1N1 will be positively associated with this ratio: the denominator (the unknown sources) decreases but not the numerator. Our estimates strongly reject this prediction of misclassification. We find no association between the ratio and the H1N1 (Table 4 , column 1). Specifically, our point estimate using hospitalizations is 0.0001 (bootstrapped p-value = 0.853) and a similarly small and not statistically different from zero estimate when using the morbidity dataset (coefficient = 0.0001, p-value = 0.136). 23 Taken together, for the period of analysis. a The dependent variable is annual hospital discharges as specified in each column. b Known causes include International Classification of Diseases (ICD-10) A00-A08; Unknown causes include ICD-10 code A09X. c Injuries include ICD-10 codes S00-S798. d Hip-related procedures include codes that capture fracture of femur (S72), arthrosis of hip (M16), complications of surgical and medical care (T80-T85), presence of orthopedic joint implants (Z96.6), fracture of bone following insertion of orthopedic implant (M96.6), and fitting and adjustment of orthopedic device (Z46.7). e H1N1 denotes the number of confirmed H1N1 cases in a given state during the treatment period (2009), and zero otherwise (2008). f p-value denotes the p-value of wild bootstrapped standard error to correct for small number (32) of clusters. g The dependent variable is the annual number of diarrhea cases or deaths due to diarrhea as specified in each column. h Conjunctivitis includes ICD-10 codes B30 and H10. all these results indicate that our findings are not driven by some misclassification of the diagnosis. 24 A key advantage of comparing diarrhea cases between 2009 and 2008 is their proximity in time. In such a short period, the inclusion of state fixed effects serves as a more credible assumption because unobservables, such as culture or geography and maybe even institutions, are less likely to vary compared to the use of longer periods. Nonetheless, it could be the case (however unlikely) that some of the unobserved characteristics specific to the 2008 cross-sectional distribution of the diarrhea cases could "predict" the spread of the 2009 swine flu and therefore bias our estimates. With that in mind, we explore whether redefining or expanding the base period alters our findings. We consider two alternative specifications using the hospitalization and morbidity outcomes. First, following Eq. (1) we estimate two pairwise comparisons separately: 2009 vs. 2007 (column 2 of Table 2 ) and 2009 vs. 2006 (column 3 of Table 2 ). The second specification discards the pairwise comparison and expands the sample 24 We also explored the possibility that, before 2009, the new strain of the H1N1 virus might have been a latent disease and those cases were incorrectly labeled as diarrhea, because the new virus has not been identified yet. Based on the number of H1N1 cases in 2009 for children under five (7238) and that 13% of cases, that at most, could include a diarrhea-like symptom, we estimated that the highest number of cases incorrectly labeled as diarrhea for 2008 cannot exceed 2.3 percent of the hospitalizations and 0.06% of the morbidity data. Re-running our equation with those reductions applied to 2008 do not change our results (available upon request). We thank an anonymous referee for this point. to include all years 2006-2009 (columns 4 and 5 of Table 2 ). The inclusion of more years permits us to add state-specific trends that were not possible in the pairwise comparison. The results of these new specifications indicate that our findings are not sensitive to these changes. Specifically, defining 2007 or 2006 as the base year does not qualitatively alter our conclusion. We continue to find a negative association (but with lower precision) and these parameters are not statistically different from the estimates using the original base year. Furthermore, expanding the data to include 2006 through 2009 we conclude that controlling for state trends (column 5) or not (column 4) provides robust findings: areas with a higher incidence of H1N1 see a larger decline in the number of diarrhea cases (with significant levels of 10 percent for hospitalizations and even lower for morbidity). All these findings hold when using the hospitalization data (Panel A) or the morbidity (Panel B). Thus, it is unlikely that our results are capturing pre-existing trends, as we continue to find a negative association between H1N1 and diarrhea cases after we control for time varying unobserved characteristics by state. We further explore the issue of possible, though unlikely, preexisting trends. In Fig. 3 Panel A we first show that states with a higher incidence of the 2009 swine flu had a larger decline in the number of diarrheal cases relative to the years preceding the outbreak. We then examine the impact of the prevalence of the H1N1 pandemic on diarrhea cases for the periods preceding 2009. Fig. 3 , Panel B, illustrates that there is no association between diarrhearelated cases between 2008 and 2007 (before the H1N1 outbreak) and the number of confirmed swine flu cases observed in 2009. The regression analog to this figure is shown in Table 2 , column 6. There we run the same specification as in Eq. (1) for the diarrhea outcomes in 2006-2007 and 2007-2008 but incorrectly assign the 2009 H1N1 distribution to these previous periods of time (for hospitalizations and morbidity, Panels A and B, respectively). As before, we should find no effect of the 2009 H1N1. Otherwise, this would be evidence in favor of unobserved variables predicting the 2009 cross-sectional distribution of the swine flu. In each case, hospitalizations and morbidity data, the estimates for this falsification test indicate true zero effects. These zero estimates and the lack of statistically significant effects provide a much stronger validation of our identification strategy and it is consistent with the visual evidence provided in Fig. 3 Panel B as well as the two maps presented before (Fig. 1) . For our next set of falsification tests, we continue to compare data between 2008 and 2009 but alter the outcomes examined. In particular we use: injuries caused by external factors 25 (e.g. traffic accidents, ICD-10 codes S00-T98), hip-related procedures (ICD-10 codes S72, M16, T80-T85, Z96.6, M96.6, and Z46.7), all hospital discharges (all ICD-10 codes excluding H1N1 cases) as well as mortality due to gastrointestinal problems (see Appendix Table 2 in Supplementary material for more details about the ICD-10 codes for these diseases). First, as a way to start introducing some of the possible mechanisms behind the observed effect of the swine flu (i.e., hand washing) we show that the H1N1 does not affect discharges unrelated to hand washing. Specifically, hospital discharges due to injuries serve as a valid placebo effect and we would expect to find statistically insignificant effects when we estimate Eq. (1) using injuries as an outcome. This is precisely what we observe in col- 25 Injuries includes trauma to body, burns, poisoning due to external factors such as falls, traffic accidents, self-inflicted injuries, exposure to inanimate falling, thrown or projected objects, and aggressions. umn 3 of Table 4 . The effects are again true zeroes: very small effects with smaller standard errors. For example, the point estimate is 0.005 (SE = 0.007, bootstrapped p-value = 0.782), which is twenty times smaller than the corresponding estimate for diarrhea (in absolute value). Second, we want to rule out the possibility that we are attributing our main findings to changes in healthcare-seeking behavior, namely, that there were fewer people going to the hospital in areas with higher prevalence of H1N1 in order to avoid contact with sick individuals. We provided evidence against this possibility earlier as we showed that the effects are also observed among morbidity cases, beyond hospitalizations. We explore this using hospital discharges associated with hip-related procedures for all age groups, and hospital discharges (excluding H1N1) for children under five. Hip-related procedures try to capture hospital visits that could be delayed. If these procedures were negatively related to the H1N1, we would find evidence that adults were avoiding hospitals. In column 3 of Table 4 we identify a null result. Similarly, if we observe a statistically significant decline in all types of hospitalizations for children, it would also suggest that parents were not taking their children hospitals. A null result for this outcome is presented in column 4 of Table 4 . If anything, in both cases we find a (very small) positive parameter that is not statistically significant. Thus, we can rule out the possibility that our findings come from people avoiding hospitals during the swine flu pandemic. 26 In column 3 of Table 4 Panel B, we show the results of the analysis where mortality due to diarrhea is the outcome measure. The estimated coefficient is 0.0003 and is not statistically significant, implying that there is no change in overall child mortality due to diarrhea in areas with more cases of the swine flu. This is important as we can rule out deaths from diarrhea happening at home, rather than at hospitals, for areas of high H1N1 prevalence. 27 How did the swine flu reduce diarrhea cases? In the previous section we have already suggested one possible explanation: the swine flu created a change in hygiene practices that led to more hand washing with soap (or use of antibacterial gels) and this led to fewer diarrhea cases requiring hospitalization, as well as fewer morbidity cases. In this section we provide further evidence in favor of this mechanism and rule out other possible pathways. 28 In this subsection, we examine several avenues that could explain the mechanisms underlying our findings. First, we consider "business-as-usual" variables. These include, state as well as federal health expenditures, the distribution of oral rehydration salts, the total number of vaccines administered and changes in health infrastructure measured by number of hospital beds. In Table 5 we display the results of using our difference-in-difference approach described in Eq. (1) when these variables are considered as outcomes, after controlling for time and state fixed effects. In columns 1 and 2, we show that the estimated coefficients for state and federal expenditures, respectively, are positive, very small and clearly not statistically significant based on the bootstrapped p-value. Similar null results are found for oral rehydration salts, vaccines and hospital beds (columns 3-5). The spread of the H1N1 is not related to these variables ruling them out as possible mechanisms to explain the improvements in health outcomes. We now present evidence in favor of a pathway where the 2009 H1N1 pandemic changed health behaviors in Mexico. We start by exploring production and consumption patterns of hygiene products. Mexican manufacturing data indicate that between 2008 and 2009, there was a 6.4 percentage point increase in production of soaps, cleaners and cosmetics; compared to a 2.3 percentage point increase from 2003 to 2007. 29 In addition to the changes in production of soaps there is other evidence that suggests changes in hand washing behavior might have occurred during the pandemic. A survey conducted in Mexico City and two states with varying prevalence of the swine flu showed that the top three mitigation efforts adopted to protect against the H1N1 virus included frequent washing of hands with soap, use of a mask, and hand sanitizer (Aburto et al., 2010) . 30 We reproduce these findings in Appendix Table 5 in Supplementary material. This table also shows that people in states 27 We also conducted additional tests to examine alternative functional specifications. These analyses suggest that the linear specification with counts appears to be the preferred specification for our paper (see Appendix Tables 3 and 4 in Supplementary material). 28 Recent papers on infectious diseases are exploring the role of viral interference; a process where individuals infected with the swine flu could become immune to other viruses (e.g., Gröndahl et al., 2014; Ånestad and Nordbø 2011; Casalegno et al., 2010) . See also Wrammert et al. (2011) for related possibilities. Whether the H1N1 served as an "antibody" for the viruses causing diarrhea (e.g., rotavirus) goes beyond the scope of this paper. 29 These numbers underestimated purchases as imports of hand sanitizers are not included. Furthermore, these data are not available at the state level. 30 Although we are uncertain about the methodology, others have indicated that a survey conducted by Nielsen showed that the top two adopted measures by consumers in Mexico during the swine flu outbreak were: 1) the use of face masks, and 2) hand washing with soap and water or hand sanitizers. Nielsen also reported an increase in sales of soaps and hand sanitizers. http://economia.terra.cl/noticias/ noticia.aspx?idNoticia=200906171913 TRM 78156849, accessed on November 24, 2013, and www.cnnexpansion.com/get content.php?q = print&url = mi-dinero/ 2009/06/17/autoservicios-ganan-con-la-influenza, accessed June 11, 2013. with higher incidence of the swine flu at the time of the survey had higher usage of hand sanitizer. This supports our hypothesis that the H1N1 pandemic changed hygiene practices, leading to more hand washing with soap or at least more use of hand sanitizers, and this change led to a reduction in hospitalizations due to diarrhea. 31 We complement these results by showing that Mexicans became more aware of the need to have better hygiene practices and increased their demand for knowledge about preventive behaviors. Fig. 4 , Panels A and B, show the trend of public interest for hand sanitizers between 2007 through 2011 using data from Google searches originated in Mexico. To understand the y-axis of Fig. 4 , it is important to note that Google Trends does not release the actual number of searches but instead provides an index, which Google describes it as a measure of "relative popularity" of searches. As mentioned in the data section, the data represent an index ranging from 0 to 100. In Fig. 4 , Panel A, we show the weekly index of searches throughout 2009. The pattern is clear. Prior to week 15 (early April) there are few searches for the expression "hand sanitizer" (gel or gel antibacterial in Spanish). However, at the beginning of the swine flu outbreak in early April we observe a spike in the number of searches of more than five times relative to first weeks of the year. The post-outbreak trend remained at a level that was higher than the pre-outbreak period. We further expand this analysis in Panel B of Fig. 4 32 We test whether this demand for knowledge is a possible mechanism by including it as an outcome in Eq. (1). We do this by constructing a panel dataset from searches by the state and year. In columns 6 and 7 of Table 5 , we find a positive and large relationship between the incidence of H1N1 and Google searches. The results of column 6 indicate that 1000 cases of the H1N1 are associated with an increase of 3.2 units in Google searches for hand sanitizers and it is statistically different from zero at the 10% level using bootstrapped standard errors. This represents a substantial increase (42 percent) with respect to average value during the period of analysis (mean = 7.63). Note that due to low search volume there are no data for eight states, 33 so in column 7 we ran our regression for Google searches replacing the missing values with zeroes (the lowest possible value in the Google Trends index) for those eight states. This way, we are able to utilize the full dataset. This imputation leads to similar findings −more Google searches in areas with a higher incidence of the H1N1-and it allows us to gain precision for the estimates by "recovering" those missing values (p-value 31 Similar behavioral changes were reported during an outbreak of Severe Acute Respiratory Syndrome or SARS. See Leung et al. (2004) for details. Also, there was a spike of purchases for hand sanitizers in the United States during the swine flu in 2009 (see Appendix Fig. 3 in Supplementary material). Furthermore, some research indicates that the H1N1 pandemic led to an increase in hand washing behavior in the US (e.g., Jones and Salathe 2009; Rubin et al., 2009 ) making our mechanism even more plausible. 32 Similar patterns −a spike around week 19 and higher searches relative to the pre-outbreak period-can be observed for Google searches of the word "cubrebocas" or face masks (not shown but available upon request). Also, when dividing the sample of states by high and low levels of H1N1, we find that the spike happens in both types of states at around the same time (figure not shown but available upon request). 33 These states are, in alphabetical order, Baja California Sur, Campeche, Colima, Chihuahua, Durango, Nayarit, Tlaxcala, and Zacatecas. (6) and (7) captures the intensity of search volume per state and year, which ranges from 0 to 100. The dependent variable in columns (1)-(5) is the log of the outcome of interest. Columns (1) and (2) denote the log of millions of expenditures in Mexican pesos. b p-value denotes the p-value of wild bootstrapped standard errors for each specification to correct for small number (32) of clusters. c In columns (6) the specifications exclude the states of Baja California Sur, Campeche, Colima, Chihuahua, Durango, Nayarit, Tlaxcala, and Zacatecas for which Google data were missing. In column (7) we assigned a value of zero to states with missing Google searches. < 0.10). These analyses indicate that the main mechanism arises from the demand for knowledge regarding hygiene practices and not so much from the other channels examined above. Our key hypothesis is that the 2009 H1N1 shocked or nudged people into changing their behavior (washing their hands) and this behavioral change led to a decline in diarrhea cases for children. Is this effect also observed with the spread of respiratory infections arising due to the seasonal flu? Theoretical models predict that engagements in preventive behavior are triggered only when the (contagious) disease crosses a threshold (Philipson 2000) . Such models would predict a null effect from the seasonal flu but an important reaction from the H1N1 pandemic. To study this question we modify Eq. (1) as follows, where y st represents the hospital discharges of diarrhea for children under five in state s and year t (A00-A09X). In Eq. (2) we are interested in the effect of the seasonal flu (Flu st ) for all ages (similar to our H1N1 variable). We obtained these data from Mexico's SINAVE for years prior to the H1N1 pandemic (2006) (2007) (2008) . This variable represents morbidity cases of the seasonal flu. While both the swine and the seasonal flu could be considered as health shocks, the latter did not exhibit the unexpected magnitude and the uncertain nature of the H1N1 pandemic. Thus we can test whether small, expected health shocks (the seasonal flu) have similar effects to larger and unexpected health shocks (the swine flu). Analogous to Eq. (2), is the parameter of interest and as before, we control for state ( s ) and time ( t ) fixed effects. We also include state-specific trends ( st ). The results of estimating this equation are presented in column 7 of Table 2 . Unlike the H1N1, there is no link between cases of the seasonal flu and diarrhea cases. Each case of the seasonal flu is associated with 0.003 additional cases of diarrhea but this parameter is not statistically different from zero. The spread of the seasonal flu does not have an effect on diarrhea as the 2009 H1N1 does. This evidence suggests that small and expected health shocks like the seasonal flu do not matter. Large and unexpected shocks like the 2009 H1N1 pandemic do. This evidence is consistent with theoretical models reviewed by Philipson (2000) . An important contribution of our paper is its capacity to test whether the effects continue over time. In the previous sections we have shown that the onset of swine flu in 2009 is associated with a reduction in diarrheal diseases as measured by hospital discharges and morbidity cases. We have presented robust evidence in favor of the causal nature of these effects, thereby ruling out pre-trends affecting both H1N1 and diarrhea cases and other possible alternative explanations. While other interventions have been able to show the contemporaneous effect of information campaigns on reduction in diarrhea cases (see for examples the 14 papers reviewed by Ejemot-Nwadiaro et al. (2008) the evidence of whether those reductions would be sustained after the campaign ends is scant. 34 While there are papers examining the persistence of the effects of hand washing campaigns (e.g., Cairncross et al., 2005; Wilson and Chandler, 1993) they concentrate mainly on hand washing practices rather than measuring possible declines in diarrhea. Our paper represents a significant advantage as we directly test whether the 2009 pandemic led to sustained declines in diarrhea. To address this issue we add more years to the control and treatment period so that our analysis includes data from 2006 to 2012. By including the years after 2009, we can test whether the subsequent years had a similar impact as 2009, and also whether the effects remain the same when there are fewer cases and concerns about the swine flu. We note that in 2009 there were over 70,000 confirmed cases of H1N1, but these numbers plummeted in the following years. In 2010, there were 2698 swine flu cases followed by only 372 in 2011 and 4507 in 2012. If the 2009 pandemic served as a "natural nudge" then the post-2009 H1N1 cases could be thought as "reminders" following the literature of behavioral economics. As reviewed by Luoto and Carman (2014) , reminders have been used in several settings where agents need follow-up nudges to sustain their health-related behavioral changes. In column 1 of Table 6 , our variable of interest captures the crosssectional and time series variation in H1N1 cases between 2009 and 2012 and as before, zero for all states and years between 2006 and 2008. In other words, we replicate Eq. (1) but with more years for both the control and the treatment periods. The coefficient of interest is −0.046, which is smaller than the coefficient of −0.105 in the main analysis. This suggests that post-2009 H1N1 cases have, on average, a smaller impact compared to the original. We then alter Eq. (1) slightly to estimate the persistence of the 2009 effect, controlling for the contemporaneous effect of the  The results of estimating Eq. (3) are shown in column 2 of Table 6 . We find evidence of a persistence effect: an increase in cases of the 2009 H1N1 is associated with a decline in the number of diarrhea cases for children under five. For every case of the 2009 H1N1 we observe 0.221 fewer cases of diarrhea, even after controlling for contemporaneous cases of H1N1 (p-value = 0.024). Furthermore, note that the contemporaneous effect of H1N1 is now positive (but not statistically significant based on the bootstrapped p-value). This positive sign is not surprising in the absence of behavioral change since 13 percent of the swine flu cases were associated with diarrhea as one of the symptoms. This reinforces our hypothesis that it is the actual health shock of the 2009 H1N1 pandemic that triggered the behavioral change. Next, we consider an event study by replacing the H1N1 variable in Eq. (3) with four interaction terms of this variable with binary indicators for each year in the period 2009-2012. This allows us to evaluate the contemporaneous impact of each year separately. Likewise, if the contemporaneous effect exists only in 2009 and disappears with future H1N1 cases this would be evidence of a nudge: people adjusted their behavior in the presence of a new shock. The shock allowed them to reach an equilibrium behavior in the sense that future shocks no longer change their hygiene practices. Now consider a situation where the first nudge changed behavior, but it did not lead to an optimal solution. In this case, there is still "room for improvement" and further "reminders" are needed to foster improvements in the production of health outcomes. In this case, it seems possible that further nudges could have larger, or smaller, effects than the first nudge depending on the degree of dynamic complementarities between nudges in periods 1 and 2. 35 The results are presented in columns 3 and 4 of Table 6 . Column 3 indicates that the effect is negative in both 2009 and 2010 but with larger effects for the latter. The results, however, disappear with the 2012 H1N1. These findings suggest that further reminders help reducing diarrhea cases but for a limited time. Most importantly, in column 4, and analogous to column 2, when accounting for the persistence effect of the 2009 pandemic, we find that the contemporaneous effects become substantially less relevant −much smaller in magnitude and no longer statistically different from zero-but the 2009 effect remains. These results suggest that as a health shock, the 2009 H1N1 pandemic had a contemporaneous and a long-lasting effect in the reduction of diarrhea cases of young children. This paper shows that severe health shocks such as the H1N1 pandemic in Mexico led to a long-lasting improvement in health outcomes by reducing diarrhea cases among young children. Several placebo and robustness checks validate our difference-indifference identification strategy and strengthen the interpretation of our estimates as causal. While other mechanisms are possible, we present evidence supporting the hypothesis that the pandemic was a shock that induced changes in hygiene practices and motivated people living in areas with higher prevalence of the swine flu to acquire information about better hygiene practices and to wash their hands or increase their use of hand sanitizers. These findings expand our knowledge of health economics in several ways. First, as reviewed by Cawley and Ruhm (2012) , previous studies emphasizing the role of health behaviors as key inputs 35 When the H1N1 impacts become larger for all periods after 2009 an alternative explanation could come from structural changes in the health system or mass vaccinations. However, we do not observe such pattern in the post-2009 analysis. Furthermore, we have already ruled out vaccinations as a possible mechanism in section 5.1. We thank an anonymous referee for this insight. in the production of health outcomes have focused on chronic rather than infectious diseases and on advanced economies instead of developing countries. In that regard, by focusing on gastrointestinal infections in Mexico, our study expands our knowledge of the role of behavioral changes in a much less investigated setting and addresses an important gap in the literature. Second, the fact that the 2009 pandemic matters for behavioral change, compared to the null effect found from the smaller (and more predictable) seasonal flu, provides empirical support for theoretical models where a decision to engage in preventive behavior is triggered only when the (contagious) disease crosses a threshold (e.g., Philipson 2000) . Furthermore, our paper complements recent advances in behavioral economics by exploring how health shocks, such as pandemics, can act as "natural nudges" that affect the production of health outcomes and generate long-lasting effects. Our findings raise several issues regarding policy implications. First, we show that business-as-usual strategies, such as overall government health expenditures, vaccinations campaigns as well changes in infrastructure (e.g., hospital beds) are unlikely to be behind the reasons for the decline in diarrhea cases. 36 During major health emergencies, such as pandemics, individuals increase their demand for knowledge about ways to remain healthy (e.g. learning about better hygiene practice increases). If governments facilitate access to low cost information sources, such as search engines, (or hot lines, TV or radio spots), especially in areas where the disease is more prevalent, our results indicate that the public will use these resources to acquire information. In that setting, health outbreaks or emergencies could have unanticipated positive effects as long as the population is willing to change behaviors under the appropriate environment, e.g., with adequate information, incentives, and social acceptability. Furthermore, our results from Google searches indicate that agents seek information broadly by using search engines. Thus, the information to help address their demand does not need to be provided exclusively by the government. During these emergencies, profit-seeking firms could provide a public health service too. This is consistent with the points made by Mathios (1990, 1995) indicating that when producers are allowed to reveal the advantages of their products, firms could provide key information to consumer who then react to this news. For example, producers and sellers of hand sanitizers or other products that improve hygiene practices could complement government efforts by advertising the benefits of their products, especially when government's health budgets are small as in the case of many developing countries. An open research question is whether market friendly policies, such as low entry costs, could complement government efforts during health emergencies by allowing more firms to enter the market and supply the demand for health products that consumers are seeking, as shown by our findings. Finally, health shocks such as the swine flu, HIV or cancer must be considered "high-water marks" as indicated by Smith et al. (2001) regarding the effectiveness of information campaigns. Therefore, the question remains whether it is indeed possible to design information messages that could alter and sustain health behaviors analogous to the effects of the H1N1 pandemic in Mexico, but without the obvious adverse consequences of a health emergency. A possible way to identify these messages could be found in the marketing strategies implemented in another Latin America country, Uruguay, as part of a nationwide antismoking campaign. Harris et al. (2015) find that the inclusion of warning messages in cigarette packages, showing explicit pictograms of newborns affected by smoking during pregnancy, led to a higher smoking cessation for the targeted population of pregnant women. Future research, should explore whether an analogous system of effective messages could be applied to the context of promoting long-lasting and improved hygiene practices. 
paper_id= fff6febdd287d474d7950b14faa899c4095557b3	title= 	authors=   Puri;Vinod K Puri;	abstract= 	body_text= pneumonia had not been the "friend of the aged." Nuland observed, "By and large dying is a messy business." The stark contrast between the findings of the study by Heyland et al and the SUPPORT studies is troublesome. Is that difference due to cultural, attitudinal, and organizational differences for the delivery of critical care in Canada and America? The easy explanation that American patients want more treatment even at the risk of discomfort may or may not be true. After years of public and often acrimonious debate, á la Quinlan and Cruzan, physicians may feel unsettled with the following question. Are our medical practices regarding the dying more humane than they were 30 or 40 years ago? Legally and ethically, a lot of ground has been covered. The death-with-dignity movement, living wills, durable power of attorney, and even assisted suicide (in Oregon) are society's attempts to deal with difficult bioethical issues. Yet why do most family members feel betrayed and burdened when their next of kin die in the ICU? The vigorous ethical debates do nothing for the anguish of surrogates caught in the maze of "full code" and "DNR" designations in the hospital. Practically, who decides the question of whether to institute mechanical ventilation or artificial feeding becomes more important than the essential goodness of the decisions. Although the current study did not report too many out-of-control treatments, many families are fearful. Callahan 3 has referred to the illusion that we could master our medical choices: "Yet there is hardly below the surface, a remarkable and rising anxiety about dying-not necessarily death as such but the combination of an extended critical illness gradually transformed into an extended dying." His personal considerations border on accepting decline and death in an almost fatalistic manner, which is unusual in Western thought. In an ever-shrinking world, we should not underestimate the effect of life-support technology and medical know-how in societies in which ethical and legal constraints are weak or nonexistent. One often hears of the "illegality" of discontinuing mechanical ventilation in dying patients! Yet, with few support systems, these interventions may be stopped abruptly after the financial ruin of the families. Unfortunately, the immorality of such practices is rarely questioned. Decision making in these highly paternalistic medical systems requires some scrutiny. I feel that we have an obligation to our colleagues in less affluent societies. A universal ethical code for the use of life-support technology in this young century is a laudable goal. 1, 6 Vietnam, Singapore, 2 and Canada. 3 The index patients of these cities had been exposed to the Guangdong physician while they were visiting China or had been staying on the same floor of the same hotel. While investigating the outbreak of SARS in Hanoi, Dr. Carlo Urbani unfortunately contracted the disease and died. SARS appears to spread by close person-to-person contact via droplet transmission or fomite. 7 The high level of infectivity of this viral illness is highlighted by the fact that 158 patients were hospitalized with SARS within 2 weeks as a result of exposure to one single patient on a general medical ward in Hong Kong. The use of a jet nebulizer for administering bronchodilators to the index case, who presented clinically with community-acquired pneumonia, could increase the droplet load around the patient and, together with the overcrowding condition on the hospital ward, had contributed to this major hospital outbreak. 1 A novel coronavirus (CoV) is now identified as the main pathogen responsible for SARS, 8 -11 although the presence of a metapneumovirus also was inferred in studies from Canada 11 and Hong Kong. 1 Several laboratories have recently completed the sequencing of the genome of the CoV that has led to the global epidemic of SARS, and they have noted that the SARS-CoV is not closely related to any of the previously characterized CoVs. [12] [13] [14] Clinical and Laboratory Features The mean incubation period of SARS is estimated to be 6.4 days (95% confidence interval, 5.2 to 7.7), and the mean time from onset of clinical symptoms to hospital admission varied between 3 and 5 days. 15 The major clinical features on presentation include persistent fever, chills/rigor, myalgia, dry cough, headache, and dizziness. Less common symptoms include sputum production, sore throat, coryza, nausea and vomiting, and diarrhea. 1-3 Watery diarrhea has been reported in a subgroup of patients 1 week down the clinical course. This was reported in a cohort infected in a community outbreak that has been linked to a faulty sewage system, presumably due to involvement of the GI tract via the fecal-oral route. 16 Lymphopenia (ie, the destruction of both CD4 and CD8 lymphocytes), features of low-grade disseminated intravascular coagulation (ie, thrombocytopenia, prolonged activated partial thromboplastin time, and elevated d-dimer levels), and elevated lactate dehydrogenase levels (reflecting lung injury) and creatinine kinase levels (reflecting myositis) are common laboratory features of SARS. [1] [2] [3] 8, 11 The clinical course of SARS appears to follow a triphasic pattern. Phase 1 (viral replication) is associated with increasing viral load and is clinically characterized by fever, myalgia, and other systemic symptoms that generally improve after a few days. Phase 2 (immunopathologic damage) is characterized by the recurrence of fever, oxygen desaturation, and radiologic progression of pneumonia with falls in viral load. The majority of patients will respond to treatment with a combination of ribavirin and IV steroids, but 20% of patients may progress into the phase 3, which is characterized by ARDS necessitating ventilatory support. 16 Compared with adults and teenagers, SARS seems to run a less aggressive clinical course in younger children, with no children in one case series 17 requiring supplementary oxygen. The radiographic appearances of SARS share features in common with other causes of pneumonia. At fever onset, almost 80% of patients with SARS have abnormal chest radiographs, all of which show airspace consolidation. All patients will eventually develop airway opacities during the course of the disease. In our study, the opacities occupy a peripheral or mixed peripheral and axial location in 88% of patients. 18 The predominant involvement of the lung periphery and the lower zone, in addition to the absence of cavitation, hilar lymphadenopathy, or pleural effusion, are the more distinctive radiographic features of SARS. 1,18 Radiographic progression from unilateral focal airspace opacity to either multifocal or bilateral involvement during the second week of the disease course, followed by radiographic improvement with treatment, is commonly encountered. 1, 18 In one case series, 16 12% of patients developed spontaneous pneumomediastinum and 20% of patients developed evidence of ARDS over a period of 3 weeks. In general, the incidence of barotrauma in ICU admissions seems higher than expected despite treatment with low-volume and low-pressure mechanical ventilation. Chest radiographs and CT scans have not demonstrated excessive hyperinflation or bullous lung disease, and it is difficult to explain this observation. 19 High-resolution CT scanning of the thorax is useful in detecting lung opacities in patients with unremarkable chest radiograph findings. Common findings include ground-glass opacification, sometimes with consolidation, and interlobular septal and intralobular interstitial thickening, with predominantly peripheral and lower lobe involvement. The characteristic peripheral alveolar opacities are very similar to those found in patients with bronchiolitis obliterans-organizing pneumonia. 1, 19 The postmortem examination of lung tissues in SARS patients has shown various levels of disease severity. Changes include gross consolidation of the lungs, the presence of interstitial mononuclear inflammatory infiltrates, desquamation of pneumocytes in alveolar spaces, pulmonary edema with hyaline membrane formation, and cellular fibromyxoid-orga-nizing exudates in airspaces, indicating the organizing phase of alveolar damage. Viral inclusions were not detected. 1, 10 Diagnostic Criteria The diagnosis of SARS is based on clinical, epidemiologic, and laboratory criteria that have been laid down by the Centers for Disease Control and Prevention. 20 The clinical criteria include the following: (1) asymptomatic or mild respiratory illness; (2) moderate respiratory illness (ie, temperature, Ͼ 100.4°F or 38°C) and at least one respiratory feature (ie, cough, dyspnea, difficulty breathing, or hypoxia); (3) severe respiratory illness (features of the second criterion and radiographic evidence of pneumonia, the presence of respiratory distress syndrome, autopsy findings consistent with pneumonia, or the presence of respiratory distress syndrome without an identifiable cause). The epidemiologic criteria include travel (including transit in an airport) within 10 days of the onset of symptoms to an area with current, recently documented, or suspected community transmission of SARS, or close contact within 10 days of the onset of symptoms with a person known or suspected to have SARS infection. Laboratory criteria include the following: (1) the detection of an antibody to SARS-CoV in specimens obtained during acute illness or 21 days after illness onset; (2) the detection SARS-CoV RNA by reversetranscriptase polymerase chain reaction (PCR) that was confirmed by a second PCR assay by using a second aliquot of the specimen and a different set of PCR primers; or (3) the isolation of SARS-CoV. A case of probable SARS is defined as having met the clinical criteria for severe respiratory illness of unknown etiology with onset since February 1, 2003, and epidemiologic criteria, irrespective of the laboratory result. A case of suspect SARS is defined as having met the clinical criteria for moderate respiratory illness of unknown etiology with onset since February 1, 2003, and epidemiologic criteria, irrespective of the laboratory result. 20 The treatment of SARS has been empirical during the recent outbreak. Anecdotal experience using a combination of ribavirin and steroids has been described by two studies in Hong Kong. 1, 21 Oral ribavirin (1.2 g tid orally or 400 mg q8h IV) and corticosteroids (ie, prednisolone, 1 mg/kg/d) were prescribed as combination therapy. 1 During phase 2, when there was radiologic progression of pneumonia and/or hypoxemia, IV high-dose methylprednisolone, 0.5 g daily for up to 6 doses in most cases, is administered to prevent immunopathologic lung injury, with the rationale that progression of the pulmonary disease may be mediated by the host inflammatory response. 16 The majority of our cohort (90% of 138 patients) appeared to have a favorable response to the combination treatment with resolution of fever and lung opacities within 2 weeks, whereas about 23% and 14%, respectively, of the same cohort required ICU admission and invasive ventilatory support. 1 The use of ribavirin therapy in SARS patients is associated with significant toxicity, including hemolysis (76% of patients) and a decrease in hemoglobin of 2 g/dL (49% of patients), elevated transaminase levels (40% of patients), and bradycardia (14% of patients). 3 Any treatment regimen for SARS needs to be tested with a randomized placebo-controlled design. New antiviral agents and immunomodulating agents are also under investigation. Noninvasive positive-pressure ventilation has been used for treatment with some success in a small number of SARS patients with respiratory failure. 21 However, therapy with noninvasive positive-pressure ventilation should be carried out only if there is adequate protection for the health-care workers (eg, an isolation room with adequate air exchange) because of the potential risk of viral transmission via mask leakage and flow compensation causing the dispersion of a contaminated aerosol. The calculation of case fatality rates in the situation of an emerging epidemic is difficult, but it has been estimated to be 13.2% (95% CI, 9.8 to 16.8%) for patients Ͻ 60 years of age and 43.3% (95% CI, 35.2 to 52.4%) for those Ն 60 years of age. 15 The prognostic factors associated with a poor outcome (ie, ICU admission or death) include age, 1, 15, 16 chronic hepatitis B treated with lamivudine, 16 high peak lactate dehydrogenase, 1 high neutrophil count on presentation, 1 or presence of diabetes mellitus or other comorbid conditions. 3 In conclusion, with the recent onset of the SARS epidemic worldwide, research on the development diagnostic tests and an effective treatment is urgently needed. We hope that the availability of the genome sequence of the SARS-CoV 12-14 will facilitate efforts to develop new and rapid diagnostic tests, antiviral agents, and vaccines in the long run. SARS patients who have recovered from the acute illness should be monitored carefully for the possibility of continued viral shedding 16 and the potential development of pulmonary fibrosis or late postviral complications. The prevention of spread is most important for this highly infectious disease. Isolation facilities, strict precautions against droplet exposure (ie, hand hy-giene, and the wearing of gowns, gloves, N95 masks, and eye protection) among health-care workers managing SARS patients, 22 the avoidance of the use of nebulizers on a general medical ward, 1  
paper_id= fff75f3cec2b458462cba1891ce88aced71b2b86	title= Cross-sectional adherence with the multi-target stool DNA test for colorectal cancer screening in a large, nationally insured cohort	authors= Lesley-Ann  Miller-Wilson;·  Lila;J  Finney;Rutten  ;·  Jack;Van  Thomme;· A Burak Ozbay;·  Paul;J  Limburg;	abstract= Purpose Colorectal cancer (CRC) is the second most deadly cancer in the USA. Early detection can improve CRC outcomes, but recent national screening rates (62%) remain below the 80% goal set by the National Colorectal Cancer Roundtable. Multiple options are endorsed for average-risk CRC screening, including the multi-target stool DNA (mt-sDNA) test. We evaluated cross-sectional mt-sDNA test completion in a population of commercially and Medicare-insured patients. Methods Participants included individuals ages 50 years and older with commercial insurance or Medicare, with a valid mt-sDNA test shipped by Exact Sciences Laboratories LLC between January 1, 2018, and December 31, 2018 (n = 1,420,460). In 2020, we analyzed cross-sectional adherence, as the percent of successfully completed tests within 365 days of shipment date. Results Overall cross-sectional adherence was 66.8%. Adherence was 72.1% in participants with Traditional Medicare, 69.1% in participants with Medicare Advantage, and 61.9% in participants with commercial insurance. Adherence increased with age: 60.8% for ages 50-64, 71.3% for ages 65-75, and 74.7% for ages 76 + years. Participants with mt-sDNA tests ordered by gastroenterologists had a higher adherence rate (78.3%) than those with orders by primary care clinicians (67.2%). Geographically, adherence rates were highest among highly rural patients (70.8%) and ordering providers in the Pacific region (71.4%). Conclusions Data from this large, national sample of insured patients demonstrate high cross-sectional adherence with the mt-sDNA test, supporting its role as an accepted, noninvasive option for average-risk CRC screening. Attributes of mt-sDNA screening, including home-based convenience and accompanying navigation support, likely contributed to high completion rates. 	body_text= Colorectal cancer (CRC) is the second leading cause of cancer deaths and fourth most diagnosed cancer in the USA [1] , representing an ongoing public health concern, with estimates of 149,500 incident and 52,980 fatal cases in 2021 [2] . Average-risk CRC screening can favorably impact the CRC public health burden by identifying patients with premalignant or localized malignant neoplasia for earlier, more effective intervention. National organizations such as the United States Preventive Services Task Force (USPSTF) in their 2021 Final Recommendation Statement and the American Cancer Society (ACS) recommend lowering the age for average-risk CRC screening (beginning at the age of 45 years) using one of several equally endorsed test options, including the multi-target stool DNA assay [3] [4] [5] (mt-sDNA; marketed as Cologuard®; Exact Sciences, Madison, WI). Since receiving approval from the US Food and Drug Administration in August 2014, the mt-sDNA assay has been prescribed by over 200,000 providers and completed by more than 4 million patients nationwide. Between 2015 and 2018, estimated CRC screening rates increased overall by 4.2%, from 61.7 to 65.9% according to National Health Interview Survey data [6, 7] . The same National Health Interview Survey (NHIS) data showed that for those who completed screening, 4.1% used mt-sDNA, and use was consistent across demographic subgroups such as sex, age, and race/ethnicity, with no apparent disparities. The number of people screened with the mt-sDNA test further increased in 2019, with 1.7 million patients successfully screened using mt-sDNA testing [8] . Despite the COVID-19 disruption, the at-home mt-sDNA test with its built-in navigation component for patients and providers and door-to-door shipping is uniquely positioned to sustain colorectal cancer screening efforts, potentially tempering the backlog of CRC screening delays and corresponding late-stage disease. One critical component of effective CRC screening programs is patient adherence with completing the selected test option. The practical effectiveness of available screening strategies may be reduced by suboptimal adherence to screening recommendations [9] . Discouragingly, prior research has demonstrated relatively low completion rates for other stool-based CRC screening tests [9] [10] [11] [12] [13] [14] , and several recently conducted trials have revealed differences in CRC screening completion rates by test modality [9] [10] [11] 14] . These differences in completion rates may be exacerbated by multiple factors, such as socioeconomic status [15, 16] and race [17, 18] . Given the differences between endoscopic, radiologic, and stool-based screening strategies, accurate understanding of test-specific patient adherence is critical for population-, provider-, and payor-level discussions. To date, analyses of mt-sDNA adherence have demonstrated high cross-sectional adherence rates (71%) but have been limited to Medicare beneficiaries to minimize the influence of insurance variability on test completion rates [19] . Given the growing adoption of mt-sDNA screening by commercial insurance plans, nearly all insured (> 94%) average-risk patients now have access to mt-sDNA screening with no out-of-pocket costs [20] . Thus, additional real-world assessment of mt-sDNA test adherence in non-Medicare patients is both feasible and timely. Here, we evaluated cross-sectional mt-sDNA test completion in a fully insured population and identified associated factors, to better inform shared decision-making, quality monitoring, and comparative effectiveness studies related to average-risk CRC screening. Aggregate laboratory data from Exact Sciences Laboratories LLC (ESL; Madison, WI), the sole-source national laboratory for mt-sDNA testing, were retrospectively reviewed as part of ongoing laboratory quality management processes and in compliance with the Health Insurance Portability and Accountability Act. Per Mayo Clinic Institutional Review Board (IRB) criteria, this study was deemed as exempt from review. Eligible study participants included individuals who met the following criteria: ages 50 years and older, covered by commercial insurance or Medicare, with a valid mt-sDNA test shipped to the order-specified address from ESL between January 1, 2018 and December 31, 2018. At the time of the study, the USPSTF recommendation was to start screening at 50 years old. Valid mt-sDNA test shipments were defined as having all information required for ESL to analyze the sample and report a positive or negative test result. Deidentified data referent to available patient (sex, age), provider (specialty, practice location), and test order features (testing status, time to completion) were sourced from the ESL internal data systems. Of note, patient race/ethnicity is listed as an optional field on the mt-sDNA order form and was additionally collected when available. Information from ESL internal data systems regarding order characteristics was initially collected for 1,508,087 patients. Inclusion and exclusion criteria (including provider/patient cancellations, duplicate orders, missing information, and reasons for ineligibility) were utilized to comprise the final analysis cohort consisting of 1,420,460 patients. The cohort attrition flow chart outlines that crosssectional adherence was defined as the percentage of eligible participants who successfully completed the test within 365 days of the shipment date ( Fig. 1) . Sociodemographic characteristics including income and education are not tracked in the ESL test order data system. Therefore, these data were ascertained at the residential zip code level from public data sources, including the United States Census and American Community Surveys database [21] [22] [23] .  Descriptive statistics were used to describe the baseline characteristics of the study population overall and by insurance type. Counts and percentages were used to describe population-level statistics while distribution statistics (mean and median) were provided, where appropriate, to further describe the study populations' time to adherence, overall adherence, and age. Descriptive statistics were principally stratified by patient insurance type. Distribution statistics relating to skewness and other appropriate tests (e.g., chi-square test) were used as needed to describe the study population. Given that the study population comprises the entirety of mt-sDNA patients during this time (N = 1,420,460), point estimates and p-value testing do not provide necessary context or support better comparison. In essence, very small differences, even if significant, are likely to arise due to the population size though do not reflect meaningful differences among populations. In contrast to significance tests, effect size is independent of sample size, while statistical significance can be dependent upon both sample size and effect size [24, 25] . To avoid confounding the meaning of significance due to such a large sample size, we decided not to report two-sided 95% confidence intervals for point estimates or two-sided p-values for subgroup comparisons. Of 1,420,460 patients who met the study criteria ( Fig. 1) , 61.2% were female with a mean age of 64.9 years. Age distributions were 50-64 years (47.3%), 65-75 years (40.0%), and 76 + years (12.7%), with the majority of the cohort residing in urban areas (70.4%). Of those who specified a race/ethnic category, the majority of participants were white, although most participants (85.6%) did not provide this optional information (Table 1) . Within the study population, 46.1% had commercial insurance coverage, 33.5% had Traditional Medicare coverage, and 20.4% were covered by Medicare Advantage (Table 2 ). Patients residing in zip codes with above average Bachelor's degree attainment rates (35.0% of all adults above 25 in the USA had a Bachelor's degree as of 2018) [22] accounted for 31.2% of all shipments. Patients in zip codes with a median income greater than the national median ($33,706 per the American Community Survey conducted in 2018) [23] accounted for 43.3% of mt-sDNA test orders. Geographically, the highest adherence in the study population was observed for those dwelling in highly rural locations: urban (66.0%), rural (68.5%), and highly rural (70.8%) ( Table 3 ). Most mt-sDNA test orders were placed by primary care clinicians (72.2%), followed by nurse practitioners and physician assistants (19.3%), gastroenterologists (3.2%), and obstetrics and gynecologists (1.8%), with the remaining mt-sDNA test orders placed by other specialties (3.5%) ( Table 4 ). The overall cross-sectional adherence rate for the entire study population was 66.8% (948,769/1,420,460 patients received and completed the mt-sDNA test and had a valid result) with a median time-to-adherence (TTA) of 22 days. The difference in adherence between males and females was small (67.2 vs. 66.5%, respectively). Cross-sectional adherence was highest in patients with Traditional Medicare coverage (72.1%) and lowest in patients with commercial insurance coverage (61.9%). Adherence increased with age: 50-64 years (60.8%), 65-75 years (71.3%), and 76 + years (74.7%) overall, and within each of the insurance types ( Table 2 ). Relative to those with commercial insurance, increases in adherence by age were more pronounced among those with Traditional Medicare or Medicare Advantage plans, who demonstrated lower rates of adherence among those 50-64 compared to those with commercial insurance (Table 2) . Moreover, adherence was 73.6% for Traditional Medicare patients ages 65-75. Statistically, cross-sectional adherence with the mt-sDNA test was not shown to be associated with level of education or median income of the zip code wherein patients resided. Adherence was 66.0% in zip codes with an above average Bachelor's degree attainment rate compared with 67.2% in zip codes with a below average Bachelor's degree attainment rate. Similarly, adherence was 68.2% compared to 65.7% in zip codes with median incomes above and below the national median, respectively (Table 3) . Adherence also varied among ordering provider specialty and practice location. Patients with mt-sDNA tests ordered by gastroenterologists exhibited a higher adherence rate (78.3%) than those with orders placed by primary care clinicians (67.2%), nurse practitioners and physician assistants (63.5%), obstetricians and gynecologists (63.1%), and other specialties (67.3%). Adherence rates were highest among patients with ordering providers in the Pacific region (71.4%) and West North Central region (70.1%). Contrastingly, the lowest adherence was observed in the Mid-Atlantic region (65.7%), New England (65.2%), West South-Central region (64.6%), and Puerto Rico/US Territories (60.7%) ( Table 4 ). Results from this retrospective study of a large, insured population demonstrate relatively high cross-sectional adherence with the mt-sDNA test for CRC screening. Adherence rates were similar between males and females and increased with age. Of note, area-level education and median income of patients' residential zip codes were not significantly associated with differences in adherence rates. Consistent with previous observations in Medicare patients [19] , adherence rates were also higher for orders placed by gastroenterologists compared to other provider specialties. Geographically, the greatest adherence was observed among those living in highly rural areas; however, that population was only 3% of the overall cohort. Although GIs ordered the fewest mt-sDNA tests, the rate of adherence to recommendations by disease specialists may relate in part to more detailed discussion during encounters regarding the importance of completing CRC screening; albeit, this speculative interpretation requires further evaluation. With nearly 1.5 million participants included in the study population, our data contributes to the emerging literature regarding mt-sDNA test adherence in the clinical setting, although in a younger and larger cohort than seen previously [19] . Overall adherence approached 67%, with meaningful differences observed between age groups. The highest rate was seen in the 76 + years subgroup having Traditional Medicare coverage. Adherence also increased with age, from 60.8% in those 50-64 to 74.7% in patients ≥ 76 + years, while Traditional Medicare patients ages 65-75 exhibited 73.6% adherence. The mt-sDNA test is offered in conjunction with patient and provider navigation support (available 24 h per day, 365 days per year, with translation services in over 240 languages). Previous studies have shown that patient navigation has a beneficial effect on patient health behavior, demonstrating increased CRC screening rates [19, 26, 27] . The amalgamation of test and navigation services was shown to be successful in a separate study of Medicare beneficiaries where individuals who were previously non-adherent with CRC screening were offered the mt-sDNA test, resulting in 88% completion and 96% diagnostic colonoscopy exam follow-up for those with a positive mt-sDNA result [28] . This intricate feature adds inherent value to the mt-sDNA test at no additional cost to the payor, health system, patient, or provider, while also improving the probability of test completion compared with no intervention [19] [20] [21] [22] [23] [24] [25] [26] [27] [28] [29] . This information may be helpful to consider as a guiding input variation for CRC modeling studies to investigate comparative effectiveness between screening modalities. There is general agreement concerning the effectiveness of colorectal cancer screening; however, in contrast to other organ sites, major guideline groups have endorsed more than one test option for average-risk CRC screening in order to maximize engagement. Informed health care provider discussions and screening decisions are better guided by implementation considerations that can help patients select the testing option they are most likely to complete, and this factor is essential for reducing the number of CRC-related deaths in the USA. Prior research has demonstrated low completion rates for other CRC screening tests [9, 10] , and existing literature on repeat screening with other non-mt-sDNA stool-based screening tests suggests a need for improvement [9] [10] [11] 30] . A more sensitive test, such as mt-sDNA, is less susceptible to drops in adherences rates. Mt-sDNA has a higher single application sensitivity for all stages of CRC, which distinguishes it from single marker fecal occult blood tests (FIT/gFOBT) [31] . The 10,000-person mt-sDNA pivotal study by Imperiale et al. [31] reported 92% sensitivity for CRC (vs 74% for FIT). This performance difference held true in receiver operating characteristic (ROC) analysis, regardless of the cutoff chosen for FIT. Higher one-time test sensitivity is relevant given that FIT adherence to recommended screening frequency can drop off substantially after the first cycle of annual testing [32] . As an integral part of the screening process, the follow-up colonoscopy to a positive non-colonoscopy CRC screening exam is becoming increasingly appreciated in the literature. Follow-up colonoscopy after a positive first-line test is necessary for complete and effective CRC screening [33, 34] . Failure to, and delay of follow-up after a positive stool-based test has been associated with increased risk of later stage CRC and CRC mortality [33, [35] [36] [37] . Doubeni et al. found that patients failing to follow-up on abnormal FIT results face a sevenfold higher risk of dying of CRC than those who complete the follow-up process [37] . Differences have also been revealed in CRC screening completion rates by modality [13, 14] . In a study by Rutten et al. [14] among residents of Olmsted County, MN, eligible and due for CRC screening, it was revealed that CRC screening incidence rates remained stable from 2016 to 2018, while test-specific rates for mt-sDNA significantly increased. Moreover, the study revealed that adherence with follow-up colonoscopy within 6 months after a positive stool-based test was significantly higher among patients who underwent mt-sDNA screening versus FIT/FOBT (84.9% vs 42.6%, respectively). At real-world (imperfect) adherence rates of 40% for annual FIT and 70% for triennial mt-sDNA derived from a critical assessment of meta-analyses and retrospective cross-sectional data [19, 38] (in systems using FIT without a navigation program) [39] , modeling illustrates that the number of LYG and reductions in CRC incidence and mortality were higher for triennial mt-sDNA [40] . Several geographic variations in adherence aligned with the currently reported US percentage of adults being up-todate with CRC screening tests by state [41] , with the highest rates seen in the Pacific and West North Central regions in our study, although adherence was lower in the New England and Middle Atlantic regions where up-to-date screening hovers around ≥ 70% and ≥ 65%, respectively. Additionally, practice specialty was associated with higher cross-sectional adherence. Although gastroenterologists ordered fewer tests, adherence was lower among primary care providers. The observed difference in adherence by specialty could be related to various provider factors (strength of screening recommendation, patient perception of specialty-related knowledge), patient factors (attitude concerning referral/follow-up completion), or a combination. It is also likely that patients who present to a gastroenterologist bring with them specific concerns which may serve as motivation to screen. Although test adherence rates differed by practice, recent literature has shown widespread awareness of the 2018 ACS CRC guidelines among primary care providers [42] . A 2020 survey study by the Montana Cancer Control Program revealed that fecal DNA testing has become much more popular since 2016, with more than a third (35.2%) of primary care providers reporting discussing it with patients in 2020 compared to only 4% in 2016 [43] . The same report from the Montana Department of Public Health and Human Services highlighted that in 2020 the fecal DNA test was the 3rd highest ranked CRC screening test prescribed with 86% of ordering providers considering the test effective. Future CRC screening studies can be tailored to garner information on provider ordering characteristics to better inform screening strategy, assess prescription differences, and develop provider education initiatives. We would like to acknowledge several limitations of our study. First, there was limited reporting of ethnicity in this study population, with less than 15% of patients specifying a racial category. The unavailability of such data prohibited us from examining an important demographic aspect associated with screening adherence; thus, the differences observed among race/ethnicity were not overly considered due to a large percentage of participants not providing the optional information. Patient and provider factors analyzed in our study were limited to those characteristics that could be captured from the existing laboratory database. A more detailed assessment of factors suspected to influence mt-sDNA screening adherence, such as socioeconomic status along with a closer look at social determinants of health, will benefit future investigations. Secondly, we relied on determination of the mt-sDNA ordering clinician to define CRC average-risk status. Although indicated for mt-sDNA, we assumed that each provider appropriately followed test eligibility indications before prescribing, and we did not have access to sufficient data to confirm risk status for all participants in this real-world study. Thus, a percentage of off-label use can be logically assumed. Lastly, given the relative recency of mt-sDNA availability and adoption in clinical practice, we did not assess longitudinal adherence for the current study. Mt-sDNA test adherence was only evaluated on a cross-sectional basis with a review of associated patient and provider factors over a 1-year study period. Further evaluation of longitudinal adherence with triennial mt-sDNA screening is anticipated in future studies. Novel data from this large retrospective cohort of both Medicare and commercially insured patients revealed favorable adherence to CRC screening with the mt-sDNA assay test among eligible patients who met the study criteria, having a valid mt-sDNA test shipped to their orderspecified address from ESL between January 1, 2018, and December 31, 2018. Innovative features of the mt-sDNA test such as the accompanying patient navigation system, together with its noninvasive approach and feasibility, likely impacted test completion rates. The highest adherence rates were observed among those covered by Medicare and for tests ordered by gastroenterologists. Our findings contribute valuable real-world data to the existing evidence base and provide clinical decisionmakers with supportive information to recommend a guideline-endorsed CRC screening strategy, with the goal of increasing adherence through test completion. Future investigations examining longitudinal completion rates in accordance with national guidelines, as well as more detailed analyses of associations with socioeconomic, race/ethnicity, and other demographic factors on mt-sDNA test completion rates, are anticipated to complement this study. 
paper_id= fff784e2a806b4425ffa677f1dda631ae988632d	title= Cellular senescence: friend or foe to respiratory viral infections?	authors= William J Kelley;Rachel L Zemans;Daniel R Goldstein;	abstract= @ERSpublications Senescence associates with fibrotic lung diseases. Emerging therapies to reduce senescence may treat chronic lung diseases, but the impact of senescence during acute respiratory viral infections is unclear and requires future investigation. https://bit.ly/2SAPpx6 ABSTRACT Cellular senescence permanently arrests the replication of various cell types and contributes to age-associated diseases. In particular, cellular senescence may enhance chronic lung diseases including COPD and idiopathic pulmonary fibrosis. However, the role cellular senescence plays in the pathophysiology of acute inflammatory diseases, especially viral infections, is less well understood. There is evidence that cellular senescence prevents viral replication by increasing antiviral cytokines, but other evidence shows that senescence may enhance viral replication by downregulating antiviral signalling. Furthermore, cellular senescence leads to the secretion of inflammatory mediators, which may either promote host defence or exacerbate immune pathology during viral infections. In this Perspective article, we summarise how senescence contributes to physiology and disease, the role of senescence in chronic lung diseases, and how senescence impacts acute respiratory viral infections. Finally, we develop a potential framework for how senescence may contribute, both positively and negatively, to the pathophysiology of viral respiratory infections, including severe acute respiratory syndrome due to the coronavirus SARS-CoV-2. 	body_text= Cellular senescence describes a state of permanent replicative arrest in normally proliferative cells. Originally discovered in vitro in human fibroblasts by HAYFLICK and co-workers [1, 2] , the concept of cellular senescence was met with controversy. Initially, some investigators were sceptical and believed that senescence was an in vitro artefact. But with accumulating in vivo evidence, including the presence of senescence in aged human skin cells [3] , scientists accepted senescence as a true biological phenomenon. Senescence is now known to contribute to a variety of age-related diseases including type 2 diabetes, obesity, atherosclerosis, COPD, pulmonary fibrosis and others [4] [5] [6] . With ageing, our cells continually divide, shorten their telomeres and accumulate mutations and DNA damage, all of which ultimately increase the likelihood of oncogenesis [7, 8] . Under typical physiological conditions, oncogene-induced senescence may hinder the development of certain cancers including lymphoma [9] and prostate cancer [10] and improve patient response to chemotherapeutics [11] . In addition, senescent cells may upregulate phagocytic receptors, therefore increasing phagocytosis and immune surveillance to reduce cancer development. Additionally, senescent cells secrete extracellular matrix proteins and growth factors that contribute to some restorative processes including wound healing [12] . Similarly, senescent cells have been shown to reduce liver [13, 14] and pancreatic fibrosis and promote wound healing [15] . Furthermore, fibroblasts made senescent by the matricellular protein CCN1 limit fibrosis in cutaneous wounds [16] . Senescent cells may contribute to tissue growth during embryonic development [17, 18] . In summary, senescence probably evolved to respond to the acute stress of organ development, mitigate against the acute effects of damaged cells to return tissue to homeostasis, and ultimately reduce the development of cancer [19] . With improvements in public health in the 20th century and subsequent extensions of the human lifespan, senescence has emerged as a contributing factor to several chronic age-associated diseases. Indeed, we now recognise senescence as one of the key hallmarks of ageing, as senescence occurs as we age naturally [20] . Although a key hallmark of ageing, senescence is not sufficient for all ageing phenotypes [20] , and senescence can occur in young hosts as a result of acute stressors (see next section). However, the importance of senescence in acute inflammatory conditions, especially respiratory viral infections, is less well appreciated. Here, we first briefly review how senescence contributes to chronic inflammation, particularly in the context of lung diseases, and emerging therapies to treat senescence. Then, we discuss the role senescence plays in acute respiratory viral infections and implications for such a role during infection with severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). What leads to senescence and the resulting senescence-associated secretory phenotype? Senescence can arise from both repeated cellular division and cellular stressors. Replicative senescence arises from repeated cellular divisions, which occur naturally when we age. Stress-induced senescence arises from cellular stressors such as increases in reactive oxygen species (ROS), which correlate with DNA damage [21, 22] . These can result from irradiation, chemotherapeutic agents, chronic exposure to pollutants (cigarette smoke), exposure to pathogens and some forms of accelerated ageing syndromes, such as progeria [23] [24] [25] [26] . Therefore, cells from both young and aged hosts can exhibit senescence. As previously mentioned, senescence is one of the key biological hallmarks of ageing and contributes to ageing via inflammation, tissue exhaustion and impaired stem-cell renewal [20] . Importantly, stem cells can become senescent as we age. Specifically, mesenchymal stem cells isolated from older humans (aged >70 years) exhibit senescence including reduced proliferation and a chronic inflammatory secretome, which impairs haematopoietic stem and progenitor cell (HSPC) replication and increases HSPC monocyte chemoattractant protein-1 and interleukin (IL)-8 expression [27] . Consequently, senescent mesenchymal stem cells cannot be used in stem-cell therapeutics due to their inability to proliferate and their reduced expression of pro-angiogenic factors [28] . Additionally, senescent neural progenitor cells contribute to the development of multiple sclerosis [29] . Specifically, senescent neural progenitor cells inhibit oligodendrocyte differentiation and contribute to demyelination via the production of high mobility group box (HMGB)1, which suppresses oligodendrocyte progenitor cell proliferation and induces pro-inflammatory transcriptomic changes in oligodendrocyte progenitor cells [29] . Thus, senescent stem cells impair wound healing, promote the development of inflammatory diseases and limit the use of stem cells in therapeutic applications. In addition to the direct effects on stem cells, there are two potential pathways by which senescence may contribute to disease. First, senescent cells stop proliferating, which prevents tissue repair after injury. Second, as senescent cells accumulate, they produce, for unclear reasons, low levels of inflammatory mediators, a phenomenon which has been termed "inflammaging", particularly when evident in older people [30] [31] [32] [33] . Senescent cells contribute to chronic inflammation via the senescence-associated secretory phenotype (SASP), the collective term for the pro-inflammatory chemokines and cytokines released by senescent cells [34] . The SASP is comprised of a number of pro-inflammatory molecules, including tumour necrosis factor (TNF)-α, IL-1α/β, IL-6, IL-8, CC-chemokine ligand (CCL)-2 and others [35] [36] [37] [38] . The SASP seems to be a double-edged sword. On the one hand, long-term exposure of SASP inflammatory mediators can lead to over-recruitment of damaging immune cells, resulting in chronic inflammatory diseases and even fibrosis [5, 39] . On the other hand, in the short term, the SASP promotes wound healing that is required to respond to acute cellular damage [12, 13, 40] . Importantly, the specific characteristics of the SASP may be dependent on the cell type and the mechanism of senescence induction [41] . For example, the SASP associated with fibroblasts contains significantly more SASP factors than the SASP associated with renal epithelial cells [41] . Some SASP pathways are shared between fibroblasts and epithelial cells, while some pathways are distinct to each cell type. Specifically, senescent epithelial cells upregulate pathways associated with protein translation and degradation; senescent fibroblasts upregulate pathways associated with apoptosis, ROS generation and extracellular matrix reorganisation; and both senescent fibroblasts and epithelial cells upregulate pathways associated with vesicle transport, metabolism and detoxification [41] . Note that in this particular study, senescence was induced in vitro in human cell lines. However, the investigators identified several secreted proteins in their in vitro models which were also found in the plasma of older subjects, suggesting that the findings of the study could be translatable in vivo. In vitro senescence is associated with the accumulation of sterile inflammatory mediators, like HMGB1, a nuclear protein that can activate inflammation [41] . Furthermore, SASP mediators such as TNF-α can induce senescence, which probably further enhances the SASP in a feed-forward loop [42] . Thus, one of the mechanisms by which senescence leads to chronic inflammation may be by the release of sterile inflammatory mediators, also known as damage-associated molecular patterns. Additionally, the SASP is temporally dynamic and the dynamics change depending on the cell type [43] . Overall, senescence induces distinct cellular programmes that lead to the secretion of proteins that promote chronic inflammation. Clearly, dissecting the physiological from the pathophysiological aspects of senescence will be required for the development of effective therapies targeting senescence to alleviate age-related diseases. Recently, there has been a focus on alleviating the negative effects of cellular senescence using senescence-targeted therapeutics. Such therapeutics fall into two primary categories: SASP inhibition and senescent cell removal. SASP inhibition involves downregulating various components of the SASP, or inhibiting the inflammatory pathways that SASP factors engage, with the goal of ameliorating the deleterious effects of SASP factors. This strategy is successful in a variety of contexts. For example, Janus kinase inhibition alleviates inflammation, both locally in the adipose tissue and systemically, and reduces frailty in aged mice [44] . Evidence that the SASP may be related to the pathogenesis of age-related diseases comes from reports that pharmacological tools to inhibit NF-κB signalling ameliorate age-related diseases. Inhibition of the mammalian target of rapamycin by rapamycin selectively reduces NF-κB signalling (a major inflammation signalling hub) in senescent cells through suppression of IL-1α activity, reduces age-associated cognitive decline, improves immune function in older people and increases the lifespan of mice [45] [46] [47] [48] [49] . Metformin, a commonly used type-2 diabetes drug, also inhibits NF-κB, resulting in improved lifespan in mice, and reduces all-cause mortality and age-related disease in humans [50] [51] [52] [53] . However, inhibiting the SASP results in off-target side-effects including neutrophilia, nephrotoxicity and changes in T-cell phenotype due to the myriad interconnected pathways involved in inflammatory signalling [54] . Specifically, inhibition of the NF-κB pathway, either by genetic deletion or small-molecule inhibition, has been shown to inhibit T-cell, B-cell and lymphoid progenitor cell development [54, 55] ; induce apoptosis of thymocytes, B-cells and macrophages [56, 57] ; impair lymphocyte growth and cytokine production [58] ; and potentially increase susceptibility to various infections [59] . All of these strategies target inflammatory pathways that senescence may activate, but do not specifically target senescent cells. Removing senescent cells involves directly targeting senescent cells by inducing apoptosis. The first therapeutics designed to directly kill senescent cells (termed "senolytics") targeted the BCL-2 protein family, an anti-apoptotic pathway that is involved in oncogenesis. However, the most widely studied therapeutic strategy is a combination therapy involving the administration of the tyrosine kinase inhibitor dasatinib (D) and the flavonoid quercetin (Q). The "D+Q" combination reduces senescent cell burden and improves healthspan and lifespan in aged mice, reduces vascular stiffening, improves vasomotor function and improves pulmonary function in murine models of pulmonary fibrosis [60] [61] [62] [63] [64] . Indeed, preliminary clinical trials have shown evidence that D+Q treatment reduces senescent cell burden and may improve physical function in patients with pulmonary fibrosis [65, 66]. However, we still do not fully appreciate the potential off-target effects of senolytic agents such as D+Q, as well as their mechanism of action in humans, which will require elucidation before they can be used to treat age-related diseases. In addition to replicative senescence induced by ageing, several other factors contribute to the development of stress-induced senescence in the lung, including radiation therapy, smoking, mechanical ventilation and chronic use of supplemental oxygen (figure 1) [24, [67] [68] [69] [70] , which supports the idea that other processes besides natural ageing lead to senescence. Increasing numbers of senescent cells correlate with increasing concentrations of SASP factors. This is supported by the finding that the cytokines IL-6 and IL-10 and neutrophil counts are elevated in the bronchoalveolar lavage fluid of older donors (note: "older" is designated age >65 years in this review), which may exhibit senescence in the lung [71] . This age-related phenotype correlates with age-related lung diseases such as COPD. Furthermore, patients with COPD exhibit increased concentrations of senescent type II alveolar epithelial cells [72, 73] and lung fibroblasts [74] , as well as elevated levels of inflammatory cytokines including IL-1β, IL-6, IL-8, IL-10 and CCL-2 [75] [76] [77] [78] . While a causal link is difficult to prove, the low-grade inflammation caused by lung senescent cells may contribute to the development of and/or exacerbate COPD. Similar patterns of elevated inflammatory cytokine levels and senescent cells are linked to other chronic lung diseases such as asthma [79, 80] . Furthermore, the exuberant wound-healing response mediated by SASP factors, combined with the inability to effectively repair tissue due to the cessation of proliferation, and possibly impairments in stem cell functions, may promote fibrosis and contribute to idiopathic pulmonary fibrosis [81] [82] [83] [84] . Thus, lung senescent cells probably contribute to the severity of many chronic lung diseases. The potential contributions of lung cellular senescence in acute respiratory conditions remain unclear. In a murine model of acute lung injury, senescent alveolar macrophages exhibit enhanced activation and secretion of pro-inflammatory cytokines, resulting in enhanced disease severity [85] . Similarly, aged mice, which probably exhibit increased lung senescence, exhibit enhanced lung permeability and increased levels of ROS in response to a lipopolysaccharide challenge, which is linked to deficient Nox4 (a ROS-generating mitochondrial protein) ubiquination [86] . Additionally, older trauma patients are significantly more likely to develop acute respiratory distress syndrome (ARDS), and older patients with ARDS exhibit a higher mortality rate [87] . However, in all of these studies the pathological role of senescence was not tested. Additionally, a recent study found that lung cellular senescence protects against lung injury induced by mechanical ventilation by preventing apoptosis [68] . Clearly, the impact of cellular senescence on acute lung conditions requires future investigation. The coronavirus disease 2019 (COVID-19) pandemic continues to claim thousands of lives every day and disproportionately affects older people. Given this, how cellular senescence impacts the host response to acute respiratory viral infections is particularly relevant. Older people experience increased susceptibility to respiratory viruses such as influenza, respiratory syncytial virus (RSV), SARS and now SARS-CoV-2 [88] [89] [90] [91] [92] . With COVID-19, the disease that results from SARS-CoV-2 infection, older people are ∼20-fold more likely to die than younger people [93] . This effect has been partially attributed to defects associated with the ageing of the immune system, termed "immunosenescence", which is distinct from classical cellular senescence as defined earlier and reviewed elsewhere [94] [95] [96] . Briefly, immunosenescence has Apart from natural ageing, other sources of senescence include chronic exposure to pollutants and cigarette smoke, chemotherapy, ionising radiation, respiratory viral infections and the use of mechanical ventilators. Each of these sources may induce senescence to varying degrees, and it remains unclear whether senescence originating from different sources and in different cells contributes to pathology similarly or differently. https://doi.org/10.1183/13993003.02708-2020 pleiotropic effects on the immune system including 1) a decrease in the proliferative capacity of haematopoietic stem cells; 2) dysregulation of innate immunity; 3 [94, [97] [98] [99] . Specifically, both CD4 + and CD8 + T-cells from aged hosts (e.g. rodents, humans) exhibit a reduced proliferative capacity associated with downregulation of surface costimulatory receptors, CD27 and CD28, which is accompanied by shortened telomeres. Collectively, these alterations within the immune system with ageing result in an impaired ability to fight viral infections and lead to reduced vaccine efficiency [90] . In addition to natural ageing, chronic viral infections (e.g. HIV, cytomegalovirus infection) induce a phenotype of reduced T-cell function (i.e. reduced proliferation and cytokine secretion) that shares features with immunosenescence [100, 101] . Although ageing of the immune system impairs host defence to acute respiratory viral infections [90] , the role that cellular senescence of lung cells plays in the pathophysiology of acute respiratory viral infections is not well understood. Clearly, older people are highly susceptible to viral infections due to myriad effects exerted by ageing on the immune system. But how much of this is contributed by cellular senescence, as defined earlier, remains to be elucidated. Indeed, it is not clear whether cellular senescence, due to natural ageing or due to an age-independent stressor, promotes or prevents acute respiratory viral infections. To the best of our knowledge, there are only a few studies on the role of cellular senescence and acute respiratory viral infections, with some showing contrasting results. There are studies that claim that cellular senescence enhances antiviral immunity [102] , which is biologically plausible, as SASP factors, which includes chemokines (e.g. CxCL1/2) that recruit innate immune cells like neutrophils, may augment host defence to viral infections [103] . Specifically, one study showed that cellular senescence inhibits vesicular stomatis virus (an RNA virus) replication in both primary murine fibroblasts with replication-induced senescence and in a tumour cell line with chemotherapy-induced senescence [102] , possibly due to SASP factors, including the release of type I interferons (IFN), important antiviral cytokines [104] . Additionally, mice that were induced to exhibit senescence within the lung via bleomycin exposure were more resistant to vesicular stomatis virus infection in vivo than mice that were not exposed to bleomycin. This is compatible with in vitro studies that found that type I IFN, in addition to other SASP components such as IL-6 and IL-8, induces cellular senescence [105, 106] , and vice versa. Additionally, cellular fusion caused by the fusogenic measles virus induces cellular senescence, including the release of SASP factors, in fibroblasts in vitro [107] , although the in vivo implications of this finding are unclear. Collectively, these studies suggest that senescence that is induced acutely by viruses may be a part of the body's antiviral immune response [108, 109] . This defence mechanism may have evolved to allow virus-infected cells to secrete SASP factors to limit viral replication in neighbouring cells and ultimately viral spread throughout the lung. Furthermore, the SASP factors may enhance host defence by promoting immune cell recruitment to the lung. But if these SASP factors are produced exuberantly, immune pathology could ensue (figure 2). Finally, some viruses, including human papillomavirus and hepatitis B, have evolved machinery specifically to overcome cellular senescence [110, 111] , suggesting that senescence plays a role in antiviral defence in some contexts. However, there is evidence that senescence contributes to the pathophysiology of respiratory viral infections. RSV induces DNA damage resulting in senescence in both mononuclear cells and lung epithelia in young mice [25] , which enhances airway tissue remodelling (exhibited by a loss of ciliated cells and an increase in secretory cells [112] ), potentially leading to permanent tissue damage and fibrosis. Furthermore, senescent cells may promote viral replication. For example, the influenza virus replicates more efficiently within senescent human bronchial epithelial cells than in non-senescent cells [113] . Similar results are found with varicella zoster virus, a herpes virus, which replicates more efficiently in senescent human dermal fibroblasts than nonsenescent cells [113] . A potential explanation for this phenomenon may be that senescence induced by these viruses downregulates the antiviral type I IFN programme, including key signalling proteins, e.g. STING and interferon regulatory factor-3, upon infection in vitro with varicella virus [113] . This downregulation could lead to reduced secretion of antiviral IFNs, promoting viral replication. However, it is not known whether this occurs in vivo. Differences between this study and the contrasting one discussed earlier, which found that senescence reduces replication of vesicular stomatis virus [102] , could be due to the different viruses employed (i.e. vesicular stomatitis versus influenza, varicella), different cell types (MCF7, A549 cell lines and primary murine embryo fibroblast cells in the vesicular stomatitis study versus primary human bronchial epithelial cells and primary human dermal fibroblasts in the influenza/varicella study), and different approaches to induce senescence (replicative senescence and bleomycin in the vesicular stomatitis study versus only replicative senescence in the influenza/varicella study). Clearly, future studies are required to determine if senescence is harmful or beneficial to pathologically relevant respiratory viruses. The overall impact of cellular senescence on acute respiratory viruses may depend on host resilience factors (figure 2). For example, the SASP may have evolved to enhance host defence by increasing the recruitment of immune cells. This is probably beneficial in resilient young hosts who do not overproduce SASP factors. Indeed, initial neutrophil recruitment to the lung is critical to host defence to influenza infection [103] . But in vulnerable aged hosts, the SASP factors may be produced at pathologically high levels to promote immune pathology. Specifically, CXCL-1, CXCL-2 and IL-17 are increased in the lungs of aged mice even before viral infection, and accompany excessive neutrophil recruitment into the lung to enhance mortality during acute influenza infection [103] . In this study, senescent alveolar epithelial cells were found to produce more neutrophil chemoattractants and thus increase neutrophil chemotaxis. Similarly, aged rhesus macaques express higher levels of CCL-2, IL-6 and IL-8 in the lungs in response to influenza infection, suggesting an exaggerated innate immune response [114] . Based on these studies, inhibiting the SASP or reducing lung senescent cells might be beneficial in aged hosts infected with acute respiratory viral infections. However, formal investigation is required to determine whether removing senescent cells prior to infection is sufficient, or if treatment during infection is required. Importantly, vulnerable hosts, including older people and people with comorbidities such as diabetes, are more susceptible to SARS-CoV-2, which emerging studies suggest is due to the cytokine storm and excessive neutrophil effector functions within the lung [90] . We still do not know whether senescent cells in the lung ultimately increase or reduce viral replication. Respiratory viruses may "hijack" the senescent machinery within the infected cell to downregulate the IFN pathway to promote viral packaging and ultimately enhance viral replication within the cell. It is not clear yet if this occurs in vivo and, if so, what underlying mechanisms are involved. In contrast, activation of the senescence machinery within a virally infected cell may be a damage-response mechanism that leads to the secretion of antiviral cytokines to limit viral replication, an issue that also requires future investigation. Alternatively, senescence may simply be a side-effect of viral infection; during and after infection, various inflammatory cytokines are upregulated, resulting in the induction of senescence in infected cells. Ultimately, the impact of senescence on respiratory viral infections requires further investigation before firm conclusions can be drawn. The impact of senescence on both viral replication and mortality in respiratory viral infections such as influenza, RSV, SARS and SARS-CoV-2 should be studied comprehensively to determine if senescence-targeted therapies, such as senolytics, might be effective, or not, at reducing the age-dependent mortality of acute respiratory viral infections. Furthermore, it remains unclear whether senescence associated with natural ageing results in the same impact on respiratory viral infections as stress-induced senescence; thus, a direct comparison between such models of senescence is warranted. For example, do mice with bleomycin-induced lung senescence exhibit a similar phenotype and mortality response to aged mice when infected with influenza, RSV or other respiratory viral infections? Additionally, the effectiveness (or lack thereof ) of senescence-targeted therapies broadly, and senolytics, specifically, has yet to be assessed during respiratory viral infections. While senolytics have shown some therapeutic efficacy in pre-clinical models of chronic illnesses such as pulmonary fibrosis, it is unknown if these effects translate to more acute conditions, and if so, how the therapeutic regimen might be optimised. For example, it may be that prophylactic senolytic treatment is effective at reducing the total senescent cell concentration in the lungs, thereby improving disease outcomes, while post-infection treatment may not be as effective, especially if removing senescent cells impairs wound healing and reduces inflammation resolution. Examining these outstanding questions will shed light on how senescence impacts our host defence to respiratory viruses, which will continue to disproportionately impact older people for the foreseeable future, particularly during the current COVID-19 pandemic. Thus, moving forward, significant research efforts focused on the interactions between cellular senescence and respiratory viral infections are clearly warranted. Author contributions: All authors were involved in the design of the review. W.J. Kelley wrote the first draft, which was then edited by D.R. Goldstein and R.L. Zemans. All authors approved the final manuscript. 
paper_id= fff7be65e11816e8c830066334b3a867214be885	title= Development and evaluation of the Good Grief program for young people bereaved by familial cancer	authors= Pandora  Patterson;Fiona E J Mcdonald;Elizabeth  Kelly-Dalgety;Bianca  Lavorgna;Barbara L Jones;Anna E Sidis;Thomasin  Powell;	abstract= Background: Adolescents and young adults (AYAs) bereaved by the death of a parent or sibling from cancer report unique psychosocial needs and can have difficulty adjusting to their loss. Unaddressed, this can result in poor long-term bereavement outcomes. This paper describes the development and evaluation of Good Griefa 3day camp-based program focused on meeting coping, social support, and respite needs of AYAs bereaved by familial cancer. Methods: One hundred and nine Australian AYAs (68% female; age: 12-25 years, M = 16.63) participated in the evaluation. Grief intensity (Texas Revised Inventory of Grief), meaning-making (Grief and Meaning Reconstruction Inventory), trauma coping (Perceived Ability to Cope with Trauma Scale) and unmet needs (Bereaved Cancer Needs Instrument) measures were administered pre-program and 3-months post-program. Acceptability was measured after each session and at the program's conclusion. Appropriateness was measured at 3-month follow-up. Thirteen participants were interviewed three months post-program on their perceptions of the program. Results: Participants reported high program satisfaction, engagement with psychosocial sessions, and enjoyment of recreational activities. Significant improvements were observed in trauma coping abilities and reductions in unmet needs for managing emotions, social support, respite, future planning, and accessing information and support domains. No change was evident in grief intensity or meaning-making as measured quantitatively. Interviews supported these quantitative findings but also identified evidence of personal growth, a component of meaningmaking. Conclusions: Good Grief is a highly acceptable and beneficial intervention that addresses the unique needs of AYAs bereaved by familial cancer. 	body_text= The impact of bereavement on AYAs The death of a parent or sibling can be a distressing and traumatic event at any life stage. However, during adolescence and young adulthood (AYA; approximately 12-25 years of age)a time characterised by emotional, psychological and physical development and identity formationsuch a loss can be particularly stressful and impactful, having short-and long-term effects on wellbeing [1, 2] . AYAs who are bereaved of a parent or sibling can experience decline in academic performance, school abandonment, feelings of anger, loneliness, isolation, and symptoms of anxiety and depression following their loss [3] [4] [5] [6] [7] . Bereaved sibling AYAs also commonly report sleep disturbance [6] , and parentally bereaved AYAs are more likely to experience separation anxiety, conduct problems, and substance abuse, and show increased functional impairment relative to non-bereaved AYAs [8] . A parent or sibling's death also often results in numerous practical impacts, including changes to family structure, financial burden, physical and emotional caregiver unavailability, and changes to future plans due to taking on caregiver or income earner responsibilities [7, 9, 10] . Regarding cancer bereavement specifically, a systematic review of the child and AYA sibling and offspring literature in this area was recently conducted by Hoffmann, Kaiser, and Kersting [11] . They found that adolescents in particular tend to have a greater vulnerability to behavioural problems and depression/anxiety symptoms in comparison to children and young adults. This is potentially due to adolescents having a greater depth of understanding of the consequences and complexities of death and loss than children [11] , but having poorer social and emotional skills to cope with the loss than young adults [12] . Furthermore, Hoffmann et al. [11] reported unresolved grief in approximately 50% of cancer bereaved AYAs two to nine years after the death, and heightened risk of self-injury in young adults who had a parent die of cancer during their teenage years compared with their nonbereaved peers. The review also found that unmet needs in social support were associated with greater risk of poor bereavement outcomes in AYAs (e.g., severe anxiety, unresolved grief). Patterson and Rangganadhan [13] identified seven key areas of need amongst AYAs as a result of their parent dying of cancer: support and understanding, help coping with feelings, to talk to people who have had a similar experience, information, to have a break/have fun, space and time to grieve, and help with household responsibilities. Building on this work and research into the needs of AYAs who had experienced the death of their parent or sibling to cancer, the Bereaved Cancer Needs Instrument (BCNI) [14, 15] was developed. The authors found that for both offspring and sibling AYAs, the two domains with the greatest percentage of unmet needs were social support from other bereaved AYAs and respite (having a break from grief/having fun) [14] . These were followed by broader social support, information about grief, and coping with emotions domains. Furthermore, AYAs with more unmet needs reported higher levels of psychological distress. Although there is a clear need for psychosocial support for bereaved AYAs (both those bereaved by cancer or due to other causes) to assist them effectively cope with and adjust to loss, there is a dearth of bereavement interventions for AYAs. Those that do exist are mostly targeted at children and younger AYAs (generally ages [6] [7] [8] [9] [10] [11] [12] [13] [14] [15] [16] [17] [16] [17] [18] [19] [20] [21] rather than being AYA specific and therefore, particularly for older AYAs, may not adequately address the needs unique to their developmental stage [22] . Additionally, few existing interventions are specific to cancer bereavement. In a recent review by Ing et al. [22] , only three of the twenty-two bereavement interventions for young people that were examined were specifically designed for those who had experienced the death of a family member from cancer, and of these, none were specific to AYAs. Design and measurement issues also exist in the literature. As highlighted in several systematic reviews of the current literature [22] [23] [24] , many existing bereavement support programs lack the use of theoretical frameworks to drive and guide the design of the program's intervention and evaluation. Further, programs have tended to focus on normalising grief and providing a space for participants to talk about and process their grief, but do not teach coping strategies for adjusting to the loss [22] . Those that did mainly employed cognitive behavioural therapy techniques, emphasising restructuring of the 'negative' emotions that arise with grief and loss [22] . It has been argued that this focus may encourage inflexible avoidance of these normal emotions, potentially inhibiting healthy grieving processes in participants [22, [25] [26] [27] . Finally, measurement issues are also evident, including a lack of pre-post and follow-up outcome measures, and reliance on parent-reports and facilitator-reports rather than by participants themselves [22, 23] . To minimise the likelihood of negative short-and long-term consequences, it is essential that evidencebased and age-appropriate support is available to AYAs who have experienced the death of a parent or sibling due to cancer. To address this, we developed Good Grief (GG)a 3-day, 2-night psychosocial program. The GG program was designed to provide a safe and supportive environment for AYAs to achieve the following outcomes: increased coping abilities to better manage grief; reduced unmet information, respite, social support, psychological support, and practical support needs; and increased post-loss meaning-making (sense of personal growth and valuing of life since the death). The name 'Good Grief' was chosen for the program in consultation with some of Canteen's clients, encapsulating the idea that the grief process is a natural, normal and healthy (as opposed to a negative or abnormal) outcome following the loss of a loved one. The following theoretical and therapeutic frameworks were used in developing the GG program and informing its outcomes. The DPM posits that within the context of daily life, bereaved individuals engage in coping processes in relation to two kinds of stressorloss and restoration [28, 29] . Loss-oriented coping refers to an individual's focus on and engagement with the grief/loss experience (e.g., thinking about the lost loved one, their death, life prior to their passing; tearfulness and longing, memories of time spent with their loved one) whereas restorationoriented coping refers to an individual's focus on adjusting to life post-loss (e.g., forming new social roles and relationships, mastery of new tasks previously completed by the deceased person, continuing with daily responsibilities, distraction from or avoidance of grief). Stroebe and Schut [28] postulate that processing of loss does not occur in phases but is an oscillation between loss-orientation and restoration-orientation, which is key to optimal adjustment. Whilst at times bereaved individuals may be caught up in the grief of their loss, at other times they may seek distraction from this, or may have no choice but to attend to other life stressors such as work. The constructivist approach theorises that an individual's understanding of their identity is formed through "the stories that we construct about ourselves and share with others" [30] . In the context of grief and bereavement, the stories and ideas a person may take up to make sense of life can be disrupted [30, 31] . Constructivist approaches such as narrative therapy [32] consider meaning making and re-storying the connection between the young person and the deceased to be central to the grieving process [33, 34] . By reflecting on the way in which the loved one has had and continues to have an impact on their values and characteristics, the bereaved young person can make some sense of their loss and consider how they wish to engage with life following it. Difficulty engaging in this process has been associated with complicated grief symptoms in young adults [31] . Additionally, in a sample of 1022 recently-bereaved university students, sense-making predicted bereavement adjustment [35] ; and adolescents' meaning-making following a life-changing event (including loss) has been associated with higher wellbeing, controlling for baseline wellbeing [36] . Research pioneered by Neff [37] defines self-compassion as openness to (as opposed to avoidance of) one's own pain and suffering as a common human experience, offering oneself non-judgmental understanding and kindness. In this way, self-compassion can be thought to have three core componentsself-kindness in moments of pain or failure (as opposed to self-criticism), common humanity (seeing one's experiences as being part of the human experience and condition as a whole), and mindfulness (holding unpleasant thoughts and emotions in a "balanced awareness") [37] . Research has pointed to selfcompassion playing a role in psychological resilience, with a number of studies linking self-compassion with well-being and adaptive coping strategies in AYAs [38] [39] [40] [41] . In the bereavement literature, a continuing bond is defined as "the presence of an ongoing inner relationship with the deceased person by the bereaved individual" [42] , and is posited as a normal and core feature of the grieving and grief resolution process [43] . A number of studies report maintenance of ongoing attachment to deceased family members in AYAs [44] [45] [46] , with Hansen et al. [45] finding that this continued bond assisted with meaning-making and bereavement adjustment. This paper describes the development and evaluation of the Good Grief (GG) programa manualised campbased program based on the DPM and aimed at meeting the social support, coping and respite needs of AYAs who have had a parent or sibling die from cancer. An advisory group comprising of clinicians with expertise in working with cancer bereaved AYAs, AYA psycho-oncology researchers, and bereaved young people, guided the program design and development. The dual process model of coping with bereavement (DPM) was selected as the primary guide for the development of the program. With this model in mind, GG was designed to encourage active oscillation between the loss and restoration orientations during the camp; moving between therapeutic grief work and recreation/respite activities throughout the camp. This model was also explicitly discussed over the course of the program to assist participants to understand the grief process in greater depth and normalise their experience. In the development of this program we were particularly considerate of the developmental stage of those attending the camp. Therapeutic activities adopted for this program were based on approaches that attend to the formation of identity and values, and normalise rather than pathologise grief responses. Such approaches include acceptance and commitment therapy (ACT), constructivist therapies such as narrative therapy and Kristen Neff's concept of self-compassion [37] . We also utilised the concept of continuing bonds, known to be helpful in familial bereavement, to assist us in the development of these activities. During the program, participants were given the opportunity to share stories about their deceased loved one. A modified version of the Life Imprint [33] (see [47] for a detailed description and evaluation of this session) involved inviting friends or family members to write about the values, characteristics and mannerisms that the young person has inherited, or learned from the deceased, and continues to carry with them. A later session adopted aspects of White's [32] outsider witness process to support young people to share memories and stories, and to respond to other group members supportively. Both of these activities were focussed on increasing social support and connection between group members. The self-compassion approach was a core feature of GG's group discussions, as participants were encouraged to show themselves and others kindness and care as they encountered a range of enjoyable and difficult experiences and emotions during the program. Finally, the principles of continuing bonds were implemented throughout GG as participants were given opportunities to practice maintaining a connection with their loved one and commemorate them. Some examples of this are participants creating memory boxes or artworks to remember their loved one, and the Honouring Ceremony session, in which young people participated in a group ritual honouring their loved ones (e.g., writing a letter to their loved one on dissolvable paper and then releasing it into the ocean). GG is a two-night three-day camp-based program to support AYAs following the death of a parent or sibling from cancer. GG aims to provide a space for young people to: increase their coping abilities in order to better manage their grief, address unmet needs (including information, respite, social support, psychological support and practical needs) that often arise following the death of a loved one due to cancer, and make meaning from their loss. Moreover, GG provides a place for young people to share about the life and legacy of their loved one with their peersto acknowledge the impact their deceased family member has had on their lives, and to honour, grieve and remember them with peers who are going through a similar experience. GG has six psychosocial sessions (see Table 1 ) interspersed with recreational activities (e.g. water sports, outdoor challenges, group games). Typically, 20-25 AYAs and 4-6 psychosocially trained facilitators (with tertiary education and experience in social work, psychology or equivalent) attended each camp. Psychosocial sessions were conducted in groups of between 8 and 10 AYAs, led by two facilitators. AYAs were eligible to participate in the GG program and its evaluation if they were aged between 12 and 25 years and were bereaved of a parent or sibling due to cancer. AYAs who were already receiving support from Canteen Australia, a national organization that provides psychosocial support to AYAs (12-25 y/o) impacted by cancer (themselves or in their family), were recruited to the program via multiple methods, including flyers, emails, phone calls and face-to-face conversations. The program was provided at no cost to attendees. Information sheets were sent to participants, informing them of the purpose of the program's evaluation, and that participation was voluntary and would not affect their ability to attend the program or their relationship with Canteen. Consent/ assent was received from all participants and a parent/ guardian if the participant was under age 18. Data were collected via paper surveys and interviews. AYAs completed demographics questions and baseline outcome measures at the beginning of the program (T0). Missing demographic data was obtained from Canteen's client management system. They completed a program satisfaction survey at the conclusion of the program (T1). In the T2 survey, completed three months post-program, participants responded to a measure assessing usefulness, relevance and practicability and outcome measures. AYAs also completed session engagement surveys at the immediate conclusion of each program session. Participants were assigned an ID number which was used to match their surveys at each timepoint (apart from the session satisfaction surveys, which were anonymous). AYAs who consented to be contacted for an interview were contacted after T2, at which point an interview was scheduled if they agreed to take part. To understand the impact of this program we used aspects of a framework for implementation outcomes [48] to assess the acceptability and appropriateness of the program and examined objective outcome measures. Qualitative semi-structured interviews were conducted with participants to gain deeper understanding of AYAs' perceptions of the program. Details are provided below. See Table 2 for a description of acceptability, appropriateness, and outcome measures used in surveys to evaluate the program. AYAs who consented to be interviewed participated in a 20-min semi-structured phone interview with a researcher which was recorded and transcribed for accuracy. Interview questions explored AYAs' personal experience of the program in greater depth than surveys, with a focus on expectations of the program, skills and knowledge acquired, and areas for program improvement. Where more than 20% of a participant's data was missing for a scale or score calculation, their data was Young people are given an opportunity to introduce the loved one they have lost to the rest of the group and share their experience of loss. -Begin exploration of personal narratives about grief experience. -Normalise the experience of grief and build the sense of shared experience amongst the group. Understanding Grief (90mins) The grief process and uniqueness of individuals' experiences is explored. The dual process model of grief is discussed. Mindfulness is introduced as a grounding activity. Personal strengths are identified by each participant. -Increase understanding of and normalise the grief process. -Explore personal narratives about grief experience. -Introduce mindfulness for grounding when distressed. -Assist young people to articulate personal strengths as a resource. Sharing Memories (90 mins) Young people share a memento/photo of their loved one with the group and create a memory box OR a create an artistic representation of their memories of their loved one and share it with the group. A mindfulness-based activity ends the session. -Facilitate the grieving process and continuing bond with loved one who died. -Support young people in engaging with and communicating memories of loved one. -Develop mindfulness skills. Honouring Ceremony (60mins) The concept of using rituals to keep a connection with deceased loved ones introduced. Young people participate in a group ritual activity to honour the memories of deceased loved ones. Another mindfulness-based grounding activity is completed. -Commemorate young people's loved ones and provide an opportunity for them to continue their bond with their loved one. -Explore ways to live well with grief (balance between keeping connections with the loved one and moving forward). Coping and Resources (90mins) Grief Support Kit: A mindfulness-based grounding activity is completed. The group discusses how they can use different items or places at home to selfsoothe using their senses. A personalised grief kit is created by each participant using self-soothing items. The session ends with participants identifying strengths they see in one another. OR a Finding Support In Our Lives: Young people's values, knowledge, skills and important supports available to them are explored and expressed through metaphorbased activities. The group brainstorms challenges they might face and generates possible coping strategies. To wrap up, a mindfulness activity is completed. -Identify and increase awareness of existing internal and external coping resources. "Since this loss, I'm a stronger person"; "I value and appreciate life more" The GMRI (overall and its subscales) has good convergent and discriminant validity [51] . Subscales demonstrated good internal consistency (α Personal Growth = .83; α Valuing Life = .76) [51] . Trauma coping abilities -Perceived Ability to Cope with Trauma Scale (PACT) [52] 20 items, 2 subscales (Trauma Focus -one's ability to spend time processing the trauma; Forward Focus -one's ability to move beyond the trauma) Seven-point numeric rating scale (Not at all able to to Extremely able to "Look for the positive in things" The Coping Flexibility Scale shows good incremental, convergent and discriminant validity [52] . excluded from that analysis. Where up to 20% of a participant's data was missing for a scale or score, a prorated scale or score was calculated using the average of valid items. AYAs' session engagement scores represent the average of their helpfulness, meaningfulness and interestingness scores for that session. It should be noted that attendees at some programs did not complete ratings for the Honouring Ceremony due to those sessions finishing late, or facilitators not wanting to disrupt the emotional flow of the session. For AYAs' ratings of overall program helpfulness and topic usefulness/relevance/practicability, the percentage endorsing the program/topic as helpful represents the combined number of Helpful and Very helpful responses, divided by the total number of responses. The same was done for overall program satisfaction (i.e., Mostly satisfied and Very satisfied responses). AYAs' responses to the open-ended satisfaction questions (what they liked most, what they found most useful, why they would/wouldn't recommend the program, suggestions to improve the program) were categorised into key topics. Measure totals were calculated. For the TRIG, responses are summed to produce an overall score (total TRIG), with higher scores representing greater grief intensity [49] . For the GMRI, scores are calculated for each subscale by summing its items, with higher scores representing a greater sense of meaning in that domain [50] . For the PACT, each subscale is scored by calculating the average rating of its items [52] . An overall Coping Flexibility Scale is calculated as the sum of the Trauma and Forward Focus subscales minus the polarity of these subscales. Higher Coping Flexibility scores represent a greater ability to employ coping strategies in both the Trauma and Forward Focus domains. For the BCNI, a score is calculated overall (total BCNI) and for each subscale by summing its items, with higher scores representing greater need in that domain [15] . To determine the effect of time on participants' outcome scores, a one-way repeated measures multivariate analysis of variance (MANOVA) was conducted with time (T0, T2) as the independent variable and grief intensity (total TRIG), meaning-making (GMRI Personal Growth and Valuing Life subscales), trauma coping abilities (PACT Coping Flexibility Scale), and unmet needs (total BCNI) as dependent variables. For measures that had subscales and were found to be significant, further exploratory analysis was completed. A summary of participant interview responses was prepared under the key interview topics of program acceptability, meeting expectations, program benefits, personal growth, and improved coping strategies. Semantic coding of each response was completed and then codes were organised into categories (e.g., positive impression of program). The most common categories are presented under the key topic headings, with selected quotes as illustrative examples. One person conducted detailed coding of interviews, which was examined by another reviewer for accuracy. One hundred and nine bereaved AYAs (M age = 16.63, range 12 to 25 years; out of 151 total attendees (72%) who attended a GG program participated in the program evaluation. The majority of AYA participants were female (68%), born in Australia (97%), and currently studying (82%). Seven percent of AYA participants identify as Aboriginal or Torres Strait Islander, and 6% reported they spoke a language other than English at home. Most AYA participants had experienced the death of a parent (n = 96, 88%). See Table 3 for details. Of the 109 total participants who provided data for at least one timepoint, 108 provided data at T0, 108 participants provided data at T1, 67 participants provided data at T2, e.g., "I currently need to be informed about grief and loss in a way that I can understand" Good convergent validity with a measure of psychological distress and high internal consistency for all subscales when validated in a sample of cancerbereaved sibling and offspring AYAs [15] . and 13 participants took part in interviews (comparisons between participants who did and did not complete T2 found no differences for gender, age, or baseline BCNI or PACT scores). One participant completed T2 and session data without completing T0 or T1. AYAs reported high levels of session engagement. Life Imprint was the highest scoring session (n = 120, M = 8.25, SD = 1. Reflecting on the overall program at its completion, 99% of participants agreed that GG was helpful or very helpful, and 99% reported they were mostly or very satisfied with the program. High enjoyment of recreational activities was also reported (M = 8.73, SD = 1.53). All but one AYA (99%) stated they would recommend the program to another young person whose parent or sibling has died from cancer. AYA ratings of satisfaction with facilitators ranged from 86 to 98% for all items, with high internal consistency (α > 0.8). Of the 108 AYAs who completed the measures immediately post-program (T1), in the open-ended questions section when asked what they most liked about the program, the greatest number of AYAs (n = 33) mentioned the psychosocial sessions, with 14 specifically discussing the Honouring Ceremony session. This was followed by making new friends and meeting people (n = 29), recreation activities (n = 21), and the supportive and social environment of the program (n = 10). When AYAs were asked about the most useful aspects of the program, the psychosocial sessions were mentioned most (n = 80). Of these, one-third discussed the Honouring Ceremony session, 12 specifically discussed the Coping and Resources session, and 11 mentioned the Life Imprint session. Following this, 11 AYAs mentioned being able to talk to and share their experiences with others. When detailing why they would recommend the program, the most common responses were because it was helpful (n = 33), it provided an opportunity to meet other young people going through a similar experience (n = 26), they gained useful knowledge/skills (n = 22), and it was an enjoyable/positive experience (n = 13). When asked about suggestions they have for improving the program, the greatest number of respondents (n = 50) mentioned making changes to the program structure. Of these AYAs, 20 reported wanting the program to be longer in duration and 19 reported wanting changes to be made to sessions (e.g., content, frequency, group size). This was followed by young people suggesting having more or different recreational activities (n = 16). Eighteen AYAs stated that they did not believe any changes should be made to the program. Reflecting on the usefulness, relevance and practicability of the program topics for their day-to-day lives when asked as part of the three-month post-program survey, all program topics were perceived to be helpful by most young people (see Table 4 ). The data was examined to ensure that assumptions were met for the MANOVA. The assumption of normality could not be met for the GMRI Valuing Life subscale due to a ceiling effect and so this variable was excluded from analyses. Removal of univariate and multivariate outliers identified visually using box plots and through computation of Mahalanobis Distances did not change the outcome of the main MANOVA and as such these cases were retained. For the 63 participants who contributed sufficient data at T0 and T2 to be included in analyses, MANOVA revealed a statistically significant difference in AYAs' outcome scores from T0 (baseline) to T2 (three months post-program), F(4, 59) = 6.73, p < .001, partial η 2 = .31. Analysis of the individual dependent variables was conducted. To account for multiple comparisons, a Bonferroni correction was applied. Analysis revealed no difference in total TRIG from T0 to T2 (F(1, 62) = 3.14, p = .08, partial η 2 = .05). However, the GMRI Personal Growth subscale (F(1, 62) = 6.73, p = .01, partial η 2 = .10), PACT Coping Flexibility Scale (F(1, 62) = 10.26, p = .002, partial η 2 = .14) and total BCNI (F(1, 62) = 16.64, p < .001, partial η 2 = .21) were all significant at an adjusted alpha of 0.0125. Compared with T0, participants at T2 reported significantly greater personal growth- These analyses were repeated with outliers removed. All significant results were retained with the exception of the GMRI Personal Growth subscale, which became non-significant (p = .04) at the Bonferroni adjusted alpha. To see whether our non-significant results could have occurred due to a lack of statistical power, we conducted power analyses using G*Power [53] with power (1 -β) set at 0.80 and α = .0125 (i.e. with Bonferroni adjustment), two-tailed. This showed us that sample sizes would have to increase to N = 409 for the TRIG and N = 107 for the GMRI Personal Growth subscale for T0 to T2 differences to reach statistical significance. Exploratory analyses were conducted on each of the BCNI subscales in order to identify possible domains contributing to the total BCNI's reduction from T0 to T2. As t-test normality assumptions were not met for many of the subscales, a series of non-parametric Wilcoxon Signed Rank Tests were performed. Results indicated that needs scores were significantly lower at T2 in comparison to T0 for all subscales except Family Connectedness (see Table 5 ). There are many different ways to grieve 87.88 There are ways to connect with positive memories of the person who died 86.15 There are ways to manage intense emotions that come from your grief 80.30 You are able to seek support when you need it 80.30 It is okay to talk about the person who died 80.00 There are ways to express your grief to yourself, your family and your friends 75.76 Mindfulness (e.g., being in the present moment) 61.54 It should be noted that despite the T0 and T2 medians being equal for this subscale, the Wilcoxon Signed Rank Test was significant. This is likely because this test is not calculated using the median, but rather is based on the sums of positive and negative ranks. Interviews Key interview topics included program acceptability, meeting expectations, program benefits, personal growth, and improved coping strategies. The high satisfaction reported in surveys was also reflected in interviews, with the majority (n = 11, 85%) of interviewees reporting an overall positive impression of GG. In particular, the program's structure and safe and welcoming environment was praised: "I think it was one of the best experiences that I've had, and I think that especially the structure that it went through was really a well thought out, and well designed, and applicable camp for the people that went… I really think it was an incredible experience". Other interviewees described the program's meaningfulness to them, and what they valued about it: "through acknowledging our loved one and letting us talk about who they are, what they mean to us, what they will always mean to you, and how they shaped who we are…even though that they're not here, we're able to use this space to honour, to just speak about how we feel, and to always remember [them]." "It's an experience that will change your life, and will go through the sections of acknowledging, of reflecting, and honouring in a really thoughtful way. And you'll meet wonderful people who will be some of your greatest friends, and that you'll have with you for, hopefully, the rest of your life." Most interviewees (n = 11, 85%) reported that GG had met their expectations. Common expectations discussed were learning skills to express grief, meeting others with similar experiences, and fun and respite. The remaining two interviewees reported their expectations were only partially met, with reasons including being unable to make friends, feeling that there was not equal opportunity given for group members to share in the group space, or learning fewer coping skills than expected. Interviews also reflected several program benefits that young people experienced during the program and in the months afterwards. Ten out of the thirteen AYAs interviewed (77%) talked about feeling less alone as a result of the program and identifying with other participants, with one AYA stating: "It was good to know people in my area that were also going through the same thing. Like having other people, younger people, I know I can connect with and you know, talk to about these things and not feel isolated as much". Just over half talked about the benefit of psychoeducation about grief, with one young person saying "They [the sessions] were really helpful for me just dealing with my grief and getting to know what grief is was really helpful", and another referring specifically to learning about the DPM: "We did this thing where we were talking about… the emotional side and then the productive, like, continue with your life side and so you're constantly moving between them. And that's helped me realise that I could go in-between them when I needed to… So then that helped". Eleven of thirteen interviewees (85%) identified making friends as a key benefit lasting well beyond the program, and nearly two-thirds reported having met others with similar experiences to their own who could understand what they were going through, with one AYA stating "that's something really huge that I gained from that camp, was the friends I made" and "everyone in there could relate to what I was going through" and another explaining: "if you're trying to talk to your friends or someone [else] about it they don't understand how to explain and talk to you about things. They'll find it awkward and avoid it, but all these people [at the GG program] know where you're coming from". Developing skills in expressing grief openly to others was also identified as a key benefit by just over half of the young people: "if I'm struggling with something, I will tell the friends that I met [on GG] or I'll let my dad know… I'm more accepting of the fact that I need to ask for help". Finally, two of thirteen interviewees mentioned feeling closer to their loved one as a result of the program, with one young person describing "I wanted to feel closer to Mum and also, sort of understand how I was feeling…I think that's the biggest thing that I took from the camp, is that I can still have a connection with Mum". Interviews additionally reflected AYA participants' awareness of personal growth following their loss, with eleven of thirteen interviewees (85%) citing that they felt different since their loved one's death. AYAs reported this personal growth occurring in a number of areas, including maturation, gaining of new perspective and assuming more responsibility, with one AYA stating: "I think it's definitely made me a lot more stronger and a lot more aware of other people around me… it's just changed a lot of aspects of my life, really. How I treat other people. And how I expect to be treated by other people". Another young person said: "I think there's a process of maturation that occurs, and I think that it makes you a more…-it makes you a wiser person than you necessarily would be for your age, and I think it makes you a more compassionate person. I think that it's… fast tracked the whole maturation process." Improvement of coping skills and abilities was also supported by interview data, with all interviewees mentioning various coping strategies they had learned from the program. These included talking about their emotions with others, accepting their grief and acknowledging the uniqueness of the individual grief experience, openness to facing grief rather than avoiding it, writing letters to their loved one and keeping their loved one's memory alive. Illustrating this, one AYA stated that "I've learnt to be able to cry and it's okay to show your emotions", and others said: "I kind of used things that I have done with dad previously [to cope]. Whenever I contribute to an activity I kind of remember, it helps me get over it a little bit. Like whenever I go fishing or camping, like even the scent of the tents kind of remind me of him a little bit". "I think sometimes we can get a little bit lost through life and through just day to day things, but the camp really gave us a chance to sort of sit down with our feelings, and sort of go, 'This is what's going on. This is how I feel. It's okay to feel this way'". This paper provides evidence that GG was well received by young people bereaved due to cancer and that it significantly improved their psychosocial wellbeing. In particular, significant improvements were found in the core program outcomes, which had been identified as critical for preventing poor bereavement outcomes in this cohort. Our findings show that young people who attend the program experience the benefits that it was designed to deliver. It is expected that these benefits will provide AYAs with greater resilience and internal and external resources to walk through their grief experience and adjust to life following the death of their loved one. Young people were highly satisfied with the program, with strong convergence of survey and interview results. Almost all young people found the program helpful, satisfying and would recommend it to others. They also reported high enjoyment of recreational activities and satisfaction with facilitators. AYAs reported high engagement with all psychosocial sessions and for seven of the nine program topics at least 80% of young people reported them as helpful for their day-to-day lives. Regarding the helpfulness of the GG program's topics, the examination of strengths and values shared with the deceased loved one, and the exploration of the shared grief experience between participants was the most highly endorsed topic. Supporting this finding, McDonald et al. [14] reported that support from other AYAs who have had a similar experience was the most highly endorsed unmet need domain for AYA siblings and offspring who have had a family member die from cancer. The next most endorsed GG topic in terms of its helpfulness was the examination of strengths and values shared with the deceased loved one. This topic is a major focus of the Life Imprint session -the session with the highest engagement scorewhich aims to facilitate meaning-making and strengthen AYAs' continuing bonds with their loved ones. Examining shared strengths and values in this session may specifically strengthen AYAs' internalised connections (an inner representation of the deceased person's beliefs, values and personality) to their loved ones as a means of continuing bonds [44] . Previous research examining the Life Imprint session in detail also found it to be highly engaging and meaningful for AYAs, promoting reframing of life narratives in some participants [47] , which can assist with coping and bereavement adjustment [33, 34] . Aligning with the existing literature, the present findings clearly highlight the importance of ensuring peer support provision is a core component of any AYA grief intervention. Trauma coping abilities A significant increase in trauma coping abilities was observed from pre-program to the three-month postprogram follow-up. This was further substantiated by AYA interviews conducted after the post-program follow-up, in which participants reported the learning and employment of a variety of coping strategies, with one AYA describing GG as being "a very good opportunity to learn coping skills. And to learn more about why you feel the way you feel and grief as a whole". The PACT's conceptualisation of coping as the flexible use of strategies relating to both facing the trauma experienced (e.g., engaging with painful emotions, remembering the event) and moving forward with one's life (e.g., distraction from painful emotions, making plans, having fun) aligns with the DPM's hypothesis of optimal adjustment to bereavement [28, 52] . As such, improvements seen from pre-program to follow-up suggest an increase in participants' abilities to flexibly and adaptively engage in both loss-and restoration-oriented coping processes as a result of attending GG. Indeed, GG not only explicitly teaches AYAs the DPM's core tenets, but also provides young people with a lived experience of it. In other words, at its core, the program's format models and allows AYAs to practice oscillation between time that is spent focusing on grief, remembering the deceased, and engaging with the associated emotional and cognitive experiences, and time that is spent focusing on having fun with other young people and taking a break from grief. For all unmet need subscales other than Family Connectedness, significant reductions were observed from pre-program to three-month follow-up. The finding of no change in family connectedness needs was expected, as GG does not have a strong primary focus on improving family relationships and expressions of grief within the family unit. Reductions in most of the other areas of need were also as expected, as many of them represent the core aims and focus of GGincreasing knowledge about grief and support resources, facilitation of peer connections with other young people who have experienced the death of a family member due to cancer, facilitation of opportunities to express and learn about how to cope with grief-related emotions, and provision of respite and fun. Reductions in unmet needs associated with getting support from friends and practical assistance in planning for the future were also observed. It is possible that the opportunities provided on GG for AYAs to speak about their grief and loss experience enabled them to feel more comfortable talking to their friends about this once they returned home. There is also the possibility that at follow-up, AYAs' new friendships with other AYAs they met on GG were meeting this need. Finally, whilst the BCNI's Planning For The Future subscale reflects mostly practical needs (e.g., assistance with household chores, developing independence, budgeting), it is possible that GG assisted AYAs to know where and how to ask for help in these areas following the program. Interview data also reflected likely reductions in several of these unmet need areas. In particular, receiving of various forms of peer support were the most mentioned program benefits. Interviews with AYAs demonstrated awareness of several areas of personal growth following the death of their loved one. It is possible that this resulted from the opportunity provided on GG for AYA participants to reflect on meaning made as a result of loss, and therefore to further integrate the loss into their self-narrative [30, 31, 33] . Furthermore, given the power analysis findings, we cannot confidently rule out a possible impact of low power on the lack of significant change in personal growth-related meaning making. No change was observed in grief intensity, with post-hoc power analysis suggesting that this finding is unlikely to be due to insufficient power. Whilst we do not know with certainty why there was no change, it may have been that some young people had avoided engaging with grief prior to the program, and so the program may have provided a space for them to engage with their grief, potentially increasing or sustaining their grief intensity scores in the months following GG. Illustrating this possibility, in an interview, one AYA reported, "I was always pushing [grief] back and I needed a set time or a set thing telling me what to do… I think, for me, it was just that I actually did get to spend that time reflecting… Which I, in all honesty, I hadn't really done". Another possibility is that perhaps no change in these symptoms should be expected. On reflection, as this is a measure of grief symptoms that for the most part constitute a normal and even healthy response to loss [54] , then perhaps we would expect these to persist for some time, especially within the timeframe this evaluation was conducted, regardless of whether an intervention is administered or not. Without a control group, we were unable to account for potential changes in outcomes over time due to natural growth and recovery processes unrelated to the intervention, or due to participants potentially receiving other forms of support from Canteen. Additionally, whilst McDonald et al. [14] found that the overall needs of AYA siblings and offspring bereaved due to cancer are very similar, subtle differences were evident. However, as the majority of AYAs in the present study were bereaved offspring with only a small number of bereaved siblings participating, it was not possible to assess any differences in how the program may have impacted the two groups. In addition, although personal growth and meaningmaking were described in interviews, given the small number of interviewees, it is unclear if this meaningmaking occurred equally for all AYA participants. In this way, the potential of bias being present in the small subset of AYAs who agreed to be interviewed must be considered. For instance, it may be that interviewees tended to have had a more significant meaning-making experience on GG or were better able to recognise and articulate personal growth than the majority of attendees. Finally, given Family Connectedness was the only BCNI subscale that did not show significant reduction from pre-program to follow-up, future research could consider including family in the intervention (e.g., [21] ) to improve familial relationships and support. GG is a sustainable community-based program to which healthcare providers can refer young people who have experienced the death of a family member to cancer. This is especially important given these young people's unmet needs and that the provision of such services is generally out of scope for primary, secondary and tertiary healthcare. Reflecting GG's long-term sustainability, since its initial piloting, GG has been run 10 times, and has informed the development of an online grief program alternative for AYAs who cannot attend GG in person. This has been especially important during travel and social distancing restrictions associated with COVID-19. Furthermore, GG offers AYAs therapeutic opportunities beyond those that can be provided through typical one-to-one counselling servicesnamely peer support and the lived experience of the DPM. Healthcare services and hospitals may be unaware that their patients have adolescent or young adult children, so when providing end-of-life care to cancer patients, they should implement systems to assist with the identification of AYA family members who may benefit from referral to community-based support programs like GG (e.g., [55] ) either as a standalone program or in conjunction with other psychological support. Additionally, greater awareness within the community of both the needs of bereaved young people and how and why they should access support, will assist them to access the support they need. Given the indelible impact that a parent or sibling's death to cancer can have on young people, it is critical that theoretically driven and evidence-based programs addressing this impact are available and accessible to bereaved young people. The current study provides strong evidence for the acceptability, appropriateness and effectiveness of the GG program in meeting this need and demonstrates the vital role community organizations have in the provision of such support.  
paper_id= fff7c69795a54700aa16a0d243811ddd2d8809c0	title= Journal Pre-proofs Current knowledge of COVID-19: advances, challenges and future perspec- tives Current knowledge of COVID-19: advances, challenges and future perspectives	authors= Yuhan  Wu;Zhuobing  Peng;Yongxue  Yan;Jintao  Hu;Yalong  Wang;Xiaoyu  Wang;Ruchao  Peng;Daniel  Watterson;Yi  Shi;Y  Wu;Z  Peng;Y  Yan;J  Hu;Y  Wang;X  Wang;R  Peng;D  Watterson;Y  Shi;	abstract= The pandemic Coronavirus Disease 2019 has already evoked massive influence. The global pandemic has been ravaging the whole world for a year, with the number of confirmed human infection cases over 150 million and a death toll exceeding 3 million. Although the genomic sequence of the cognate pathogen SARS-CoV-2 (Severe Acute Respiratory Syndrome Coronavirus 2) has been quickly determined, there are still many unknown aspects, including the virus origin and evolution trend, and also the effectiveness of current vaccines and drugs against the mutating virus. This review summarizes current knowledge and advances about COVID-19, including virus origin, transmission and infection, with the aim to improve understanding of COVID-19 and provide a new perspective for future studies. 	body_text= The emergence of COVID-19 is a milestone in mankind history, irrespective of whether SARS-CoV-2 can be completely eliminated, as achieved for SARS-CoV, or whether it becomes a seasonal epidemic in the human population like other human-infecting coronaviruses. To date, the scientific knowledge gained in response to this pandemic has improved our understanding of SARS-CoV-2 and the cognate disease, and will benefit the control and prevention of emerging infectious disease in the future. However, our concerns have been deepened by the warning from WHO (World Health Organization), that the world is in a "new and dangerous phase": small-scale reemergence, and the accelerating deterioration of the situation in many countries, especially in South America, Africa and India. Within this context the COVID-19 pandemic continues (Figure 1 ), and the virus population is rapidly diversifying in different countries [1] (Figure 2 ). The most difficult choice for governing bodies is to balance pandemic control and social/economic sacrifices in response to this unprecedented situation. To combat with the ongoing COVID-19, we reviewed and summarized current knowledge across diverse aspects, with the aim to provide a new perspective for COVID-19 studies now and going forward. Many scientists believed that identification of the "original host animal" is vital to contain the COVID-19 pandemic, and to prevent further pandemics in the future [2] . But current research about the virus origin is still unclear. Phylogenetic analysis, through the comparative analysis of genome sequences, has the potential to provide evidence for the original or intermediate host(s) of a pathogen, which is essential for us to understand the origin and transmissive pathways of pathogens. According to the sequence identity between different coronaviruses, SARS-CoV-2 is most similar to a bat coronavirus RaTG13 (with a sequence identity of 96.2 % across the entire viral genome), suggesting SARS-CoV-2 may originate from bats [3] [4] [5] . Many studies have shown that some other animals can be infected by closely-related coronaviruses, such as pangolin (The pangolin coronavirus isolate shares a sequence identity of 91.02% to SARS-CoV-2) [6]. These species could therefore be potentially used as model animals for evaluating vaccines and therapeutic drugs. Avian An alternative possibility is that SARS-CoV-2 virion in contaminated water or environment may maintain infectivity at low temperatures, which offers the chance to cause human infections via contact with the contaminated water or animals. The use of reverse genetics systems together with synthetic biology to recreate CoVs, including SARS-CoV-2, has been reported by labs in the USA and other European countries [9] [10] [11] [12] [13] . Humans still do not know why the virus suddenly emerged. Identifying the "Patient Zero" would be particularly important to understand this pandemic and control future possible pandemics [14] . It is possible that the virus may gradually acquire key mutations during ongoing transmission in the human or animal populations which could lead to a highly adapted and more difficult to eradicate pathogen [15] . It has been reported that dominant D614G substitution in S protein may increase infectivity by nearly ten fold [16] . E484K reduced susceptibility to neutralization by antibodies, identified as part of lineage of South Africa. In addition to the E484K and D614G mutations in the spike protein, the Indian mutant strain can evade and weaken the human immune response to the virus. It also has a chromosomal rearrangement of 6 nucleotides (H146del and Y145del) [17] . This step is thus a critical determinant for the efficiency of virus entry [18] and host tropism [19] . It has been reported that SARS-CoV and MERS-CoV can enter host cells via endocytosis and require the proteolysis of S protein by cathepsin in the endosome to trigger membrane fusion [20] . The SARS-CoV-2 might utilize similar mechanisms for cell entry and membrane fusion [21] . Compared with SARS-CoV S protein, the unique features of SARS-CoV-2 S protein may lead to differences in the ability to bind the receptor [22] , which may partly explain why it is more infectious than other human coronaviruses. Other putative receptor molecules in addition to ACE2 have also been proposed, including CD209 (cluster of differentiation 209) and CLEC4M (C-type lectin domain family 4 member M) and neuropilin-1 [23, 24] . These molecules also indicate candidate targets for antiviral intervention. These facts also suggest that the infection mechanism of SARS-CoV-2 has not been fully understood. There are still many other questions remain in unanswered, such as whether other receptors/factors are involved. Non-proteinaceous factors, such as fatty acids, should not be overlooked, as they may also play some important roles in the interaction between the viral S protein and host receptors [25] . Since the identification of SARS-CoV-2, this virus has undergone several significant changes (e.g. D614G from UK [26] , N501Y from South Africa [27] , E484K from Brazil [28] ) that boosted its ability of infection, transmission and immune evasion. However, the underlying mechanisms for these properties not fully understood and warrent detailed analysis at the molecular and disease level. The clinical manifestation of SARS-CoV-2 are similar to that of SARS-CoV. The main organ of virus infection is lung, and by attacking the respiratory system, the patients develop ARDS (Acute Respiratory Distress Syndrome). ARDS can lead to respiratory failure and even death [29] . Of note, some clinical cases also revealed other clinical manifestations in addition to respiratory system pathology [30] . More and more clinical studies have shown that SARS-CoV-2 not only attacks the lung but also causes injury in other organs of the human body, especially in critically ill patients. SARS-CoV-2 can directly infect extra-pulmonary organs which express ACE2 and TMPRSS2 [31] . Further, SARS-CoV-2 infection may bring some unexpected complications, such as impaired sensory capacity, abnormal hepatic and renal function, brain and heart damage, impaired gastrointestinal function [32] . Moreover, increase of XIAP associated factor 1 (XAF1)-, tumor necrosis factor (TNF)-, and FAS-induced T cell apoptosis in COVID-19 patients was observed [33] . In addition, severe COVID-19 patients showed a stronger response to interferons and virus infection compared to mild patients and healthy ones [33] . A striking clinical feature of the SARS-CoV-2 infection is the significant increase in thrombotic and micro-vascular complications, or COVID-19-associated coagulopathy (CAC) [34] . The incidence rate of thrombotic complication in COVID-19 patients in ICU is as high as 31% [35] . SARS-CoV-2 may also infect vascular endothelial cells (EC) [36] , which express the receptor ACE2 [37] . A recent study suggests that endothelial cell (EC) injury plays a significant role in the pathogenesis of CAC, and the level of soluble thrombomodulin in the blood are correlated with mortality [38] . In addition, it is reported that the injury of ECs may be a key driver of COVID-19 severity and death [38] . SARS-CoV-2 can also destroy the blood-brain barrier and invade the central nervous system by attacking the vascular system [39] , causing some neurological complications, such as partial loss of sight/smell/taste [40] (see Supplementary Table) , headache, aortic ischemic stroke and spinal cord injury (SCI) [41] . It has also been reported that about 10% of patients have gastrointestinal symptoms, such as diarrhea, vomiting and etc [39] . This may be driven by infection of gastrointestinal epithelial cells which express high levels of ACE2. These evidences indicate that the SARS-CoV-2 can actively infect the digestive system. The early reports of epidemiological studies did not reveal significant differences in the susceptibility to SARS-CoV-2 of human populations with different ages or genders. The accumulating data implies that the elderly bears a higher chance of infection and would result in a higher mortality rate due to the degeneration of immunity. Therefore, these people should be given a high priority for testing and protection. Although children are much less likely than adults to experience severe complications from the infection because of healthy blood vessels, kids with Kawasaki syndrome and other artery diseases are at a high risk of developing severe symptoms by SARS-CoV-2 infection [42] . It noteworthy that the relatively higher infection rate of younger population could be attributed to the insufficient social distancing and less mask protection [1] . Generally, males reveal a higher rate of infection and mortality than females. This might be the result of several factors, including the higher expression of ACE2 in lung cells of males, primarily type II lung cells, perhaps other tissues as well, and also sex hormones levels, immunological factors and smoking [43] [44] [45] . Interestingly, the ACE2 receptor is expressed in the reproductive organs of males, such as the testes and prostate, but not in the ovaries of females [46] . It is proposed that AR (androgen receptor) activation induces the expression of ACE2 and TMPRSS2 genes, resulting in gender differences [39] . The long-term exposure to smoke triggers the expansion of respiratory secretory cell populations, which as a result up-regulates the expression of ACE2 and may increase the susceptibility for SARS-CoV-2 infection [47] . GWAS also revealed that the susceptibility to SARS-CoV-2 infection may also be related to blood types. The epidemiological data indicated that the blood group A was associated with a higher risk of acquiring COVID-19, whereas blood group O might be more resistant to the infection [48] . The SARS-CoV-2 is thought to be transmitted mainly by close contact with one another which is generally defined within a distance of about 6 feet or 2 meters [49] . This concept leads to the proposal of "social distancing" as an effective measure for preventing infections [50] . Because the virion itself is very fragile, it would be inactivated when exposed outside the cells for a certain period. The virus is mainly transmitted through respiratory droplets and airborne when an infected person coughs, sneezes or talks, thus the virus transmission can be cut off by a face mask [51] . It is also publicly accepted that face mask is one of the most critical PPEs (Private Protection Equipment). Mother-to-child transmission of COVID-19 is controversial and there is a lack of data to confirm this route as a major concern. Although increased SARS-CoV-2 specific IgM (immunoglobulin M) antibodies have been detected in some new-born infants [31, 52] , viral nucleic acid tests of amniotic fluid and cord blood were negative. Recent studies have reported the positive detection of SARS-CoV-2 in breast milk [53] and semen [54] , which alerts the potential risk of virus infection by breastfeeding and sexual transmission [55] . Another issue is how long the live virus could keep alive on the surface of solid substances that people could touch with bare hands. An experiment on how long the virus could survive reveals a significant difference in wood, stainless steel, copper and cardboard, which is widely used for packages [56] . Another study showed the 9-hour survival of SARS-CoV-2 on human skin may increase the risk of contact transmission, thus, it is widely accepted that hands should be washed after touching any hard surfaces [57] . It is also well known that most living cells and viruses are sensitive to the extreme environmental conditions, such as high temperature and low moisture. This leads to the misunderstanding that the virion could only be stable under the "cold &wet" conditions in winter and would be inactivated by the "dry and hot" weather in summer. It is not the case for COVID-19, as evidenced by the outbreaks in tropical countries. The infection of workers in slaughterhouses in Germany, the USA, and other countries, as well as the detection of live virus in seafood markets, raise concerns for another transmission-enhancing factor, for example, "cold-chain". The advancement of technologies to sequence trace amount of DNA/RNA, has opened the door to detect viruses or viral products at various environments. Italian scientists detected traces of SARS-CoV-2 RNA in sewage water samples collected in December 2019, which suggested that the virus had been circulating much earlier in the country, several months before the outbreak at the end of February 2020. It is also hypothesized that the SARS-CoV-2 might originate from the cryosphere that can hide many unexpected and ancient pathogens, which needs to be confirmed in the future. Developing vaccines is a crucial measure for long-term protection against SARS-CoV-2 [58] , as evidenced by the successful eradication of smallpox in humans and rinderpest in cattle. Great efforts have been made globally for developing vaccines against the virus. A hallmark feature of coronaviruses is the presence of a 3'→5' exonuclease that proofreads RNA products in transcription and replication [59] , which suggests a more stable genome than many other RNA viruses and indicates the possibility of successful vaccine development. This is in sharp contrast to the situation of developing universal vaccines against diverse strains of influenza virus or HIV that have a higher rate of genomic mutation and in the case of influenza, re-assortment. Most of current vaccine strategies, have been applied to generate a SARS-CoV-2 vaccine, such as attenuated or inactivated virus , adenovirus-vector-based [60] , mRNA [61] and recombinant protein [62] . In addition, a team of researchers are proposing to give a booster dose of a mixture of the measles, mumps and rubella (MMR) vaccine to people to test whether it can elict broad-spectrum viral immunity, which may help to prevent some of the most severe effects of COVID-19 [63] . Considerable efforts have been made for the development of effective drugs to treat the disease. For example, common steroids could be an affordable and effective treatment for COVID-19 [64] . A clinical trial revealed that dexamethasone reduced the risk of death in critically ill patients [65] . Some other drugs, like baricitinib, which has been used for treating adults with moderate to severe rheumatoid arthritis (RA) [66] , have also been evaluated in clinical trials. There is a rationale for therapies to stabilize the endothelium while tackling viral replication, particularly with anti-inflammatory/anticytokine drugs, ACE2 inhibitors [18] , and statins. The American FDA (Food and Drug Administration) and agencies in several other countries have cancelled the emergency use authorization of hydroxychloroquine [67, 68] and chloroquine [69] for COVID-19 treatment. Clinical evidence has revealed that these two drugs do not appear effective against SARS-CoV-2. The broadly-reactive nucleoside analog drugs remesdivir and favipiravir have shown antiviral efficacy in clinical reports but with a certain side effects. Mode of action studies of these two drugs will enable modification of these two drugs, which present as promising first response measures to deal with emerging SARS-CoV-2 variants and threat of newly emerging viruses [70] [71] [72] . Although there has been massive scientific effort to understand COVID-19, there are many knowledge gaps yet to fill. Although similar, SARS-CoV-2 has proven starkly different to SARS-CoV which was eliminated without the need for vaccine development ( Figure 3 ). The key to understand the biology and virus-host interactions of SARS-CoV-2 requires knowledge of mutation and evolution of this virus at both inter-and intra-host levels. However, despite quite a few polymorphic sites that have been identified among SARS-CoV-2 variants, intra-host variant spectra and their evolutionary dynamics remain mostly unknown. Recently, a SARS-CoV-2 mutant has appeared in London, UK, and caused global attention as the mutant virus has a stronger transmission ability [26] . This development warrents additional and more detailed viral surveillance at the genomic level. We should guarantee the annual surveillance of coronaviruses in humans and animals, and also in different environment, and then combine the coronavirus genomic sequence information as a database. With this database, we can easily trace the orignin of potential novel human-infecting coronaviruses, just like what we have done for influenza viruses. The emergence of new variants will also challenge the efficacies of current vaccines and drugs. Moreover, functional studies are required as the phenotypes that arise through the interaction between the virus, its hosts and environmental changes cannot be simply extrapolated by the genome sequence information. Therefore, methods should be applied to systematically study the COVID-19, and also the potential emerging infectious pathogens. To deal with the challenge of future threats of new pathogens, we should call for a change from a reactive culture to a proactive one, including the establishment of global network of virus surveillance and development of broad-spectrum antiviral drugs and universal vaccines [73] .  
paper_id= fff81d0b050c0f49eb59bb9e85ab3a90b46017b2	title= Examining injury severity in truck-involved collisions using a cumulative link mixed model	authors= Mingyang  Chen;Peng  Chen;Xu  Gao;Chao  Yang;	abstract= Background: Trucks play a vital role in promoting regional freight transportation and economic development, but truck-involved collisions often have more severe consequences and create greater losses for society. Research purpose: This study examined the relationships between injury severity and various explanatory factors in truck-involved collisions to identify preventive countermeasures for safety improvement. Data: Los Angeles' collision records from 2010 to 2018 were analyzed. Method: A cumulative link mixed model was applied, where the heterogeneities among drivers were highlighted. Result: Our findings confirmed that various driving mistakes, such as speeding, improper driving, and drinking alcohol, contributed to severe injuries. Male drivers were more likely to be severely injured, while female occupants were more likely to be severely injured. The use of safety equipment always helped mitigate injury severity. Collisions at night on dark roads with no streetlights and collisions on slippery road surfaces had higher risks of causing severe injuries. In addition, collisions on ramps were more likely to result in severe injuries. Drivers in old trucks were also at a higher risk of suffering from severe injuries. Conclusions: Freight companies are encouraged to monitor drivers' performance using remote cameras. Policy-wise, local agencies should regulate improper driving behavior and safety equipment use for truck drivers. Improving lighting conditions, periodically testing the skid resistance of road surfaces, adjusting speed limits, and applying weigh-in-motion technologies may greatly help mitigate injury severity. Old trucks should be brought in for frequent tests or abandoned after many years of usage. 	body_text= With the development of modern shipping and logistics, most companies now engage in overseas manufacturing and global trade. The freight industry has similarly been greatly reshaped with a growing demand for consumer goods (Hesse, 2007; Yuan, 2018a) . More recently, with advancements in smartphone technology, online retail has exploded, and its logistics network has expanded Prior studies examining truck-involved collisions have emphasized different groups of factors when analyzing injury severity. These factors can be classified into the following categories: (1) socio-demographic profiles, such as age and sex, (2) behavioral factors, such as alcohol use, distraction, improper driving, unsafe driving speed, and the misuse of safety equipment, (3) temporal factors, such as collision time, season, and day of the week, (4) environmental factors, such as weather and street lighting conditions, (5) road conditions, such as roadway geometry, road damage, road surfaces, and road obstructions, (6) collision characteristics, such as collision type, and (7) vehicle characteristics, such as a truck's length, weight, and years of usage. Among these factors, truck drivers' socio-demographic characteristics, behavioral characteristics, and vehicle characteristics are correlated with the injury severity of occupants in truck-involved collisions. Frequently discussed factors include truck drivers being distracted or intoxicated, the misuse of safety equipment, and a truck's weight and size. Truck drivers' higher degrees of distraction and intoxication have been directly linked with more severe injuries (Khorashadi et al., 2005; Lemp et al., 2011; Teoh et al., 2017; Zhu and Srinivasan, 2011a, b) . Interestingly, a positive relationship was identified between the number of truck trailers and the fatality rate, but the length and the weight of trucks both suggested negative correlations with the fatality rate (Bedard et al., 2002; Lemp et al., 2011) . Meanwhile, injuries caused by improper driving behavior such as speeding were far less severe when safety equipment was used (Zhu and Srinivasan, 2011b) . Studies have not paid much attention to victims' characteristics, with only a few studies suggested that males tend to be more severely injured than females in truck-involved collisions (Uddin and Huynh, 2017) . In terms of temporal factors, several studies indicated that victims were more severely injured in collisions that occurred at night (Osman et al., 2016; Pahukula et al., 2015) . Injuries were less severe during the summer months, probably due to there being longer daytime and better road conditions (Pahukula et al., 2015) . Environmental factors were correlated with injury severity (Ahmed et al., 2018; Lemp et al., 2011; Lyman and Braver, 2003; Naik et al., 2016; Uddin and Huynh, 2017; Wang and Shi, 2013; Zou et al., 2017) . Bad weather conditions, including fog, ice, and snow, showed positive relationships with severe injuries. In terms of lighting conditions, areas with no street lights had more severe collisions (Uddin and Huynh, 2017) . Road surface conditions were also found to be related to injury severity. On dry road surfaces, the risks of collision and severe injury were greatly reduced, and vice versa (Ahmed et al., 2018) . When road surfaces were covered by ice, the probability of death in truck-involved collisions was greatly increased (Lemp et al., 2011) . In terms of collision types and location characteristics, injuries caused by head-on collisions tended to be more severe. Injuries caused by truck-involved collisions at state and interstate highways' intersections were also more severe (Zhu and Srinivasan, 2011a) . Specifically, truck-involved collisions on state and interstate highways were 2.3 and 4.5 times more likely to be severe injuries, respectively (Osman et al., 2016) . For methodology, various discrete choice models have been used to examine injury severity in truck-involved collisions, such as the multinomial logit (MNL) model and the nested logit (NL) model. Some studies treated injury severity as an ordered categorical variable and employed an ordered logit (OL) model or a generalized ordered logit (GOL) model, as shown in Table 1 . With increased modeling capacity, recent studies have tried to include random parameters to capture individual heterogeneities (Al-Bdairi and Hernandez, 2017; Naik et al., 2016; Zou et al., 2017) . Different approaches with a focus on individual or group heterogeneities that capture autocorrelations among multiple occupants are expected. This paper employed the cumulative link mixed model (CLMM), also known as the ordered response mixed model, which is an extension of the cumulative link model (CLM). The CLM is also referred to as the ordered logit (OL) model or the proportional odds (PPO) model. The advantage of employing a CLMM lies in its inclusion of random effects to model group heterogeneities. In this study, a default assumption is that drivers of the same sex and age, performing similar driving behavior, are likely to result in similar injury severity outcomes for all occupants in truck-involved collisions. The failure of capturing this feature may overstate the injury severity in truck-involved collisions. The general form of the CLMM is denoted as Eqs. (1~2). where Y i denotes the injury severity that takes values from 1 to J, X i represents the vector of fixed effects, β is the vector of coefficients for each regressor, a j is the threshold for level j, j = 1, … J for an ordinal variable with J levels, G − 1 is the link function, and μ t represents the vector of coefficients corresponding to the group-level predictors Z t [i] for observation i in group t. This model has the added assumption that random effects are normally distributed and centered at zero. The CLMM was estimated with the R 'ordinal' package (Christensen, 2019) . The collision data were documented in the US Statewide Integrated Traffic Records System (SWITRS). This study employed the truck-involved collision data from January 2010 to December 2018 from the city of Los Angeles (LA), California. LA was selected for analysis for three reasons. First, LA is a famous harbor city with high freight demand. As such, trucks play an important role in the local economy. Second, LA has a high traffic volume, correlating with a higher rate of truck-involved collisions. Third, the SWITRS is a wellreported collision data system, which serves as the basis for an insightful analysis of injury severity in truck-involved collisions. The response variable, injury severity, had five levels, including property damage only, possible injury, evident injury, severe injury, and fatal injury. When processing the raw data, any collision involving a truck or large vehicle was selected, while collisions between or among other types of vehicles were filtered out, which resulted in a sample of 24,514 observations. However, the data had many missing values. After removing cases with missing values, the final sample was reduced to 21,258 observations. A descriptive summary of the response variable is given in Table 2 . The percentages of injury types were largely consistent between the original and final samples, indicating that using the final sample for analysis would not introduce any serious sampling bias. Due to fatally injured victims not being able to provide information, some observations of fatal injuries were removed from the final sample, which may Table 1 Methods examining injury severity in truck-involved collisions. Authors Model Khattak et al. (2003) Binary Probit and ordered Probit Khorashadi et al. (2005) Multinomial logit Chen and Chen (2011) Mixed logit Lemp et al. (2011) Ordered Probit and heteroskedastic ordered Probit Zhu and Srinivasan (2011a) Ordered Probit Zhu and Srinivasan (2011b) Mixed ordered Probit Islam and Hernandez (2013) Random parameter ordered Probit Pahukula et al. (2015) Random parameter logit Naik et al. (2016) Random parameter ordered logit and mixed logit Osman et al. (2016) Multinomial logit, nested logit, ordered logit, and generalized ordered logit Al-Bdairi and Hernandez (2017) Ordered random parameter Probit Uddin and Huynh (2017) Mixed logit Zou et al. (2017) Random parameter ordered Probit and spatial generalized ordered Probit Ahmed et al. (2018) Bayesian logit incur some biases. In this study, over half of the fatally injured observations were kept in the final sample, accounting for 0.92% of the total number of injuries. The other levels, including property damage only, possible injury, evident injury, and severe injury, accounted for 20.95%, 56.98%, 17.96%, and 3.19% respectively in the final sample. A collision is often caused by many parties. In this study, any vehicle involved in a collision was treated as a party, and each party had one driver and possibly multiple passengers. Moreover, both drivers and passengers were considered as occupants. Following the safety system approach (Larsson et al., 2010) , all selected variables were grouped as factors of human behavior, natural and road environment, collision and vehicle attributes. In detail, this model included the individual profiles of drivers and other occupants, behavioral factors, temporal factors, environmental factors, location characteristics, collision characteristics, and vehicle characteristics. Fig. 1 presents the conceptual model framework, which followed the safety system approach and was built based on existing studies (Apronti et al., 2019; Lemp et al., 2011; Zhu and Srinivasan, 2011a, b) and data availability. The data cleaning process involved several steps. The first step was to handle the missing values. For example, vehicles' lengths and weights were largely missing in the data. Although their importance in explaining injury severity has been widely recognized, these two variables were excluded from the final sample. The vehicle size was included as a substitute for this information. The second step was to work with rare events. For behavioral factors, drug use was excluded because seriously intoxicated truck drivers were rarely present in the data. These rare observations can be picked out for more microscopic case studies but should not be included in the final model. To simplify analysis and to obtain the maximum amount of information with useful implications for real-world practices, some attributes that had similar indications were merged. For instance, among the various driving behavior-related factors, following too closely, driving on the wrong side of the road, improper passing, unsafe lane changing, and improper turning were merged into a variable titled improper driving. Similarly, rain, snow and fog were combined into the variable of bad weather. Wet, snowy, icy, and slippery road surfaces were merged into slippery road surfaces. Roadway damage, obstructions on roadways, and narrow roadways were merged into poor road conditions. A descriptive analysis of selected variables is shown in Table 3 .  Among the 21,258 injury cases, property damage accounted for 20.95%, possible injuries accounted for 56.98%, evident injuries accounted for 17.96%, severe injuries accounted for 3.19%, and fatal injuries only accounted for 0.92%. For explanatory variables, the number of drivers accounted for 57.54% of all occupants, the mean age of drivers was 39.95, and the percentage of male drivers was 59.82%. The mean age of all occupants was 36.55, and the percentage of male occupants was 56.69%. Although the maximum number of injured occupants per collision was 34, the mean of injured occupants per collision was 1.91. In many cases, drivers made errors when driving, including alcohol use, improper driving, and driving at unsafe speeds, which accounted for 7.26%, 36.51%, and 39.05% respectively. It is worth noticing that the percent of drivers and the percent of all occupants using safety equipment were at the same level, accounting for 57.83% and 56.75% respectively. This indicates that the use of safety equipment needs to be promoted. For temporal factors, about 12% of the collisions occurred during the summer, about 26% of the collisions occurred at night, and about 84% of the collisions occurred on weekdays. In addition, collisions occurred less frequently at intersections, only accounting for 15.72%. Collisions more often occurred at ramps. In terms of collision characteristics, collisions were more often rear-end or side-swipe collisions, and the percentage of head-on collisions was quite low. These three types of collisions accounted for 44.48%, 26.84%, and 3.54% respectively. Furthermore, older trucks did present a greater level of collision risks. For the final sample, trucks had been used for an average of 9.12 years, with the oldest one being used for 58 years. The CLMM was used to examine relationships between various explanatory factors and injury severity in truck-involved collisions. The modeling outcome is shown in Table 4 . This study specifically includes drivers' socio-demographic factors and driving behavioral factors as random effects to capture group heterogeneities to account for intra-class correlations created by drivers. The four thresholds between different injury types, including the threshold between property damage only and possible injury, the threshold between possible injury and evident injury, the threshold between evident injury and severe injury, and the threshold between severe injury and fatal injury were all significant. In addition, only the threshold between property damage only and possible injury was negative, which indicates that the injury outcomes in truck-involved collisions were unlikely to be just property damage, and brings more attention to mitigating the negative consequences on our society. Among human factors, both demographic profiles and behavioral characteristics suggest important findings. For demographic factors, the driver's sex and occupant's age had a positive effect on injury severity, suggesting that male drivers and older occupants were more likely to be severely injured. In comparison, older drivers and male occupants were less likely to be severely injured. When compared with passengers, drivers were more likely to be severely injured. For driving behavioral factors, some variables showed significant correlations with injury severity. The use of safety equipment helped mitigate injury severity for both drivers and other occupants. Drivers driving improperly or drunk both indicated positive relationships with severe injuries. Among various environment-related factors, summer and nighttime variables showed positively significant correlations with injury severity. Dark/no streetlight showed a positive relationship with injury severity while slippery road surfaces showed a negative relationship with injury severity. As for collision location, the injury outcomes of collisions occurring at intersections were less severe, while the injury outcomes of collisions occurring on ramps were more severe. In terms of collision types, the injury outcomes of side-swipe and rear-end collisions tended to be less severe, while the injury outcomes of head-on collisions did not have a significant effect in this study. Significance codes: '***' 0.001; '**' 0.01; '*' 0.05; '.' 0.1 In terms of vehicle characteristics, years of vehicle usage showed a positive relationship with injury severity. This study specifically placed a focus on interpreting the effect of trucks' kinetic energy on other vehicles. As shown, unsafe speeds seemed to not have any significant effect on the injury severity, while vehicle size had a negative relationship with injury severity. This indicates that occupants on trucks were unlikely to be severely injured, but property damage on trucks was still possible. It is worth highlighting that the interaction effect of vehicle size and unsafe speed showed a positive effect on injury severity. Taking advantage of the CLMM, this study split the sample into multiple groups based on drivers' age and sex and included drivers' behavioral factors as random effects to account for intra-class correlations within the clusters. Among the three included random effects, alcohol use presented great variations across different driver groups, followed by unsafe speeds, and then improper driving. The high intra-class correlation indicated that drivers who had improper driving behavior were also likely to drive at unsafe speeds. This study examined the effects of various factors on injury severity in truck-involved collisions. According to our results for the threshold coefficients, truck-involved collisions deserve more attention because injury outcomes in such collisions are unlikely to only cause property damage. As shown in Table 4 , most of the selected explanatory variables were significant, suggesting that preventive countermeasures can be designed around significant indicators. Most of our findings were consistent with prior studies (Al-Bdairi and Hernandez, 2017; Lemp et al., 2011; Naik et al., 2016; Uddin and Huynh, 2017; Zhu and Srinivasan, 2011a, b) . The variables we utilized included alcohol use, safety equipment use, improper driving, nighttime, dark/no streetlight, and vehicle size, all of which were consistent with prior research. Among the selected human factors, truck drivers who drove improperly and drank alcohol were more susceptible to severe injuries (Al-Bdairi and Hernandez, 2017; Zhu and Srinivasan, 2011a, b) . It is worth mentioning that the use of safety equipment greatly mitigated injury severity for both drivers and other occupants. These findings highlight the importance of safety education to prevent human fatalities. Regarding socio-demographic factors, if the driver was older or a female, collision injuries tended to be less severe. In contrast, if the occupant was older or a female, the injuries tended to be more severe. Generally, females are more cautious than males when driving, and young drivers tend to drive more aggressively. Drivers who have records of reckless or improper driving should attend training programs for critical road safety topics, especially for young, male drivers. Furthermore, the number of injured occupants was positively correlated with injury severity. This is sensible as when more people are injured, the more likely there are severe injuries. When it comes to the natural environment factors and road conditions, slippery road surfaces and the season of summer contradicted existing literature (Islam and Hernandez, 2013; Pahukula et al., 2015) . However, for the slippery road surfaces, several other studies have suggested similar findings (Ahmed et al., 2018; Lemp et al., 2011) . To clarify, LA is located in the south of the US and the temperature is generally high throughout the year. Snowy weather, cold winters and slippery road surfaces are rare in LA. LA has dry road surfaces for most of the year. When road surfaces are wet and slippery, truck drivers become more cautious when driving, hence reducing the injury severity on wet and slippery road surfaces. This may partially explain why the findings are not consistent with prior studies examined in other cities. On the other hand, negative consequences related to extremely hot weather could impact the performance of drivers in LA (Naik et al., 2016) . For example, individuals drive more aggressively on hot days. Reflections on asphalt road surfaces can temporarily damage eyesight and increase collision risks. These alternative explanations need to be further examined in future research. However, such a contradictory finding sounds reasonable and could be applicable to cities with similar climates. For truck-involved collisions happening at nighttime and in the dark without street lighting, injuries were far more severe (Uddin and Huynh, 2017) . All in all, manufacturers should continually improve techniques for both electronic stability control and tire design to prevent vehicle wheel skidding. Local authorities should check road surfaces and lighting systems periodically and add anti-skid materials and street lights in collision hotspots. As for location characteristics, our results indicate that collisions occurring at intersections tended to be less severe, but those on ramps were more severe. Generally, vehicles slow down when approaching intersections, which greatly reduces the likelihood of collisions and severe injuries. However, when driving on ramps, most drivers are accelerating, failing to notice vehicles coming from other directions, which may result in collisions and severe injuries. This finding indicates the importance of adding signs and signals and regulating driving behavior on ramps. As for collision types, prior studies have investigated vertical collisions, such as head-on and rear-end collisions (Zhu and Srinivasan, 2011a) , and the results of our research for rear-end collisions are fully consistent. Another collision type, namely side-swipe collisions, was included in this study. As expected, the injury severity in side-swipe truck-involved collisions was less severe. In addition, compared with previous studies, this study had some interesting findings. We noticed that the longer a truck was used for, the higher likelihood of it being involved in collisions with severe injuries. Therefore, local authorities should closely supervise annual vehicle inspection in order to maintain the quality of on-road vehicles. This study also examined the injury outcomes triggered by the kinetic energy transferred from trucks to passenger vehicles, which represents a unique feature of truck-involved collisions when compared with collisions between regular passenger vehicles. As shown, occupants sitting in larger vehicles were less likely to be severely injured, while occupants in smaller vehicles were more vulnerable to severe injuries. Additionally, driving at unsafe speeds had an insignificant effect, which is quite different from prior road safety research regarding passenger car collisions. These findings suggest different takeaways for both vehicle manufacturers and local authorities. For vehicle manufacturers, more investments should be made in vehicle collision buffering systems to offset the kinetic energy transferred in collisions. Local authorities are encouraged to adopt dynamic weighing technology in order to better detect and reduce the number of overweight and overloaded trucks on the road. In densely populated urban environments, a shift from heavy and large trucks to light transport vehicles is also strongly encouraged. Trucks play a vital role in promoting regional freight transportation and economic development, but injuries from truck-involved collisions can create great losses for society. Currently, the ongoing coronavirus pandemic results in many changes in shopping that people are getting used to, such as ordering foods and goods online. Even after the vaccine is successfully developed and things are back to the new normal, shopping habits may be changed permanently. The urban fright system faces an expanded volume of online sales, and shipping companies should make large investments to increase their workforce and improve their operational safety. Such a trend confirms the research significance of this study. Therefore, it is important to examine the relationships between injury severity in truck-involved collisions and various explanatory factors to identify evidence-based preventive measures to mitigate negative consequences. This study employed the data from the SWITRS for LA to develop a CLMM. Following the safety system approach, this study incorporated human factors, natural road environment factors, collision characteristics, and vehicle characteristics. This study also examined considerable random effects of explanatory variables to capture group heterogeneities and intra-class correlations. For the human factors, the findings of this study confirm that various driving errors contribute to severe injuries, such as speeding, improper driving, and drinking alcohol. Male drivers were more likely to be involved in severe collisions, but female occupants were more likely to be severely injured. It is important to recognize the intra-class correlation between improper driving and unsafe speeds. Drivers who drive improperly are very likely to be those who drive at unsafe speeds. The use of safety equipment always mitigated injury severity. Hence, monitoring truck drivers' performance helps regulate improper driving behavior and prevent reckless driving. Education, enforcement, and engineering tools all play important roles in preventing human mistakes and improving road safety. For environmental factors and vehicle characteristics, driving at night and driving on dark roads with no streetlights led to higher risks of collisions and severe injuries. Streetlights are important for drivers to identify pedestrians and other vehicles in dark or inclement weather conditions. Improving lighting conditions on main corridors of traffic may greatly help reduce the number of collisions. Meanwhile, regulating speeds contributes to the reduction of kinetic energy, which then helps mitigate injury severity. Another component relating to kinetic energy is vehicle weight. To mitigate injury severity, we encourage authorities to apply weighin-motion technologies to reduce the number of overweight and overloaded trucks on the road. Switching from large, heavy trucks to light delivery vehicles is strongly encouraged in dense urban environments. Most states have drunk driving laws to prevent driving with a blood alcohol content (BAC) greater than 0.08%. Many trucks are owned by companies or fleets that have alcolocks in them to prevent drivers from drinking and driving. Therefore, alcohol use is rare among truck drivers. Additional safety countermeasures can be enforced on ramps. For example, lowering speed limits, installing cameras, and improving the visibility of signals and signs on ramps. Local and state agencies should periodically test the skid resistance of road surfaces. Lastly, the expiration of very old trucks should be considered. More frequent tests should be required for old trucks to ensure that they operate safely on roadways. Lastly, it is worth mentioning one limitation of the police-reported collision data, where the severity of injury is usually a quick decision that is made at the scene. According to the prior research, the severity of the victims' injuries can be underestimated or overestimated based on police officer's subjective measures (Amoros et al., 2007; McDonald et al., 2009) . Data improvement by integrating the police-reported collision records and hospital-recorded injuries will contribute to build a more complete data system, and support a more accurate understanding of issues in road safety. This study receives no funding to support its research.  
paper_id= fff840d04e68c20bf3bba19a4cd4ec5023cd1a4e	title= 	authors= 	abstract= 	body_text= Dear Editor-Pneumonia caused by aspiration of different materials such as oropharyngeal secretions or gastric content into the airway may result in serious complications. Altered mental status, neurological disorders, and trauma are among the major predisposing factors for aspiration, secondary to impaired natural defense mechanisms such as gag and cough reflexes (1) . Aforementioned conditions contribute to a large number of admissions to the emergency departments. Some of these patients may initially undergo Chest CT scan either because of respiratory manifestations or nonrespiratory signs and symptoms (as in trauma patients with suspected fractures). In routine clinical practice, detection of patches of consolidation in posterior parts of upper lobes or superior segments of lower lobes in a patient with a known risk factor for aspiration suggest a correct diagnosis (1,2), however, the interpretation of chest findings may not be necessarily easy for the emergency radiologists and physicians in the setting of corona pandemic, as some radiologic features overlap those of COVID-19 pneumonia. This was one of the problems we encountered during the epidemic, because of the occurrence of a parallel outbreak of alcohol poisoning (3). There were several cases of methanol intoxication referred with altered mental status and simultaneous findings in chest CT scan. Similar cases potentially pose a challenge in making an initial decision for appropriate allocation of the patients to the COVID or non-COVID wards or ICUs before the results of Reverse transcription polymerase chain reaction (RT-PCR) are ready. This is of utmost importance in places where ICU beds are limited. Moreover, correct triage of these patients is mandatory to decrease the risk of spread of infection and to protect the medical personnel from inadvertent exposure to the infection. On the other hand, prompt administration of antibiotics in the presumed cases of aspiration pneumonia is mandatory to prevent morbidity and mortality (4) . Although the gold standard of the diagnosis of COVID infection is PCR testing, radiologic findings together with the clinical data have also been implicated at least for the initial decision-making (5) . In some cases, definite discrimination of the two entities might be impossible solely based on the imaging, however, some radiologic features may suggest one diagnosis over the other. While lobar or segmental pneumonia, lung abscess, and empyema have been reported as complications of aspirations, none of these findings are regarded as typical for COVID-19 pneumonia (5) . Centrilobular nodules and tree-in-bud sign are commonly seen in cases with aspiration (64%À74%) (2) . Interestingly, these CT findings are not frequent in COVID-19 pneumonia and have been categorized as "Atypical" by Radiological Society of North America (RSNA) (5) and therefore could be of some value in proposing a differential diagnosis. Bilateral subpleural patches of ground glass opacity (GGO), especially in basal distribution, have been described as typical for the diagnosis of COVID-19 pneumonia in suspected cases. Such a presentation is also fairly common in aspiration pneumonia. These changes which had been present in up to 74% of the patients (2) make the discrimination more difficult. In such cases, attention to the pattern of distribution of the lesions may be helpful. Although posterobasal lung involvement is the most common distribution pattern in both conditions, anterior lung involvement has rarely been reported in aspiration pneumonia, thus detection of radiologic findings in anterior parts is more suggestive of COVID pneumonia rather than aspiration. In addition, Atol sign and subpleural sparing which have been reported in some of the COVID-19 cases, are not usual findings in aspiration pneumonia, although further studies are needed to assess the predictive value of these radiologic signs. In conclusion, despite possible radiologic and clinical similarities between aspiration and COVID pneumonia, one could suggest a correct diagnosis by careful examination of the CT images together with attention to the clinical scenario and judicious utilization of laboratory tests. Timely diagnosis and treatment positively influence prognosis and reduce mortality. 
paper_id= fff8b9e88db122ffcbaf1daf6b697e44eaaffd93	title= Septic shock caused by Mycobacterium tuberculosis in a non-HIV patient	authors= D  Angoulvant;I  Mohammedi;S  Duperret;P  Bouletreau;	abstract= 	body_text= Sir: Septic shock due to Mycobacterium tuberculosis is rarely reported. Most cases concern patients with proven HIV infection or who are at risk of it but have unknown serologic status [1±5] . We report a case of refractory septic shock caused by M. tuberculosis in a non-HIV patient leading to multiple organ failure and death. A 44-year-old male was admitted to our intensive care unit with a combination of profound asthenia, anorexia, weight loss, acute respiratory failure, and septic shock. His past medical history included inferior myocardial infarction, tobacco and alcohol abuse, and cancer of the right tonsil considered cured 2 years previously. On admission, the physical examination revealed the following vital signs: temperature 39.5 C, respiratory rate 22 breaths/min, blood pressure 80/45 mm Hg, and pulse 100 beats/min, after volume expansion and under dopamine (8 mg/kg per min) and dobutamine (5 mg/kg per min) infusion. Crepitant rales were found bilaterally. The patient was alert and oriented. Chest radiograph showed diffuse bilateral pulmonary infiltrates consistent with ARDS. Arterial blood gases obtained under oxygen therapy were pH 7.46, arterial carbon dioxide tension 6.3 kPa, arterial oxygen tension 7 kPa, and oxygen saturation 87 %. Lactic acid was 3.2 mmol/l. His white blood cell count was 2400/mm 3 , hemoglobin 9.9 g/ dl, and platelet count 96 000/mm 3 . Cardiac tissue enzymes were normal. The enzymelinked immunosorbent assay test for HIV 1 and 2 was negative. The patient was intu-bated, and mechanical ventilation was promptly instituted because of refractory hypoxemia. A two-dimensional Doppler transthoracic echocardiography showed left ventricular depressed systolic function without any indication for a preload increase, consistent with severe septic shock. Then the dobutamine dose was increased to 20 mg/kg per min and norepinephrine infusion was started due to persistent hypotension. Specimens of urine, blood, and bronchial secretions were sent for culture, and empirical antibiotic treatment consisting of ceftriaxone, ofloxacin, and antituberculosis drugs (rifampin, isoniazid, and ethambutol) was begun. Direct examination of bronchoalveolar lavage revealed numerous acid-fast bacilli, subsequently identified as M. tuberculosis susceptible to the drugs given. Routine blood and urine cultures were negative. Despite antituberculous therapy, mechanical ventilation, and maximum use of catecholamine combination therapy, his condition continued to worsen and he died on day 3 in multiple organ failure. No autopsy was performed. To date, nine well-documented cases of septic shock due to M. tuberculosis have been reported in the English literature. Tumor necrosis factor production from mycobacterial products is assumed to be responsible for septic shock hemodynamic alteration [2] . Of these nine cases, seven were HIV-infected patients [1, 3±5] , and two were considered to be at high risk for HIV disease, although testing had not been performed [2] . It is well known that adults infected by HIV have increased susceptibility to M. tuberculosis and progress more rapidly to disease. Conversely, the case presented herein is one of the first reported of hemodynamic septic shock due to M. tuberculosis in a patient without HIV. Because two-dimensional echocardiography is considered a reliable technique to evaluate left ventricle preload and contractility in septic shock [6] , we did not used pulmonary artery catheterization to monitor our patient's hemodynamic profile. For this patient, with a previous history of cancer, several weeks of weight loss, anorexia, asthenia, combined with respiratory symptoms and chest radiographic aspects on ad-mission led to the diagnosis. However, despite initiation of prompt and appropriate empirical antibiotic therapy and maximum supportive therapy, the infection proved refractory and the patient died. We conclude that M. tuberculosis should be considered in the differential diagnosis of septic shock, even in non-HIV patients, especially in patients with other causes of immunosuppression such as cancer or transplantation. 
paper_id= fff916d39d32964557ff04905160d769542720dc	title= Clinical characteristics of 10 children with a pediatric inflammatory multisystem syndrome associated with COVID-19 in Iran	authors= Leila  Shahbaznejad;Mohammad Reza Navaeifar;Ali  Abbaskhanian;Fatemeh  Hosseinzadeh;Golnar  Rahimzadeh;Mohammad Sadegh Rezai;	abstract= Background: Although symptoms and signs of COVID-19 (Coronavirus disease 2019) in children are milder than adults, there are reports of more severe cases which were defined as pediatric inflammatory multisystem syndrome (PIMS). 	body_text= In January 2020, China reported a novel coronavirus called the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) [1] . The world is currently in the rapid emergence of the pandemic caused by SARS-CoV-2 now which is called COVID-19 (Coronavirus disease 2019) [2] . The real incidence of this infection in children is indefinite. The most common presentations of COVID-19 are fever and cough. However, the disease can have other manifestations such as myalgia, headache, dizziness, vomiting, diarrhea, and abdominal pain. Despite the lack of the full presentation of the disease and outcomes in children, the most commonly reported symptoms and signs are milder than adults [3] . This may be due to differences in immune responses to the virus [4] . Unusual cases of the COVID-19 with signs and symptoms similar to atypical Kawasaki disease (KD) and toxic shock syndrome (TSS) have been reported recently [5] [6] [7] . In April 2020, the first report of a child with Kawasaki disease and concurrent positive reverse transcriptase polymerase chain reaction (RT-PCR) of COVID-19 was published in the United States [8] . Since then, other countries have reported some cases with prolonged fever, dyspnea, irritability, diarrhea, vomiting, abdominal pain, as well as conjunctivitis, rash, and cardiogenic shock [6, 7, 9] . According to these reports, the World Health Organization (WHO) and the US Center for Disease Control and Prevention (CDC) shared a clinical guideline as Pediatric inflammatory multisystem syndrome (PIMS) [10] [11] [12] . Finally, the WHO provided a preliminary case definition criterion for PMIS in children and adolescents 0-19 years of age with fever more than 3 days and two of the followings: (a) Rash or bilateral non-purulent conjunctivitis or mucocutaneous inflammation signs (oral, hands or feet); (b) Hypotension or shock; (c) Features of myocardial dysfunction, pericarditis, valvulitis, or coronary abnormalities (including Echocardiography findings or elevated Troponin/N-terminal pro b-type natriuretic peptide (NT-proBNP)); (d) Evidence of coagulopathy (by PT, PTT, and elevated d-Dimers); (e) Acute gastrointestinal problems (diarrhea, vomiting, or abdominal pain) and elevated markers of inflammation such as ESR, Creactive protein (CRP), or procalcitonin. And No other obvious microbial cause of inflammation, including bacterial sepsis, staphylococcal or streptococcal shock syndromes. And Evidence of COVID-19 (RT-PCR, antigen test or serology positive), or likely contact with patients with COVID-19 [11] . A possible temporal causality has been hypothesized between COVID-19 and PIMS because some of the children tested for COVID-19 infection were either positive by polymerase chain reaction (PCR) or serology [5, 7] . There are limited relevant evidence and no concrete working hypothesis for pathogenesis, to examine the association between exposure to COVID-19 and PIMS. Immune response to COVID-19 involves both cellmediated immunity and antibody production. In COVID-19 infection, one suggestive mechanism that causes PIMS in children could be via antibodydependent enhancement [7] . Early diagnosis and management of PIMS are very critical in order to decrease the risk of long-term complications, morbidity and mortality. We lack knowledge about COVID-19-induced presentations mimicry of other diseases like status epilepticus, acute abdomen, and immune response and their possible association with PIMS. Therefore, the management of PIMS remains unclear and difficult. The purpose of this report was to provide a description and characterization of PIMS, increase the level of evidence about the possible association between COVID-19 and PIMS in our pediatric referral hospital in the north of Iran. We focused on novel manifestations and the best approach for the management of suspected PIMS and early outcomes. From 28 March to 24 June 2020, 10 febrile children were admitted with COVID-19 infection showing characteristics of PIMS in a tertiary hospital in the north of Iran, Mazandaran province, Sari. Demographic and clinical characteristics, laboratory and imaging findings and also therapeutic modalities were recorded in all of them, separately. In this manuscript, we reported the most compatible clinical syndrome as the first impression in each case, complications and outcome in detail. For detection of SARS-CoV-2 infection in our patients, in addition to chest CT scan, we used COVID-19 real-time polymerase chain reaction (Made by Roge Network Technology and Sansure Biotech Inc., China), serum immunoglobulin M and G: IgM and IgG (Made by Pishgam Teb Co, Tehran, Iran). Due to vitamin D insufficiency of all patients based on laboratory test results, and the high prevalence of zinc deficiency in our region and their proven immunomodulatory role, we prescribed them for most of the patients with PIMS if tolerable. Based on the age of the patient, a single dose of vitamin D pearl (50.000 IU once a week) or 1000 IU droplet (1 cc/kg) was prescribed daily for all of the patients. Vasoactive drugs including dobutamine, dopamine and milrinone were used and doses were adjusted based on condition of the patients. Hydroxychloroquine was prescribed at a single dose of 3-5 mg/kg/day for 5 days. Other medications including oseltamivir, meropenem, vancomycin and kaletra were used based on the national protocol for the management of COVID-19 in children [13] . Toxic-appearing was defined as pale or cyanotic, lethargic or inconsolably irritable. Besides, they may have tachypnea and tachycardia [14] . The ethics committee of Mazandaran University of Medical Sciences approved the study protocol (No = IR.MAZUMS..REC.1398.7277). Written informed consent was obtained from parents of all patients before treatment. A 12-year-old boy with history of chronic renal failure and recurrent hemodialysis, admitted with fever and chills from 7 days and rashes from 3 days prior to admission which disappeared before admission. Also, the patient complained of diarrhea, weakness, and fatigue without respiratory complaints. He had a history of COVID-19 in family members. He was toxic at admission and physical examination revealed the following: body temperature, 39.5°C; pulse, 120 beats/min; blood pressure, 70/50 mmHg; and oxygen saturation, 92% in the ambient room. Although he didn't have tachypnea or abnormal lung sounds, respiratory distress developed hours later. Vasoactive drugs, oseltamivir, meropenem, vancomycin, hydroxychloroquine, and Kaletra were prescribed. In the chest computed tomography (CT) scan, patchy ground-glass opacity and interlobar septal thickening were found compatible with COVID-19. In the complete blood count (CBC), lymphopenia, anemia, and thrombocytopenia were observed. Raised blood urea and creatinine, increased liver transaminases, proteinuria, hematuria, and marked acidosis in the arterial blood gas (ABG) were also noted, but creatine phosphokinase (CPK), lactate dehydrogenase (LDH), and electrolytes were in normal values. The patient's general condition deteriorated and he underwent tracheal intubation. Echocardiography showed mild mitral and tricuspid regurgitation, mild diastolic dysfunction, decreased left ventricular ejection fraction on the first day which deteriorated on the second day (Table 1) . Packed red blood cell (packed cell), intravenous immunoglobulin (IVIG), and hydrocortisone prescribed. Before completion of the IVIG infusion, the patient's condition deteriorated and he died on the third day after hospitalization. The result of COVID-19 RT-PCR was positive for him. A 5-year-old girl presented with a history of 3 days highgrade fever (39-40 o c), vomiting of one episode and skin rash during fever, loss of appetite, intermittent cough, otalgia, and diarrhea. On admission, she was ill and irritable without respiratory distress and SPO2 was 99% in room air. She had bilateral otitis media and bilateral non-purulent conjunctivitis. Her parents had a suspicious history of COVID infection, so the chest CT-scan was performed and was normal. Treatment with ceftriaxone and zinc gluconate was started. Gradually, presentation of the disease changed and periorbital edema (day 3), headache, limb pain, pharyngeal congestion with punctuated exudate and purulent conjunctivitis appeared. Due to abdominal pain and tenderness, abdomino-pelvic ultrasonography was performed which was normal, except for mild to moderate free fluid was seen and also, mild to moderate bilateral pleural effusion was detected. Liver transaminases, serum amylase and lipase were in a normal range. Because of a family history of COVID-19 infection and new changes in the second chest CT-scan compatible with COVID-19 infection, hydroxychloroquine and azithromycin were started on the 3rd day and the patient was isolated. On day 5, COVID-19 RT-PCR result was positive, she was still febrile and her condition deteriorated. She became drowsy, tachypneic (respiratory rate: 38 /min) without retraction and dry cough and generalized edema developed. She became hypoxemic in room air with SpO2 of 88% and was subsequently transferred to the pediatric intensive care unit (PICU). On CBC, severe anemia and thrombocytopenia in addition to hypoalbuminemia were noted (Table 1) . So, packed red blood cells (packed cell), 1 g/kg IVIG and albumin were transfused and antibiotics changed to meropenem. Blood urea, Cr, LDH, peripheral blood smear, prothrombin time (PT), partial thromboplastin time (PTT), bilirubin, triglyceride, and fibrinogen level were in the normal range and urine analysis was normal. Blood and urine cultures did not yield any organism. Echocardiography showed mild tricuspid regurgitation, trivial mitral regurgitation with normal coronary arteries. During the first 8 days, fever subsided, but the patient was still tachypneic. Finally, the patient was discharged after 13 days with a good general condition. He was a 13-month-old boy presented with 4 days history of fever and 3 days of rash prior to admission. Skin rash started from the palms of the hands and the soles of the feet and then, erythematous patches, papule, and some target shape lesions on the edematous base on the trunk, limbs and face developed without itching sensation. He also defecated loose stool 2 to 3 times and had a loss of appetite and irritability without any respiratory signs except left tympanic membrane erythema. His parents worked in a COVID-19 referral ward. On admission day, chest CT-scan was normal and he was treated with hydroxychloroquine and ceftriaxone. On day 3, the patient's condition deteriorated and generalized edema in addition to purulent conjunctivitis, intercostal and subcostal retraction, and respiratory distress with tachypnea appeared. While SPO2 was 84% in the ambient room, the second chest CT-scan changed to typical COVID-19 findings (bilateral pleural effusion, basilar patchy infiltration, and reverse halo sign). Anemia and hypoalbuminemia occurred while both of them were in the normal range on the admission day (Table 1) . So, the patient was transferred to the PICU, and oxygen was administered with a hood and packed cell, albumin and IVIG (1 gr/kg) were transfused. Antibiotic was changed to meropenem and vancomycin. Abdominopelvic ultrasonography was normal except mild fluid observed in sub-hepatic, perisplenic, and interloop space. Echocardiography showed mild tricuspid and mitral regurgitation and normal coronary arteries on two occasions. The patient gradually improved, skin rashes got better, he became afebrile and without any distress. He has discharged after 8 days and COVID-19 RT-PCR result was positive for him. A 10-year-old girl presented with fever and itching skin rash from 5 days prior to admission. One day before admission, her general condition worsened; she got toxic and cough, abdominal pain, generalized edema and oliguria developed. The maculopapular and target shape skin rashes appeared with more accumulation around the neck, trunk, and axillary. Mucous membranes were intact except for bilateral conjunctivitis and cracked lips. She had a history of COVID-19 in family members. Physical examination at admission revealed the following: body temperature, 39.4°C; pulse, 120 beats/min; respiration, 36 breaths/min; blood pressure, 66/40 mmHg; and oxygen saturation, 90%. Laboratory evaluation revealed anemia, hypoalbuminemia, and impaired renal function tests (Table 1 ). According to hypotension and shock, vasoactive drugs in addition to meropenem, clindamycin, vancomycin, IVIG, packed cell, and albumin prescribed ( Table 1 ). The hemodynamic status of the patient stabilized after 3 days. Liver transaminases, PT, PTT, CPK, troponin, LDH, fibrin degradation products (FDP), C3, C4, and total hemolytic complement (CH50) were in a normal range. Antistreptolysin O, antiphospholipid antibody, and antinuclear antibody (ANA) were negative too. Blood and urine culture were also negative. Chest roentgenogram (CXR) and CT-scan were normal on the first admission day. COVID-19 RT-PCR test result was negative but COVID-19 immunoglobulin G (IgG) test was positive 7 days following admission. On day 3, edema, severe pain of right lower extremity, and venous stasis appeared due to central venous catheter insertion in the right femoral vein. Color Doppler ultrasonography of lower limb veins was normal. Enoxaparin was started as prophylaxis of deep vein thrombosis (DVT). Chest CTscan on the fourth day showed COVID-19 compatible changes with mild bilateral pleural effusion. The urinary system was normal in ultrasonography. Echocardiography was performed for three times and showed mild tricuspid regurgitation, mild mitral regurgitation, mild pulmonary insufficiency, with normal ejection fraction and coronary arteries. Fever eased 4 days after admission, vitamin D, and zinc gluconate were added to the patient's drugs. The general condition improved and vasoactive drugs tapered gradually. After 11 days, the patient was discharged with complete improvement. He was a 14-month-old boy with a history of COVID-19 in family members, presented with fever and irritability from 5 days and skin rash from 3 days prior to admission. Maculopapular erythematous rashes presented from the trunk and upper limb. Then, generalized and hands and feet edema developed. Cracked lips, erythematous lips and tongue, and bilateral non-purulent conjunctivitis also appeared. During the first admission day, the patient became toxic and was transferred to the PICU. The CBC showed leukocytosis with a significant neutrophil count. Elevated ESR, CRP, and liver transaminases and were also found in addition to hypoalbuminemia. Urine analysis and CXR were normal. COVID-19 RT-PCR test was negative and chest CT-scan showed non-significant changes. So, cefotaxime, hydroxychloroquine, 2 gr/kg IVIG and 60 mg/kg aspirin, zinc, vitamin D, and albumin started. Echocardiography showed normal coronary arteries, mild right pleural effusion (5 mm), mild mitral regurgitation with a normal coronary artery on the first admission day. Because of prolonged Prothrombin Time (PT) and the partial thromboplastin time (PTT), fresh frozen plasma (FFP) and vitamin K were prescribed. Fever continued for 2 days after IVIG infusion and he was still toxic. So, the second dose of IVIG was infused on the fourth day of admission. Echocardiography showed diastolic dysfunction, mild right and left coronary artery dilatation in the left anterior descending artery (LAD) and the left circumflex artery without aneurysm. Liver transaminases, PT and PTT decreased to the normal value but leukocytosis continued and packed cell was transfused for severe anemia. Also, ceftriaxone changed to vancomycin and meropenem. The second chest CT-scan showed non-significant changes. While fever subsided on day 7, hydroxychloroquine discontinued but echocardiography showed progression in coronary arteries dilatation, moderate mitral and tricuspid regurgitation, decreased ejection fraction, and mild diastolic dysfunction. So, 2 mg/kg/day prednisolone, vasoactive drugs and furosemide started. On day 10, the patient became hemodynamically stable, so vasoactive drugs tapered but abdominal distension occurred with non-significant findings in the physical examination. Ultrasonography showed mild hepatosplenomegaly, intra-abdominal fluid, and bilateral pleural effusion. In CBC test result, leukocytosis and thrombocytosis (platelet count: 420.000/µL) were reported. Echocardiography showed progressive coronary artery aneurysm and beading and clopidogrel started. Abdominal distension improved during the last 5 days, and the skin rash disappeared with pilling on day 14. Still afebrile, he was toxic. COVID-19 RT-PCR result was negative but COVID-19 IgM was positive. After that, marked thrombocytosis appeared in the CBC (platelet count: 1.168.000/µL), ESR and CRP normalized but coronary artery dilatation progressed to a giant aneurysm in day 17 as follows: right coronary artery (RCA): 8.3 mm, left main coronary artery (LMCA) and LAD: 6.7-7.2 mm with good left ventricular ejection fraction. So, warfarin and infliximab were prescribed. The patient was discharged from the hospital with aspirin and warfarin. Results of other evaluations during admission like serum vitamin D, LDH, CPK, and antiphospholipid antibodies were unremarkable. Two weeks later, the coronary diameters decreased to 6 mm in RCA and near to 5 mm in LMCA and LAD in the follow-up echocardiography. A 6.5-year-old boy with a history of COVID-19 in family members referred with fever from 3 days prior to admission, anorexia, and abdominal pain in the periumbilical and hypogastric area with occasional vomiting (3 times), one episode of loose defecation, and skin erythematous rash around ankles spreading to the trunk. He had no respiratory complaints and received a diclofenac and acetaminophen suppositories for fever and pain relief. He had a history of repaired duodenal atresia at birth. On physical exam, the patient was ill and febrile, had macular erythematous rashes around feet, hands, trunk and perioral, periorbital edema, erythema of oropharynx, right eardrum erythema and hypogastric tenderness. He had elevated ESR and CRP in addition to abnormal urine sediment. Ceftriaxone started and chest CT-scan were performed with non-significant changes for COVID-19. the urine culture of the first day yielded no organism. On the second day of admission, he got toxic and irritable with respiratory distress, low SPO2: 87%, mild abdominal distension, and anasarca edema. The patient was transferred to the PICU. Serum albumin was 2.6 g/ dL, while other indexes like amylase and lipase were in normal values. So, albumin started. During the last 2 days, general condition of the patient got worst, he became anemic and more toxic, so albumin, 1 gr/kg IVIG, packed cell, hydroxychloroquine, and vitamin D were administered and ceftriaxone changed to vancomycin and meropenem. A second chest CT-scan showed bilateral opacity compatible with COVID-19. COVID-19 RT-PCR was negative but COVID-19 IgG was positive which was measured a week later. Abdominal ultrasonography showed mild splenomegaly, free interloop fluid, and several reactive lymph nodes. Echocardiography was performed two times and reported normal coronary arteries. Other investigations like Wright, Widal, blood, urine, and stool culture were negative. Transaminases, PT, PTT, LDH, FDP, and D-dimer were in normal values. The patient's abdominal pain improved on day 6, and he was discharged after 11 days. A 7.5-year-old girl presented with a history of fever, irritability, myalgia, vomiting, diarrhea, abdominal pain, generalized erythematous maculopapular and patches from 4 days prior to admission without respiratory complaints. Her parents were infected with COVID-19 nearly 2 weeks before admission. She received diclofenac and azithromycin before admission. On physical exam, she was ill with no distress. Erythema of the throat and generalized erythematous maculopapular and patches were observed. On the admission day, lymphopenia and vitamin D deficiency were noted (Table 1) . Chest CTscan, abdominal ultrasonography and stool exam were normal and cultures of the urine, blood, and stool were negative. So, ceftriaxone, hydroxychloroquine, zinc, and vitamin D prescribed. Evaluation for infection with COVID-19 with RT-PCR COVID-19 was negative. On the third admission day, facial edema, tachypnea and tachycardia developed and the patient got toxic with a gallop in heart auscultation. Due to marked hypoalbuminemia and anemia, packed cell and albumin were transfused. Blood urea and Cr raised, with normal values for serum electrolytes, PT, PTT, CPK, LDH, troponin, and liver transaminases. Anti-phospholipid antibodies were negative. Echocardiography showed low ejection fraction with dilated right and left ventricle, so, the diagnosis of myocarditis was raised; vasoactive drugs and 1 g/kg IVIG started and ceftriaxone changed to meropenem and vancomycin with magnesium sulphate for hypomagnesemia. CXR showed bilateral mild groundglass opacity. Gradually, during last days, facial edema improved a little, but tachycardia and tachypnea were persistent on the 5th day. The second echocardiography report included mild pleural effusion, valve insufficiency, low ejection fraction, lack of tapering and brightness in RCA and LAD. So, another dose of IVIG (1 g/kg) with 60 mg/kg/day aspirin was prescribed. With the improvement in the hemodynamic status, vasoactive drugs gradually tapered. On the 7th day, the patient became afebrile and was discharged after 12 days with normal echocardiography and laboratory tests. A 20-month-old boy with a history of COVID-19 in the family presented with intermittent fever from 7 days prior to admission. Gradually, coryza, vomiting, severe diarrhea, abdominal pain, irritability during urination, and loss of appetite appeared. He also had an erythematous papule with 2-centimeter diameter in the forehead and erythema of oropharynx. Before admission, he received metronidazole, cefixime, nalidixic acid, diclofenac, and acetaminophen without improvement. CBC at admission showed marked leukocytosis, thrombocytosis, anemia, and increased levels of ESR and CRP ( Table 1 ). The stool exam had 4-5 white blood cells and 2-3 red blood cells. PT, PTT and blood levels of creatinine, urea, LDH, and CPK were in normal values. Urine analysis was unremarkable and cultures of blood, urine, and stool were negative. Abdominal ultrasonography was normal but Chest CT-scan showed bilateral ground-glass opacity compatible with COVID-19. So, the patient isolated and ceftriaxone, hydroxychloroquine, vitamin D, and zinc gluconate started. Hours after admission, he got tachypnea and SPO2 was 90% without supplementary oxygen. Unilateral tongue swelling and drooling with discrete ulcers under the tongue were seen. So, he was transferred to the PICU and ceftriaxone changed to clindamycin and meropenem. Echocardiography found a lack of tapering in RCA and LAD, mild dilatation of left atrium, and diastolic dysfunction. Before starting 2 g/kg IVIG and 80 mg/kg/day aspirin, fever subsided but he was still toxic. Seven days after admission, the patient's condition improved and the second echocardiography after one week was normal. Results of COVID-19 RT-PCR, IgM, and IgG in the first week of admission were negative. He was a 7-year-old boy with a fever from 3 days prior to admission. He also suffered from epigastric pain which was shifted to the right lower quadrant (RLQ) after 2 days and was not associated with eating. He had nausea, vomiting, and a normal defecation pattern. He was admitted 10 days ago for acute intravascular hemolysis due to glucose 6 phosphate dehydrogenase deficiency (G6PDd) and fava bean exposure and was treated with packed cell and hydration. During the recent admission, he was still ill and febrile without respiratory symptoms. On physical examination, tenderness in RLQ and rebound tenderness were noted. Ultrasonography showed reactive lymph nodes and free interloop fluid. In the CBC, leukocytosis with a shift to left was found. ESR and CRP were markedly elevated but urine and stool were unremarkable. So, the patient received ceftriaxone plus metronidazole and underwent appendectomy, but normal appendix with some exudative secretion in the peritoneal cavity was seen during the operation. After surgery, the patient's condition worsened and became toxic, developed abdominal distension and recurrent vomiting occurred. Abdominal X-ray was normal and repeated ultrasonography reported mild interloop fluid only. Chest CT-scan showed findings compatible with COVID-19 infection. Ceftriaxone changed to meropenem and vitamin D, hydroxychloroquine, and zinc gluconate started. Three days following admission, his fever subsided and abdominal complaints improved. Other investigations like liver transaminases, serum LDH and echocardiography were normal. After 4 days, the patient was discharged. Results of COVID-19 IgM, IgG and RT-PCR during hospitalization were negative. An 18-month-old girl referred for a prolonged febrile seizure. The fever started from 4 days prior to admission and seizure was generalized tonic colonic accompanied with loss of consciousness, upward gaze, and foaming, which lasted for 40 minutes and was controlled with diazepam and phenobarbital in another hospital. The family was passenger from another province of Iran. She had an epileptic sister. On physical exam, she was lethargic and febrile without abnormal findings. She had lymphopenia and decreased serum albumin. Cerebrospinal fluid (CSF) analysis and electrolytes were normal and CSF, blood, and urine culture yielded no organism. Meropenem, clindamycin, phenobarbital, hydroxychloroquine, and vitamin D were prescribed. Brain CT-scan and CXR had no pathologic finding and chest CT-scan showed bilateral nonspecific opacity in inferior lobes. She was ill and lethargic during the first two days. So, albumin, 1 g/kg/day IVIG, and zinc gluconate were prescribed. On the third admission day, fever continued and maculopapular rashes appeared in the trunk. So, phenobarbital discontinued and echocardiography was performed which was normal. On this day, tachypnea developed, ABG was normal and second chest CT-scan on the 4th day had the same features of the first. COVID-19 RT-PCR result was positive. On the 5th admission day, fever subsided. Tachypnea improved within 4 days. After 3 days, the patient was discharged with good general condition. In this study, 10 patients with COVID-19 associated PIMS who presented the disease with unusual symptoms and signs like status epilepticus, appendicitis, TSS, KD and KD shock syndrome were reported (Table 2 ). Due to the similarity with other diseases, these patients may be admitted to a ward without respiratory isolation and may spread the disease to other patients and health care workers. Given that standard treatment for the disease has not been identified yet, this report provides our experience in the management of these patients. Although children with COVID-19 are more likely to present fever and respiratory or gastrointestinal symptoms [15, 16] , the most common presentations of our cases (8 cases of 10) were fever and rash with no respiratory symptoms, like the report of Whittaker et al. [7] and only 3 of our cases had a cough or any other respiratory complaints. On the other hand, two of our patients had no respiratory complaints. One of the diagnostic criteria for COVID-19 is respiratory symptoms, so according to WHO and AAP, the diagnosis of the disease is possible without respiratory symptoms [11, 17] . However, three to four days after hospitalization, almost all patients developed respiratory symptoms like Shekerdemian et al.'s report [15] . One of these patients had refractory seizures and fever, with no other serious symptoms at the time of admission. The onset of the disease with unusual symptoms can delay the diagnosis and have irreparable consequences for the patient, or might be misdiagnosed with other diseases and management is performed based on the disease. The primary diagnoses of our patients were KD, TSS, prolonged febrile seizure, appendicitis, and suspected sepsis or COVID-19. In the case series reported by Whittaker et al. [7] , the primary diagnosis was almost the same as this report, although one of our patients had prolonged febrile seizures, which could be the first report. Sixty percent of our patients had positive laboratory evidence for COVID-19, and all of them had a history of contact with COVID-19 patients. Like the report of Chiotos et al. [18] , most of the patients in our study had anemia, lymphopenia, hypoalbuminemia and all of them had increased ESR and CRP. It is worth noting that abnormal laboratory results may not exist at the admission time, but such measures deteriorated when the general condition of the patient worsened. In two patients, PT and PTT decreased. Interestingly, regarding more severe conditions, LDH was normal in all cases in contrast to Chiotos et al.'s study [18] . Three patients suffered from acute kidney injury and two patients had hepatic dysfunction. These findings have had different results in the studies of Verdoni et al. [5] , Whittaker et al. [7] , Shekerdemian et al. [15] and Chiotos et al. [18] . In these studies, the increase in ESR and CRP, anemia, and leukopenia were similar to our case series. In this report, chest CT-scan was normal at the time of admission in all patients. However, 3 days after hospitalization, like the report of Whittaker et al. [7] , 5 patients had concurrent findings compatible with COVID-19 on chest CT-scan along with respiratory symptoms. The most common chest CT-scan findings included patchy infiltration, ground glass opacity with halo or reverse halo sign in the lower lobes of the lungs and pleural effusion. In echocardiography, ejection fraction was reduced in three cases, and in two cases, ectasia and in one patient aneurysm were reported. As reported by Verdoni et al. [5] , Whittaker et al. [7] and Chiotos et al. [18] , evidence of echocardiography in some of these patients was in favor of KD, TSS, and myocarditis. Like Whittaker et al. [7] , Shekerdemian et al. [15] , and Chiotos et al.'s study [18] , almost all of the patients were admitted to the PICU. In addition to supportive care like hydration, albumin infusion, packed cell transfusion, appropriate antibiotics, and hydroxychloroquine, IVIG was given to 6 patients with a dose of 1 g/kg and three patients at a dose of 2 g/kg. Corticosteroids were prescribed for two patients with resistant Kawasaki disease with giant aneurism and multiorgan failure. Similar to other studies, the prognosis of most patients was good [4, 5, 7, 15] . Almost all our patients were discharged without complication, except one case with a giant aneurysm. There was only one death in our patients, which was due to a history of uncontrolled renal failure and late referral. Our study was the first report of PIMS in Iran. We aimed to share clinical characteristics, paraclinical findings and therapeutic experiences regarding more severe pediatric COVID-19 cases who fulfill PIMS criteria. All of them were previously healthy children, except one case with CRF. The limitation of this study was few number of the patients which. Since our cases were limited to northern Iran, the number of children in this study is not able to represent COVID-19 of the pediatric population. Increasing the number of patients can help to generalize the findings. Also, our data is descriptive which is not able to imply any possible association and treatment benefit. Furthermore, since our follow up was limited during a rather short period, cases included in this series does not exclude the possibility of worse or better outcomes yet to evolve. In addition, some tests such as IL-6 have not been performed in some patients. Therefore, multicenter studies and systematic reviews for better understanding and management of the disease are recommended. Although clinical manifestations and inflammatory biomarkers in most of the children are nonspecific and milder than that in adults (16), children with COVID-19 infection may present symptoms similar to Kawasaki disease and inflammatory syndromes. So, PIMS should be considered in children with fever and rash, seizure, cough, tachypnea, and gastrointestinal symptoms such as vomiting, diarrhea, and abdominal pain.  
paper_id= fff9b10e6c5eb955d4bb7c541423e688ee0ffd8f	title= General synthetic strategy for regioselective ultrafast formation of disulfide bonds in peptides and proteins	authors= Shay  Laps;Fatima  Atamleh;Guy  Kamnesky;Hao  Sun;Ashraf  Brik;	abstract= Despite six decades of efforts to synthesize peptides and proteins bearing multiple disulfide bonds, this synthetic challenge remains an unsolved problem in most targets (e.g., knotted mini proteins). Here we show a de novo general synthetic strategy for the ultrafast, highyielding formation of two and three disulfide bonds in peptides and proteins. We develop an approach based on the combination of a small molecule, ultraviolet-light, and palladium for chemo-and regio-selective activation of cysteine, which enables the one-pot formation of multiple disulfide bonds in various peptides and proteins. We prepare bioactive targets of high therapeutic potential, including conotoxin, RANTES, EETI-II, and plectasin peptides and the linaclotide drug. We anticipate that this strategy will be a game-changer in preparing millions of inaccessible targets for drug discovery. 	body_text= A fter production in the ribosome, most proteins undergo further maturation through covalent modifications, which alter their structure, localization or function and aberrations in these steps are associated with numerous diseases 1 . While unmodified proteins can be readily obtained by biological expression, the preparation of posttranslationally modified proteins remains challenging, increasing demand for their chemical synthesis 1, 2 . Disulfide bonds, one of the most widespread covalent modifications, influences the three-dimensional architecture and function of peptides and proteins exist in many target of therapeutic interest such as in the insulin hormone, the coronavirus (SARS-CoV-2) spike protein 3, 4 , and in natural libraries of toxin peptides among others. Unfortunately, performing studies with isolated or biologically expressed peptides and proteins that contain disulfide bonds, is time-consuming and often impractical and even impossible for various targets, driving the great interest in finding approaches for their efficient synthesis. Yet, their chemical synthesis remains as a long standing synthetic challenge for over six decades [5] [6] [7] [8] , leaving various potential therapeutic targets inaccessible 3,6,7,9 . This synthetic challenge led to exploration of two main strategies; (1) oxidative folding, which is common and depends on the protein folding pathway conferring the correct disulfide connectivities under thermodynamic control (e.g., the correct isomer out of the 75 possibilities for three disulfide bonds). This strategy suffers from various drawbacks such as long reaction times (up to several days), low yields or lack of success in many targets, probably due to the accumulation of kinetically trapped folded intermediate (dead-ends) (Fig. 1a) 3, 6, 7, 10 . (2) The second strategy is based on stepwise formation of disulfide bonds employing orthogonally protected cysteine (Cys). Here, the removal of protecting groups (PGs) can lead to significant irreversible amino acids (AAs) side reactions and disulfide bonds isomerization, in addition to the lengthy process (several days) due to long reactions time and the necessity for multiple purification steps, which also leads to significant loss of material ( Fig. 1b) [11] [12] [13] [14] . Here we show, an effective synthetic approach based on a general design for the synthesis of peptides and proteins containing disulfide bonds. Our strategy relies on small molecule activation of the Cys side chain via the disulfiram (DSF) and ultraviolet (UV) light/Pd chemoselective chemistries for one-pot and ultrafast formation of two or three disulfide bonds (Fig. 1c) . We demonstrate the power and potential of this strategy for the preparation of various challenging targets in the efficient chemical syntheses of α-conotoxin SI peptide, RANTES protein from the chemokine family, EETI-II trypsin inhibitor knotted mini protein, plectasin antimicrobial peptide and the linaclotide peptide drug. We set out to design an effective set of sequential reactions guided by the following considerations. First, the synthesis must be accomplished rapidly to eliminate accumulation of side products and/or reshuffling. Second, the chemistry should be carried out under aqueous denaturation conditions to avoid aggregation and kinetically trapped folding intermediates. Third, all steps must be performed in a one-pot operation for efficient synthesis 3,9 . Devising efficient synthesis of α-conotoxin and RANTES with two disulfide bonds. As a model system we chose the well characterized and studied α-conotoxin SI peptide, composed of 13 Stepwise deprotection and oxidation strategy, main limitations. c Ultrafast high-yielding formation of multiple disulfide bonds via our strategy (PG is abbreviation for protecting group, UV is abbreviation for ultraviolet-light and DSF is abbreviation for disulfiram). AAs and having two disulfide bonds between Cys (2&7) and Cys (3&13) that induce the 3 10 helix shape 15 . This peptide belongs to the Conus venoms conotoxin family-bearing multiple disulfide bonds that stabilize their compact structures. Such properties are responsible for their potency against voltage-gated ion channels, G-protein-coupled receptors and neurotransmitter transporters, making them attractive leads for drug development 6 . Direct disulfide bond formation from a protected Cys precursor has been challenging due to several side reactions and/or to reshuffling when multiple disulfide bonds exist. A recent study has shown that Pd and diethyldithiocarbamate (DTC) shown an effective removal of acetamidomethyl (Acm) PG and the direct formation of a single disulfide bond in peptides with no side reaction 16 . Yet, avoiding reshuffling remains a significant challenge in exploring these reagents in the efficient synthesis of peptides and proteins with multiple disulfide bonds. Nevertheless, we speculated that the Pd thiophilic nature and disulfiram (DSF), the oxidized and possibly more reactive form of DTC, could be employed to tackle this synthetic problem. To examine this notion, we prepared the linear α-conotoxin SI peptide by solid phase peptide synthesis (SPPS), with two Cys (3&13) protected with the trityl PG to afford the free S-H of these Cys residues after trifluoroacetic acid (TFA) cleavage, and two additionally modified Cys (2&7) with the (Acm) moiety, (Supplementary Fig. 1 ). Exposing α-conotoxin SI to DSF at 37°C , pH 7, led to the immediate formation of the first disulfide bond between Cys (3&13), as detected by HPLC-ESI MS. To form the second bond between Cys (2&7) we added, in situ, PdCl 2 for 5 min in pH 1. Notably carrying out this step at higher pHs led to significant disulfide reshuffling. Although the exact pathway of the reshuffling step still unclear, this on the other hand highlights the challenge in this synthetic endeavor. Subsequently, we added DTC, as a Pd scavenger 16 , and a fresh amount of DSF followed by adjustment of the reaction to pH 7. The second disulfide bond was formed immediately as shown by HPLC-MS analysis and we isolated the desired product in 48% yield (Fig. 2a-c) . Circular dichroism (CD) measurements showed the expected signature of the 3 10 helix native isomer (Supplementary Fig. 2 and Fig. 2f ) confirming the formation of the native α-conotoxin SI peptide with the correct disulfide bonds 15 . In addition, the chromatographic retention time of our synthetic product matched perfectly to the commercially available peptide (Fig. 2d , e, respectively). These encouraging results motivated us to examine our set of conditions in more challenging synthetic systems. Therefore, we chose the RANTES chemokine protein from the cytokine family that directs trafficking of leukocytes during inflammation 17 . We prepared this protein, composed of 68 AAs and two disulfide bonds between Cys (10&34) and Cys (11&50) 18 , via SPPS with Cys 10 and 34 in the free form, and Cys 11 and 50 modified with the Acm (Supplementary Fig. 3 ). Applying our developed synthesis afforded the native protein with the correct disulfides within 5 min, which was isolated in 35% yield for the two oxidation steps (Fig. 3a-c) . The chromatographic retention time of the product matched perfectly with the commercially available protein and the CD signature of the correct isomer folding was detected (Fig. 3d-f ). One-pot formation of three disulfide bonds: plectasin and linaclotide syntheses. Next we sought to develop conditions for formation of three disulfide bonds in a one-pot operation taking advantage of the above describe conditions. We searched for possible orthogonal chemistry with the Pd and DSF before or after the Acm removal and in presence of at least a single disulfide bond. Therefore, we envisioned a possible pathway involving Pd/ light irradiation, as disulfide bond is known to be stable to light irradiation 19 . To evaluate this notion, we synthesized the αconotoxin peptide bearing two Cys (3&13) modified with 2nitrobenzyl (NBzl) and other two Cys (2&7) modified with Acm. We examined the use of the NBzl photosensitive PG under exposure to our conditions because of its relatively easy incorporation to the Cys side chains and its high stability in SPPS 20,21 . To our pleasant surprise when the α-conotoxin peptide was exposed to UV light in presence of DSF at pH 7, a fast (within 8 min) and simultaneous decaging and selective disulfide formation, was observed without affecting the Acm ( Supplementary  Fig. 4) . Notably, this decaging step was expedited by the disulfide formation in presence of DSF 20 . Following the UV step, we then added Pd II and DTC/DSF, which led to the formation of the desired product within only 5 min without any detectable side products (Supplementary Fig. 4) . Based on these results with the model peptide, we designed the following synthetic one pot operation for the synthesis of peptides and proteins with three disulfide bonds, which includes; (1) DSF mediated formation of the first disulfide bond (<10 s), (2) UV light exposure (~8 min) to form the second disulfide bond from a Cys (NBzl) precursor, (3) Pd II DTC/DSF mixture mediated third disulfide bond formation from a Cys (Acm) precursor (~5 min). To evaluate the practicability of this design, we chose the plectasin peptide, known as a fungal defensin potently active against drug-resistant Gram-positive bacteria (e.g., streptococci) 22 . This peptide is made of 40 AAs with three disulfide bonds between Cys (4&30), Cys (19&39), Cys (15&37) and shares structural features with the other members of this family. The linear polypeptide was assembled via SPPS protocol, bearing two free Cys (4&30), two Cys modified with NBzl (19&39) and two Cys modified by Acm (15&37) (Supplementary Fig. 5 ). The peptide was subjected to our synthetic design and a single product was formed within 13 min and was isolated in 43% yield ( Fig. 4a-d) . The folded protein eluted with the same retention time as the product obtained via oxidative folding following previously used protocols ( Supplementary Fig. 6 ), exhibited the expected biological antibiotic activity (Fig. 4e) and showed the expected CD signature of the natural isomer laddered shape and (Fig. 4f) 23 . Using our protocol we have also prepared the linaclotide drug for chronic constipation and irritable bowel syndrome, made of a 14 AAs peptide and bearing three disulfide bonds ( Fig. 5 and Supplementary Figs. 7, 8 ) 3 . With these results in hands, we decided to examine our synthetic design on a more challenging example from the knotted protein family. The synthesis of molecular knots has been a challenging problem that has spurred synthetic efforts for the past 50 years 24 . In particular the synthesis of Cys knot structural motif that has exceptional effects on peptide/protein chemo, mechano and proteolytic stability has remained difficult to the point of seemingly impossible 7 . We chose as a representative example the well-studied EETI-II mini protein, composed of 28-residues and belonging to the squash family of trypsin inhibitors, which has never been prepared using the stepwise approach 25, 26 . Using native chemical ligation in solution or on resin coupled with oxidative folding the protein was successfully prepared. Yet, the oxidative folding step took up to 10 days for completion 25 . To examine our strategy we used SPPS to afford the linear peptide bearing free Cys (9&21), Cys (15&27) modified with NBzl and Cys (2&19) modified with the Acm (Supplementary Fig. 9 ). Applying our synthetic three steps operation, the protein was formed with native disulfide connectivities within 13 min ( Fig. 6 and Supplementary Fig. 10) . The product was isolated in 32% yield and showed the native biological activity 25 in addition to the expected unique CD signature (Fig. 7) . In summary, we introduced here a straightforward synthetic scheme applying activation of the Cys side chain by DSF, UV light, and Pd for chemoselective and regioselective formation of multiple disulfide bonds, tackling a decades old standing problem in peptide and protein chemistry. This efficient three step, onepot operation was demonstrated in the syntheses of representative examples from various large peptide/protein families. This strategy is currently restricted to three disulfide bonds and the synthesis of targets with more than three disulfide bonds will require the development of additional chemistry for chemoselective and regioselective disulfide bond formation. We anticipate that this synthetic design will pave the way for the synthesis of currently inaccessible targets due to its excellent efficiency and simplicity. Moreover, we envision with the recent technological progress in SPPS employing flow [27] [28] [29] and robotic chemistry 30 , the generation of combinatorial libraries of peptides and proteins bearing multiple disulfide bonds for drug discovery and various other applications. The α-conotoxin SI synthesis. The lyophilized conotoxin peptide (0.5 mg, 0.3 nmol) was dissolved in 670 µl (0.5 mM) 6 M Gn . HCl buffer, pH 7, and treated with 10 equiv. DSF (20 µl from stock) for 10 s at 37°C. Subsequently, the pH of the reaction was adjusted to 1 using 0.1 M HCl and 15 equiv. PdCl 2 (30 µl from stock #1) was added for 5 min at 37°C. Then, 30 equiv. (20 µl from stock #3) DTC followed by 10 equiv. DSF were added. pH adjustment to 7 and incubation at 37°C for 10 s immediately afforded the native α-conotoxin SI. Plectasin synthesis. The lyophilized plectasin peptide (0.5 mg, 0.1 nmol) was dissolved in 208 µl (0.5 mM) 6 M Gn . HCl buffer, pH 7, and treated with 10 equiv. DSF (6 µl from stock #2) for 10 s followed by exposure to UV irradiation at 350 nm for 8 min at room temperature. Subsequently the pH of the reaction was adjusted to 1 by 0.1 M HCl and 15 equiv. PdCl 2 (9 µl from stock #1) was added for 5 min at 37°C. Then, 30 equiv. (3 µl from stock #3) DTC was added followed by in situ addition of 2 equiv. LiS (1 µl from stock #4) and 10 equiv. DSF. pH adjustment to 7 and incubation at 37°C for 10 s immediately afforded the native plectasin. The addition of 2 equiv. LiS was found to facilitate the recovery of the peptide from the bounded Pd residues. EETI-II synthesis. The lyophilized EETI-II peptide (0.5 mg, 0.1 nmol) was dissolved in 303 µl 6 M Gn . HCl buffer, pH 7 (0.5 mM), and treated with 10 equiv. DSF (9 µl from stock #2) was added for 10 s followed by exposure to UV irradiation at 350 nm for 8 min at room temperature. Subsequently the pH of the reaction was adjusted to 1 using 0.1 M HCl and 15 equiv. PdCl 2 (13 µl from stock #1) was added for 5 min at 37°C. Then, 30 equiv. (4 µl from stock) DTC followed by in situ addition of 10 equiv. DSF, pH adjustment to 7 and incubation at 37°C for 10 s afforded the native EETI-II. Linaclotide synthesis. The lyophilized linaclotide peptide (0.5 mg, 0.2 nmol) was dissolved in 515 µl 6 M Gn . HCl buffer, pH 7, (0.5 mM) and treated with 10 equiv. DSF (15 µl from stock #2) for 10 s followed by exposure to UV irradiation at 350 nm for 8 min at room temperature. Subsequently the pH of the reaction was adjusted to 1 by 0.1 M HCl and 15 equiv. PdCl 2 (23 µl from stock #1) was added for 5 min at 37°C. Then, 30 equiv. of DTC (7 µl from stock #3) followed by 2 equiv. GSH 10 equiv. (16 µl from stock # 5) and 10 equiv. DSF were added in situ. pH adjustment to 7 and incubation at 37°C for 10 s immediately afforded the native linaclotide. The addition of 2 equiv. of GSH was found to facilitate the recovery of the peptide from the bounded Pd residues. a b c d Fig. 6 EETI-II synthesis. a HPLC-ESI MS analyses: Reaction at time zero, the main peak corresponds to reduced EETI-II with the observed mass 3297.6 ± 0.1 Da, calcd. 3297.4 Da (average isotopes). b Reaction after 10 s, the main peak corresponds to EETI-II with single disulfide bond with the observed mass 3295.2 ± 0.1 Da, calcd. 3295.4 Da (average isotopes). c Reaction after 8 min: the main peak corresponds to EETI-II with two disulfide bonds with the observed mass 3022.5 ± 0.2 Da, calcd. 3023.4 Da (average isotopes). d Reaction after 13 min: the main peak corresponds to EETI-II with three disulfide bonds with the observed mass 2879.2 ± 0.1 Da, calcd. 2879.4 Da (average isotopes). R 1 = NBzl, R 2 = Acm. Fig. 7 Synthetic EETI-II characterization. a HPLC-ESI MS analyses of purified and folded EETI:, the main peak corresponds to EETI-II bearing three disulfide bonds with the observed mass 2879.2 ± 0.1 Da, calcd. 2879.4 Da (average isotopes). b EETI-II structure (PDB). c Trypsin inhibition biological activity assay. Data are represented as mean ± SD, n = 3 biologically independent samples, error bars represent the SD. d CD spectrum of synthetic EETI-II. 
paper_id= fff9e4a9dd9f1bbf769deeebe1d651baede057d6	title= Supplementary appendix	authors= 	abstract= 	body_text= Le et al. 29 Hanoi, Vietnam Hospital Case series 12 29.5* 3 (25) URT Li et al. 30 Wuhan China Hospital Case series 36 57.5 (52) (53) (54) (55) (56) (57) (58) (59) (60) (61) (62) (63) (64) (65) 23 (64) URT Liang et al. 31 Wuhan, China Hospital Case series 120 61.5 68 (  Ling et al. 32 Shanghai, China Hospital Case series 66 44 (16-778) 38 (58) URT, stool, blood, urine Liu et al. 33 Wuhan, China Hospital Case series 238 55 (38.3-65) 138 (58) URT Liu et al. 34 Nanchang, China Hospital Case series 76 48.3 48 (63) URT Lo et al. 35 Macau, China Hospital Case series 10 54 3 (30) URT, LRT, stool, urine Lou B et al. 36 Zhejiang 65 Guangzhou, China Hospital Case series 10 6.6 6 (60) URT, rectal swab Yan et al. 66 Hubei 82 Hong Kong, China Hospital Case series 1041 NR NR URT, LRT, stool, urine Kwan et al. 83 Hong  Arons M et al Y Y Y Y Y N Y Y Lavezzo E et al Y Y Y Y Y Y Y Y RANDOMISEDHung I et al. Y N Y N N N Y Y Y Y Y Y Y Research question: What is the duration and dynamics of viral shedding in various body fluids in coronaviruses? The following databases will be searched: MEDLINE and EMBASE. For the following conferences we will hand search abstracts: European Congress of Clinical Microbiology and Infectious Diseases (ECCMID). Additionally, if any literature reviews are identified, reference lists of those review articles will be searched. Language: only English language articles will be reviewed. Search terms: 
paper_id= fff9f7976a67abc8dd73b8d488451c23387bdc92	title= 	authors= Marie  Lannelongue;Le  Plessis-Robinson;France  ;	abstract= 	body_text= The endothelium forms the innermost layer of blood ves sels and lymphatic vessels and is best viewed as a multi functional organ with both systemic and tissue specific roles. At the whole organism level, the endothelium reg ulates oxygen and nutrient supply, immune cell traffick ing and inflammation 1 , haemostasis and coagulation 2 , vasomotor tone 3 , blood vessel permeability 4 and angiogenesis 5 . In addition, the endothelium has a num ber of organ specific functions including regulation of organ size and function (myocardial hypertrophy 6 , liver size and function 7 , pulmonary alveolar repair 8 and kidney function 9, 10 ) . Given this heterogeneity of endothelial cell function, it is not surprising that studies show a remarkable het erogeneity of gene expression profiles in endothelial cells from different organs 11 . Interestingly, these expression profiles are functionally matched to local tissue needs. Microenvironment stimuli (shear stress, hypoxia and the presence of specific growth factors, cytokines and hormones) and epigenetics define and continu ously optimize local characteristics of endothelial cells. Epigenetic signatures that regulate the basal expression of endothelial specific genes in different organs are specified during embryonic development and con served during mitotic cycles 12 . Transcriptome analysis of endothelial cells from different tissues revealed hetero geneous gene expression signatures even after several passages in cell culture, indicating that tissue specific epigenetic modifications participate in the regulation of organotypic transcriptomic profiles 13, 14 . However, after long term cell culture, which removes endothelial cells from their in vivo microenvironment, approximately 50% of gene expression patterns are lost 15 , and major architectural characteristics, such as fenestrae, also disappear 16 . To characterize organotypic endothelial specificity as close to in vivo conditions as possible, many groups have utilized microarray or RNA sequencing (RNA seq) of endothelial cells isolated by flow cytometry without the cell culture step 11, 17 . Single cell RNA seq of endothe lial cells isolated from adult male mice identified tran scriptomic signatures of quiescent arterial, venous, capillary and lymphatic endothelial cells in 11 differ ent tissues 11 . Interestingly, lymphatic endothelial cells The quiescent endothelium: signalling pathways regulating organ-specific endothelial normalcy Nicolas Ricard 1 , Sabine Bailly 2 , Christophe Guignabert 3, 4 and Michael Simons 1, 5 ✉ Abstract | Endothelial cells are at the interface between circulating blood and tissues. This position confers on them a crucial role in controlling oxygen and nutrient exchange and cellular trafficking between blood and the perfused organs. The endothelium adopts a structure that is specific to the needs and function of each tissue and organ and is subject to tissue-specific signalling input. In adults, endothelial cells are quiescent, meaning that they are not proliferating. Quiescence was considered to be a state in which endothelial cells are not stimulated but are instead slumbering and awaiting activating signals. However, new evidence shows that quiescent endothelium is fully awake, that it constantly receives and initiates functionally important signalling inputs and that this state is actively regulated. Signalling pathways involved in the maintenance of functionally quiescent endothelia are starting to be identified and are a combination of endocrine, autocrine, paracrine and mechanical inputs. The paracrine pathways confer a microenvironment on the endothelial cells that is specific to the perfused organs and tissues. In this Review, we present the current knowledge of organ-specific signalling pathways involved in the maintenance of endothelial quiescence and the pathologies associated with their disruption. Linking organ-specific pathways and human vascular pathologies will pave the way towards the development of innovative preventive strategies and the identification of new therapeutic targets. from all the tissues cluster together, suggesting that the molecular signature of lymphatic endothelial cells is not tissue specific. By contrast, arterial and venous endothe lial cells from a specific tissue clustered together, show ing that vascular endothelial cell heterogeneity comes mainly from tissue specificity rather than arterial, capil lary or venous identity. Moreover, capillary endothelial cells that are involved in gas, ion, metabolite and hor mone exchange between the blood and tissues have the highest heterogeneity among tissues 11 . Structural differences in the capillary endothelium were first described in the 1960s with the use of elec tron microscopy 18 . Three major types of capillaries exist (continuous, fenestrated and discontinuous). The capillary type of an organ is related to its functions 19 . Most organs have barrier forming, continuous capillar ies (lungs, brain, skin and heart) with tightly connected endothelial cells surrounded by a continuous basement membrane. This architecture permits diffusion of water, small solutes and lipid soluble materials, while preclud ing the passage of cells or pathogens. By contrast, fenes trated capillaries have intracellular pores (windows) with a diaphragm and are found in renal glomeruli, exocrine glands, endocrine glands and intestinal mucosa. These fenestrae increase permeability to fluids and solutes, but not macromolecules 20 . Sinusoids are fenestrated capillar ies with gaps instead of pores between endothelial cells and a thinner basement membrane than in continuous or fenestrated endothelia and are present in the liver, spleen and bone marrow. The gaps found in sinusoids facilitate selective exchange of materials. Structural differences notwithstanding, normal endothelial cells everywhere are quiescent. This quies cent state is defined by minimal or absent endothelial proliferation and migration, minimal or no vascular leakage across the endothelial barrier and minimal (or fully absent) expression of leukocyte adhesion mol ecules. Indeed, the half life of a normal endothelial cell is an estimated 6 years in the heart as measured by 14 C incorporation 21 , and proliferative activity is absent (except in the liver and spleen, where about 1% of endothelial cells proliferate in a quiescent state 11 ). However, this 'quiescent' endothelium performs lots of active work, from secretion of paracrine and endocrine factors to active support of barrier maintenance for cell survival. Remarkably, little attention has been paid to what controls this 'active quiescence' and what main tains vascular normalcy under physiological conditions (Fig. 1) . Although much effort has been expended on exploration of signalling pathways underlying endothe lial cell activation and proliferation, almost no attention has been given to the signalling events that main tain endothelial normalcy and quiescence. The latest advances in this area are the subject of this Review. The fibroblast growth factor (FGF) signalling cascade includes a family of 18 ligands, four receptor tyrosine kinases (RTK) (FGFR1-FGFR4) and several accessory molecules such as Klotho proteins and syndecans 22 . After FGFs bind to their high affinity RTKs, several intracellular pathways are activated, including the phosphoinositide 3 kinase (PI3K)-AKT pathway and mitogen activated protein kinase (MAPK) pathways mediated by extracellular signal regulated kinase 1 (ERK1) and ERK2 (reF. 22 ). The extensive structural overlap and cross reactivity among FGF ligands and receptors represent a real chal lenge to identifying the function of FGF signalling in the endothelium. Results from studies in mice with knock out of individual Fgf genes or individual Fgfr genes are hard to interpret because of functional redundancy, whereas attempts to use FGFR chemical inhibitors are hampered by the low specificity and cross reactivity of these compounds. Successful strategies to circum vent the redundancy in the FGF family and investigate FGF signalling include mice with knockout of multiple 23 )), the use of soluble FGFR traps that target vari ous FGF family members 24 , and endothelial cell specific expression of a dominant negative FGFR1 construct that can inactivate all four FGF receptors 25 . Mice with con ditional endothelial cell specific deletion of Fgfr1 and Fgfr2 are viable, with no vascular developmental defects and no alterations in vascular homeostasis 26 . However, postnatal endothelial cell specific knockout of Fgfr1 in mice with global knockout of Fgfr3 results in impaired development of blood and lymphatic vessels 23 . A soluble receptor trap strategy was tested with the use of a solu ble FGFR1 trap (sFGFR1) that binds to a large number of FGFs 24 . In this study, transient FGF inhibition was achieved in vivo in mice via adenovirus mediated sys temic expression of sFGFR1. This FGF inhibition led to an increase in vascular permeability and, eventually, pul monary and myocardial haemorrhages, demonstrating the necessity for FGF signalling in the maintenance of vascular integrity 24 (Table 1) . A particularly interesting finding was the disrupted endothelial cell-cell junctions in large vessels, such as the femoral artery, carotid artery and jugular vein (Table 1) . One possible explanation for the disrupted endothelial cell-cell junctions is that the loss of FGF signalling decreases the expression of the phosphatase SHP2 (also known as PTPN11), thereby increasing phosphorylation of the junctional protein VE cadherin on tyrosine 658, which, in turn, results in loss of the VE cadherin-β catenin interaction 27 . The intracellular kinase SRC can also phosphorylate VE cadherin, especially in venous endothelial cells 28 . Phosphorylated VE cadherin is internalized and ubiq uitinated in response to inflammatory mediators 28 . However, phosphorylation of VE cadherin in the absence of inflammatory mediators is not sufficient for induction of vascular permeability 28 . • Quiescent endothelial cells require active maintenance to preserve normalcy in a tissue-specific manner. • Dysregulation of signalling pathways involved in endothelial normalcy maintenance leads to endothelial dysfunction and vascular pathologies. • Endothelial quiescence and normalcy are important for disease resilience. • Identification of organ-specific signalling pathways that maintain endothelial normalcy and quiescence will lead to new therapeutic targets supporting disease resilience and treatment of associated vascular pathologies. www.nature.com/nrcardio 0123456789();: The FGF signalling cascade also maintains endothe lial cell identity. FGF2 stimulation decreases expression of the transforming growth factor β (TGFβ) receptors (TGFβRs), TGFβ ligands (especially TGFβ2) and the major intracellular TGFβ signal transducer SMAD2 (reFs 29, 30 ). FGF driven suppression of TGFβ signal ling reduces endothelial to mesenchymal transition (EndMT) 31, 32 (Table 1) , a state in which endothelial cells lose the expression of important endothelial genes and acquire mesenchymal cell like characteristics, including increased migration and proliferation 32 . Mechanistically, FGF signalling controls the cellular levels of the let7 microRNA family 30 , which targets a number of TGFβ family members. A reduction in let7 expression results in substantial increases in the level and activity of TGFβ ligands and TGFβRs in vitro and in vivo 30 . Also, addi tion of FGF2 to human umbilical vein endothelial cells in vitro increases the expression of miR20a, which targets the TGFβRs 33 , another mechanism involved in FGF-TGFβ crosstalk. In addition, FGF signalling is cru cial for the expression of the endothelial receptor vas cular endothelial growth factor receptor 2 (VEGFR2), the primary signalling receptor of vascular endothelial growth factor A (VEGFA) 25 . Endothelial cells are located at the interface with the circulating blood and, therefore, are subject to shear stress resulting from the flow of blood in the vasculature. Shear stress can be linear, pulsatile or disturbed depend ing on location (for example, near arterial bifurcations and along the aortic arch inner curvature) and vary in strength. Endothelial cell sensing of shear stress is a com plex process that is reviewed elsewhere 34 . The proteogly can syndecan 4 is one element of the shear stress sensing complex. Indeed, loss of endothelial syndecan 4 in mice results in impaired alignment of endothelial cells under flow 35 . Given that syndecan 4 is an important modula tor of FGF2 signalling [36] [37] [38] , this study suggests that flow modulates FGF signalling. Furthermore, shear stress can directly affect the expression of endothelial FGF recep tors. In particular, decreased FGFR1 expression has been observed in endothelial cells in areas of disturbed shear stress 39 . In vitro studies have confirmed the relationship between the level of endothelial FGFR1 expression and the magnitude and type of shear stress 39 . Loss of FGF signalling in endothelial cells in areas of high shear stress has been linked to increased athero sclerotic plaque growth 39 (Table 2 ). Atherosclerosis is a progressive disease characterized by gradual intracel lular lipid deposition in the vasculature leading to the formation of atherosclerotic plaques 40, 41 . At these sites, endothelial cells acquire a pro inflammatory pheno type, which predisposes to atherosclerotic plaque development 41 Paracrine stimulation confers a tissue-specific microenvironment to the endothelium. For example, vascular endothelial growth factor (VEGF) signalling is crucial to the formation and maintenance of fenestrated endothelia, whereas paracrine WNT signalling induces formation and maintenance of continuous endothelial lining, which is crucial for vascular integrity in general and that of the bloodbrain barrier in particular. Autocrine transforming growth factorβ (TGFβ) signalling is a signature of endothelial cell dysfunction. ANG1, angiopoietin 1; ACVR2A, activin A receptor type 2A; ALK, activin receptor-like kinase; BMP9, bone morphogenetic protein 9; BMPR2, bone morphogenetic protein receptor type 2; FGF, fibroblast growth factor; FGFR, fibroblast growth factor receptor; LRP, lipoprotein receptor-related protein; TGFβR, transforming growth factorβ receptor; TIE2, angiopoietin 1 receptor; VEGFR, vascular endothelial growth factor receptor. EndMT 39 . TGFβ signalling further promotes an inflam matory phenotype in endothelial cells, thereby estab lishing a feed forward loop between inflammation and TGFβ pathway activation 39, 42 . In patients with coronary artery disease, a strong correlation exists between dis ease progression and loss of FGFR1 expression and activation of TGFβ signalling in endothelial cells of the left main coronary artery, with up to 70% of endothe lial cells overlying atherosclerotic plaques expressing mesenchymal markers 39 (Table 2 ). Syndecan 4, a proteo glycan that increases FGF2 signalling, protects against atherosclerotic plaque formation in mice 35 , highlighting the protective role of endothelial FGF signalling against atherosclerosis. VEGF signalling VEGF is the most studied angiogenic growth factor. All members of the VEGF family have important functions in the endothelium. VEGFA and VEGFC are key drivers of angiogenesis and lymphangiogenesis, respectively 43 . VEGFB is involved in the regulation of endothelial cell metabolism 44 , whereas the function of VEGFD is less clear 45 . A closely related growth factor, placental growth factor, is crucial for placental angiogenesis 46 . All the Bmpr2 endothelial knockout Not inducible Spontaneous pulmonary hypertension in 40% of adult animals 168 All studies were in mice except where indicated. ALK1, activin receptor-like kinase 1; BMP, bone morphogenetic protein; ERK, extracellular signal-regulated kinase; FGF, fibroblast growth factor; FGFR, fibroblast growth factor receptor; PV1, plasmalemma vesicle protein 1; SHH, sonic hedgehog; TNF, tumour necrosis factor; VEGF, vascular endothelial growth factor; VEGFR2, vascular endothelial growth factor receptor 2. www.nature.com/nrcardio VEGF ligands function via three related high affinity tyrosine kinase receptors (VEGFR1-VEGFR3) and a host of auxiliary signalling molecules including neuro pilins and syndecans 43 . VEGFR2 is the principal sig nalling VEGFR in blood endothelial cells, whereas both VEGFR2 and VEGFR3 are involved in lymphatic endothelial cell VEGF signalling 45 . Similar to FGF ligand binding to FGFRs, VEGF ligand binding to VEGFR2 activates several intracellular pathways, including PI3K-AKT and MAPK (including ERK1, ERK2 and p38 MAPK) pathways, among a number of others 43 . Of these VEGF induced pathways, VEGF mediated activation of ERK1 and ERK2 is thought to be crucial during embry onic vascular development and in angiogenic settings in adult tissues 43, 47 . The role of VEGF-VEGFR2 signalling in angiogen esis is well established. However, the role of this sig nalling cascade in the quiescent endothelium remains unclear. Interestingly, VEGF ligands are expressed by a number of specialized cell types, such as podocytes, choroid plexus epithelium and hepatocytes in adult mice 48 . Studies using mice expressing the VEGF-lacZ reporter construct showed that VEGFA is expressed in cells overlying fenestrated and sinusoidal blood vessels, such as podocytes in the kidney and hepatocytes in the liver, as well as in tissues with secretory functions 48 . Furthermore, VEGFR2 in the adjacent endothelial cells was phosphorylated, demonstrating paracrine activ ity of the non endothelial VEGF in these specialized environments. Local effects. The importance of paracrine VEGFA signalling in quiescent endothelium is well docu mented in glomerulus endothelium 49 ( Fig. 1 ; Table 1 ). Renal glomeruli are composed of fenestrated capillary endothelial cells and highly specialized epithelial cells (podocytes) separated by a glomerular basement mem brane. Glomerular podocytes continuously express high levels of VEGFA. Constitutive heterozygous deletion of Vegfa in podocytes in mice leads to endotheliosis (swell ing of endothelial cells with a partial loss of fenestra tions) by 2.5 weeks of age 49 . By 6.5 weeks of age, mice Nature reviews | Cardiology with heterozygous deletion of Vegfa have an expanded glomerular basement membrane and a near complete loss of endothelial fenestration. The outcome is termi nal renal failure with nephrotic syndrome at 9 weeks of age (Table 1) . Interestingly, constitutive overexpression of Vegfa in podocytes in mice leads to a different kid ney disease 49 . In this case, by 5 days of age, mice have a collapsing glomerulopathy characterized by kidney haemorrhages, proteinuria and complete collapse of the capillary loops, with no or few multinucleated endothelial cells (Table 1 ). Induction of Vegfa overex pression in podocytes of adult mice induces a different phenotype from that of mice with non inducible Vegfa overexpression 49, 50 . Inducible Vegfa overexpression in podocytes in adult mice also leads to kidney failure, pro teinuria, glomerular basement membrane thickening, slit diaphragm loss and podocyte effacement, although endothelial cells are mostly unaffected (no endotheliosis or loss of fenestration) 50 (Table 1 ). Together, these data show the crucial requirement for finely balanced VEGF signalling in the filtration barrier of the glomeruli at different developmental stages. Similar to the kidney, ablation of VEGF in endocrine glands leads to the loss of endothelial fenestrations 51 . Deletion of Vegfa 52 or overexpression of a soluble form of Vegfr1 (reF. 53 ) in pancreatic β cells in mice results in a loss of endothelial fenestrations in pancreatic islets (Table 1) . A similar loss of endothelial fenestrations is observed in thyroid capillaries following pan endothelial Vegfr2 deletion in mice 54 (Table 1 ). In addition to paracrine activation of VEGF signal ling, an autocrine VEGF loop is also crucial to the integ rity of the quiescent endothelium 55 (Table 1) . VEGFA expression in pulmonary, aortic and intestinal blood ves sels in adult mice is patchy 55 . Constitutive homozygous deletion of Vegfa in endothelial cells results in lethal ity at any age, including around 25% lethality in adult mice, with a peak in death at 20-25 weeks after birth, as a result of multiorgan haemorrhage (in the spleen, kidney, brain, intestines, heart and lungs), intestinal perforations and myocardial infarction 55 . These haemorrhages and thrombotic events are the consequence of endothe lial cell apoptosis 55 . Interestingly, endothelial specific knockout of Vegfa in mice does not induce a loss of endothelial fenestrations in renal glomeruli 55 , unlike in mice with loss of paracrine VEGFA signalling. These differences in phenotype imply differences in VEGF signalling circuits when activated in an autocrine ver sus paracrine manner. However, the identity of these circuits remains unknown. Finally, small amounts of circulating VEGF ligands are found in blood. Whether the presence of VEGFs in blood implies the existence of endocrine signalling and what that might entail has not been established. The VEGF pathway is also activated by shear stress. VEGFR2 is part of a complex with VE cadherin and platelet endothelial cell adhesion molecule 56 , which allows endothelial cells to sense shear stress. Increased shear stress leads to ligand independent activation of VEGFR2 by SRC, activation of AKT and then activation of endothelial nitric oxide synthase (eNOS; also known as NOS3) to induce vasodilatation 57 . Systemic hypertension. In addition to specific effects in various organs, VEGFs have a number of systemic effects that became evident with the widespread clinical use of VEGF neutralizing agents and tyrosine kinase inhibi tors that target VEGFR2. Use of anti VEGF therapies can often trigger the development of systemic hyper tension owing to decreased synthesis of the vasodila tor nitric oxide (NO) and increased expression of the vasoconstrictor endothelin 1 (ET1) [58] [59] [60] [61] [62] (Table 2 ). The decrease in blood NO levels is due to a reduction in the expression of eNOS, which catalyses the synthesis of NO from l arginine. However, specifically how decreased VEGF signalling leads to increased ET1 expression and whether a specific vascular bed is predominantly respon sible for NO or ET1 synthesis is unclear. Other adverse effects of anti VEGF therapies include protei nuria and membranous glomerulonephropathy, which can poten tially advance to renal failure 63 and are attributable to the requirement for VEGF in the maintenance of the renal vasculature and filtration units (Table 2 ). Finally, haemorrhages have been described in patients with can cer who were treated with sunitinib or sorafenib, two tyrosine kinase inhibitors targeting VEGFR2 signalling 64 ( Table 2 ). Lack of VEGF signalling is responsible for the hyper tension in women with pre eclampsia, a common mater nal complication of pregnancy associated with oedema, renal failure and systemic hypertension (Table 2) . Syncytiotrophoblasts, a placental cell type, secrete a sol uble form of VEGFR1 (sVEGFR1) and if this secretion is abnormally elevated, increased amounts of circulat ing sVEGFR1 sequestrate VEGFA, initiating the disease process [65] [66] [67] . Adenovirus mediated overexpression of sVEGFR1 in pregnant rats induced systemic hyperten sion, proteinuria and glomerular endotheliosis, similar to what is observed in patients with pre eclampsia 65 , and validating the requirement for sVEGFR1 in the patho genesis of pre eclampsia. Similar to the hypertension induced by the use of VEGF inhibitors, systemic hyper tension in patients with pre eclampsia is also driven by decreased NO production and increased ET1 levels in plasma 68, 69 . Interestingly, elevated plasma TGFβ2 levels in patients with systemic hypertension 70 and the changes in renal histology in patients with pre eclampsia 71 are similar to findings in mice with a systemic deletion of Erk1 and an inducible, endothelial specific deletion of Erk2 (Erk1 −/− Erk2 iEC−/− ) 10 (Table 1) . Both FGF and VEGF signalling cascades activate endothelial ERK1 and ERK2 signalling 43 . A study from 2019 elucidated the function of ERK1 and ERK2 in the quiescent endothelium 10 . Inducible, endothelial specific deletion of Erk2 in adult Erk1 null mice led to universal lethality within 4 weeks 10 . Interestingly, the phenotypes of these mice are a combination of the phenotypes found after inhibition of the FGF pathway and the VEGF path way (Table 1 ). Erk1 −/− Erk2 iEC−/− mice have increased TGFβ signalling as a result of a decrease in let7 microRNA expression, leading to EndMT. This TGFβ induced EndMT has previously been observed after endothelial FGF pathway inhibition 30 . These Erk1 −/− Erk2 iEC−/− mice also develop systemic hypertension due to a loss of eNOS expression and increased ET1 expression, lose www.nature.com/nrcardio fenestrations in endocrine gland and kidney endothe lium, and develop kidney failure with proteinuria and endotheliosis of the glomerulus endothelium 10 . These phenotypes are very similar to those found in mice with VEGF pathway inhibition. Taken together, these results demonstrate the crucial function of the ERK1-ERK2 pathway in the maintenance of quies cent vasculature integrity and highlight the differences between VEGF mediated and FGF mediated activation of ERK1-ERK2. Pulmonary vasculature. The function of VEGF in the pulmonary vasculature is less well defined. Pulmonary capillaries are continuous, which means they do not have fenestrations. Nevertheless, VEGF is important for the maintenance of the pulmonary vasculature. Inhibition of VEGF signalling in rats by treatment with the VEGFR blocker SU5416 leads to pruning of the pulmonary arterial vasculature, which, in turn, induces alveolar cell apoptosis and emphysema at high doses 72 (Table 1) . Interestingly, Notch signalling can attenuate the angio genic sprouting effect of VEGF signalling 73, 74 . Consistent with this notion, mice with a heterozygous deletion of Dll4 (which encodes the Delta like protein 4, a ligand of the Notch pathway), which is highly expressed in arte rial endothelial cells and is a target gene of VEGF signal ling, have pulmonary haemorrhages, suggesting a crucial role of VEGF signalling and the downstream Notch pathway in pulmonary endothelial cells 75 (Table 1) . Furthermore, some patients treated with neutralizing antibodies against DLL4 (demcizumab or enoticumab) or a bispecific antibody against DLL4 and VEGF (nav icixizumab) develop pulmonary hypertension [76] [77] [78] [79] ( Table 2 ). The cancer therapy drug dasatinib inhibits ephrin receptor signalling, which is directly connected to the VEGF signalling pathway by modulating VEGFR2 endocytosis 80 and activation of downstream pathways 81 . The interaction of these signalling pathways and the fact that multiple intracellular and cell surface kinases are simultaneously inhibited by dasatinib might explain why this therapy is associated with the development of pulmonary hypertension in some patients 82, 83 and aggravates pulmonary hypertension in rats 84 (Table 2) . VEGF signalling also has an important role in vascu lar protection in patients with acute respiratory distress syndrome (ARDS) 85, 86 (Table 2 ). ARDS is characterized by diffuse alveolar damage leading to impaired gas exchange and is common in several pulmonary diseases, including viral pneumonitis (such as those caused by H1N1 influenza virus infection), severe acute respiratory syndrome and coronavirus disease 2019 (COVID19), in which ARDS is associated with intense bronchial and lung parenchyma inflammation 87, 88 . The early, exuda tive phase of ARDS is characterized by diffuse alveolar damage, disruption and loss of epithelial and endothelial cell-cell junctions, and alveolar oedema 89 . The exudative phase is followed by a proliferative phase that involves the formation of hyaline membranes on the epithelial side of the basement membrane and extensive cellular infiltrates in the alveolar spaces 90 . VEGFs are present in the normal alveolar space and probably serve to main tain alveolar function 85 . The loss of this protection, such as occurs with increased levels of VEGFA trap sVEGFR1 in the lungs, is predictive of the development of ARDS and of an increased risk of adverse outcomes among patients with ARDS 86 . At the same time, these protec tive effects of VEGF signalling are counterbalanced by the capacity of the same VEGFA to induce oedema and promote inflammation 85, [90] [91] [92] . Indeed, studies of patients with ARDS associated with viral vasculitis, such as those caused by hantavirus infection, have demonstrated a strong association between VEGF levels in the lungs and pulmonary oedema [93] [94] [95] . This association renders VEGF an unappealing agent for ARDS therapy and emphasizes the need for a drug that selectively activates VEGF protective pathways while inhibiting its pro inflammatory signalling. However, the absence of a drug that differentiates between the ben eficial (cell survival) and detrimental (oedema) effects of VEGF signalling prevents the development of a promising new therapeutic modality. Whereas the VEGF-ERK signalling axis is central to the maintenance of endothelial fenestrae of the kidney and endocrine glands, WNT signalling has an equally important role in the maintenance of tight junctions and the continuous endothelium of the central nervous system 96 . The WNT family is composed of ten WNT receptors (Frizzled 1-10), four WNT co receptors (LDL receptor related protein 5 (LRP5), LRP6, RTK like orphan receptor 2 and receptor like tyrosine kinase), and 16 WNT ligands (WNT1-WNT16). Canonical WNT signalling involves the binding of WNT ligands to Frizzled receptors and the co receptors LRP5 and LRP6. Phosphorylation of LRP5 and LRP6 recruits the β catenin destruction complex (Axin, casein kinase 1, glycogen synthase kinase 3 and adenomatous polyposis coli protein) from the cytoplasm to the plasma mem brane, where the complex cannot degrade β catenin. This stabilized form of β catenin accumulates and translocates to the nucleus to activate the transcription of WNT target genes. Glia and neurons produce WNT7a and WNT7b ligands (Fig. 1) , and binding of these WNT ligands to the receptor Frizzled 4 activates canonical WNT signalling in endothelial cells of the central ner vous system [97] [98] [99] . Upregulation of β catenin in endothe lial cells leads to increased expression of genes encoding tight junction components (claudin 1, claudin 3 and claudin 5) and the glucose transporter 1 (GLUT1) 97 . Simultaneously, expression of Plvap, the gene encoding plasmalemma vesicle protein 1 (PV1), which is the prin cipal component of endothelial fenestrae, is repressed in endothelial cells 97, 100 (Fig. 2) . This combined effect of WNT signalling is crucial to the integrity of the bloodbrain barrier (BBB). Indeed, endothelial cell specific deletion of Ctnnb1, the gene encoding β catenin, in adult mice leads to severe seizures, brain haemorrhages and death 96 (Table 1 ). Endothelial deletion of Fzd4, which encodes the receptor Frizzled 4, in adult mice leads to increased PV1 levels and decreased claudin 5 levels in retinal, cerebellar, spinal cord and olfactory bulb endothelial cells 98 (Fig. 2; Table 1 ). Therefore, the WNT pathway is a perfect example of how the cellular Nature reviews | Cardiology microenvironment can induce the final differentiation step of endothelial cells, leading to a highly specialized organotypic endothelium. Similar to the paracrine WNT signalling path way, a paracrine sonic hedgehog (SHH) signalling pathway is also activated in the BBB 101 . Astrocytes in the BBB express the ligand SHH, which binds to and inactivates the receptor protein patched homologue 1 (PTCH1), which is expressed in brain endothelial cells 101 (Table 1) . Inactivation of PTCH1, in turn, results in the inactivation of the G protein coupled recep tor Smoothened (SMO), which leads to the activation of the gliomaassociated oncogene (GLI1). Genetic endothelialspecific deletion of Smo specifically led to an increase in BBB permeability in adult mice, mani fested by plasma protein leakage and decreased expres sion of junctional proteins (occludin, claudin 3, claudin 5 and tight junction protein ZO1) 101 . This increased BBB permeability induces a pro inflammatory pheno type in BBB endothelial cells with upregulation of intercellular adhesion molecule 1 (ICAM1) and recruit ment of circulating inflammatory cells 101 . Furthermore, SHH signalling also induces the expression of netrin 1 in the BBB endothelial cells, a laminin related protein that is critical for cell-cell junction and cell-substrate adhesion 102 . Dysregulation of the WNT-β catenin pathway has been implicated in various central nervous system dis orders that involve BBB breakdown, including multi ple sclerosis 103 , Alzheimer disease 104 and Huntington disease 105 . The blood-retina barrier shares high similar ity with the BBB. Mutations in NDP, a gene encoding the WNT ligand Norrin and expressed in the blood-retina barrier, are linked to Norrie disease, a condition in which the blood-retina barrier integrity is compromised, lead ing to blindness 106,107 ( Table 2 ). Dysregulation of the SHH pathway is found in HIV associated neurocognitive disorders with disruption of BBB integrity 108 . Angiopoietin signalling Angiopoietins (ANGs) are a family of secreted fac tors comprising ANG1, ANG2 and ANG3 (ANG4 in humans). Unlike the FGF and VEGF signalling path ways, which are involved in both pro angiogenic pro cesses and maintenance of endothelial cell quiescence signalling, ANG1 signalling is purely a quiescence sig nalling pathway in endothelial cells. ANG1 is expressed in mural cells and binds to the angiopoietin 1 receptor (Fig. 2 ; Table 1 ). The loss of TIE2 signalling reactivates the endothelium by weak ening endothelial cell-cell junctions 114, 115 , inducing the expression of pro inflammatory adhesion molecules, including ICAM1 and vascular cell adhesion molecule 1 (VCAM1) 112 , and increasing the levels of procoagulant proteins on the luminal surface of endothelial cells 116, 117 ( Table 1) . Intriguingly, ANG2-TIE2 interactions are context dependent -ANG2 acts as a TIE2 agonist under pathogen free conditions but as an antagonist under inflammatory conditions (such as infection and tumour necrosis factor (TNF) or lipopolysaccharide stimulation) 118 . TIE1 is an orphan receptor unable to bind any angiopoietin or other known ligands. TIE1 and TIE2 interact in the absence of a ligand 119 . Endothelial TIE1 is required for the agonist effects of paracrine ANG1 and autocrine ANG2 on TIE2 activation. During inflammation, the ectodomain of TIE1 in endothelial cells is cleaved, resulting in the loss of ANG2 agonist activity, thereby promoting vascular remodelling and leakiness 120 . TIE1 cleavage reduces, but does not abolish, ANG1 agonist activity. Despite AKT signalling functioning in different aspects of cellular regulation in multiple cell types, in the quiescent endothelium, AKT is considered to function as a survival pathway 121 . In vitro transduc tion of endothelial cells with a dominant negative Akt variant decreases endothelial viability by opposing the pro survival effects of VEGF 122 . However, a 2016 study in mice showed that the main function of the endothelial AKT pathway is not endothelial cell survival but mainte nance of adequate interactions between endothelial cells, pericytes and vascular smooth muscle cells 123 (Table 1) . Indeed, endothelial cell specific deletion of Akt1 in mice with global Akt2 deletion alters Jagged 1-Notch signalling between endothelial and mural cells, leading to apoptosis of vascular smooth muscle cells and peri cytes and subsequent vessel regression, particularly in coronary arteries 123 . However, endothelial cell apoptosis was not detected in these Akt2 −/− mice with endothelial cell specific Akt1 deletion. The regulation of Notch sig nalling by AKT following ANG1 stimulation in endothe lial cells is mediated by the endothelial transcriptional regulator ERG 124 . Inducible, endothelial cell specific deletion of Erg in mice leads to a phenotype similar to that produced by the deletion of Akt1 and Akt2, with a loss of vascular smooth muscle cell coverage and vascular regression 125 . Clinical data have added a layer of complexity to our understanding of ANG-TIE signalling. Multiple cutaneous and mucosal venous malformations have been reported in patients carrying gain of function genetic variants in TEK (encoding TIE2) 126, 127 . This vascular malformation is characterized by the develop ment of soft, blue, compressive and localized lesions 128 . Histological features of these venous lesions include uneven endothelial cell lining, disorganized extracellu lar matrix structure, enlarged lumen and sparse mural cell coverage 128 . These lesions can be present at birth or present around puberty 128 . These gain of function TEK genetic variants result in autophosphorylation of TIE2 and excessive activation of the downstream AKT pathway in endothelial cells 129 . At the same time, secre tion by endothelial cells of platelet derived growth fac tor B, which is responsible for mural cell recruitment, is downregulated 129 . These observations suggest that, although TIE2 signalling is important for the switch from an activated to a quiescent endothelium, overacti vation of this pathway is deleterious. The mechanism responsible for ensuring the proper extent of TIE2 activation is currently unclear. Vascular endothelial protein tyrosine phosphatase (VEPTP), a vascular phos phatase, seems to be crucial in limiting TIE2 activation, because neutralization of VEPTP in vivo in mice results in vascular lesions similar to those seen in mice with gain of function TEK genetic variants 130, 131 . The TGFβ superfamily includes a large pool of ligands, such as TGFβ1-TGFβ3, bone morphogenetic proteins (BMPs), growth differentiation factors (GDFs), activins and inhibins and nodal. These ligands bind to a com plex of two dimers of a combination of type I receptors named activin receptor like kinase 1 (ALK1)-ALK7 and two dimers of type II receptors (BMP receptor type 2 (BMPR2), TGFβR, activin receptor type 2A and activin receptor type 2B), leading to the activation of a number of different canonical (SMAD dependent) and non canonical signalling cascades 132 . The major distinc tion between canonical BMP and TGF pathways is the phosphorylation of SMAD2-SMAD3 by TGFβ, activins, inhibins and nodal, whereas BMPs and certain GDFs phosphorylate SMAD1, SMAD5 and SMAD8. BMP9 and BMP10 are circulating BMPs produced by the liver and the heart, respectively 133 . Both ligands bind the high affinity receptor ALK1, which is specif ically expressed by endothelial cells 134 . ALK1 signal ling is crucial for developmental angiogenesis [135] [136] [137] [138] . In addition, ALK1 also has a crucial role in the quiescent endothelium. Endothelial deletion of Alk1 in adult mice (aged 2 months) is lethal within 9-21 days of deletion, although the exact cause of death is still unknown 139 (Table 1) . Major autopsy findings included cardiac enlargement and haemorrhages in the lungs and the gastrointestinal tract 139 . Deletion of Alk1 in endothelial cells in adult mice also led to the spontaneous forma tion of arteriovenous malformations (AVMs) -direct shunts between arteries and veins -in the gastroin testinal tract and uterus 139 . AVMs were also suspected to be present in the lungs, but were difficult to assess because of the multiple haemorrhages in the lungs 139 . Furthermore, wounding can also induce de novo AVM formation in the skin of adult mice with Alk1 deletion 139 . Endoglin, the co receptor for ALK1, is also important for vascular quiescence. Endothelial specific deletion of Eng (which encodes endoglin) in adult mice resulted in wound induced AVMs in the skin, but no visceral AVMs were found 140 . However, spontaneous pelvic AVMs were shown in another study of mice with endothelial specific Eng deletion 141 (Table 1) . Interestingly, the pelvic area where the AVMs form after Eng deletion has high VEGFA levels 141 . Anti VEGF treatment blocked the for mation and maturation of AVMs in Alk1 knockout mice and Eng knockout mice 141, 142 . Together, these data sup port the concept that ALK1 and endoglin are required for the maintenance of endothelial quiescence in adult life to counteract an over exuberant endothelial proliferative response to VEGF signalling. The molecular mechanism by which ALK1-endoglin signalling maintains the integrity of the quiescent endothelium is still unknown. BMP9 and BMP10 induce endothelial quiescence by inhibiting endothelial cell migration and proliferation in microvascular endothelial cells 134, 143 . Moreover, the BMP9-ALK1 signalling pathway inhibits the pro angiogenic VEGF-AKT1 pathway 144 . BMP9, which is produced by hepatic stellate cells, induces the fenestration of the sinusoidal endothelial cells 145 (Figs 1,2) . Deletion of Bmp9 in 129/Ola mice triggers hepatic fibrosis following sinusoidal capillarization (transformation of the fenestrated hepatic sinusoids into continuous capillaries, with synthesis of a basement membrane of collagen between the endothelial cells and the hepatocytes) 146 (Table 1) . The BMP9-ALK1 pathway is also modulated by shear stress. Endoglin increases BMP9 signal ling through ALK1 in endothelial cells during shear stress 147 . This increased BMP9 activity is accomplished by an increase in the association between endoglin and ALK1 before their binding to the ligand 147 . Loss of eng in zebrafish leads to defective blood flow induced cell shape changes, resulting in enlarged vessels 148 . The primary cilia, which can function as a sensor of blood flow induced mechanical forces on endothelial cells, can also regulate BMP signalling. In vitro, the loss of BMP9 signalling through the cilium was shown to increase endothelial cell migration 149 . Together, these studies show that shear stress increases BMP9 signalling in quiescent endothelial cells. Interestingly, stimulation of human pulmonary endothelial cells with BMP9 in vitro inhibits the TGFβ pathway by inducing the expression of inhibitory SMADs (SMAD6 and SMAD7) and by decreasing TGFβR2 expression 150 . These results identify a second pathway that inhibits TGFβ signalling in endothelial cells 10, 30 (Fig. 2) . Further evidence supporting the impor tance of TGFβ pathway inhibition in quiescent endothe lium is the increased expression of SMAD6 and SMAD7 observed in pulmonary endothelial cells of adult mice compared with pulmonary endothelial cells from infant mice, which are proliferative cells 151 . These studies firmly support the notion that BMP9-ALK1 signalling inhib its the TGFβ pathway in quiescent pulmonary endothe lial cells. The endothelial specific transcription factor ERG can also activate the BMP pathway by upregulat ing SMAD1 as well as inhibiting the TGFβ pathway by downregulating SMAD3 expression in the quiescent endothelium of the hepatic vasculature 152 . Disruption of normal BMP signalling in quiescent endothelial cells is the molecular basis for hereditary haemorrhagic telangiectasia (HHT), a rare genetic vas cular disease (Table 2 ). HHT is characterized by recur rent nosebleeds, mucous telangiectasia and formation of AVMs 153, 154 . In most patients, HHT is caused by loss of function genetic variants in ENG or ACVRL1 (encoding ALK1) 155, 156 , and these variants decrease BMP9 signalling 157, 158 . Interestingly, a second variant on the somatically non mutated allele of ACVRL1 or ENG can be found in some lesions in patients with HHT 159 . These results establish HHT as a disease of decreased BMP9 and BMP10 signalling. AVMs in patients with HHT are predominantly found in the liver and lungs and, to a lesser extent, in the brain 160 . Interestingly, AVMs in the liver are more frequent in patients with HHT type 2 (patients carrying an ACVRL1 variant) than in patients with HHT type 1 (carrying an ENG variant), whereas the opposite is true for AVMs in the lung 160 . Whether AVMs are congenital or acquired during adult life is unclear. Given that most AVMs are asymptomatic, sparse data exist on AVM frequency in children and younger adults. One study found that, in patients with HHT type 2, hepatic AVMs were present in 67% of patients aged <45 years and in 93% of patients aged >45 years 161 . A similar difference was found in patients with HHT type 1 (hepatic AVMs were present in 46% of patients aged <45 years and in 78% of patients aged >45 years) 161 . With regard to pulmonary AVMs, a Canadian study compared the frequency of pulmonary AVMs in children (aged <18 years) and their parents 162 . In patients with HHT type 1, the frequency of pulmonary AVMs was similar in both groups, whereas among patients with HHT type 2, 8.3% of children had pulmonary AVMs compared with 25.9% of the parents 162 . Moreover, the incidence of HHT symptoms increases with age, and symptomatic AVMs in the liver are found in adult patients (aged >30 years) 163, 164 . Taken together, these data suggest that at least some AVMs can develop in adulthood because of alterations in endothelial quiescence and that AVM size increases with age leading to symptomatic AVMs in older patients. The hypothesis of an increase in VEGFA signalling as a result of the loss of BMP signalling owing to ACVRL1 or ENG variants was validated by data from a clinical trial showing that inhibition of VEGF in patients with HHT decreases the symptoms of HHT 165 . The BMP pathway is also involved in the develop ment and progression of pulmonary arterial hyperten sion (PAH) ( Table 2 ). Heterozygous germline variants in BMPR2 underlie the main genetic susceptibility for PAH, found in 53-86% of patients with a family his tory of PAH and 14-35% of patients with idiopathic PAH 166 . Although the presence of a BMPR2 variation is neither necessary nor sufficient to cause PAH, a reduction in BMPR2 activity is currently viewed as the major molecular defect conferring a predisposition to develop PAH as well as an increased risk of progres sion of the disease 166, 167 . Constitutive deletion of Bmpr2 in endothelial cells in mice predisposed the animals to develop spontaneous PAH 168 , supporting the notion that disrupting BMP signalling in the endothelium is a risk factor for PAH. However, given the potential www.nature.com/nrcardio inhibitory role of BMPR2 in BMP signalling 169 , the exact effect of BMPR2 variants on BMP signalling in the pulmonary endothelium is unclear. The discov ery in some patients with PAH of variants in GDF2 (encoding BMP9), leading to decreased circulating BMP9 level, and variants in BMP10 revealed another layer of complexity of the regulation of the pulmonary endothelium and PAH pathogenesis [170] [171] [172] [173] . Therapy with BMP9 has been proposed as a strategy to compen sate for the loss of one BMPR2 allele in patients with PAH 150 . However, Bmp9 null mice do not develop spon taneous pulmonary hypertension, and these mice were even protected in experimental models of pulmonary hypertension 174 . Given these contradictory findings, further research is needed to clarify the role of BMP sig nalling in PAH. In particular, understanding how the reduction in the BMPR2 activity predisposes to PAH and how the BMP9-BMP10-BMPR2 axis contributes to the pathophysiology of PAH is essential. An early event that seems to be facilitated by dysfunction in the BMP9-BMP10-BMPR2 axis is the pro inflammatory phenotype of endothelial cells in PAH 175 . In PAH, dur ing the process of vascular remodelling, quiescent pul monary endothelial cells become activated and express high levels of adhesion molecules, such as VCAM1 and ICAM1, and secrete high levels of chemokines, such as IL6 and CC chemokine ligand 2 (CCL2; also known as MCP1) 176 . TGFβ ligands bind to TGFβR1 (also known as ALK5) and TGFβR2. Type III receptors (TGFβR3) increase ligand binding to their cognate receptors. Although endothelial ALK5 and TGFβR2 are crucial for cere bral vascular development 177 and endothelial TGFβR3 for coronary vessel development 178 in mouse embryos, Alk5 or Tgfbr2 deletion in endothelial cells in adult mice does not affect vascular morphogenesis 179 . Activation of the TGFβ pathway in endothelial cells in adult mice and humans is associated with a change in endothelial cell identity referred to as EndMT, a cell fate change event underlying a number of pathological processes 32 (Figs1,2). When endothelial cells undergo EndMT, they acquire mesenchymal characteristics including fibroblast like morphology, cell junction rearrange ment, increased mobility and proliferation, a throm bogenic and inflamma tory phenotype, and increased secretion of the extracellular matrix proteins fibronec tin and collagen 180 . To date, at least three pathways that inhibit TGFβ signalling in quiescent endothelia have been identified: the VEGF/FGF-ERK-let7 pathway and the BMP9-ALK1 pathway discussed above, and the cerebral cavernous malformation (CCM)-MEKK3 pathway 10, 30, 150, 181 . Postnatal deletion of any of the three known CCM genes in mice leads to overactivation of the MEKK3 pathway 182, 183 , which induces the expres sion of Klf4 (which encodes the transcription fac tor Krüppel like factor 4 (KLF4)) 181 and Klf2 (which encodes KLF2) 184 . The exact mechanism of how KLF2 and KLF4 induce CCM lesion formation is unclear. One study showed that KLF4 induces an autocrine loop that involves BMP6, which activates the TGFβ pathway leading to EndMT 181 . However, another study found that CCM gene deletion induces MEKK3 mediated overac tivation of KLF2 and KLF4, but that this overactivation does not induce EndMT 183 . Interestingly, an ensemble computational intelligence strategy, comprising deep learning and probabilistic programming of RNA seq data, causally linked the loss of ERK1 and ERK2 in human endothelial cells in vitro to the activation of an autocrine loop driven by TGFβ2 (reF. 10 ). Verified in mice, this autocrine loop resulted not only in the induction of EndMT (seen in vitro and in vivo in Erk1 −/− Erk2 iEC−/− mice), but also in systemic hypertension. The latter was induced by suppression of eNOS expression (and there fore NO production) and induction of vasoconstrictor ET1 expression. A decrease in endothelial fenestration was also observed (caused by decreased PV1 expression as seen in vitro and in vivo in Erk1 −/− Erk2 iEC−/− mice) 10 (Fig. 2) . Systemic hypertension and the loss of endothelial fenestrations are features of VEGF-ERK pathway inhi bition. This in silico analysis suggested that the pheno types seen after the loss of VEGF signalling are, at least partially, due to increased TGFβ signalling. Fibrosis is a devastating process characterized by myofibroblast cell proliferation and abnormal extra cellular matrix accumulation, leading to organ failure. Endothelial cell injury often precedes the development of fibrosis and is suspected to be an initiating event 185 ( Table 2 ). Endothelial cells produce profibrotic medi ators, such as TGFβ, plasminogen activator inhibitor 1 and connective tissue growth factor, which induce fibroblast growth and differentiation and collagen syn thesis by fibroblasts 186 . In addition, endothelial cells can also differentiate into fibroblast like cells and secrete collagen as a result of EndMT 32 . However, the exact contribution of EndMT as the source of myofibroblasts is controversial, and lineage tracing experiments in animal models of cardiac and renal fibrosis show that EndMT is not a major source of myofibroblasts [187] [188] [189] [190] [191] . Liver sinusoidal endothelial cells (LSECs) have a major role in liver fibrosis 192, 193 . After liver injury, LSECs rap idly switch from a fenestrated to a capillarized phenotype and acquire a pro vasoconstrictive, pro inflammatory, pro angiogenic and pro fibrotic phenotype, which induces hepatic stellate cell activation that leads to liver fibrosis. VEGF and BMP9 can both function to maintain the fenestrated quiescent state of LSECs 145, 194 . Inflammatory cells also contribute to the development of fibrosis. Activated endothelial cells provide important signals, such as the expression of adhesion molecules (for example, ICAM1 and VCAM1) and secretion of vari ous cytokines and chemokines (such as IL6, CCL2 and • What is the genetic and metabolic basis of endothelial heterogeneity? • What determines disease-prone versus disease-resistant endothelial cell subsets? • What are the organ-specific signals governing endothelial cell specialization and final differentiation? • What are the main organ-specific interactions between endothelial cells and non-endothelial cells? CXC chemokine ligand 12), to recruit leukocytes and perpetuate inflammation. This pro inflammatory pheno type of endothelial cells and the recruitment of inflam matory cells contribute to the pro fibrotic environment by inducing the secretion of collagen 195 . Activation of the TGFβ pathway in endothelial cells triggers an endothelial inflammatory phenotype 39 . In addition, TGFβ secreted by endothelial cells can induce resident fibroblasts to become myofibroblasts 196 . Finally, activation of the TGFβ pathway is also a major trigger of plaque formation in atherosclerosis, as a consequence of decreased FGF signalling (see the section on FGF signalling) 39, 42 . Endothelial quiescence has emerged as an important area of investigation in the field of vascular biology research. The vascular endothelium is central to the regulation of tissue and organ homeostasis and is cru cial for disease resilience. Understanding the signalling pathways that regulate the numerous functions of the quiescent endothelium in different organs is central to the understanding of normal physiology as well as the pathophysiology of numerous diseases, and addressing important knowledge gaps is the current challenge in the field of vascular biology (box 1). Although many functions of the quiescent endothe lium are organ specific (Fig. 1) , other functions are general to all quiescent endothelial cells. Thus, the TGFβ signal ling pathway is inhibited in the healthy quiescent endothe lium regardless of organ or location, and activation of TGFβ is linked to the loss of normal endothelial cell fate (via EndMT) and to the development and progres sion of disease states (Figs 1,2) . Indeed, the capacity of an endothelial bed to avoid the activation of TGFβ signalling is closely linked to its capacity to resist disease develop ment and might account for different disease susceptibil ities in different patient populations. It is interesting to speculate that an increased susceptibility of endothelial cells to TGFβ activation might be a risk factor for some of the most common vascular diseases, such as atheroscle rosis. The ability to understand and assess this endothelial cell susceptibility, both genetically and functionally, would allow better risk assessment and the development of therapies aimed at disease prevention. The importance of keeping TGFβ signalling in check is further illustrated by the variety of signalling cascades that inhibit TGFβ signalling in endothelial cells, including VEGF-ERK, FGF-ERK, BMP9-ALK1 and CCM-MEKK3 (Fig. 2) . The existence of these signalling cascades suggests that control of TGFβ signalling is crucial for maintaining cell homeostasis and that abnormalities in this pathway can trigger specific diseases. Endothelial cell senescence and ageing are crucially linked to endothelial cell quiescence, and endothelial normalcy is probably one of the most crucial factors contributing to a healthy lifespan. Examples of such a link include arterial stiffness and hypertension, two hallmarks of the ageing process. Although this subject is outside the scope of this Review, the mechanisms of ageing related endothelial cell senescence have been well described previously 197 . A thorough knowledge of the dynamic control of endothelial quiescence is required. To this end, we need to understand how a signalling pathway that is involved in angiogenic stimulation, such as VEGF signalling, can also maintain endothelial quiescence. This dichotomy could be a result of different VEGF dosages, differential VEGF signalling through different co receptors, such as neuropilin 1 (reFs 198, 199 ) and syndecan 2 (reF. 200 ), alter ations in the duration of VEGF stimulation, paracrine versus endocrine versus autocrine activation of VEGF signalling, or crosstalk with other signalling pathways. All these factors might differentially affect VEGF stimu lation and point towards the existence of regulators that modulate the effects of VEGF signalling to achieve the desired physiological objective 201, 202 . We also need to understand the molecular basis of the organotypic effects of endothelial cell signalling. For example, although CCM proteins are expressed in all endothelial cells, variants in CCM genes seem to affect only the central nervous system vasculature. Another related problem is that organ specific mutation of genes in a given signalling pathway does not have a consistent pheno type across organs. Genetic variants in ACVRL1 and ENG lead to the development of HHT (with AVM mainly in the lungs and liver), whereas variants in BMPR2 predispose to the development of pulmonary hypertension, with no effect on the vasculature of other organs. Advances in research into endothelial cell metab olism show a difference in the metabolic signature between quiescent and activated endothelial cells 203 . Of note, alterations in endothelial cell metabolism could be very important to regulate cell quiescence and warrant further investigation. Another important unknown is the link between endothelial cell quiescence and disease resilience. Emerging data from single cell RNA seq studies high light the heterogeneity of endothelial cells between tissues but also within each tissue 11 (Fig. 3a,b) . These single cell RNA seq data suggest that in many cases, disease progression is due to the expansion of a single population of normal cells (for example, endothelial or smooth muscle cells) that are susceptible to a particular disease stimulus 42, 204, 205 (Fig. 3c) . These findings also sug gest that other normal populations of these cell types are disease resistant. An in depth understanding of this phenomenon is crucially important. Further studies to characterize the genetic, molecular and metabolic signa tures of disease resistant versus disease prone cell popu lations are also needed (box 1; Fig. 3c ). To summarize, the understanding of the active regulation of the organotypic endothelial quiescence is currently one of the biggest challenges in vascular biology research. Published online xx xx xxxx Fig. 3 | Endothelial heterogeneity in health and disease. a | Quiescent endothelial cell (EC) heterogeneity in structure, function, immune regulation (interferon response and leukocyte adhesion molecule expression) and metabolism between tissues and within tissues. The information shown in this panel a is from reF. 11 . b | Heterogeneity in healthy capillary ECs between organs. c | Development of endothelial dysfunction. This is a stepwise process, progressing from activation of ECs to the development of endothelialto-mesenchymal transition to the full-blown pathological end state. This sequence of events leads to ECs losing their normal fate and acquiring features of mesenchymal cell types, including fibroblasts, smooth muscle cells and macrophages, in a process known as endothelial-to-mesenchymal transition. These events result in the initiation and propagation of inflammation, loss of normal endothelial structures and function, increased vascular permeability and formation of pathological lesions, such as atherosclerotic plaques. 
paper_id= fffa00fd44c423c523e8623ae2285a481bc5ec41	title= Cardiology in the Young COVID-19 crisis: will online learning have negative consequences to our students?	authors= George  Angelidis;Panagiotis  Georgoulias;Varvara  Valotassiou;Ioannis  Tsougos;	abstract= 	body_text= COVID-19 crisis: will online learning have negative consequences to our students? Panagiotis Georgoulias, George Angelidis , Varvara Valotassiou and Ioannis Tsougos Faculty of Medicine, University of Thessaly, Larissa, Greece We read with great interest the article by Ganigara et al concerning the evaluation of online fellowship education during the coronavirus disease 2019 (COVID-19) pandemic. 1 The authors concluded that the online lecture curriculum is associated with several advantages that may contribute to a better adjustment of graduate medical education under the negative consequences of the pandemic. However, we believe that there may be significant differences regarding medical education at the undergraduate level. Nowadays, it is widely accepted that the medical education rest upon the ancient Greek foundations. These foundations include, but are not limited to, the associations between science and the medical art, as well as the role of mentoring in medical education. 2 However, COVID-19 pandemic has affected the medical education in several ways, particularly at the undergraduate level. Medical schools worldwide have been forced to a rapid scaling-up of the online teaching procedures, and this shift is expected to continue at least until an effective COVID-19 vaccine will become widely available. 3 This situation could raise questions regarding the education we are currently providing to our students. A university is much more than tuition. The academic environment and the educational interactions inside the classes are valuable components of university life. During the COVID-19 crisis, the technology has offered solutions for the provision of medical education through the online platforms. Despite the additional benefits related to the online learning, such as flexibility, mobility, and equitability, we believe that the face-to-face teaching has unique characteristics that cannot be substituted, particularly with regard to the medical education. Medicine is not only science but also an "art." Face-to-face interactions between professors and students have educational significance, and medical schools are places where future physicians learn how to have an efficient relationship with their patients. Such education cannot be provided online, at least completely. A significant progress has been made in COVID-19 research, and effective vaccines may be available by the end of 2020 to early 2021. However, there are estimations that COVID-19 outbreak will not end until more than 50% of the human population is immune to the virus, and the pandemic may last until the end of 2021 (if no longer). Consequently, a shift to blended learning (with a significant portion of conventional face-to-face instruction) could be indicated, particularly in periods with better epidemiological picture. Financial support. No specific grant from any funding agency, commercial, or not-for-profit sectors. Conflicts of interest. None. 
paper_id= fffa2f145172929f5c0e3f0b0258775ce69b4ba6	title= Journal Pre-proof Possible silent hypoxemia in a COVID-19 patient: A case report Case report Possible silent hypoxemia in a COVID-19 patient: a case report	authors= Munawar  Siswanto;Aditya Rifqi Gani;Ririn Enggy Fauzi;Maria Patricia Yuliyanti;Bagus  Inggriani;Denny  Nugroho;  Agustiningsih;  Gunadi;  Siswanto;Munawar  Gani;Aditya Rifqi Fauzi;Ririn Enggy Yuliyanti;Maria Patricia Inggriani;Bagus  Nugroho;Denny  Agustiningsih;Universitas  Gadjah;Mada / ;	abstract= Introduction: It has been hypothesized that silent hypoxemia is the cause of rapid progressive respiratory failure with severe hypoxia that occurs in some COVID-19 patients without warning. A 60-year-old male presented cough without any breathing difficulty. Vital signs showed blood pressure 130/75 mmHg, pulse 84x/minute, respiratory rate (RR) 21x/minute, body temperature 36.5 C, and oxygen saturation (SpO2) 75% on room air. RT-PCR for COVID-19 were positive. On third day, he complained of worsening of breath shortness, but his RR was still normal (22x/minute) with SpO2 of 98% on 3 liters/minute oxygen via nasal cannula. On fifth day, he experienced severe shortness of breath with RR 38x/minute. He was then intubated using a synchronized intermittent mandatory ventilation. Blood gas analysis showed pH 7.54, PaO2 58.9mmHg, PaCO2 31.1mmHg, HCO3 26.9mEq/L, SaO2 94.7%, FiO2 30%, and P/F ratio 196mmHg. On eighth day, his condition deteriorated with blood pressure 80/40mmHg with norepinephrine support, pulse 109x/minute, and SpO2 72% with ventilator. He experienced cardiac arrest and underwent basic life support, then resumed strained breathing with return of spontaneous circulation. Blood gas analysis showed pH 7.07, PaO2 58.1mmHg, PaCO2 108.9mmHg, HCO3 32.1mEq/L, SaO2 78.7%, FiO2 90%, and P/F ratio 65mmHg. Three hours later, he suffered cardiac arrest again and eventually died. Discussion: Possible mechanisms of silent hypoxemia are V/Q mismatch, intrapulmonary shunting, and intravascular microthrombi. J o u r n a l P r e -p r o o f Conclusions: Silent hypoxemia might be considered as an early sign of deterioration of COVID-19 patients, thus, physician may be able to intervene early and decrease its morbidity and mortality. Keywords: ARDS; COVID-19; early sign of deterioration; respiratory failure with severe hypoxia; silent hypoxemia J o u r n a l P r e -p r o o f Highlights Some patients with COVID-19 might experience rapid deterioration without warning. Silent hypoxemia might be considered as an early clinical sign of deterioration of patients with COVID-19. Several hypotheses of silent hypoxemia have been proposed, including V/Q mismatch, intrapulmonary shunting and endothelial injury. J o u r n a l P r e -p r o o f 	body_text= The SARS-CoV-2 virus that causes Coronavirus Disease 2019 (COVID- 19) was declared a pandemic since the World Health Organization (WHO) decree on March 11, 2020 , and has infected more than 54 million people and caused more than 1.3 million deaths as of November 14, 2020 [1,2] . The early sign of severe disease of SARS-CoV-2 infection is pneumonia with respiratory failure, similar to Acute Respiratory Distress Syndrome (ARDS). Although hypoxic acute respiratory failure causes an increase in respiratory rate (RR), in some patients, a persistent normal RR was found and inconsistent with the severity of hypoxia. Some patients with reported experiencing rapid deterioration without warning. This reaction might be caused by 'silent hypoxemia'. Research has shown that failure of pulmonary oxygen diffusion causes a gradual decrease in oxygen saturation [3, 4] . Here, we reported one COVID-19 case with the possibility of silent hypoxemia. A 60-year-old Javanese male came to outpatient clinic in our hospital with complaints of cough that was felt for two weeks before admission without any breathing difficulty. Complaints were accompanied by fever, runny nose and sore throat. He had a comorbid condition of uncontrolled diabetes mellitus (DM). He was not a smoker and had no history of chronic pulmonary disease. (Fig. 2) . Three hours later, he suffered cardiac arrest again, but was unable to be resuscitated. The patient eventually died. Here, we discuss a case with COVID-19 with the possibility of silent hypoxemia. Hypoxemia itself is defined as a potential life-threatening condition characterized by a decrease in arterial PO2 below the normal value. Hypoxemia documentation can be done by checking pulse oximetry and arterial blood gas analysis. Hypoxemia occurs when PaO2 is less than 80 mmHg, and severe hypoxemia is when it is less than 60 mmHg. There are four main factors that can impair pulmonary gas exchange and cause hypoxemia when breathing room water is at sea level: hypoventilation, diffusion limitation, shunt, and ventilation-perfusion mismatch [7] [8] [9] . This unusual 'silent hypoxemia' phenomenon showed it is possible that the virus has an idiosyncratic effect on the respiratory control system. Angiotensin-converting-enzyme 2 (ACE2) receptors are highly expressed in the carotid bodies, which are also the same sites where chemoreceptors sense oxygen. ACE2 receptors are also widely expressed in the nasal mucosa. Symptoms of anosmia-hyposmia are experienced by a third of patients with COVID-19, and the olfactory bulb can be the entrance of the virus to the brain and may also play a role in depressed dyspnea response [10, 11] . Moreover, ACE2 counteracts the physiological functions of ACE and results in the activation of the renin-angiotensin-aldosterone system (RAAS) related to blood pressure regulation through the conversion of Angiotensin I to Angiotensin II and the electrolyte homeostasis. During the hypoxia condition, Angiotensin II induces vasoconstriction to improve the ventilation-perfusion mismatch, however, at the same time, it also stimulates the pro-fibrotic effect, and both effects are aggravated by the concomitant upregulation of ACE2 [10] [11] [12] . Gattinoni et al. [13] suggested two primary phenotypes of hypoxemia in patients with COVID-19: type L and type H. Type L is caused by loss of respiratory regulation and loss of hypoxic vasoconstriction. The condition of hypoxemia is due to an increase in minute ventilation, mainly by increasing the tidal volume (up to 15-20 ml/kg). Meanwhile, type H is a transition from type L, which is associated with a more negative intrathoracic inspiratory pressure. Diffuse pulmonary microvascular thrombosis is also believed to be the cause of hypoxemia in patients with COVID-19 [14] . Pneumonia analysis of COVID-19 shows that the air sacs in the lungs of patients are not filled with fluid or pus as in pneumonia infections in general but instead the virus causes the water sacs to collapse, thereby reducing the J o u r n a l P r e -p r o o f oxygen level and causing hypoxia in the patient, but the reaction still enhances the normal lung ability to expel carbon dioxide. Since carbon dioxide removal is still effective, patients do not feel shortness of breath [15] . Another mechanism proposed is intrapulmonary shunting. Infection causes interstitial edema, loss of surfactant and superimposed pressure, which induces alveolar collapse and substantial fraction of cardiac output perfusing non-aerated lung tissue. Over time, increased edema will increase lung weight, alveolar collapse, and dependent atelectasis, leading to increased shunting [16] . is thought to also play a role in the mechanism of silent hypoxemia. SARS-CoV-2 can directly infect lung capillary endothelial cells expressing ACE2. Endothelial injury and acute inflammation provoke the formation of intravascular microthrombi. Lung autopsies in patients after severe disease have shown diffuse alveolar damage, thickening of the vascular walls, and formation of microthrombi that clog capillaries. Hypercoagulable conditions accelerate the worsening of V/Q mismatch and lung tissue damage [16] . Risk factors for silent hypoxemia are old age and having diabetes [9] and these factors are known to blunt the body's regulatory response to hypoxia [17] , as in our case: a 60-year-old-male with comorbidity of diabetes mellitus. Therefore, early detection of silent hypoxemia such as by using prehospital pulse oximetry [5] , or radiology imaging [18, 19] might provide some red flag signs of impending danger of eminent cardiac arrest or sudden respiratory failure. Silent hypoxemia might be considered as an early clinical sign of deterioration of patients with COVID-19, thus, the physician may be able to intervene early and decrease its morbidity and mortality. SARS-CoV-2: Severe Acute Respiratory Syndrome Coronavirus 2; COVID-19: Coronavirus Disease 2019, RR: Respiratory rate; ACE2: Angiotensinconverting-enzyme 2; RAAS: renin-angiotensin-aldosterone system. Written informed consent was obtained from the patient for publication of this case report and accompanying images. A copy of the written consent is available for review by the Editor-in-Chief of this journal on request. Not commissioned, externally peer reviewed.  
paper_id= fffa5c82e390af4baca6d3eb6a0a35b8b2e648a5	title= 	authors= Sarah Ceridwen Totton;Jan Merrill Sargeant;Annette  Maree O&apos;connor;	abstract= A cat owner returns from an international vacation. On returning to her country, she self-quarantines in her apartment (where she lives alone and which no one has entered in her absence) for 14 days, after which, she tests negative for SARS-CoV-2. At the end of the quarantine period, the woman does a curbside pick-up of her cat from a local cattery where the cat has been staying in her absence. The cat had become infected with SARS-CoV-2 while at the cattery. Sometime after the cat returns home, the woman (who has continued to isolate from other people and animals, apart from her cat) develops symptoms of COVID-19. She tests positive, as does her cat. Testing of other cats at the cattery reveals two cats, kept in the same room as the woman's cat, also test positive for the virus. Virus from the cats at the cattery, the woman's cat and the woman are identical genetic matches. Three other cats kept in the same room at the cattery show antibodies to SARS-CoV-2, indicating exposure. (Indirect cat-to-cat transmission between cats kept in the same room has been demonstrated in an experimental setting, Shi et al., 2020.) Speculation about the role of animals in the transmission of disease is natural during this SARS-CoV-2 pandemic. However, as veterinary epidemiologists, we wanted to point out that it seems extremely 2 | LETTER TO THE EDITOR unlikely, due to ethical and study design considerations, that direct evidence of cat-to-human transmission will ever be obtained. 	body_text= To the editor: Although human-to-human transmission is maintaining SARS-CoV-2 (the virus that causes in the world's human population (World Health Organization, 2020) , the role of animals in spreading the disease is unclear (Office International des Epizooties, 2020). Of the commonly owned companion animals, cats appear to be particularly susceptible to the virus, and cat-to-cat transmission of SARS-CoV-2 has been demonstrated under experimental conditions (Shi et al., 2020) . Additionally, in Germany a cat was reported to have contracted the virus from its infected owner (Schulz et al., 2020) . As veterinarians, we are often asked: Can cats transmit the virus to people? Although human-to-cat transmission has been reported, there is, as yet, no evidence that transmission in the opposite direction (i.e. cat-to-human transmission) has occurred. But since human-tohuman is the main route of transmission (i.e. the most likely source of a human infection is another infected human), our question is, what would constitute definitive evidence that cat-to-human transmission has occurred? Ethical considerations preclude conducting an experiment exposing an uninfected person to an infected cat; therefore, we must rely on evidence from real-world situations. The essential criteria required to demonstrate cat-to-human transmission are as follows: 1. An effective quarantine period, followed by negative PCR and serologic testing that eliminates the potential for undetected infection in the person, 2. The person must remain isolated from all other sources of SARS-CoV-2 from the start of the effective quarantine period, through exposure to the infected cat, development of symptoms and diagnosis. Using the above criteria, we propose two hypothetical scenarios in which cat-to-human transmission could be conclusively demonstrated in the natural setting. These scenarios do not represent an exhaustive list of all possible scenarios for which cat-to-human transmission can be demonstrated. In these scenarios, transmission of the virus from the cat(s) to the people occurs via petting, giving kisses or licks, or sharing food; these transmission routes have been identified as potential routes of SARS-CoV-2 transmission from humans to animals (CDC, 2020). All cats are sampled via nasal, deep oropharyngeal and rectal swabs. Blood is sampled for serological analysis. The humans are sampled via nasal or throat swabs. The SARS-CoV-2 diagnostic test used is RT-PCR. Viral isolation is used to demonstrate that live virus is being shed. The virus in the infected cats and the virus in the infected humans are confirmed to be an identical match via genomic sequencing of the viral RNA, allowing a chain of transmission from human to cat to human to be demonstrated. A person, who has no contact with any other people or animals (except for an indoor-outdoor cat that they own), completes a 14-day quarantine, subsequently tests negative for SARS-CoV-2 by both PCR and serology and continues to quarantine themselves. Their cat then goes outside, where it contacts an infected person living in the neighbourhood, perhaps a person who sometimes provides the cat with its 'second breakfast'. The cat could contract the virus through close contact (e.g. cuddling) with this other person. The cat would then bring the infection back and infect its owner. In this case, genetic sequencing of the virus isolated from the owner would indicate that the virus originated from the second-breakfast supplier in the neighbourhood. 
paper_id= fffa648d0ab0631b869e704d4139ab3fee653612	title= Medicine's metaphysical morass: how confusion about dualism threatens public health	authors= Diane  O&apos;leary;B  Diane;O &apos; Leary;	abstract= What position on dualism does medicine require? Our understanding of that question has been dictated by holism, as defined by the biopsychosocial model, since the late twentieth century. Unfortunately, holism was characterized at the start with confused definitions of 'dualism' and 'reductionism', and that problem has led to a deep, unrecognized conceptual split in the medical professions. Some insist that holism is a nonreductionist approach that aligns with some form of dualism, while others insist it's a reductionist view that sets out to eradicate dualism. It's important to consider each version. Nonreductive holism is philosophically consistent and clinically unproblematic. Reductive holism, however, is conceptually incoherent-yet it is the basis for the common idea that the boundary between medical and mental health disorders must be vague. When we trace that idea through to its implementation in medical practice, we find evidence that it compromises the safety of patient care in the large portion of cases where clinicians grapple with diagnosis at the boundary between psychiatry and medicine. Having established that medicine must embrace some form of nonreductionism, I argue that Chalmers' naturalistic dualism is a stronger prima facie candidate than the nonreductive alternatives. Regardless of which form of nonreductionism we prefer, some philosophical corrections are needed to give medicine a safe and coherent foundation. 	body_text= Something appears to be seriously amiss in the practice of medicine! (Huth 1981, p. 247)  In the 1970s and 1980s, physicians, psychiatrists, ethicists, and philosophers came together in recognition of a problem. For centuries medical science had proceeded as a mechanistic sort of endeavor. Doctors' focal concern through this model had been the body and its diseases, with a sense that disease can best be remedied through a piecemeal approach to the body's parts. Through this "biomedical model" (BMM), medicine aimed for the kind of objectivity that generally characterizes the sciences, but in the late twentieth century that approach faced a reckoning. In spite of the undeniable success of the BMM at improving and protecting health, both professional and popular culture came to see that something was missing. Medicine's exclusive focus on the body had encouraged doctors to treat patients as things. The problem, it became clear, was that medicine should not be focused on the body. It should be focused on the person. Widespread recognition of this problem created a "quality-of-care crisis" (Marcum 2008a, p. v) . On the moral side the crisis was addressed with the movement of "humanism", championed by Pellegrino and Thomasma (1981) . By failing to recognize the person, rather than the body, as its chief concern, "modern medical care conceals a dangerous ethic that encourages treating persons as [objective] things" (Thomasma 2002, p. 335) ". Humanism developed to address that problem, and its impact has been immense. Medical education, for example, now routinely includes humanities education. More importantly, humanism's central insights led to the development of bioethics as we know it today (Marcum 2008a, b) . We owe our current understanding of informed consent, for example-along with all four of the now classic principles of clinical ethics-to the continued relevance of humanism's foundations in the 70s and 80s. Indeed, the largest conference for bioethicists in the US at this time is the annual meeting of the "American Society for Bioethics and Humanities". In addition to the ethical concerns of humanism, objections to the BMM in the late twentieth century often took on a metaphysical character, and when they did, "holism", rather than "humanism", provided the remedy. From many perspectives humanism and holism are indistinguishable, and both terms have been used so loosely that distinct definitions are truly impossible. Still, it's safe to say that while humanism is generally understood to be concerned with ethical gaps in the BMM, holism set out at the start to rectify more abstract forms of misunderstanding. Though, as we will see, medical holism lacks a coherent definition, generally speaking a holistic position adjusts typical understanding of disparate elements in a way that allows us to see them as unified. The central spokesperson for holism at its origins was George Engel, a psychiatrist whose work remains as influential today as it was in his own time. Based on a sense that "appeals to humanism" are "ephemeral and insubstantial…when not based on rational principles" (Engel 1977, p. 135) , Engel dug into the conceptual misunderstanding that had led to dogmatic acceptance of the BMM. I contend that all medicine is in crisis and, further, that medicine's crisis derives from the same basic fault as psychiatry's, namely, adherence to a model of disease no longer adequate for the scientific tasks and social responsibilities of either medicine or psychiatry…. The biomedical model not only requires that disease be dealt with as an entity independent of social behavior, it also demands that behavioral aberrations be explained on the basis of disordered somatic…processes. Thus the biomedical model embraces…reductionism, the philosophic view that complex phenomena are ultimately derived from a single primary principle… (Engel 1977, p. 129) . To resolve this conceptual problem, Engel proposed that we replace the reductive "biomedical model" with a holistic "bio-psycho-social model" (BPSM). 1 Based on biological systems theory, the BPSM offers a hierarchical explanatory perspective, so that we no longer limit the focus of medicine to the biological elements of disease. Instead, "clinicians must attend simultaneously to the biological, psychological, and social dimensions of illness" (Borrell-Carrio et al. 2004, p. 576) . In addition, "the biopsychosocial model protests against the manner by which only non-subjective qualities gain explanatory power in analysing, diagnosing and treating somatic and mental health issues." (Schillmeier 2019, p. 1) . It is a core tenet of holism that medical science will fail to accomplish its scientific and ethical goals if it fails to take stock of patients' subjective experiences as persons. It's hard to overstate the value or historical importance of these insights. Engel's brand of holism had such a powerful impact on the profession that it's become central to medicine's identity in our time. The BPSM has been described as "the official philosophy of the American Psychiatric Association and the Diagnostic and Statistical Manual, DSM-5" (Rease 2014, p. 1) . We see it in the World Health Organization's continued commitment to a definition of health as "a state of complete physical, mental and social well being, and not merely the absence of disease or infirmity" (WHO 2019). Indeed, it seems clear that Engel's ideas have been powerful enough to seep out from professional discourse into the mainstream, where the notion of "integrated mind-body medicine" is now an everyday feature of popular culture. From a metaphysical standpoint, it seems the chief insight of Engel's holism is straightforward: medicine should reject reductive physicalism in favor of a perspective that recognizes the existence and importance of the mind (or mental properties) (Borrell-Carrio et al. 2004; Marcum 2008a, b; Woods 2015) . What position on mind does holism propose, exactly? That question is challenging enough on purely philosophical grounds, and I will address it in Part 4. Unfortunately, Engel used basic philosophical terms in confusing and inaccurate ways, and this has led to conceptual complications that we must first consider: …the biomedical model embraces both reductionism, the philosophic view that complex phenomena are ultimately derived from a single primary principle, and mind-body dualism, the doctrine that separates the mental from the somatic (Engel 1977, p. 130) . Not once, but consistently throughout all of Engel's writings, biological reductionism is directly equated with mind-body dualism: The biomedical model can make provision neither for the person as a whole nor for data of a psychological or social nature. For the reductionism and mind-body dualism upon which the model is predicated requires that these must first be reduced to physico-chemical terms before they can have meaning (Engel 1981, p. 103) . Philosophically speaking, dualism and reductionism are diametrically opposed, so for philosophers it's hard to understand even what Engel means to say in these passages. It would be pleasant enough to let go of the effort to sort that out, but the confusions built into holism's original characterization have now become pervasive throughout the medical professions: …the Cartesian division of the person into mind and body…is basic to the reductionist approach of medicine and therefore the source of its tendency to ignore the human dimensions of health and disease (Sullivan 1986, p. 331) . …to reject dualism is, on the face of it, to leave open the possibility that the mind can have a more significant role in illness, and in healthcare, than medicine has traditionally envisaged (Paley 2000 (Paley , p. 1294 . Dualism…discounts the significance of mental states in the maintenance of health (Gendle 2016, p. 141) . The Cartesian notion of mind and body as two completely different substances has been indicated as the source of the medical reductionist view of the human being (Arnaudo 2017 (Arnaudo , p. 1082 . If this were just a confusing strain of armchair philosophy, nothing much would hang on the question of what these claims are intended to communicate. The mind-body problem is difficult, and it's not uncommon for professionals outside of philosophy to discuss the issue in a way that misunderstands basic terms and concepts. But this is not just armchair philosophy. The World Health Organization and the American Psychiatric Association have based their goals and conclusions on the conceptual foundation of holism. More than that, in the realm of everyday medical practice the influence of the BPSM has been so strong that "rejection of biopsychosocial medicine is tantamount to a betrayal of one's profession" (Howsepian 1977, p. 179) . In spite of the development of evidence-based medicine and its unequivocally biological orientation, the BPSM is still "woven into the delivery of care at every level, including insurance and hospital inpatient and outpatient services" (Greenfield 2007) . To the extent that Engel was right-that is, to the extent that we can optimally improve health only with attention to both mind and body-it seems straightforwardly threatening for the profession to embrace an incoherent position on dualism. What are the WHO, the APA and the everyday practice of medicine actually doing when they set out to "do away with the reductionism and mind-body dualism inherent in the biomedical model" (Easton 1982 (Easton , p. 1185 ? Something is indeed amiss in the practice of medicine. In what follows, however, I will not argue that holism is irretrievably confused in itself. On the contrary, I hope to show how medicine's struggle to articulate a consistent position on the mind arises from deep philosophical tension between its commitment to improving experience and its commitment to science. I begin in Part 2 by pinning down terminological confusions that have led to incoherence in statements about mind and body in medicine. When we rectify those we see the sense in holism's confusing claims, but problems persist because holism maintains contradictory positions on reductionism. In Part 3, I show that while the nonreductive strand of holism is philosophically and clinically sound, reductive holism is conceptually incoherent. More importantly, reductive holism drives the clinical recommendation of deliberate diagnostic vagueness with patients whose symptoms might be diagnosed as either psychiatric or biomedical. Evidence suggests this recommendation leads to delayed diagnosis and treatment for substantial patient groups. Finally, having established that medicine must embrace some form of nonreductionism, I argue in Part 4 that Chalmers' naturalistic dualism is a stronger prima facie candidate for medicine's metaphysical foundation than the nonreductive alternatives. I conclude with a list of corrections needed to give medical holism a safe, coherent philosophical foundation. The more one looks at twentieth-century holism, the more elusive it becomes, the more it dissolves and reconfigures itself into its opposite (Rosenberg 2007, p. 155 ). Medical writing about holism uses central terms differently from the way that philosophers use them. First, philosophers draw a basic distinction between methodological reduction and ontological reduction. In research on holism, however, that distinction is rarely acknowledged. When it comes to methodological reduction, the defining figure is certainly Descartes, whose convictions about the value of a mechanical model had a powerful impact on the medical profession: Because it is the experience of everyone from childhood that many of our movements obey the will…this has disposed us to believe that the soul is the principle behind all of them. And the ignorance of anatomy and mechanics has contributed to this, for in considering only the exterior of the human body, we never imagined that it had enough organs or springs in it to move itself in all the different ways in which we see it move (Descartes 1998, p. 170) . It is certainly accurate to characterize Descartes' mechanistic methodology as reductionistic, and to recognize, as Engel did, the very serious limitations of that approach: this neglect of the whole inherent in the reductionism of the biomedical model is largely responsible for the physician's preoccupation with the body and disease and corresponding neglect of the patient as a person (Engel 1981, p. 107) . In other passages, however, Engel's writing about reductionism is about ontology rather than methodology. For example, "the biomedical model embraces…reductionism, the philosophic view that complex phenomena are ultimately derived from a single primary principle" (Engel 1977, p. 129) . In passages such as this, the issue of reductionism is not a matter of medicine's method; it's a matter of medicine's conceptual foundation, so we're not asking about how medicine should do things. Instead, we're asking about how the world is in itself. Can persons be reduced to bodies? More specifically, is subjective experience the same thing as physical brain states, or do minds (or mental properties) exist in addition to brains (and brain properties)? When philosophers consider the mind-body question, 'reduction' is automatically understood in this ontological sense, so it's a view that takes a position about how many kinds of things (or properties) exist. Those who think that minds or mental properties reduce to physical items and properties stand opposed to dualism. To be very clear, if you accept reductionism in this sense, you insist that experience is nothing over and above physical brain states. By contrast, those who think that minds (or mental properties) exist at the foundational level are dualists. It is not always important for medical writers to use the same terms and concepts that philosophers use, but when it comes to the distinction between methodological and ontological reductionism it's actually crucial, because our views on the methodological question readily come apart from our views on the ontological question. We can, for example, reject methodological reductionism as an approach to medicine, while we accept ontological reductionism as a view about the existence of the mental. If we do, we think Engel was right about the problem with medicine's mechanistic focus on the body, but we reject the idea that the mind or mental properties exist. This is a common combination of positions in current writing about holism, a perspective I'll call "reductive holism" that's particularly popular in psychiatry (and one that I will argue is untenable). More importantly, we can accept methodological reductionism as an approach to medical practice while we reject ontological reductionism, the idea that mental states can be reduced to the physical states of the brain. In fact, Descartes embraced this combination of views: I knew from this that I was a substance, the whole essence or nature of which was to think and which, in order to exist has no need of any place and does not depend on anything material. Thus this…soul…is completely distinct from the body…and even if the body did not exist the soul would still be everything that it is (Descartes 2000, p. 61) . While it's accurate to say that Descartes was a reductionist about methodology, it is very seriously inaccurate to say that he was a reductionist about the ontological issue, the question of whether the mind can be reduced to the body. As the quintessential substance dualist, Descartes is very adamantly opposed to ontological reductionism. This, then, is the first reason why so many of holism's claims about mind and body seem philosophically confused. Because medical writing recognizes Descartes as a methodological reductionist, but then it fails to distinguish methodological from ontological reduction, it slips into describing Descartes as a reductionist about the mind. That is straightforwardly inaccurate. While the distinction between methodological and ontological reduction is helpful in making sense of holism's philosophically confusing claims, it cannot do the whole job. A far bigger problem lies in holism's definition of dualism, which also fails to recognize philosophers' central concern with the question of the existence of mind (or mental properties). In the original formulation of the BPSM, Engel defines dualism not ontologically, but epistemically, as "the doctrine that separates the mental from the somatic" (Engel 1977, p. 130) . We see this same characterization in Eric Cassell's The Nature of Suffering and the Goals of Medicine (Cassell 1991) , another foundational work on holism: The split between mind and body that has so deeply influenced our approach to medical care was proposed by Descartes to resolve certain philosophical issues. Moreover, Cartesian dualism made it possible for science to escape the control of the church by assigning the noncorporeal, spiritual realm to the church, leaving the physical world as the domain of science (Cassell 1991, p. 32 ). This is not inaccurate, philosophically speaking. Descartes did certainly advocate a "split between mind and body", and that split did facilitate the development of medical science. Notice, though, that when we characterize dualism as a matter of separating mind and body in our thinking, we are drawn to the idea that the biomedical model is itself dualistic, a notion that will be straightforwardly inaccurate in a philosophical context. Broadly speaking, the problem with the BMM, as Engel originally characterized it, is that it is monistic. The BMM reduces the mind to the physical material of the body and, for this reason, it overlooks the person. In Duncan's "Mind-body dualism and the biopsychosocial model of pain: What did Descartes really say?" (Duncan 2000) we can track confusion about dualism in a way that helps us see how it works. Duncan begins by defining dualism as "any concept of mind and body which treats them as distinct entities" (Duncan 2000, p. 485n1 )-again, a characterization focused not on whether mind and body both exist, but on how we treat them. He then explains that "Cartesian mental philosophy is seen, often in concert with Newtonian physics, as ushering in an era in which the body is reduced to mechanistic, organic processes, quite separate from the mind" (Duncan 2000, p. 486 ). While the "separation" of mind and body at the end of that sentence is accurate as an account of "Cartesian mental philosophy", for philosophers it's jarring to imagine Descartes' views on mind as facilitating a reductive physicalist model of persons. This tie is then made explicit: "Recent attempts by medical thinkers to overcome organic reductionism in favor of a more holistic representation of disease and health routinely begin by attacking Cartesian dualism" (Duncan 2000, p. 486) . This perplexing claim is entirely in keeping with writing about holism from the 1970s to the present, so it's important to trace the steps that brought us here. Holism begins with the idea that Descartes' methodologically reductive approach to medicine is misguided. Then it defines Cartesian dualism as "separation of mind and body"-and from there it comfortably concludes that reductionism in the BMM was made possible by Cartesian dualism. For philosophers, that conclusion is deeply confused, because Cartesian dualism is not just a matter of separating mind and body in our thinking. It's the view that minds exist as substances, in addition to the physical substances of bodies. For philosophers-including Descartes-Cartesian dualism is directly opposed to reductionism. When we replace the word 'dualism' with the epistemic notion of "separation of mind and body" (and we're careful to distinguish methodological reduction from ontological reduction), holism's confusing claims become not only sensible, but insightful. Engel wrote: the biomedical model embraces both reductionism, the philosophic view that complex phenomena are ultimately derived from a single primary principle, and mind-body dualism, the doctrine that separates the mental from the somatic (Engel 1977, p. 130) . Philosophers who read that sentence literally will find it to be incoherent, but Engel does have a point to make in this passage and it's an insight that does define the movement of holism. What he means to say is that "the biomedical model embraces both ontological reductionism, and the separation of mind and body in our medical methodology". In the end of the day, when we repair Engel's terminological confusions, his view amounts to this: holism rejects Descartes' methodologically reductive approach to medicine but it agrees with Descartes that it's a mistake to reduce the mind to the body. Medicine cannot succeed unless it acknowledges the existence of the mind (or mental properties) as distinct from the body (or physical properties). Once it has accepted the mental, medicine should endeavor to manage the mental and physical features of patients in a way that sees them as unified in the whole person. It remains unclear what position on the existence of mind (or mental properties) holism is reaching for, and I will address that question in Part 4. Medicine's working version of dualism is a facet of philosophy's version. Philosophically speaking, a dualist is someone who holds that mental experiences are distinct from physical brain states in a fundamental account of what exists in the world, so at that level we find two kinds of things: minds (or mental properties) and bodies (with physical properties). In medicine, however, a "dualist" is someone who "separates mind and body" in their thinking or medical practice. A dualist in the philosophical sense will indeed "separate mind and body", so as long as we're affirming dualism, medicine's conception doesn't take it too far afield of philosophy's conception. In the negative, however, problems arise very quickly. When philosophers reject dualism, they insist that only physical items (and properties) exist in a fundamental account of the world. For philosophers, then, rejecting dualism means accepting a fundamental account of human beings and their properties as entirely physical. When holists reject dualism, however, they are chiefly concerned to reject the BMM. For holists, then, rejecting dualism means rejecting a fundamental account of human beings as entirely physical. That disparity is the heart of the problem. The source of the trouble is this. While it's really not inaccurate to define dualism as "separation of mind and body", when you reject dualism in that form, you're not actually committed to one or the other side of the real dualism debate. You can go either way. You can reject separation as a dualist by insisting that minds (or mental properties) exist but we should think about them as inextricably unified with bodies in whole persons-or you can reject separation as a monist by insisting that minds (or mental properties) aren't actually there at the fundamental level, because only the physical material of the body exists (or the physical properties of the body). Philosophically speaking, these are opposed positions. For this reason, even after we correct terminological problems with 'reduction' and 'dualism', we find that the banner of holism does continue to fly over contradictory positions. That would be no problem at all if some holists defended a dualistic or nonreductive approach while others defended a reductive approach, but that's rarely what occurs. 2 Instead, holism is somehow construed as a perspective that can embrace opposing views on ontological reduction. Along the nonreductive strand we have The Rise and Fall of the Biopsychosocial Model (Ghaemi 2010) , where Ghaemi understands holism as it seems Engel intended, and in the way that philosophers tend to do-that is, as a position chiefly defined by rejection of reductive physicalism. Ghaemi suggests that the original BPSM served as the antidote to reductive physicalism in medicine, and that it now serves as the antidote to the biological approach to psychiatry. Woods presents a similar perspective, where holism "requires that the medical gaze toward the person is not one of eliminative reductionism" (Woods 2015, p. 14) , and Zucker takes an even stronger position, suggesting that holism is "a strategy…within medicine that stresses…a dualist, interactionist theory of mind and body" (Zucker 1981, p. 159) . Similarly, in Humanizing Modern Medicine, Marcum characterizes the BMM as "mechanistic monism", while the remedy to the BMM is consistently framed as "dualism/holism" (Marcum 2008a, p. 19) . Along the reductive strand, by contrast, many researchers, particularly in psychiatry, have seen Engel's literal rejection of "dualism" as holism's defining feature. These researchers insist that holism's central goal has always been to do away with "the mind-brain dichotomy", and from this perspective holism is conceptually aligned with a biological foundation for psychiatry. In "The persistence of mind-brain dualism in psychiatric reasoning about clinical scenarios", for example, Miresco and Kirmayer (2006)  Mind-brain dualism is the idea that the mind is somehow distinct from the brain and that its essence cannot be reduced to purely material and deterministic neurological mechanisms… Although many continue to support the view that mental phenomena result from emergent levels of organization of the brain, the idea that mind and brain are different entities is no longer credible in medical science. Indeed, the last few decades have seen efforts to develop integrative models heralded by claims that dualism in psychiatry has finally been overcome (Miresco and Kirmayer 2006, p. 913). 3 While it's great to see a philosophically accurate definition of dualism here (at least property dualism in the broadest sense), these authors seem to be sure that reductive physicalism has firmly been established as the only credible position in medicine and psychiatry. Unfortunately, to support that assumption they primarily cite Engel, suggesting that his "integrative" BPSM is the primary tool medicine has used to advance the reductionistic program. In another study along these lines, "Dualism persists in the science of the mind" (Demertzi et al. 2009 ), researchers similarly assume that the BPSM is grounded in reductive physicalism. What the study aims chiefly to show is that in spite of the diligent efforts of holists and advocates of the BPSM, healthcare professionals and the lay public continue to be dualists. The study supports this conclusion by examining responses to a series of claims-centrally that "mind and body are two separate things" and that "the mind is fundamentally physical" (Demertzi et al. 2009, p. 2)-then noting that a great many people accept the former and reject the latter. What's most important here is that, like Miresco and Kirmayer, these authors never see a need to argue that reductive physicalism is the best way to understand or improve holism. Instead, they assume from the start that holism and the BPSM are naturally, and indeed universally, understood to be based on a reductionistic perspective. Ghaemi is correct to point out that advocates of the BPSM stand firmly on the non-reductive, non-biological side in the context of debate about pharmaceutical versus psychotherapeutic approaches to mental illness, as Engel did. It's important to recognize, though, that it's also common for advocates of the BPSM to assume that mind (or mental properties) should be "reduced to purely material and deterministic neurological mechanisms" (Miresco and Kirmayer 2006) -not as if that would be a novel new way of doing things, but as if Engel's own holism has always been understood in these terms. For philosophers unfamiliar with medical writing, this conceptual confusion may seem impossibly far-fetched, but it's difficult for any field to reimagine its foundations once they've become entrenched. Where the philosopher finishes, there begins the physician. Aristotle As I've presented the core problem, holism sets out with an incoherent philosophical goal-to "do away with the reductionism and mind-body dualism inherent in the biomedical model" (Easton 1982 (Easton , p. 1185 . As I've diagnosed it, the problem arises from inaccurate definitions of basic philosophical terms. If by 'reductionism' we mean methodological reductionism, and by 'dualism' we mean separation of mind and body in our thinking or medical practice, then it will make sense to insist that Descartes' dualistic reductionism is responsible for the ills of the biomedical model. But this is not what 'dualism' and 'reductionism' mean for philosophers in the context of the mind-body problem, so for philosophers that claim-and all the claims that derive from it-are incoherent. When we understand the philosophical meaning of these terms correctly, we see that dualism and reductionism are opposing positions. It is not possible for the BMM to be grounded in dualistic reductionism, so it is not possible for holism to offer a remedy for that combination. What, then, are holists actually aiming to do? I have argued that as a result of terminological confusions, understanding of holism's goal is conceptually divided. Nonreductive holists see its chief aim as rejection of reductionism, while reductive holists see its chief aim as rejection of dualism-not actual dualism, but dualism mistakenly defined as the separation of mind and body in our thinking, language and medical practice. When we correct the terminological confusions, nonreductive holism emerges as internally consistent and philosophically viable. As Engel saw it, the primary problem with the BMM is that it reduces experiences to biology, and this is why it addresses patients as bodies rather than persons. Nonreductive holism fixes that problem by insisting that medical science must broaden its scope to include the realm of experience. When it does that well, clinicians see patients as whole persons in whom biological, psychological and social elements are inextricably unified. There is still work to do in pinning down the specific position on mind that nonreductive holism demands or implies, and I'll address that question in Part 4. Whatever its views on mind turn out to be, there is nothing about the general picture of nonreductive holism that's conceptually problematic. Reductive holism, on the other hand, has severe conceptual problems. Most obviously, the term 'reductive holism' is an oxymoron. Holism of any kind proposes that many disparate elements are present and unified. Reductionism of any kind proposes, by contrast, that while it might seem that we have many disparate elements, in reality all of them can be reduced to one element. For a reductionist we do not have multiple things or ideas that we can set out to conceptualize as "whole". When we think about the foundations for medicine, this is more than just a word game. Reductive holists insist that "the mind is indivisible from the brain" (White et al. 2012 ) because mind can be "reduced to purely material and deterministic neurological mechanisms" (Miresco and Kirmayer 2006) . While it might well be possible to defend reductionism as the right approach for medicine and psychiatry, it is very difficult to see how that approach could align with holism. This is the heart of the problem, conceptually speaking. Reductive holists routinely cite Engel and the BPSM as the foundation for the reductionist program-not as a new and philosophically complex innovation, but as if Engel's own holism has always been understood to be reductive. That is simply not accurate. The trouble for reductive holists, then, is not reductionism per se, but the combination of reductionism with holism. It is a mistake to suggest that holism and the BPSM have always been understood as reductionistic, and it is difficult to see how reductionism and holism can be combined even if we do recognize the effort as an innovation. Finally and most simply, even if we managed to develop some fancy philosophical footwork that makes it possible to align reductionism with holism, that project would directly oppose holism's historical purpose. However we conceive of it, holism has to be an antidote to the biomedical model, a corrective for the mistaken idea that patients are bodies rather than persons with experiences. We might feel strongly that reductionism is the right way to go in medicine-but if we do then we favor the BMM, not the antidote of holism. In this sense too, reductive holism is a conceptual contortion that cannot be put right. Medicine, however, is not philosophy, and it is not a theoretical science. Medical science does not play out in the conceptual realm, that is to say. It plays out in practice. If we want to understand the viability of reductive holism versus nonreductive holism it is not enough to consider the conceptual merits and challenges of each approach. The real test lies at the level of practice. What does reductive holism look like in everyday clinical medicine, and how does that picture differ from practice that embraces nonreductive holism? Deep conceptual confusion rarely leads to viable empirical results. I will suggest that this truism holds in medicine as it does in other sciences. The conceptual incoherence of "reductive holism" is manifested in clinical practices that are threatening to patient health. The central innovation that holism introduces at the practical level is to broaden the scope of clinical practice beyond biology so that mind is welcomed into the exam room. No matter what picture of holism we might prefer, that is to say, holists will all agree that psychological explanations for symptoms should be drawn into the professional framework as medically legitimate, and medically relevant. For this reason, just as holism invigorated the profession of bioethics, it also facilitated the development of psychosomatic medicine as a profession whose research has relevance for doctors in everyday practice. In this sense, though psychosomatic medicine is actually a subdiscipline of psychiatry, Engel often presented it as a paradigm for the sort of "integrated mind-body medicine" that holism aims for (Engel 1967) . Unfortunately, because holism is conceptually splintered into nonreductive and reductive strands, it offers two very different approaches to the mind-body diagnostic line. If we stick with the nonreductive strand of holism that Engel recommends, when we bring the mind into the exam room we must continue to recognize it as mind rather than reducing it to the biology of the body. As a result, though we accept that bodily health is inextricably interwoven with psychosocial well-being, we insist that at the level of medical practice psychiatric conditions must be clearly distinguished from biomedical conditions. When doctors construe psychiatric problems as biological problems, that is to say, they overlook the person in favor of the body, reverting to the BMM. This will be very important in Part 4, when we try to sort out the specific nonreductive view on mind that holism demands. At the level of actual medical care, when clinicians see all of patients' maladies as biological diseases, they straightforwardly fail to practice in the way that is recommended by nonreductive holism. If we accept the reductive strand of holism, however, our picture of the clinical mind-body line will look radically different. This approach is laid out directly in "Time to end the distinction between mental and neurological illnesses". Like Miresco and Kirmayer, author P.D. White and colleagues begin with a reductive programme, where mind can "be reduced to purely material and deterministic neurological mechanisms" (Miresco and Kirmayer 2006, p. 913) . Citing a long series of neuroscientific studies on mental health disorders, White and colleagues suggest that neuroscience has now definitively established that "the mind is indivisible from the brain" (White et al. 2012, p. 1) . Based on that emphatic rejection of dualism-not as it is in philosophy, but as Engel misdefined it-they conclude that we must "end the distinction between mental and neurological illnesses" (White et al. 2012, p. 1) . Because the reductive strand of holism embraces Engel's confused definition of dualism, it leads researchers to recommend rejection of separation in very literal terms at the level of practice. So it's not just that we refuse to separate mind and body in our conceptual cogitations. It's that we refuse to separate them in actual diagnostic practice. Holism in its reductive form is routinely understood to imply that the boundaries between medical and mental health conditions are intrinsically vague-an idea I will refer to as the "vagueness thesis". It's important to note that in itself the reductive picture, and the vagueness thesis that grows out of it, are perfectly coherent. As reductionists we reduce the mental to the physical, then we reject the idea that mind and body are separate, not only conceptually but also diagnostically. The trouble arises when we combine that picture with holism-when White and colleagues insist, as if it's perfectly obvious, that the reductionist programme they propose will "promote a biopsychosocial model". First, as a matter of factual accuracy, Engel's BPSM definitively rejects reductionism. Second, even if we do recognize that reductive holism would be a radical innovation to the BPSM, it's hard to see how we can actually make the combination work. Third, we can now see the most serious challenge that project would involve: nonreductive holism demands a continual distinction in practice between problems caused by psychology and problems caused by biology. When doctors reject that distinction, addressing every malady as a biological disease, they are not practicing holism. They are practicing the biomedical model. Philosophers might be inclined to assume that it's rare for those in the medical professions to attempt the conceptual backbends required by reductive holism. That is not the case. Though nonreductive holism clearly rejects the idea that psychiatric disorders should be addressed as biological diseases, the vagueness thesis has become everyday dogma in the medical professions, particularly in psychiatry and bioethics. In spite of serious conceptual problems at the origins of the thesis, that is to say, it is common practice to equate holism with the idea that medical and mental health conditions are not distinct. We see this clearly in the explanatory introduction to the DSM-IV in 1994: The term mental disorder unfortunately implies a distinction between 'mental' disorders and 'physical' disorders that is an… anachronism…. The term persists in the title of DSM-IV because we have not found an appropriate substitute (APA 1994, p. xxx). We see it again in more explicit form in discussion of the terms 'mental disorders' and 'psychiatric disorders' during the development of DSM-5: "Mental" implies a Cartesian view of the mind-body problem…. The term 'psychiatric disorder' may be preferable insofar as it emphasizes that these conditions are not purely 'mental', and that the line between 'psychiatric disorder' and 'other medical disorders' is not a sharp one (Stein et al. 2010 (Stein et al. , p. 1760 ). The vagueness thesis marks the juncture where armchair philosophy on the mindbody problem is transformed into action that directly affects the health of living human persons, particularly those with bodily symptoms that remain undiagnosed. Undiagnosed or unexplained symptoms are very common in everyday medicine. While research on prevalence has led to widely varied results, textbooks and guidelines commonly settle in a middle range, where roughly half of outpatients seeking medical care get a medical diagnosis, while half have symptoms that are unexplained (O'Leary 2018a). According to these guidelines, patients with unexplained symptoms are as common as all patients with medical diagnosis combined (Greenberg 2017 ; Joint Commissioning Panel for Mental Health 2017). Every case of unexplained symptoms could be caused by an unrecognized biomedical condition. As long as we're holists it will also be possible that unexplained symptoms are actually "not attributable to verifiable, conventionally defined diseases" (Fink and Schroder 2010, p. 415 ) because they're caused by "psychological conflict" that is "transformed or transduced into bodily distress" (Kirmayer and Young 1998, p. 420 ). There are a great many terms in use to describe symptoms of this kind, and for our purposes "psychosomatic symptoms" will be the clearest and most neutral option. 4 Psychosomatic symptoms sit right at the mind-body divide in medicine. If we want to see what clinicians actually do in relation to mind and body, this will be the place to look, and if we want to understand what philosophical picture of mind underlies medical practice, we will see it best if we investigate standards for diagnosis at this line. In this sense, patients with unexplained symptoms are uniquely gathered in the mind-body borderlands. They are those whose symptoms could be either biomedical or psychological, that is, they are patients who could require either medical or psychiatric care. How should doctors handle this very common form of the mind-body divide in everyday practice? If we accept Engel's nonreductive approach, we will continue to insist on practice that distinguishes between biological and psychological disorders-so when a doctor is unsure which kind of problem she's dealing with, she should set out to resolve that question with all the caution and clarity that typically define diagnostic practice. When we start from the reductive strand of holism, however, the vagueness thesis will direct the doctor not to double down on her diagnostic efforts in cases of uncertainty, but to abandon them. There is no clear line between medical and mental health conditions, the thesis literally tells her, so in every case where a doctor sets out to draw a crisp diagnostic distinction between disease and psychological distress, she is acting on philosophical misunderstanding. The field of psychosomatic medicine is unequivocally grounded in the reductive strand of holism, and it is the field of psychosomatic medicine that develops guidelines for managing the large portion of medical cases where symptoms remain unexplained (O'Leary 2018b). This is Schwab describing the goals of the field back in 1984: In full accord with the established principles of psychosomatic medicine, there is a lessening of the tendency to view the patient dichotomously as being "organic or functional." Instead, collaborative work by internists and psychiatrists enables each specialist to conceptualize the patient as a total person, a psychobiologic unit. (Schwab 1985, p. 584 ). Psychosomatic medicine continues to characterize itself in this way, most recently working to revise the term "medically unexplained symptoms", because it "reflects dualistic thinking-regarding symptoms as either 'organic' or 'non organic/psychological" (Creed et al. 2010, p. 5) . In this sense, both originally and in our time, psychosomatic medicine has been defined by a commitment to diagnostic vagueness at the mind-body line, by "the necessity of not trying to force these disorders into either a 'mental' or 'physical' classification" (Creed et al. 2010, p. 5) . As an approach to diagnostic uncertainty, the vagueness thesis has had a profound impact on patient care, or at least on the principles that direct doctors' response to unexplained symptoms in medical education. Based on the directive to avoid diagnostic distinctions at the line between psychiatry and medicine, research in this area routinely proceeds as if all patients with unexplained symptoms should be understood to suffer from psycho-somatic conditions (O'Leary 2018a). That assumption is now so commonplace that the research summary system known as "UpToDate" asserts in its chapter on psychiatry: "More than 50% of patients presenting to outpatient medical clinics with a physical complaint do not have a medical condition" (Greenberg 2017) . In the UK's National Health Service that astonishing idea has been put into practice with a program that directs all patients with unexplained physical symptoms onto a track where their care will be focused on mental health support (NHS 2018). Outside the conceptual framework of psychosomatic medicine, the problem with deliberate diagnostic vagueness seems immediately evident. No matter what metaphysical account we prefer to understand mind and body, it will still be the case that diagnostic error is possible at the boundary between psychiatry and medicine. More than that, no matter what metaphysical account we prefer, it will still be dangerous to provide only psychiatric care to patients in need of medical care. Imagine a patient who suffers from undiagnosed early-stage lung cancer, someone who begins as one of the great many patients with unexplained symptoms. Insofar as her doctor chooses to follow the directives of UpToDate and the UK's National Health Service, she will understand this patient as someone for whom continued diagnostic effort would be misguided, someone with a mind-body interface disorder best managed with mental health support-but of course a patient with early-stage lung cancer who receives only psychiatric care will not survive. The British press covered a story of this kind not long ago (Philipson 2013) , where an assistant professor in international relations suffered for years from unexplained symptoms construed as psychosomatic. "How is it possible", she asked in a public blog just before her death, "that a 36-year-old, health…conscious, occasionally social smoking, middle class, fiancé of a doctor can develop metastatic lung cancer unnoticed. How?!?" (Smirl 2012 ). If we accept the vagueness thesis as a principle for practice, we understand undiagnosed symptoms to be caused by a condition that can't be rightly classified in psychiatry or in medicine, so we take it to be impossible to make a diagnostic error at the mind-body line (Sharpe and Bass 2003; Edwards 2019) . Outside of psychosomatic medicine, however, we find evidence that error of this kind is problematic for public health (Butler and Zeman 2005; Koyama et al. 2018) . For example, more than half of patients surveyed with autoimmune disease report that they've been wrongly denied the medical care they needed at some time on the basis of mistaken psychosomatic diagnosis (AARDA 2014) . Similarly, studies show strong ties between mistaken psychosomatic diagnosis and delay in accurate diagnosis and treatment of rare disease (Kole and Faurisson 2009) . Based on conservative prevalence estimates (NIH 2012; Global Genes Project 2013), if we assume only half of rare disease patients are affected by this problem, we find 12 million Americans in these two groups alone have suffered without the care they need as a result of diagnostic error at the line between psychiatry and medicine. The most decisive evidence we have for the seriousness of this problem is "chronic fatigue syndrome", known in research as myalgic encephalomyelitis/chronic fatigue syndrome (ME/CFS). Because a clear biological explanation for this condition has long been elusive, and psychiatry strongly advocates the vagueness thesis as a diagnostic practice, ME/CFS has long been construed as a paradigm of psycho-somatic illness. While that approach has guided ME/CFS care across the globe for decades, in 2015 the Institute of Medicine insisted that it has been a mistake (IOM 2015) . Biological research, they concluded, has now shown that ME/CFS is a serious biomedical disease along the lines of diabetes or heart disease (Komaroff 2019) . This conclusion has now been accepted by the NIH (Green et al. 2015) , the Agency for Healthcare Research and Quality (2014) , and the Centers for Disease Control (2018) who have implemented the change in research orientation and practice guidelines. Recently the UK government awarded substantial funding for DNA research on ME/CFS-that is, for ME/CFS defined as a biological disease. Even if we agree with psychiatrists who still insist that ME/CFS is a psycho-somatic disorder (Sharpe and Greco 2019) , dissolution of professional consensus on that point will still force us to question the safety of the vagueness thesis as a principle for practice. Regardless of whether we construe ME/CFS as a biomedical disease or a psychiatric condition, the existence of the debate does establish that it's possible to make a diagnostic mistake at the line between psychiatry and medicine, and that doing so can be harmful for patients in need on a very large scale (O'Leary 2020a). It is important to note that this issue has suddenly become pressing (Raskin et al. 2020; Harding 2020; Vastag and Mazur 2020; Lowenstein 2020; Yong 2020) . A substantial portion of patients who've suffered from acute Covid-19 are reporting ongoing fatigue in the form that's typical of ME/CFS (Open Medicine Foundation 2020; Covid-19 Recovery Awareness 2020; Assaf et al. 2020) , following the pattern of those who've suffered from SARS, MERS, Epstein-Barr and other viruses. While many biological researchers have coordinated efforts to address this problem (Open Medicine Foundation 2020), many in psychiatry have relied on the vagueness thesis, proposing that post-Covid fatigue must a mind-body disorder best managed by psychiatry (Holmes et al. 2020; University College London 2020; Gill 2020; Chalder and McCormack 2020 ). It will not be possible to determine the best course of care for this increasingly large and vocal patient group unless we're clear about the reasoning that supports the vagueness thesis as a principle for practice. More specifically, we will need to be very clear about how philosophical confusion can undermine standards for safe, cautious patient care in the context of diagnostic uncertainty. Eventually it will become intolerable to have a theory of medicine that cannot accommodate the evidence for the effect on health of the meaning of experience, expectations, beliefs, intentionality, and relationships (McWhinney 2002, p. x) . Driven by Engel's holistic concerns, medicine has embraced a distinctly philosophical identity in the last 40 years. Medical professionals and patients alike now see a new and improved version of the profession that rejects the traditional model's wholly physical account of patients-but the conceptual meaning of that idea remains terribly confused. It is long past time that we set out to clarify the metaphysical foundation for holism in a way that aligns with current understanding in philosophy. Before we dig into that task, it's important to articulate a few straightforward corrections to discourse about holism that comments on philosophy. First, though research in psychiatry often suggests otherwise, philosophy in our time is not a prima facie justification for reductionism. As Kim succinctly put it back in 1989, "reductionism in the mind-body problem has been out of fashion for two decades" (Kim 1989, p. 32) . Second, contrary to conceptual discussions in DSM-IV and DSM-5, philosophy does not provide a prima facie justification for the vagueness thesis, the idea that boundaries between psychiatric and medical disorders must be intrinsically vague. As we saw in Part 3, the vagueness thesis arises from reductive holism's effort to "eliminate the mind-brain dichotomy" in our thinking, language and practice. Effort of that kind is generally not defended in philosophy of mind. In fact, as Davies and Roache describe things in "Reassessing biopsychosocial psychiatry", current orthodoxy in philosophy proposes that "descriptions and explanations expressed in the language of psychology are irreducible to descriptions and explanations expressed in the language of biology" (Davies and Roache 2017, p. 4 ). While it is certainly possible to defend the vagueness thesis on philosophical grounds, it is inaccurate to suggest that contemporary philosophers are generally united behind that kind of effort. Reductive holism is, first, a conceptually incoherent combination of ideas, or at least it is immensely problematic. Second, we've seen evidence that, through literal application of the vagueness thesis in clinical recommendations, reductive holism has led to problems in practice, particularly with the large portion of patients whose symptoms might have either psychological or biological explanations. We see now that, third, while reductive holists often imply that philosophy demands such a combination of ideas, that is actually not the case. For all three reasons it seems clear that reductive holism is a nonstarter. If we want to find a way to make sense of mind in medicine as holists, we must begin with nonreductionism. The real challenge in pinning down holism's metaphysical foundation is in sorting out the nonreductive position on mind that it requires. What's made this confoundingly difficult is holism's persistent, deeply confused insistence on doing away with "the reductionism and mind-body dualism inherent in the biomedical model" (Easton 1982) . Because there is no coherent position that accepts both reductionism and Cartesian dualism, there is no coherent position that can be characterized as an antidote to it. Medicine's metaphysical morass results from decades of effort to grapple with this impossible challenge. The fact is that we can make sense of the BMM either as a reductive approach, where mental states dissolve into states of the body, or as an approach based on Cartesian dualism, where the difference between mental states and bodily states is explained by the existence of separate mental and physical substances within each person. These are not the same foundation. In fact, though either will work as a metaphysical ground for the body-focused approach of the BMM, they are diametrically opposed. In this passage, Engel comes close to conceptual clarity about the opposing options, and if he had just taken a few more steps in this direction, medicine might well have avoided its current confusions: Biomedical dogma requires that all disease, including "mental" disease, be conceptualized in terms of derangement of underlying physical mechanisms. This permits only two alternatives whereby behavior and disease can be reconciled: the reductionist, which says that all behavioral phenomena of disease must be conceptualized in terms of physicochemical principles; and the exclusionist, which says that whatever is not capable of being so explained must be excluded from the category of disease (Engel 1977, p. 380) . What holism really wants to do is to find a way of understanding mental states where we avoid the two poles of denying mind and accepting mind in a way that sets it outside the scope of medical science. Philosophically speaking, that is to say, when holism rejects a purely biological model for medicine it seeks to (1) reject reductionism, while it also (2) rejects Cartesian substance dualism. Research on holism in the last 10 years has become increasingly clear on this point, and increasingly vocal about the importance of finally pinning down that middle way: Does…the notion of mental disorder lead to a dilemma that consists in either implying a Cartesian account of the mind-body relation or in the need to give up a notion of mental disorder in its own right? Many psychiatrists seem to believe that denying substance dualism requires a purely neurophysiological stance for explaining mental disorder. However, this conviction is based on a limited awareness of the philosophical debate on the mind-body problem (Schramme 2013, p. 1). According to Schramme, when we pursue the middle way, "mental illness is not reducible to brain illness, even when mental phenomena have their basis in the brain" (Schramme 2013, p. 8) . Gabbard (2000) and Jungert (2013) have also argued for this kind of approach, as has Natalie Banner (2013), who states directly, "one can legitimately hold both that ultimately, the mind is made up of brain stuff, and that mental disorders are not reducible to brain disorders" (Banner 2013, p. 510) . Philosophy does offer these kinds of alternatives. In fact, intensive effort in the last five decades has highlighted the space between the poles of reductionism and Cartesian dualism and populated it with a range of subtler options (Putnam 1967; Davidson 1979; Fodor 1974; Block 1980; McLaughlin 1991; Chalmers 1996; Kim 1996 Kim , 1998 Kim , 2005 . If we want to clarify medicine's position in this realm, however, we must begin with accurate and consistent use of the term 'dualism'. To be very clear, dualism is not the epistemic idea that we should "separate mind and body" in our thinking, language or practice. Dualism is an ontological position, a view about how many kinds of entities or properties exist in a fundamental account of the world. Moreover, though it's clear that holists must indeed reject the ontological position of Cartesian substance dualism, it is not at all clear that they should reject dualism generally. As Susan Schneider has put it, "contemporary philosophy of mind sees the question of the nature of substance as being settled in favor of the physicalist. Dualism about properties, in contrast, is regarded as being a live option" (Schneider 2012, p. 51) . Philosophy offers two central alternatives in the territory between reductionism and substance dualism: "nonreductive physicalism" and "naturalistic dualism". Confusing as the terminology may be, both embrace physicalism about substances, so both align against the Cartesian dualist's view that persons are split into mental and physical substances. In addition, both are nonreductive positions, so both align against reductionism just as Engel does. What makes it possible to combine rejection of Cartesian dualism with rejection of reductionism is the idea that some physical brain states have mental properties. For nonreductive physicalists and naturalistic dualists alike, "mental properties are real, and not just heuristic devices for making convenient predictions" (Kallestrup 2006, p. 468) . The point of debate between nonreductive physicalists and naturalistic dualists is a matter of how far we're willing to go with the existence of mental properties. For the nonreductive physicalist, mental properties do genuinely arise in the world. They "supervene on" or "emerge from" physical properties. When we count the things that exist in the end of the day, however, mental properties are ultimately identified as physical properties. For the naturalistic dualist, by contrast, the difference between mental and physical properties does not disappear at the fundamental level. When we count the things that exist, we find that while all substances are physical, and most of their properties are physical, some properties exist that are mental in the end of the day. The difference between these two perspectives is immensely important for medicine because medicine is a science. For the nonreductive physicalist all sciences can proceed with business as usual, because while mental properties will sometimes supervene on or emerge from physical properties, mental properties will still be fundamentally physical in the end of the day. For the naturalistic dualist, on the other hand, science needs to respond to the fundamental reality of the mental. As David Chalmers puts it, "the fact that consciousness accompanies a given physical process is an additional fact, it is not explainable by telling the physical facts" (Chalmers 1996, p. 107) . Which approach should medicine choose? Generally speaking, given Engel's confusion about what dualism amounts to, and his bizarre insistence that dualism and reductionism are one and the same view, many now assume that holism is opposed to dualism in all its forms. Bioethics has taken a particularly strong stance in this way -so much so that rejection of dualism seems to carry the force of an ethical imperative. In 2007, however, Rachel Cooper explored dualism in the context of psychiatry, pointing out that "within the psychiatric literature dualism is sometimes presented as being a dead theory. But is not". She concludes, …[naturalistic] dualism is compatible with all neuroscientific findings, and with the fact that psychoactive medications can change mental states. One can thus adopt dualism without this causing any problems for psychiatric research (Cooper 2007, p. 106) . In 2018, Hane Maung followed up on Cooper's conclusion with a substantial defense for naturalistic dualism as a "a way of maintaining the irreducibility of subjective experience while accepting the scientific claims of biological psychiatry" (Maung 2019, p. 68) . In "Dualism and its place in a philosophical structure for psychiatry" (Maung 2019) , Maung presents the tenets of naturalistic dualism as defended by Chalmers in a way that finally does refute knee-jerk resistance to dualism in psychiatry. First, he clarifies, Chalmers does not propose that all mental states or properties must exist at the fundamental level. Instead, he distinguishes psychological and phenomenal concepts of the mental, arguing that while cognitive science can explain psychological states and properties, no account of that kind will be adequate to explain phenomenal states-that is, states of experience. Maung characterizes the difference in this way: The phenomenal concept of the mental is that which concerns the subjective quality of experience. When one is involved in perceiving, thinking, and acting, there occur various complex causal processes in one's brain. However, these processes do not usually go on "in the dark", but are associated with a phenomenal feel (Chalmers 1996, p. 4) . They are accompanied by a first-person subject of experience, that is to say, by consciousness. To borrow a phrase from Thomas Nagel (1974), there is usually "something it is like" to be in a given mental state. For this reason, naturalistic dualism is not dualism about mental and physical properties generally. It is dualism specifically about experiential properties as distinct from physical properties. Moreover, because Chalmers insists that there are psychophysical laws that tie experiential properties to physical properties, naturalistic dualism demands a rethinking of our scientific sensibilities. If we accept naturalistic dualism we accept that science has some work to do in explaining how we get from physical facts to experiential facts. Second, naturalistic dualism is motivated by "the perceived failure of physicalism to account for the subjective experience of consciousness" (Maung 2019, p. 63 ). Chalmers asks, How can we explain why there is something it is like to entertain a mental image, or to experience an emotion? It is widely agreed that experience arises from a physical basis, but we have no good explanation of why and how it so arises. Why should physical processing give rise to a rich inner life at all? It seems objectively unreasonable that it should, and yet it does (Chalmers 1996, p. 3) . If naturalistic dualism is compatible with biological psychiatry-and it's hard to refute Cooper and Maung on this point-then it will be compatible with holism. More than that, while Engel does generally work with the language of the "psychological" in contrast with the "biological" and the "social", holism's concern with the psychological is specifically focused on experience. For this reason, naturalistic dualism seems to capture the spirit of holism better than nonreductive physicalism. Roughly speaking, until the 1970s medicine understood itself straightforwardly as a physical science, one that "assumes disease to be fully accounted for by deviations from the norm of measurable biological (somatic) variables" (Engel 1977, p. 130) . At that point the profession found itself in a sort of double crisis. In the realm of ethics, humanism demanded recognition of patients as persons and not merely bodies, and on that basis it clarified the core ideas of clinical ethics as we now understand them. At the same time-and as a distinct kind of movement-holism proposed that humanism's ethical demands must be grounded in a change in medicine's understanding of itself as a science. If we want to succeed in the goal of improving and protecting health, Engel insisted, we must broaden the scope of medical science to include experience. As Borrell-Carrio and colleagues have described it… Engel did not deny that the mainstream of biomedical research had fostered important advances in medicine, but he criticized its excessively narrow focus for leading clinicians to…[ignore] the possibility that the subjective experience of the patient was amenable to scientific study (Borrell-Carrio et al. 2004, p. 576 ). When we consider options for holism's metaphysical foundation, then, we must seek an approach that will not only (1) reject reductionism and (2) reject Cartesian substance dualism, but also one that will (3) demand that we revise our understanding of medical science to include experience. In this way, holism seems to have a natural home in naturalistic dualism. If medicine had overtly adopted the picture of naturalistic dualism in the late twentieth century, that conceptual change would certainly have forced a sharp turn away from the BMM in the direction of holism. More than that, naturalistic dualism can make sense of 40 years of subsequent effort to understand and implement a new approach to medical science that makes room for mind. If we adopt the nonreductive physicalist approach of supervenience we can successfully explain the metaphysical ideas we end up with in holism, and that would be a substantial improvement on the confused status quo. When it comes to medicine's history, though, and the force with which holism has demanded a change to medical science, it seems that supervenience falls short. If experiences do disappear in a fundamental scientific account, why should we see the BMM as deeply and irretrievably mistaken? On this issue, Kim's comments in "The myth of nonreductive materialism" are surprisingly apt: "Given all this, it's difficult to see what point there is in recognizing mentality as a feature of the world…. It allows mentality to exist, but this doesn't strike me as a form of existence worth having" (Kim 1989, p. 32) . Emergence is a different approach to nonreductive physicalism, and it argues for a deeper metaphysical distinction between experiential properties and physical properties. For this reason, emergence seems like a better candidate than supervenience to motivate and explain decades of effort to turn away from a purely biological model. Both Borrell-Carrio and Marcum have imagined, moreover, that emergence is the metaphysical foundation that holism actually has in mind (Borrell-Carrio et al. 2004; Marcum 2008a) . 5 Chalmers takes a general stand on this issue, however. When emergence does succeed in distinguishing itself from supervenience, he suggests, it continues to "require new fundamental laws" (Chalmers 1996, p. 129 ) so it collapses into naturalistic dualism. It may be that emergentists can distinguish their position from naturalistic dualism in a way that would still force a strong turn away from the BMM, but until we have an account of that kind, naturalistic dualism will have stronger prima facie plausibility when it comes to (3). Finally, based on problems that have resulted from the reductive strand of holism in practice, it will not be enough for medicine's metaphysical foundation to (1) reject reductionism, (2) reject Cartesian substance dualism, and (3) demand that we bring experiences into the scope of medical science. It must also (4) bring experiences into medical science in a way that retains their experiential character. If clinicians invite experiences into the exam room but they equate them with bodily properties in the end of the day, it seems we end up in practice with reductive holism-that is, with the conceptual confusions and public health threats that arise from the vagueness thesis. Once again aligning with Kim, it seems the realities of human health force us to get very clear about the difference between nonreductive physicalism and reductionism at the level of medical science. We know the supervenience physicalist will insist that experiential properties disappear in an accounting of the world at the fundamental level of physics, but we do not know what she has to say about experiential properties in an accounting at the level of medical science. It may be that supervenience can meet Kim's challenge, distinguishing itself from reductionism at the level of physics, and it may be that supervenience can do this in a way that manages to (4) retain the experiential nature of experiential states at the level of medical science. It remains unclear how supervenience could accomplish this in a way that will also accomplish (3), providing not merely an explanation for holism's current perspective, but a formative demand to reject a purely biological model of medical science. Though my defense of naturalistic dualism for medicine has been preliminary and provisional, evidence has established now that Engel's general conclusion was right, that medicine is more successful in accomplishing its scientific goals when it considers the complex role that experience plays in human health. Importantly, this is the case even if we continue to evaluate medicine's success in wholly biological terms (e.g. Gouin and Kiecolt-Glaser 2012; Sgambato et al. 2012) . While these successes arise from our ability to track the body's responses to the neurological correlates of experience, it is clear that exercise will lose its value if we fail to distinguish experiences from their neurological correlates. The ubiquitous medical catch-phrase, "mindbody integration", is helpful on this score. Incoherent as it is to imagine that we must do something in order to align the mental with the physical, the phrase aligns with Chalmers's expectation that science will ultimately make room for experience as experience. Medicine is thoroughly permeated with…philosophical questions which can, and do, masquerade as being simply empirical (Gold 1985, p. 664) . Holism is now "woven into the delivery of care at every level" (Greenberg 2017, p. 1) . In spite of the distinctly biological focus of evidence-based medicine in our time, the core ideas of holism remain central to medicine, psychiatry, bioethics, psy-chosomatic medicine, health policy, health law, health insurance, medical education, and diagnostic coding. Nearly 40 years after Engel originally articulated the BPSM, it continues to ground the foundational claims of the DSM, and WHO's working definition of health. Indeed, holism has become central to cultural understanding of what medicine aims for, to the point where medical marketing makes everyday promises of "integrated mind-body practice" and "whole person care". In all of these areas, holism's original philosophical confusions routinely lead to incoherence. The WHO, the APA, and the everyday practice of medicine cannot possibly "do away with the reductionism and mind-body dualism inherent in the biomedical model" (Easton 1982) because that goal is incoherent. Holism itself is not incoherent, however. Its usefulness for improving and promoting health has been verified again and again, in theory and in practice, to an extent that Engel likely could not have imagined. So it certainly is important for medicine, and all of its related professions, to aim for a holistic approach. To accomplish that goal, holism's terminological and conceptual confusions must be repaired. As a position that centrally stakes a claim in mind-body territory, holism requires a philosophically stable foundation. First, dualism has been inaccurately defined as "separation of mind and body". In reality, dualism is the view that we cannot give a wholly physical account of persons and their experiences, and so we must accept the fundamental existence of mind or mental properties. Second, ethical medical practice does not demand rejection of dualism. This idea runs deep in bioethics, but it is mistaken. Bioethics certainly does not aim to see patients as physical and only physical, and that is what it actually means to reject dualism. On the contrary, bioethics requires a metaphysical picture that will force us to distinguish experiential states from brain states in medical practice. 6 The only position that maintains this distinction at a fundamental scientific level is naturalistic dualism. Third, holism is an anti-reductionistic perspective, so it cannot aim to "eliminate the mind-brain dichotomy". Because holism's central insight marks the difference between the body and the person who experiences it, the pervasive strain of holism that sees it as reductive is confused. This point is particularly pressing for psychiatry, and it will be concerning for many, given the common sensibility that rejecting reduction means accepting Cartesian substance dualism. That sensibility is misguided. There are other options, and philosophers in our time have gone to considerable lengths to try to work them out. The fact is that holism's anti-reductionistic stance is well aligned with philosophy in our time, so medical and psychiatric professionals can openly reject reductionism without concern that doing so suggests they're philosophically uninformed. Fourth, the boundary between medical and psychiatric conditions is not intrinsically vague, or at least that is an idea that currently lacks support. Because the vagueness thesis arises from a reductionistic foundation, it is not supported by holism or by philosophy's general approach to mind and body at this time. Rooting out the vagueness thesis, however, or even tempering enthusiasm for it, is a massive undertak-ing-because giving it up means we must revise the foundational claims of the DSM. We must re-evaluate the areas of bioethics where we rely on the idea that medical illness and mental illness are indistinguishable in the end of the day. We must redefine the field, and the goals, of psychosomatic medicine. Most importantly, we must give up the notion that deliberate diagnostic vagueness is a good idea when patients might suffer from either psychiatric or medical conditions-but there is an immediate payoff on this last endeavor that makes sense of the whole project. For those with autoimmune diseases, rare disorders, and ME/CFS, and for the half of outpatients whose symptoms are undiagnosed, deliberate diagnostic vagueness at the mind-body line has led to substantial suffering and harm, and continues to do so at this time. Medicine, it turns out, is an arena where reductionism can have harmful human consequences. Finally, corrected holism has implications for philosophy in the armchair. To the extent that holism is the right view (and it's tough to dispute its scientific success), it appears that medical science cannot succeed with a reductive foundation. It appears, moreover, that supervenience physicalism and emergence face challenges in accommodating the changes that holism brings to medical science. I don't claim to have offered a substantial defense of naturalistic dualism as the only viable approach, but I think it's clear that the spirit of holism is captured by naturalistic dualism. We pursue the practice of medicine not because we have bodies, but because we experience them and we dread the day when that will no longer be the case. This is the core revelation that holism carries through the medical professions, and it does make a strong case for dualism. 
paper_id= fffa727d0acb374b3441ad3fe0bb41196c729195	title= Development and validation of the long covid symptom and impact tools, a set of patient-reported instruments constructed from patients' lived experience Running title: the Long covid symptom and impact tools	authors= Viet-Thi  Tran;Caroline  Riveros;Bérangère  Clepier;Moïse  Desvarieux;Camille  Collet;Youri  Yordanov;Philippe  Ravaud;Viet-Thi  Tran -Md;	abstract= To develop and validate patient-reported instruments, based on patients' lived experiences, for monitoring the symptoms and impact of long covid. The long covid Symptom and Impact Tools (ST and IT) were constructed from the answers to a survey with open-ended questions to 492 patients with long covid. Validation of the tools involved adult patients with suspected or confirmed covid-19 and symptoms extending over three weeks after onset. Construct validity was assessed by examining the relations of the ST and IT scores with health related quality of life (EQ-5D-5L), function (PCFS, post-covid functional scale), and perceived health (MYMOP2). Reliability was determined by a testretest. The "patient acceptable symptomatic state" (PASS) was determined by the percentile method. Validation involved 1022 participants (55% with confirmed covid-19, 79% female and 12.5% hospitalised for covid-19). The long covid ST and IT scores were strongly correlated with the EQ-5D-5L (r s = -0.45 and r s = -0.59 respectively), the PCFS (r s = -0.39 and r s = -0.55), and the MYMOP2 (r s = -0.40 and r s = -0.59). Reproducibility was excellent with an interclass correlation coefficient of 0.83 (95% confidence interval 0.80 to 0.86) for the ST score and 0.84 (0.80 to 0.87) for the IT score. In total, 793 (77.5%) patients reported an unacceptable symptomatic state, thereby setting the PASS for the long covid IT score at 30 (28 to 33). The long covid ST and IT tools, constructed from patients' lived experiences, provide the first validated and reliable instruments for monitoring the symptoms and impact of long covid. All rights reserved. No reuse allowed without permission. We developed the long covid Symptom and Impact Tools (ST and IT) from the experiences of 492 patients, captured during a survey with open-ended questions, and assessed their validity and reliability in a sample of 1022 patients with long covid. 	body_text= The development and validation of these instruments applied a two-stage method. 23 The study was nested within ComPaRe (Communauté de Patients pour la Recherche, www.compare.aphp.fr), an e-cohort of patients with chronic conditions who volunteer to participate in research by regularly answering patient reported outcome measures (PROMs) and patient reported experience measures (PREMs) . 24 Because of the similarities between long covid and chronic conditions, recruitment in ComPaRe was extended to patients reporting symptoms of covid-19 lasting more than three weeks past initial onset. All participants provide electronic consent before participating. The Institutional Review Board of Hôtel-Dieu Hospital, Paris, approved ComPaRe (IRB: 0008367). We sought to develop the contents of the instruments from the patients' lived experiences of long covid. We invited adult patients in the ComPaRe cohort who reported a covid-19 infection (laboratory confirmed or not) with symptoms persisting more than three weeks past the initial infection to complete an online survey of open-ended questions that asked them to describe in detail the symptoms, potential triggers, and the effects of long covid on their lives. The survey, which took place from October 14 to November 29, 2020, used broad questions to avoid directing patients about long covid's consequences (Supplementary material 1). Participants were recruited through a social media and media campaign and by partner patient associations. Participants who had participated were invited to encourage people in their entourage who may have had a COVID-19 infection, to participate, by a 'snowball' sampling method 25 . Two investigators (VTT and CR) used an inductive multiple-round qualitative content analysis to examine the participants' responses. First, they independently coded the responses (i.e., they identified within each response any expression found in the text expressing a symptom or impact of long covid and assigned it a code). Answers related to the acute phase of covid-19 (that is, symptoms during the first three weeks of infection) were removed from analysis. For example, when a participant stated that "I had a cough and fever for one week All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. and joint pain for 7 months", we considered only "joint pain" to be a potential long covid symptom. If the response did not specify a timeframe, all reported symptoms were kept in the analysis. Next, the investigators grouped the identified codes into a standardised set of symptoms and consequences of long covid, based on their medical knowledge and the literature. 4 18 Analysis continued on until a comprehensive list of symptoms was obtained, i.e., until data saturation was reached. The point of data saturation was assessed with a mathematical model to predict the number of new symptoms that could be identified by adding new participants to the study. 26 In a third step, they reduced the number of symptoms by: 1) grouping closely related symptoms (eg, loss of taste and changes in taste were grouped as loss/change of taste) and 2) eliminating those expressed by less than 2% of participants in the open-ended survey. The two investigators used the results of the open-ended survey to develop a preliminary patient reported instrument for monitoring long covid's current symptoms and impact. We chose to split the instrument into two independent parts. The first part aimed at assessing patients' long covid-related symptoms over the previous 30 days with the checklist of symptoms identified above, a list we named the long covid Symptom Tool (ST). The second part aimed at assessing the disease's impact on their lives over the past 30 days from the responses to six questions constructed from themes identified and examples provided by patients in their open-ended answers. Items used a numeric scale ranging from 0 (no impact) to 10 (maximal impact). This second instrument was called the long covid Impact Tool (IT). These two instruments were reappraised by the two author patients (BC and CC) for content validity, clarity, and wording during a telephone interview with the main investigator (VTT) that used the double interview method. 27 A second sample of adult patients reporting a SARS-CoV 2 infection (laboratory confirmed or not) with symptoms persisting three weeks past the initial infection served as the validation sample. Patients who had participated in the first stage could also participate in the validation stage. In addition, we increased our sample through a call for participation on the "TousAntiCOVID" app, the official French contact tracing app used by 12 million people. As our instrument intends to evaluate patients' current symptoms and their impact, only patients reporting at least one symptom during the previous 30 days could participate in the validation study. Each tool was validated independently. The long covid ST score was defined as the number of symptoms reported by patients, and the long covid IT score as the sum of the responses to each impact question. Construct validity was determined by confirming several theories or conceptions (ie, constructs) about long covid. First, we hypothesised that both the ST and IT scores would be negatively correlated with patients' quality of life, which was evaluated with the EuroQol five-dimension five-level (EQ-5D-5L) questionnaire and the EuroQol visual analogue scale (EQ-VAS). 28 Statistical analyses were performed with R v. 3.6.3. All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint this version posted March 20, 2021. The tools were developed from qualitative data about patients' lived experiences, captured during a survey with open-ended questions. The qualitative results and the implementation of the items in the patient reported tools were discussed in detail with two patients meeting the criteria for long covid. They also participated in the critical revision of the manuscript and are co-authors of the paper (BC and CC). Step 1 Participants' open-ended answers represented a corpus of 116 657 words. Inductive content analysis of patients' answers was stopped after the analysis of 380 answers and the definition of 53 symptoms of long covid because models showed that 99.99% of all possible symptoms had been identified in both the total sample and in the subgroups of confirmed and suspected cases (Supplementary material 3) . The list of symptoms was further organised in 10 categories: general symptoms (n=11), thorax (n=6), digestive (n=3), ear/nose/throat (n=5), eyes (n=3), genitourinary (n=2), hair and skin (n=4), musculoskeletal (n=4), neurological (n=11), and blood and lymph circulation (n=4) symptoms (Supplementary material 4) . The analysis of patients' open-ended answers also enabled us to identify six aspects of patients' lives that were affected by long covid: 1) difficulties in performing personal activities such as driving, shopping, or doing household chores; 2) difficulties in their professional lives; 3) difficulties in fulfilling their family roles and/or feeling that they're a burden to their family or friends; 4) difficulties related to social activities; 5) morale, fear of the future and of not returning to normal, and 6) negative impact on relationships with care providers. Examples of what patients wrote are provided in Supplementary material 5. All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint this version posted March 20, 2021. ; https://doi.org/10.1101 https://doi.org/10. /2021 Thus the list of symptoms and the themes and examples related to the impact of long covid on patients' lives were used to develop preliminary versions of the long covid tools, which were further reviewed and improved by two patients. The Box presents the items for both tools. Step 2: Validation of the long covid symptom and impact tools From November 30, 2020 , to February 15, 2021 , 1360 ComPaRe participants reported they had had long covid. In all, 1022 had experienced at least one covid-19-related symptom in the 30 days before responding to the survey and were included in the validation sample. Among them, 418 (40%) had participated in the development sample, in the first stage. Table 1 presents the characteristics of all patients included in the validation sample. Their median age was 45 years (IQR 37 to 52), and 817 (79.9%) were women. Overall, 564 (55%) had tested positive for SARS-CoV2 by PCR swab or serological assay and were considered confirmed cases. Among all cases, 128 (12.5%) patients had been hospitalised for covid-19, and 17 (1.7%) had been admitted to an ICU. The median time since symptom onset was 263 days (IQR 123 to 289), with a bimodal distribution corresponding to the first (260 to 300 days) and second waves (50 to 100 days) in France. The median long covid ST score was 16 (IQR 11 to 23). This score reports the number of symptoms patients experienced over the last 30 days and has a theoretical range from 0 (no symptoms) to 53 (all symptoms identified during step 1). The symptoms most frequently reported were fatigue (n=899), headaches (n=709), difficulties concentrating/mental fog (n=650), sleep disorders (n=603), and dyspnoea (n=570) (Figure 1) . Symptom frequency was similar in confirmed and suspected cases except for "change/loss of taste" and "change/loss of smell", both more frequent in confirmed cases. Besides symptoms, 824 patients (80.6%) reported a relapsing-remitting disease course with daily (n=285, 34.6%), weekly (n=320, 38.8%), or less than weekly (n=219, 26.6%) relapses. Patients' median long covid IT score was 36 (IQR 24 to 45). This score has a theoretical range of 0 (no impact) to 60 (maximum impact) and represents the sum of item scores for the six questions related to the disease's impact on their personal activities, family lives, professional lives, social lives, their morale, and their relationships with care providers. In all, 265 (26%) patients rated the impact of the disease on their work lives at 10 (out of 10). In general, the disease's impact on patients' lives was similar for patients with confirmed and suspected All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint this version posted March 20, 2021. ; https://doi.org/10.1101 https://doi.org/10. /2021 infections, except for the item about its effect on their relationships with care givers, which was rated higher in suspected cases (Supplementary material 6) . The long covid ST and IT scores were highly correlated (r s =0.54, p<0.0001) and did not seem to differ by time from symptom onset (Supplementary material 7) . Overall, 970 (95%) patients completed the EQ-5D-5L quality of life questionnaire, the EQ-VAS scale, and the PCFS functional status scale, while 746 (73%) answered the MYMOP2 questionnaire. For health-related quality of life, the median EQ-5D-5L and EQ-VAS values in our sample were respectively 0.83 (IQR 0.60 to 0.89) and 51 (IQR 40 to 70). As hypothesised, we found that the long covid ST score was moderately and negatively correlated with the EQ-5D-5L questionnaire (r s = -0.45, p<0.0001) and the EQ-VAS (r s = -0.39, p<0.0001), while the long covid IT score had a strongly negative correlation with the EQ-5D-5L questionnaire (r s = -0.59, p<0.0001) and the EQ-VAS (r s = -0.54, p<0.0001). For functional assessments, 469 (48%) patients in our sample indicated that they were no longer able to perform some activities at home or at work by themselves (grade 3 or 4 of the PCFS), and 371 (38%) reported that they had had to reduce some of their activities (grade 2 of the PCFS). We found a moderate correlation between the long covid ST score and the PCFS score (r s = -0.39, p<0.0001) and a high correlation between the long covid IT score and the PCFS score (r s = -0.55, p<0.0001). Similarly, for patients' perceived health state, we found that the long covid ST score was moderately correlated with the MYMOP2 score (r s = -0.40, p<0.0001) while the long covid IT score was highly correlated with it (r s = -0.59, p<0.0001) ( Table 2) . Of the 351 patients invited to complete the long covid ST and IT twice for the test-retest, 235 (67%) did so. The symptom score had an ICC of 0.83 (95% CI 0.80 to 0.86), with Bland and Altman plots showed a mean difference of 0.8 (95% limits of agreement, -14 and 16). The impact score's ICC was 0.84 (95% CI 0.80 to 0.87), with Bland and Altman plots showing a mean difference of 0.5 (95% limits of agreement, -11 to 12 (Figure 2) . Finally, Cronbach's alpha was 0.89 (95% CI, 0.88 to 0.90) for the long covid ST score and 0.86 (95% CI, 0.85 to 0.88) for the IT score. All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint this version posted March 20, 2021. ; Only 229 (22.4%) patients reported an acceptable symptomatic state, while 793 (77.5%) patients reported it was unacceptable. Long covid's impact was considered acceptable for >75% of patients with IT scores < 30 (95% CI, 28 to 33), that is, below 50% of the maximum score. (Figure 3 ). In this study, we present the long covid ST and IT. They are the first set of validated and reliable instruments for monitoring this disease's symptoms and impact and assess respectively 53 long covid symptoms and 6 dimensions of patients' lives that it can affect. Until now, two issues have limited the assessment of long covid. First, the sets of symptoms used have been both unreliable and defined from the care givers' perspective. The number of symptoms assessed in studies has ranged from 8 to 205 manifestations, the latter also including triggers and laboratory test result anomalies 3 7 8 19 38 and usually failed to cover difficulties in concentration, even though several studies report that this symptom is frequent in long covid. 8 38 The long covid ST and IT were built from patients' lived experiences, captured during a survey with open-ended questions. Our tools thus provide a comprehensive picture of the disease. By using mathematical models to assess data saturation, we ensured that our instruments covered all manifestations and impacts of long covid relevant to patients. Both of our tools showed robust psychometric properties. Construct validity was demonstrated by the high correlations with patients' quality of life, functional status, and perceived health state. Reliability was excellent with an ICC ≥ 0.8 during the test-retest. Finally, we defined a PASS (Patient Acceptable Symptomatic State) for long covid that provides thresholds to be met by pharmacological and/or non-pharmacological interventions aimed at reducing its impact on patients' lives. Second, most studies have focused on counting symptoms, but have failed to assess the disease's impact on patients' lives. 4 5 7 18 19 The long covid IT fills this gap by providing validated, and reliable questions for assessing its burden. This is critical, in view of the major impact that long covid has on the quality of life and functioning of the patients in our ComPaRe cohort. The health related quality of life for patients with long covid, assessed with the EQ-VAS, was on average 40% lower than in the reference general population, 39 and similar to that of patients living with epilepsy or multiple sclerosis. 40 Half reported impaired All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint this version posted March 20, 2021. ; https://doi.org/10.1101/2021.03.18.21253903 doi: medRxiv preprint functioning, that is, they were no longer able to perform some activities unassisted at home or at work (PCFS grade 3 or 4), a figure similar to that described in the study by Davis et al. 38 The symptoms captured in the long covid ST overlap with those presented in sections 2.5 and 2.6 of the recent case report form issued by the WHO for the follow-up of patients after their acute illness. The WHO form covers 45 symptoms of long Covid (all included in our instrument) and includes questions on patients' functioning (e.g., washing themselves, dressing, ability to join in community activities, etc.). 22 Our set of instruments goes further by inquiring about patients' perceptions of the disease's impact on their lives, roles, and relationships, beyond assessing whether they can or cannot perform specific activities. Moreover, besides collecting information, our instruments provide a scoring method producing valid and reproducible measurements of the disease-and patient-relevant cut-offs to interpret these measures. In all, the overlap of the long covid ST and IT with the WHO case report form strengthens the content validity of our instruments and provides a glimpse of the potential validity and reliability of the WHO form. This study has limitations. Generalisation of the estimates obtained in this study must be cautious. Our study recruited volunteer patients who reported persistent symptoms. This might have selected younger, more educated, and more often female patients willing to share their experiences with others and/or with more severe conditions. Nonetheless, we were able to involve and validate our instrument in a diverse sample of participants by using a broad media campaign and a call for participation on the "TousAntiCOVID" app. 41 Second, our study may not be appropriate for examining the longitudinal impact of the disease. Although we recruited patients with various times from symptom onset, the assessment of symptoms and impact was cross-sectional. Future work following up patients with standardised, validated tools is required to investigate the duration of the disease and the course of the symptoms over time. Finally, in view of the limited number of patients who were hospitalised in ICUs in our study, we cannot confirm that our tool is suitable for measuring the consequences of ICU on patients. According to the United Kingdom Office for National Statistics, 10% of people who were infected with SARS-CoV-2 still experience symptoms after three months, including those whose acute infection was asymptomatic. 42 With about 100 million cases of COVID-19 worldwide, long covid may well affect millions of patients. The severe burden of illness and the impairment of quality of life associated with long covid call for urgent research to understand this disease and to develop interventions to help patients. Using scientific, valid, All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint this version posted March 20, 2021. ; https://doi.org/10. 1101 /2021 and reliable measurements in these initiatives will enable the comparison and combination of study results. The long covid Symptom and Impact Tools, constructed from patients' lived experience, provide the first validated, reliable instruments for monitoring the symptoms and impact of long covid. It may help the development of treatment strategies to mitigate the considerable burden of this disease. Generated the idea: VTT and PR, Conceived and designed the experiments: VTT, and PR, Collected data: VTT, YY and PR. Analysed data: VTT, CR, BC, CC; Wrote the first draft of the manuscript: VTT, Contributed to the writing of the manuscript: VTT, CR, BC, CC, YY and PR; ICMJE criteria for authorship read and met: VTT, CR, BC, CC, YY and PR. Agree with manuscript results and conclusions: VTT, CR, BC, CC, YY and PR. VTT is the guarantor, had full access to the data in the study, and takes responsibility for the integrity of the data and the accuracy of the data analysis. VTT affirms that the manuscript is an honest, accurate, and transparent account of the study being reported; that no important aspects of the study have been omitted. There were no discrepancies from the study as originally planned. The results of your study will be sent to research participants and partner patients' associations, accordingly to the protocol of ComPaRe. In addition, we plan a press release for the study. All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.  The authors received no specific funding for this work. The authors declare no competing interests and no financial associations that may be relevant or seen as relevant to the submitted manuscript. The authors have no association with commercial entities that could be viewed as having an interest in the general area of the submitted manuscript. All data collected for the study, including individual participant data and a data dictionary is available for research purposes, under the rules of the ComPaRe e-cohort (https://compare.aphp.fr/) (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint this version posted March 20, 2021. ; B  o  o  n  G  ,  B  a  r  c  o  S  ,  e  t  a  l  .  T  h  e  P  o  s  t  -C  O  V  I  D  -1  9  F  u  n  c  t  i  o  n  a  l  S  t  a  t  u  s  s  c  a  l  e  :  a  t  o  o  l  t  o  m  e  a  s  u  r  e  f  u  n  c  t  i  o  n  a  l  s  t  a  t  u  s  o  v  e  r  t  i  m  e  a  f  t  e  r  C  O  V  I  D  -1  9  .   E  u  r  R  e  s  p  i  r  J   2  0  2  0  ;  5  6  (  1  )  d  o  i  :  1  0  .  1  1  8  3  /  1  3  9  9  3  0  0  3  .  0  1  4  9  4  -2  0  2  0  [  p  u  b  l  i  s  h  e  d  O  n  l  i  n  e  F  i  r  s  t  :  2  0  2  0  /  0  5  /  1  4  ]  3  0  .  M  a  c  h  a  d  o  F  V  C  ,  M  e  y  s  R  ,  D  e  l  b  r  e  s  s  i  n  e  J  M  ,  e  t  a  l  .  C  o  n  s  t  r  u  c  t  v  a  l  i  d  i  t  y  o  f  t  h  e  P  o  s  t  -C  O  V  I  D  -1  9  F  u  n  c  t  i  o  n  a  l  S  t  a  t  u  s  S  c  a  l  e  i  n  a  d  u  l  t  s  u  b  j  e  c  t  s  w  i  t  h  C  O  V  I  D  -1  9 .  • Box: Items of the long covid Symptom and Impact Tool (long covid ST and IT) • Table 1 : Characteristics of patients included in the validation step (n=1022) • (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint this version posted March 20, 2021. ; https://doi.org/10.1101 https://doi.org/10. /2021 Assess the impact of your illness during the last 30 days… • On your personal activities (not being able to do personal activities, driving a vehicle) * • On your family life (feeling isolated or as a burden to others, having to ask for help, not being able to do household chores or take care of your family) • On your professional life (having to stop working, being unable to work as well as before) • On your social life (avoiding relationships because of the way people look at you, coping with others not taking your disease seriously, being afraid of infecting others) • On your morale/mood (low morale, feeling that life is passing you by, fearing of the future or not recovering) • On your relationship with caregivers (guilt, taking the illness seriously, lack of response, complexity of the care pathway) All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. MYMOP2 score* r s = 0.40 <0.0001 r s = 0.59 <0.0001 Relations between the scores and continuous variables were explored with Spearman's correlation coefficient. Correlation coefficients range from -1 (perfect negative correlation) to 1 (perfect positive correlation) with 0 indicating no correlation. *Higher EQ5D scores indicate better quality of life; higher PCFS scores indicate more severe functional limitations; higher MYMOP2 scores indicate worse general health. All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity.  
paper_id= fffaed7e9353b7df6c4ca8f66b62e117013cb86d	title= Dengue Virus Glycosylation: What Do We Know?	authors= Akio  Adachi;Sylvie  Alonso;Sally S L Yap;Terry  Nguyen-Khuong;Pauline M Rudd;	abstract= In many infectious diseases caused by either viruses or bacteria, pathogen glycoproteins play important roles during the infection cycle, ranging from entry to successful intracellular replication and host immune evasion. Dengue is no exception. Dengue virus glycoproteins, envelope protein (E) and non-structural protein 1 (NS1) are two popular sub-unit vaccine candidates. E protein on the virion surface is the major target of neutralizing antibodies. NS1 which is secreted during DENV infection has been shown to induce a variety of host responses through its binding to several host factors. However, despite their critical role in disease and protection, the glycosylated variants of these two proteins and their biological importance have remained understudied. In this review, we seek to provide a comprehensive summary of the current knowledge on protein glycosylation in DENV, and its role in virus biogenesis, host cell receptor interaction and disease pathogenesis. Dengue Dengue (DEN) is an emerging arthropod-borne infectious disease which is caused by DENV. According to the World Health Organization (WHO) (2017), DEN cases have continually increased in recent decades. An estimation of DEN infections worldwide has indicated up to 50-100 million cases per year (WHO, 2017). The virus is transmitted primarily by female Aedes aegypti mosquitoes in tropical and subtropical regions. The spread of DEN in non-tropical areas has been associated to the transmission by the secondary vector, A. albopictus mosquito which is able to withstand winter temperature (Gould et al., 2010) . Four serotypes of DENV (DENV1-4) have been identified to date and co-circulation of these serotypes has been reported in Asia, Africa, and America (Guzman et al., 2010) . 	body_text= Most DENV infections are asymptomatic or remain as mild febrile illness. A classical DEN fever is diagnosed when the patient shows self-limiting high fever, headache, and muscle/joint pain 3-14 days after a mosquito bite. A small proportion of DEN patients may develop DEN hemorrhagic fever and/or DEN shock syndrome (DHF/DSS) which are life-threatening. The clinical manifestations of DHF/DSS include hemorrhagic fever, vascular permeability and plasma leakage, thrombocytopenia and circulatory failure in DSS. To date, there is no specific treatment for DEN and no licensed anti-DENV drug is available. For severe DEN cases, clinical complications are managed by supportive therapy to avoid mortality. The progression to severe DEN (DHF/DSS) has been linked to a phenomenon known as ADE of infection (Halstead, 2014) . The ADE hypothesis postulates that during a secondary heterologous DENV infection, preexisting anti-DENV antibodies bind to but fail to neutralize the virus, and promote increased uptake of sub-neutralized virions by Fcgamma-receptor bearing cells such as DC, macrophages, and monocytes (Kliks et al., 1989; Boonnak et al., 2008) . In addition, ligation of Fc receptor stimulates production of Interleukin (IL)-10 which in turn suppresses the cellular anti-viral response (Suhrbier and La Linn, 2003) . These events lead to increased viral loads which are believed to correlate with disease severity (Vaughn et al., 2000) . To reduce DEN morbidity and eventually eliminate the disease, an effective vaccine is urgently needed. However, the development of DEN vaccine has been greatly hampered by the potential risk of ADE. The only licensed DEN vaccine (CYD-TDV) available is a tetravalent, recombinant, live attenuated DEN vaccine developed by Sanofi Pasteur (Guy et al., 2015) . The vaccine has shown varied efficacy against different serotypes and in different age groups, with safety issues in children below 9 years of age (Capeding et al., 2014; Villar et al., 2015) . In addition, large scale efficacy studies have suggested that this vaccine works best in people with pre-existing DENV immunity (Capeding et al., 2014) . Thus, WHO recommendations have limited the use of the CYD-TDV vaccine in geographical settings with high DEN burden and in age group 9-45 years old (WHO, 2017) . Clearly, while this first-in-human tetravalent DEN vaccine will certainly provide a wealth of knowledge and improve our understanding of immune correlates of protection, a better vaccine is needed to protect the 3.9 billion people that are at risk of DEN infection. Several promising vaccine candidates are currently under development; some have entered the clinical pipeline [reviewed in (Schmitz et al., 2011)] . It is hoped that they will address the shortcomings of the CYD-TDV vaccine. DENV belongs to the family Flaviviridae of which the members are well known as human pathogens, including WNV, Zika virus, Yellow fever virus, Tick-borne encephalitis virus, JEV, and Hepatitis C virus (HVC). They are enveloped viruses with positive sense, single stranded RNA and many of them are arthropod-borne viruses. Among all flaviviruses, DENV has the highest impact on global disease burden. The virus particle is about 50 nm in size and the RNA genome (∼10.7 kb) is encapsulated by a protein shell which consists of three structural proteins, namely capsid (C), envelope (E), and (pre)membrane protein (prM/M) (Kuhn et al., 2002) . In order to establish infection, DENV first binds to the host cell receptors via E proteins on the cell surface. The ligand-receptor interaction initiates uptake of the virion through receptor-mediated endocytosis (Acosta et al., 2014) . Inside the acidic late endosome, membrane fusion occurs as the virion envelope fuses with the endosomal membrane (Allison et al., 1995; Modis et al., 2004) , followed by uncoating of the nucleocapsid and then release of the viral RNA into the cytoplasm. The RNA genome of DENV is translated into a single polyprotein by host ribosomes and is made of three structural (C, E, prM/M) and seven non-structural (NS) (NS1, NS2A/B, NS3, NS4A/B, NS5) proteins. The polyprotein is then cleaved by host and viral proteases to release individual viral proteins (Acosta et al., 2014) . The viral genome replication process within the host cell is mainly driven by the NS proteins. NS1 anchors the replication complex to the ER membrane and interacts physically with NS4B (Youn et al., 2012; Muller and Young, 2013) . NS2A is responsible for viral RNA synthesis and virion assembly (Xie et al., 2015) . NS3 functions as a serine protease, RNA helicase and nucleotide triphosphatase/RNA triphosphatase, and its protease activity is dependent on the cofactor NS2B (Falgout et al., 1991) . NS4A has been reported to induce membrane rearrangement within the host cell, thereby assisting the formation of replication vesicles (Miller et al., 2007) . NS5 is a multifunctional enzyme with a methyltransferase (MTase superfamily) domain and a RNAdependent RNA polymerase domain (Acosta et al., 2014) . During virion assembly, the newly synthesized viral RNA interacts with C proteins to form the nucleocapsid. A spiky immature virion is formed when E and prM proteins encounter the nucleocapsid Acosta et al., 2014) . The maturation process takes place in the trans-Golgi network where prM is cleaved by host furin to generate a smooth surfaced mature virion (Stadler et al., 1997; Yu et al., 2008) , which is released into the extracellular environment through the secretory pathway. Glycosylation is the post-translational modification of biomolecules such as proteins or lipids through the enzymatic attachment of complex oligosaccharide structures to the peptide backbone or lipid anchor (Varki et al., 2015) . Over 70% of the eukaryotic proteome is glycosylated (Dell et al., 2010) . The range of complexity of these structures is reflected by their covalent attachment to the protein, the monosaccharide composition of the glycan and combinations of anomeric ring linkages between these monosaccharides. This complexity affects the branching, antennae and topology of the glycan structures, which translates to the overall tertiary and quaternary structure of the glycoprotein. There are two types of protein glycosylation distinguished by their site of attachment on the protein backbone; N-linked glycosylation where the glycan is covalently attached to the asparagine (N) (consensus motif; NxS/T except where x is a proline); and O-linked glycosylation where the glycan is linked to the oxygen from some serine (S) or threonine (T) residues of the protein backbone (Chandler et al., 2013; Varki et al., 2015) . N-linked glycans share a common chitobiose core structure. Furthermore, N-linked structures fall under three different classes: (1) high mannose, where the non-reducing composition of the glycans are dominated by mannose sugars that extend from the core, (2) complex, where the branching and extension of the glycans from the core is initiated by N-acetyl glucosaminyl transferases; and (3) hybrid structures where the core is extended by both high mannose arm and a complex structure on the other arm (Chandler et al., 2013; Varki et al., 2015) (Figure 1) . Such complexity can be found with the isomerism/anomercity of a sugar. Whilst two glycans may have the same composition, the differences of their isomeric linkages can affect the selectivity of their host receptors and thus biology. For example, in the context of avian influenza, the haemagglutinin specifically and exclusively recognizes 2,3-linked sialic acids that are found in the avian host. This is in contrast to 2,6 linked sialic acid normally found in the human host receptors with very low 2,3-linked sialic acid expressed in the lower respiratory tract. Cross reaction between avian H5N1 and such sialic acids caused the recent human epidemics (Shinya et al., 2006; Walther et al., 2013) . Glycosylation is a highly organized process that involves a network of glycotransferases and glycosidases in the ER-Golgi complex that enzymatically synthesize the glycan as well as trim down the structures so as to achieve the refined structure. In mammalian cells, N-glycosylation takes place predominantly within the ER. Briefly, the initial stages of glycosylation involve the enzymatic synthesis of dolicholphosphate-NAcGlcNAc 2 Man 9 Glc 3 which usually takes place across the membrane of the ER. This involves firstly the enzymatic synthesis of dolichol-P-P-GlcNAc 2 Man 5 , before this is "flipped" across the membrane into the ER lumen where further monosaccharides are added (Shi and Jarvis, 2007; Dell et al., 2010) . The NAcGlcNAc 2 Man 9 Glc 3 is transferred to the appropriate NxS/T motif of the protein via the oligosaccharyltransferase as the incipient protein is being translated. As the N-glycans transition through the ER-Golgi complex, a series of glycosidases trim down the mannose residues before glycotransferases present in the Golgi extend the antennae of the glycans to produce larger hybrid or complex structures (Varki et al., 2015) . In contrast to N-glycosylation, O-glycosylation occurs entirely in the Golgi apparatus. It does not involve any glyco-lipid intermediates and no glycosidases appear to be involved in their synthesis and processing. Within the insect kingdom, glycosylation is far simpler yet interestingly insect are able to produce elaborate protein glycosylation in a restricted fashion compared to glycosylation of higher eukaryotes (Rendić et al., 2008) . Most our knowledge in insect glycosylation was derived from studies performed on Drosophila melanogaster and baculoviral-insect systems. In fact, the bulk of mammalian glycoproteins expressed and/or purified in insect cells have used cell lines from Spodoptera frugiperda (sf9, sf21) or Trichoplusia ni (High five) (Rendić et al., 2008) . Glycoproteins derived from these insect cell systems display glycans that contain predominantly high mannose type structures. However, the long belief that these high mannose and paucimannosidic N-linked structures are the dominant forms in insect-cell derived glycoproteins has been recently challenged by the advent of high resolution glyco-analytical tools which were able to identify glyco-epitopes such as [alpha]1-3 fucosylation (Hsu et al., 1997; Takahashi et al., 1999; Rudd et al., 2000) and double core fucosylated structures ([alpha]1-3 and [alpha]1-6 fucosylation on the core GlcNAc) (Staudacher et al., 1992; Rendic et al., 2006) . There were also reports of the extension of the [alpha]1,3-arm of the chitobiose core as opposed to the [alpha]1-6 arm extension found in most mammalian cell lines (Kubelka et al., 1993) . Higher complex N-glycans can be found in many insectderived N-linked glycoproteins, however, if grown in serum free media, lysed cell extracts from sf9 and High five cells do lack the nucleotide donors for sialic acid (CMP-NeuAc) (Rendić et al., 2008) . Further to this, the presence of a truncated trimannosyl N-glycan with an [alpha]1,6-linked fucose was reported (Shi and Jarvis, 2007; Rendić et al., 2008) . Extensive work has shown that such structures are the result of the action of an endogenous hexosaminidase specific for the NAcGlcNAc[beta]1,2-Man structure rather than low activity of the [beta]-1, NAc 2 GlcNAc Transferase II responsible for the extension of the mannose arms of complex structures (Kubelka et al., 1994) . Studies performed with mosquito cell lines A. albopictus and A. aegypti showed that glycoproteins produced in these cell lines display predominantly high mannose and pauci-mannosidic structures (Hsieh and Robbins, 1984) . Interestingly, within these initial experiments, the presence of mannosidase-resistant structures was observed (Rhomberg et al., 2006) . Glycomics as a field has experienced a significant maturation from low to high resolution analysis. In the past decades, scientists have mostly relied on digestive enzymes (Johnson et al., 1994; Mondotte et al., 2007; Hacker et al., 2009) , chromatography (Johnson et al., 1994) , lectin-binding assay (Johnson et al., 1994; Hacker et al., 2009) and radioactive labeling (Smith and Wright, 1985) to study the glycan structure on glycoproteins. Enzymatic digestion by Endo H and peptide:N-glycosidase (PNGase F) remains the most popular method due to its simplicity and allows the investigator to determine whether the glycan structure is asparagine linked (N-linked). In this approach, purified glycoprotein is subjected to specific enzymatic digestion prior to separation on SDS-PAGE. After cleavage of the attached oligosaccharide chains, the digested protein migrates ahead of the undigested form due to a lower molecular weight. Enzymatic digestion of glycoproteins reveals, however, relatively little information on the glycan structure. Over the recent decade, glycan analysis has dramatically improved through developments in fluorescent labeling, LC and mass spectrometry. There is an abundance of techniques such as LC, CE and mass spectrometry which are available for glycomic analysis. For LC and CE, the field has benefited from labels such as 2-AB, 2-AA (LC separation) and 9-Aminopyrene-1,4,5-trisulfonic acid (CE separation), whereby glycans are tagged at a glycan's reducing end and the identity of these glycans is assigned based on their retention time behavior across a HILIC or CE, respectively (Rudd and Dwek, 1997; Callewaert et al., 2001) . Approaches coupling fluorescence with mass spectrometry (FLR-MS) have helped increase the efficiency of glycomic approaches. In such a platform, the retention time and fluorescence are usually coupled with mass detection to add further confirmation (Houel et al., 2014; Zhang et al., 2016) . The use of exoglycosidase arrays adds further confidence to a glycan's structure elucidation and can help to identify co-eluting glycan species (Marino et al., 2010) . The evolving development of glycan labeling has meant that newer, faster labeling and highly sensitive labels such as procainamide or Rapidfluor Mass Spectrometry (RFMS) label can increase the throughput and efficiency of a glycomic analysis. Various modes of mass spectrometry have been applied to the analysis of released glycans. The most common are MALDI and electrospray ionization. MALDI is a straight-forward method that often requires methods such as permethylation and esterification to increase signal intensity and stabilize labile glycans that contain sialic acids. Electrospray ionization involves a milder desolvation technique and coupled with LC methods and further fragmentation of the molecule, provides high resolution techniques to qualitatively characterize a glycome (Nguyen-Khuong et al., 2015) . Detection of glycan fragments which result from fragmentation along the glycosidic bonds (detected in positive mode) and cross-ring (detected in negative mode) (Harvey et al., 2004; Everest-Dass et al., 2012) help to understand the composition and topology of the glycan without the need to adulterate the glycan through derivatization. Glycoproteomics allows investigators to understand the degree of glycosylation on various sites of the glycoprotein. The platform is adapted from proteomics and as such relies heavily upon mass spectrometry and substantial data analysis. Whilst most glycoproteomic methodologies follow similar approaches to proteomics such as trypsin digestion and analysis, key to any glycoproteomics method is the enrichment of glycopeptides (Mysling et al., 2010; Kolarich et al., 2012) . This is important to reduce the ion suppression from peptide mass spectrometry signals without enrichment. This can be performed via HILIC chromatography, in which the enrichment is centered upon exploiting a glycan's hydrophilicity. Fragmentation data is vital to glycopeptide identification and data analysis must be able to exploit the information which can come from fragmentation modes such as collisionally induced dissociation (CID), high collisional dissociation or electron transfer dissociation/electron transfer high collisional dissociation (ETD/EtHCD) available to the investigator (Scott et al., 2011; Thaysen-Andersen et al., 2016; Stavenhagen et al., 2017) . Depending on the strength of fragmentation, information such as glycan composition, site of attachment and peptide backbone are all able to be divulged from a single spectrum (Yang et al., 2016) . Glycans either directly or indirectly have diverse biological functions which span but are not limited to inflammation, immunology, infectious diseases, metabolism, embryogenesis, cancer biology and neurodegeneration. At the protein level, glycosylation is responsible for correct protein folding/structure, protein trafficking and stability, receptor/ligand recognition as well as increasing its half-life in the blood stream (Park et al., 2005; Bork et al., 2009; Sumer-Bayraktar et al., 2011; Hart, 2013; Palmisano et al., 2013) . At the cellular level, complex sugar structures modulate receptor functions and thus are integral to regulate normal cell-cell, cell-substrate communication and adhesion (Vercoutter-Edouart et al., 2008; Varki et al., 2015) . Glycosylation disorders can adversely affect immunity and cancer development. From an immunological perspective, all living cells are covered by a dense glycocalyx and indeed, pathogens and foreign objects must deal with this complex forest of cell surface glycoconjugates upon entering the host (Pickles et al., 2000; Rodrigues et al., 2015) . Viruses do not possess their own glycosylation machinery and by virtue of their opportunistic nature, are heavily dependent upon the glycosylation machinery of the host cell to glycosylate their proteins. HIV, Influenza virus, Hendra virus, Severe acute respiratory syndrome coronavirus (SARS-CoV), Hepatitis viruses and WNV are examples of viruses for which glycosylation was shown to be critical to their stability, infectivity and antigenicity (Mir-Shekari et al., 1997; Vigerust and Shepherd, 2007; Medina et al., 2013; Doores, 2015) . Firstly, glycosylation can be involved in receptor binding. This is exemplified by HIV and DENV which rely on high mannose type glycosylation to bind to their MRs or DC-SIGN that are present on host immune cells (Cambi and Figdor, 2003; Cambi et al., 2004) . Furthermore, glycosylation is required to facilitate proper protein folding and trafficking of the viral membranes using the host chaperones such as calnexin and/or calreticulin proteins (Meunier et al., 1999; Land and Braakman, 2001; Slater-Handshy et al., 2004) . Importantly, glycosylation is a means to evade immune recognition within the host by changing glycan sites (Medina et al., 2013) , which in turn can increase the diversity of the glycosylation on the virus. In addition, the glycan structure has been reported to mask particular antigenic sites from recognition by neutralizing antibodies (Doores, 2015; Walls et al., 2016) . The external protein shell of DEN virion consists of 180 copies of E (53-56 kDa) and prM glycoproteins whereby only E proteins are exposed on the surface (Kuhn et al., 2002) . Extensive research over the years has revealed multiple functions of E protein in host receptor attachment, cellular uptake of virion and membrane fusion. E protein forms dimers on the virion surface (Kuhn et al., 2002) . The ectodomain of each E monomer without the transmembrane domains and membrane-associated "stem" region displays an elongated structure under Cryo-EM, which is further defined into three distinct domains (Domains I, II, and III) (Rey et al., 1995) . The central N-terminal DI separates the dimerization DII from the C-terminal DIII. DIII has been proposed to be the receptorbinding domain (Kuhn et al., 2002) whereby neutralizing monoclonal antibodies against DIII most efficiently block virus initial attachment to mammalian cells (Crill and Roehrig, 2001) . The fusion peptide located at the tip of DII is essential for endosomal membrane fusion and is essential for virus entry (Allison et al., 2001; Kuhn et al., 2002; Huang et al., 2010) . Dimerization of E proteins at neutral pH positions the fusion loop into a hydrophobic pocket formed by DI and DIII of the adjacent E monomer. This helps to prevent premature exposure of the fusion loop before endocytosis of the virion by a new host cell. DI forms part of the flexible hinge region which facilitates structural rearrangement of E protein during virion maturation and fusion process (Zhang et al., 2004) . Inside the acidic endosome, the pH-dependent hinge at the DI-DII interface (Allison et al., 1995; Modis et al., 2003) allows E dimer to dissociate and rearrange into a trimeric form which serves as a pre-fusion intermediate promoting membrane fusion (Modis et al., 2004) . In the ER lumen of the host cell, membrane-associated E protein is generated after co-translational processing of the viral precursor polypeptide by host Signalase (Acosta et al., 2014) . The newly synthesized E protein rapidly heterodimerizes with prM (Lorenz et al., 2002) and three prM-E heterodimers further oligomerize to form a total of sixty heterotrimeric prM-E spikes per subviral particle (Konishi and Mason, 1993; Zhang et al., 2003) . This higher-order oligomer has been proposed to represent the preassembly complex (Wang et al., 1999) . Translocation of this complex from ER to Golgi is critical as the transition from immature to mature virion is completed only in the trans-Golgi network, where the spiky prM-E trimers are rearranged into 90 flat dimers in a head-to-tail orientation on the virion surface (Kuhn et al., 2002; Zhang et al., 2003) . In the ER, DENV E protein undergoes N-linked glycosylation at two asparagines, N67 and N153 located in DII and DI, respectively (Chambers et al., 1990; Johnson et al., 1994; Hacker et al., 2009) . The N67 glycosylation site is unique to DENV and has been proposed to interact directly with DC-SIGN, one of the host cell receptors (Pokidysheva et al., 2006 ) (see Virus Attachment to Cell Surface and Cell Entry Process). In contrast, N153 (N154 in other flaviviruses) represents the conserved glycosylation site in the family Flaviviridae. High resolution crystal structure of Tick-borne encephalitis virus E dimer shows the N154-oligosaccharide chain projected overhead of the hydrophobic groove where the fusion loop fits in, suggesting that it functions as an "epitope shield" over the fusion loop to stabilize the dimer contacts (Rey et al., 1995) . Consistently, DENV2 and DENV3 mutant viruses lacking N153glycans due to a single point mutation within the glycosylation motif displayed elevated fusion pH threshold compared to their parental counterpart (Guirakhoo et al., 1993; Lee et al., 1997) . The authors proposed that the altered fusion activity of these mutants was likely due to instability of the E dimers. The glycan structure on DENV E protein has been studied using digestive enzymes (Johnson et al., 1994; Mondotte et al., 2007; Hacker et al., 2009) , chromatography (Johnson et al., 1994) , lectin-binding assay (Johnson et al., 1994; Hacker et al., 2009) and radioactive labeling (Smith and Wright, 1985) . Endo Hand PNGase F-enzymatic digestion revealed the presence of N-glycosylation in DENV E protein. No O-linked glycan has been detected to date (Johnson et al., 1994) . In mosquito cell-derived virions, the N-glycans attached to E protein display heterogeneity in structure and sugar composition where high mannose and paucimannose with terminal mannose residues are the dominant glycoforms (Figure 2A ) (Smith and Wright, 1985; Johnson et al., 1994; Hacker et al., 2009) . Recently mass spectroscopy has been applied to DENV glycoprotein studies to provide a comprehensive and detailed profile of the glycan moieties (Dubayle et al., 2015; Lei et al., 2015) . Using an integrated mass spectroscopy strategy consisting of lectin microarray and MALDI-Time of Flight Mass Spectrometry (MALDI TOF-MS), Lei et al. (2015) have successfully determined the detailed composition of N-glycans attached to the E protein from mosquito cell derived mature DENV2. Among the 19 distinct N-glycans detected, 15 contain terminal galactosylation while the remaining glycans were identified as high mannose type, complex type, fucosylated and sialylated N-glycans. In a separate study, the N-glycans from DENV1-4 (vaccine CYD-TDV) produced in mammalian Vero cells have been reported to consist of high mannose, complex and hybrid glycans with complex glycans as the major glycan species (Figure 2A ) (Dubayle et al., 2015) . By performing in-gel proteolysis of E-protein, site specific N-glycans have been determined. Sialylated complex glycans and high mannose (6-8 residues) glycans were detected at N153 in all DENV except for DENV2. Besides, most of the complex or hybrid glycans at N153 were found fucosylated. Interestingly, fucosylated glycans were detected only at N153 but not at N67 across all four DENV serotypes. Since high mannose binding DC-SIGN interacts only with N67 glycans on the viral surface (Pokidysheva et al., 2006) and N153-glycan is dispensable for virus production in mosquito and mammalian cells (Bryant et al., 2007) , this suggests that N153 glycans may serve a distinct function from N67 glycans in DEN pathogenesis possibly via interaction with an unknown fucose binder or act as a viral glycan shield. For N67 specific glycans, DENV2 was reported to have a different sugar composition from the other three DENV serotypes (Dubayle et al., 2015) whereby a higher content of complex or hybrid glycans was found in DENV2. High mannose glycans were detected as the main glycan species for DENV1, 3 and 4. In addition, sialylated N-glycan was detected only in DENV2 at this site. The differential glycosylation pattern between DENV2 and DENV1, 3, 4 may impact on various aspects of dengue pathogenesis including virus tropism, virus fitness, and induction of host responses (see Role of Glycosylation in DENV Life Cycle). Non-structural Protein 1 (NS1) was first identified as a nonhemagglutinating, soluble complement-fixing antigen in the brain and serum from DENV2-infected mice (Brandt et al., 1970; Smith and Wright, 1985) . NS1 has a molecular weight range of 46-55 kDa depending on its glycosylation status. It is a multifunctional glycoprotein which presents in different oligomeric forms and locates at various cellular compartments (Westaway and Goodman, 1987; Flamand et al., 1999) . Non-structural Protein 1 monomer consists of three structural domains namely a β-roll dimerization domain, a wing domain and a β-ladder domain (Akey et al., 2014) . The monomer structure is stabilized by six intramolecular disulfide bonds and no intermolecular disulfide bond has been identified in dimeric NS1 (Winkler et al., 1988) . However, any one of the three cysteine residues at the C-terminal has been reported to be important for dimer formation (Pryor and Wright, 1993) . The β-roll domain and part of the extended wing domain form a hydrophobic protrusion surface that acts as the ER membrane and replication complex (NS4B) interacting site, which is critical for viral RNA replication (Youn et al., 2012; Akey et al., 2014) . NS1 dimer is formed when two β-roll domains dimerize at the center and these dimers tend to trimerize resulting in hexameric NS1 (Flamand et al., 1999; Gutsche et al., 2011; Muller et al., 2012) . The NS1 hexamer crystal structure revealed a barrel-shaped oligomer with a central open channel. Three dimers are arranged symmetrically in a way such that the β-roll domains are entirely facing inwards and the channel interior is lined by the hydrophobic protrusion surface contributed by each dimeric component (Akey et al., 2014) . The hydrophobic lining allows the NS1 hexamer to be secreted as a lipoprotein whereby the lipid cargo is loaded into the central channel (Gutsche et al., 2011) . In contrast to the β-roll domains, glycosylation sites and most of the linear epitopes of NS1 identified are facing outward, representing the most accessible parts of NS1 hexamer by host antibodies (Akey et al., 2014) . Intracellular NS1 is predominantly in dimeric form whereas secreted NS1 is mainly in hexameric form ( Figure 2B ) (Flamand et al., 1999) . During protein synthesis, NS1 is cleaved from the viral polypeptide and translocated into the ER lumen. In ER, newly synthesized E protein is glycosylated and heterodimerizes with prM protein to form a higher order oligomeric preassembly complex. The immature virus particle with prM-E spikes is formed when the nucleocapsid associates with prM-E-rich membranes which buds into the ER lumen (1). The glycans remain of high mannose type on the immature virus particle as it is translocated to Golgi apparatus along the secretory pathway (2). The conformational rearrangement of prM-E spikes and cleavage of prM by host protease furin occurs in the Golgi to produce a mature, smooth virus particle. In mammalian cells, the glycans are further processed and modified into complex glycans before the virus particle is released to the extracellular milieu (route 2a). In mosquito cells, majority of the glycans are high mannose or galactosylated due to the different glycosylation enzymes expressed in insect cells (route 2b). High mannose glycan on the E protein, particularly the N67-glycan facilitates DC-SIGN(+) cell infection and virus propagation. The function of complex glycan on E protein is currently unknown. The glycosylated pr peptides are bound to E protein after furin cleavage and only dissociate at neutral pH in the extracellular milieu. (B) Monomeric NS1 protein is glycosylated with high mannose glycans at N130 and N207. The monomer rapidly dimerizes in the ER and membrane-associated NS1 dimers (1) are involved in virus RNA replication. Three NS1 dimers form a soluble hexameric NS1 but the exact location of hexamer formation remains unknown (2). In mammalian cells, the N130 glycans are modified into complex glycans before the soluble NS1 hexamer is secreted out of the cells (route 2a). In mosquito cells, generation of complex glycans doesn't happen and the lack of complex glycans (N130) on NS1 hexamer affects hexamer stability and greatly reduces its secretion (route 2b). A subset of dimeric NS1 are found on the infected cell surface but the trafficking pathway has yet to be determined (route 2c). High-mannose glycan at N207 stabilizes NS1 dimer. The soluble monomer undergoes dimerization to gain partial hydrophobicity (Flamand et al., 1999) , allowing membrane association of NS1 dimer in the absence of a transmembrane domain (Winkler et al., 1989) . The exact mechanism of NS1 hexamer formation remains unclear and two possible locations have been proposed including along the Golgi secretory pathway, or immediately after dimerization at the ER (Muller and Young, 2013) . The functions of NS1 are closely associated to its cellular location throughout the virus replication cycle. ER membraneassociated dimeric NS1 has been found to co-localize with viral dsRNA (Mackenzie et al., 1996) . Circulating hexameric NS1 is able to bind to the plasma membrane of mammalian cells via the interaction between its N-glycans and cell surface glycosaminoglycans, heparin sulfate and chondroitin sulfate E (Avirutnan et al., 2007) . Recently, it has been reported that hexameric NS1 contributes to disease pathogenesis of severe DEN (Beatty et al., 2015; Modhiran et al., 2015) . The soluble protein acts as a viral toxin that induces pro-inflammatory cytokine response and vascular leakage via Toll-like receptor 4 expressed on immune cells and endothelial cells (Modhiran et al., 2015) . Beatty et al. (2015) showed that NS1 vaccination protects mice from NS1-induced vascular leakage which was independent of complement components. Glycosylation of DENV NS1 occurs right after its cleavage in the ER (Winkler et al., 1988) at two asparagines, N130 and N207 (Putnak et al., 1988; Winkler et al., 1989; Flamand et al., 1999) . These two N-glycosylation sites are conserved in the family Flaviviridae. Recently, a less conserved glycosylation site at N175 has been reported in WNV, St. Louis encephalitis virus and Murray Valley encephalitis virus but is absent in all four serotypes of DENV (Akey et al., 2014) . Intracellular and extracellular DENV NS1 display different types of N-glycans as the oligosaccharides undergo modification during the maturation process (Winkler et al., 1989; Pryor and Wright, 1994; Flamand et al., 1999) . Intracellular dimeric NS1 N-glycans are of high mannose composition regardless of the host cell type (mammalian or mosquito cell) (Mason, 1989) . On the other hand, in extracellular hexameric NS1, the N130-glycans consist of complex oligosaccharides whereas the N207-glycans are made of high mannose type sugar chains (Mason, 1989; Pryor and Wright, 1994; Flamand et al., 1999) . As dimeric NS1 passes through the Golgi apparatus, two N130-glycans are further modified into the Endo H-resistant, multi-branched complex type before the protein is released (Winkler et al., 1989) . The differential modification at these two sites is due to the inaccessibility of N207-glycan by Golgi-resident enzymes after the dimerization of NS1 (Flamand et al., 1999) . In the DENV replication cycle, prM interacts with E protein and acts as a chaperone to ensure proper E protein folding (Lorenz et al., 2002) and to prevent premature fusion of the virus particle along the secretory pathway by concealing the E fusion loop Yu et al., 2009) . Glycosylation of the prM/M glycoprotein in DENV has not been extensively studied. The protein is glycosylated at N69 (Table 1) with circumstantial evidence for N-linked glycosylation at sites 7, 31, and 52 (Courageot et al., 2000) . It was found that α-glucosidase inhibitor reduced the amount of prM-E heterodimer, suggesting the N-glycans are required for productive folding pathway of these glycoproteins (Courageot et al., 2000) . Triglucosylated N-glycan at N68 of DENV1 affects the folding of prM by causing a delayed formation of prM-E heterodimer (Courageot et al., 2000) . N-glycosylation on both E and NS1 proteins has been shown to play important roles throughout the DENV infection cycle from virion attachment, entry, maturation, assembly to secretion. Carbohydrate chains on the DENV E proteins play a critical role in host cell infection at the early step of host receptor binding. Indeed, virus attachment and penetration into mammalian and mosquito cells were blocked by pre-incubation of virus with Concanavalin A, a plant lectin that binds to alpha-linked terminal mannose of high mannose or hybrid glycans (Hung, 1999) . Lectins are a group of proteins that recognize carbohydrates through a carbohydrate recognition domain [reviewed in ±The N-glycosylation site is located at residue 64 to 69 depending on the serotype. (Zelensky and Gready, 2005) ]. To date, various lectin families such as C-type, P-type, L-type, Galectin and Calnexin have been shown to interact with viral components . C-type lectins are particularly important for DENV infection as they have been shown to be involved in host cell attachment and disease pathogenesis (see Disease Pathogenesis). The cell membrane-anchored C-type lectin DC-SIGN has been identified as host cell receptor for many viruses , among which DENV infects DC and monocyte via DC-SIGN (Navarro-Sanchez et al., 2003; Tassaneetrithep et al., 2003) . The interaction between DC-SIGN and DENV can be inhibited by the addition of mannose and mannan and has been further examined at the molecular level by structural analysis. Cryo-EM data of DENV/DC-SIGN complexes reveals that the carbohydrate recognition domain of DC-SIGN interacts directly with the N67-glycan of E dimers (Pokidysheva et al., 2006) . Consistently, lectin (HHA)-resistant DENV which lacks both N-glycosylation sites on E protein failed to infect DC-SIGN(+) DC, in contrast to productive infection and replication in DC-SIGN(−) and carbohydrate-independent cells such as Vero, Huh7, C6/36 and Baby Hamster Kidney fibroblasts (BHK-21) (Alen et al., 2012) . The presence of N67-glycan on E protein also allows DENV to infect endothelial cells in liver and lymph node via DC-SIGN-related proteins known as DC-SIGNR and L-SIGN, the close homologues of DC-SIGN (Tassaneetrithep et al., 2003; Alen et al., 2012) . In addition to DC-SIGN, MR has been identified as another C-type lectin utilized by all four serotypes of DENV to infect macrophages and DC (Miller et al., 2008) . Both MR and DC-SIGN bind to DENV E protein with high affinity (K D in the sub-nanomolar range) (Lo et al., 2016) , despite a different ligand specificity for these two host receptors (Miller et al., 2008) . MR shows a preferential binding to terminal mannose, fucose and N-acetyl glucosamine while DC-SIGN binds to high-mannose oligosaccharides (Miller et al., 2008) . As DC-SIGN and MR have been proposed to be the primary host receptors for DENV during infection (Lo et al., 2016) , the engagement to these C-type lectin receptors with diverse glycoforms of E protein may allow DENV to infect a wide range of host cells. In contrast and interestingly, N67 deglycosylated (N67 − ) DENV1 and DENV2 were found to display enhanced infectivity in mosquito cells (C6/36) compared to wild type (WT) (Ishak et al., 2001; Lee et al., 2010; Alen et al., 2012) . For mosquito cells, the entry mode employed by Flavivirus (DENV and JEV) was shown to involve membrane fusion instead of receptormediated endocytosis (Hase et al., 1989a,b) . Hence, it is possible that absence of the N67-glycans from the virion surface reduces steric hindrance and therefore promotes cell membrane attachment and membrane fusion. Finally, N153 deglycosylated (N153 − ) DENV mutant displayed reduced infectivity (10-fold lower) in both mammalian and mosquito cells compared to WT, possibly due to impaired virus entry process (Lee et al., 1997; Hacker et al., 2009) , whereby loss of the N153-glycan affected the conformational stability of E proteins and led to premature exposure of the fusion peptide (Yoshii et al., 2013) . Early studies on DENV E protein showed that N-glycosylation is not essential for virus replication in mosquito cells (Bryant et al., 2007; Mondotte et al., 2007) . Instead, loss of the N67glycosylation site through site directed mutagenesis (N67Q) in E protein was sufficient to render DENV2 (strain 16681) growth defective in BHK-21 cells, a DC-SIGN(−) cell line (Bryant et al., 2007) . Direct transfection of N67Q mutant RNA into BHK-21 cells neither produced intracellular viral antigen nor released new virus progeny. The lack of virion release may be due to impaired virion secretion along the ER-Golgi secretory pathway in the absence of N67-glycan tag on E protein. However, the same mutant replicated and grew comparably to WT counterpart in C6/36 mosquito cells in vitro and in A. aegypti mosquito in vivo (Bryant et al., 2007) , thus supporting that N-glycosylation of E protein at position N67 is essential for productive infection in mammalian cells only, consistent with earlier studies (Bryant et al., 2007; Mondotte et al., 2007) . Further investigation on the importance of the N-glycosylation motif was done by Lee et al. (2010) through extensive point mutation within the conserved N-x-T/S motif of DENV2 (strains PUO-218 and NGC), whereby the conserved residue T69 was replaced with residues of different side chain propensity. Replacement of T69 by a larger and more hydrophobic residue (leucine and valine) either by molecular cloning (Lee et al., 2010) or by passaging the virus under selection pressure (Alen et al., 2012) generated viable virus that retained efficient growth in BHK-21 and Vero cells in the absence of N67 glycan. In addition, N67Q/D mutant virus generated in strain PUO-218 propagated in mammalian cells at reduced growth rate, which is inconsistent with previous studies carried out with DENV2 strain 16681 (Bryant et al., 2007; Mondotte et al., 2007) . The differential and virus strain-dependent outcome led to the hypothesis that in the absence of N67 glycosylation, the aminoacid composition of the DII region determines virus survival in mammalian cells (Lee et al., 2010) . Multiple sequence alignment showed indeed that strain 16681 differed from strains PUO-218 and NGC at two positions, arginine (R)120 in DII and T170 in DI (Figure 3) . Uncharged polar threonine replaces charged R120 and hydrophobic isoleucine (I) replaces T170 in strains PUO-218 and NGC. It is possible that the presence of hydrophobic residues facilitates protein folding even without the glycan tag for chaperone-assisted folding and followed by productive protein secretion. Nevertheless, structural comparison of the WT strains and their respective mutants needs to be carried out to confirm this hypothesis. In contrast to the varying outcomes obtained with N67 mutant viruses and their ability to grow in mammalian cells, it has been consistently reported that these virus variants replicate, propagate in mosquito cells but produce lower virus titers compared to WT (Bryant et al., 2007; Mondotte et al., 2007; Lee et al., 2010) . N153-glycan on E protein is important but not essential for DENV survival in mosquito cells. Successive passages (as low as two passages) of DENV in C6/36 cells in vitro resulted in mutation at T155 which ablates the N153-glycosylation motif; however, the non-glycosylated variant was able to propagate in mosquito cells (Lee et al., 1997) . N153 − DENV grows in both mammalian cells and C6/36 cells and produce lower virus titer than its WT counterpart (Bryant et al., 2007; Lee et al., 2010) , which could be due to defective virus budding. Consistently, in a study using transmission electron microscopy, it was shown that virus budding of WT WNV occurs at the plasma membrane while the mature progeny of N154 − mutant scatters at the smooth membrane vesicle within swollen ER lumen without budding (Li et al., 2006) . N130A NS1 mutant virus of DENV1 (Tajima et al., 2008) and DENV2 (NGC) (Pryor et al., 1998) failed to generate viable virus in both mammalian and mosquito cells. Mutation at N130 in DENV4 NS1 caused reduced viral growth in mammalian cells and C6/36 cells (Pletnev et al., 1993) . However, N130Q NS1 mutant of DENV2 (16681) produced infectious virus with a similar titer as the WT virus in mammalian cells but with a reduced titer in C6/36 cells (Crabtree et al., 2004) . Removal of glycan from N207 in DENV1 and DENV2 (16681) NS1 protein produced similar growth and virus titers compared to WT in mammalian cells despite a delayed cytopathic effect (Crabtree et al., 2004; Tajima et al., 2008) , which is not consistent with the observation on DENV2 (NGC) (Pryor et al., 1998) . Double mutation attempts (N130Q/N207Q and T132N/T209N) failed to generate genetically stable mutant viruses (Crabtree et al., 2004) . Taken together, the findings suggest that at least one of the two N-glycosylation sites (probably N130) in NS1 protein is essential to produce viable virus. Similar to E protein, the impact of deglycosylation at this site varies depending on the virus strain and amino acid residue used for replacement. In DENV, N130-glycosylation is important but not essential for NS1 secretion in mammalian cells (Despres et al., 1991; Jacobs et al., 1992; Pryor and Wright, 1994; Crabtree et al., 2004) . Single and double NS1 mutant proteins are secreted from infected cells even though a reduced secretion yield has been observed (Crabtree et al., 2004; Somnuke et al., 2011) . The impact of deglycosylation on secretion is thought to be associated with the stability of the NS1 oligomer. Mutation of N130 or N207 does not affect dimerization of the protein but compromises the stability of the dimer (Winkler et al., 1989; Pryor and Wright, 1994) . The dimer appeared more heat sensitive when the N-glycan was removed from the protein especially for N207A mutant. The use of tunicamycin, an enzyme targeting the host glycosylation enzymes, allowed confirm that absence of N-glycan was solely responsible for the instability of NS1 oligomers instead of changes in the polypeptide backbone in the genetically deglycosylated mutants (Pryor and Wright, 1994; Flamand et al., 1999) . Furthermore, the secretion of NS1 was reduced when complex glycans maturation was blocked by glycosylation inhibitors Swainsonine and 1-deoxymannojirimycin (Flamand et al., 1999) . Consistently, low levels of NS1 with solely high mannose glycan are secreted from infected mosquito cells, which lack the enzymes to generate complex type glycans. These findings support the proposal that N-glycosylation and complex glycan are important for NS1 secretion (Hsieh and Robbins, 1984; Mason, 1989; Thiemmeca et al., 2016) . In addition, the majority of the secreted WT and N207Q NS1 proteins are hexamers (Somnuke et al., 2011) , whereas secreted N130Q and N130/N207Q NS1 proteins showed reduced hexamer population and increase in higher order oligomer (>675 kDa) population. As compared to mammalian cell-secreted NS1, mosquito cell-secreted NS1 is less stable and undergoes degradation more rapidly at body temperature (Thiemmeca et al., 2016) . Hence, the presence of complex glycan at N130 is critical for both NS1 secretion and NS1 hexamer stability (Flamand et al., 1999; Somnuke et al., 2011) . Similarly, it has been proposed that high-mannose glycans at N207 stabilizes NS1 dimer (Pryor et al., 1998; Flamand et al., 1999; Somnuke et al., 2011) . Reduced levels of DENV 4 N207Q and N130Q/N207Q NS1 proteins were observed in culture supernatant (Somnuke et al., 2011) which could be explained by two possible scenari: (1) Stability of the secreted mutant forms is compromised due to a different protein conformation (Somnuke et al., 2011) . The misfolding of the protein may lead to a less effective secretion of functional hexamer. (2) Transport of the protein from the perinuclear region is affected which in turn compromises the maturation and secretion of the protein (Crabtree et al., 2004) . Deglycosylated NS1 mutant viruses (N130 − ) are less neurovirulent as evidenced by the reduced mortality observed with mice infected intracranially with the DENV2 and DENV4 mutants (Pletnev et al., 1993; Crabtree et al., 2004) . The reduced neurovirulence of these viruses which lack the complex type glycans may be linked to the reduced levels of extracellular hexameric NS1 (Crabtree et al., 2004) . The virulence phenotypes observed with N207 − mutant viruses varied depending on the DENV strain. DENV2 N207 − mutant displayed decreased virulence (Pryor et al., 1998; Crabtree et al., 2004) , whereas the DENV4 N207 − mutant showed enhanced virulence in mice (Pletnev et al., 1993) . The low to undetectable levels of NS1 specific antibodies in mice infected with the DENV4 N207 − mutant suggests that the enhanced neurovirulence could be attributed to the reduced immunogenicity of the virus (Pletnev et al., 1993) . The complement cascade is the central defense mechanism of innate immunity which triggers the immune effector function to remove infectious pathogens and modified self cells upon activation. The activation and amplification of the complement pathway involves a series of sequential events and the whole process is tightly regulated. Complement can be activated through three major pathways, namely the classical, lectin and alternative pathways [reviewed in (Ricklin et al., 2016) ]. The classical pathway is triggered by antibody-antigen complexes whereas the lectin pathway is activated by carbohydrate moieties on the microbial surface. The alternative pathway is activated through direct binding of C3b at the surface of pathogens, which results from the constitutive basal cleavage of C3 (Ricklin et al., 2016) . Mannose binding lectin (MBL) in the lectin pathway triggers antibody-independent activation of complement (Thielens et al., 2002) . The proposed MBL-mediated virus elimination mechanisms include (1) direct virus neutralization, (2) C3/C4 deposition on virus surface and (3) interference of host cell lectin receptor binding . MBL differentiates self-from non-self-antigens based on a sugar density-dependent recognition mechanism (Dam and Brewer, 2010) , and the micro pattern of the oligosaccharides structure in addition to the spatial geometry of the macro sugar pattern (Takahashi and Ezekowitz, 2005) . It was proposed that the additional N67-glycan in DENV (which is absent in other flaviviruses) could promote a more efficient recognition and binding by MBL . This hypothesis is supported by improved MBL binding and in vivo virus clearance of a genetically engineered WNV with additional N67-glycosylation site . MBL is reactive to high mannose oligosaccharides and thus can efficiently recognize insect cell-derived DENV with high mannose glycans present on its E proteins. Hence, the change of N-glycan profile of E protein after one round of replication in mammalian host cells may provide an opportunity to the virus to escape from effective MBL recognition (Fuchs et al., 2011) . However, mammalian cell-derived DENV was found to be effectively inhibited and neutralized by mouse MBL . A separate study instead reported preferential binding of recombinant human MBL to insect cell-derived DENV2, whereas virions produced in monocyte-derived DC were not neutralized by human MBL . It therefore remains unclear whether MBL-mediated virus clearance is optimally engaged during DENV infection in humans. Furthermore, studies have shown that NS1 interferes with the complement pathway through binding to a number of its components (Muller and Young, 2013) . N130Q NS1 was found to bind to C1s proenzyme, C1, C4, and C4b with reduced affinity compared to WT and N207Q NS1 (Somnuke et al., 2011) , indicating that the N-glycan is required for effective interaction with complement components. The role of NS1 glycosylation in the ability of the protein to interfere with the complement activation, however, has been largely ignored. The N-glycans were proposed to be involved in NS1 binding to C4 (Avirutnan et al., 2010) although direct experimental evidence has been missing. A recent study reported that secreted NS1 binds directly to C4BP, a major inhibitor of the C4b component (Thiemmeca et al., 2016) . This binding leads to the recruitment of C4BP on cell surface via NS1 and inactivates C4b thereby interfering with the formation of the membrane attack complex (MAC). Furthermore, the work has demonstrated a competitive binding of NS1 to MBL, which prevents MBL-mediated DENV destruction. The presence of secreted NS1 in the saliva of Aedes mosquito suggests that secreted NS1 protein could help DENV to escape the host innate immune surveillance during virus transmission (Thiemmeca et al., 2016) . Severe DEN (DHF/DSS) is characterized by increased vascular permeability and plasma leakage, thrombocytopenia, hemorrhagic fever and circulatory failure in DSS (WHO, 2017) . The current paradigm proposes that viral-induced proinflammatory cytokine storm drives the disease progression to DHF/DSS (Pang et al., 2007) . Direct interaction between DENV and CLEC5A expressed on macrophages indicates that virus glycosylation plays a role in DEN pathogenesis Wu et al., 2013) . Similar to the engagement to DC-SIGN, DENV binding to CLEC5A relies on sugar moieties and can be inhibited by exogenous fucose and mannose . However, CLEC5A binding does not mediate viral entry into the host cell, instead it serves as a cooperative signaling receptor to MR/DC-SIGN that activates macrophage inflammasome and triggers the production of pro-inflammatory cytokines Wu et al., 2013; Lo et al., 2016) . Consistently, anti-CLEC5A monoclonal antibody reduced DENV-induced vascular leakage in a mouse model , which further supports that targeting the viral glycoprotein-host lectin receptor interactions represents a potential therapeutic approach to counteract the excessive inflammatory responses involved in severe DEN. Glycosylation is a post-translational modification which significantly affects the conformation of a protein. It is a heterogeneous process that is highly host-cell specific. Viruses have evolved to utilize their host's glycosylation machinery so as to optimize their fitness, infectivity, replication and virulence. The role of glycosylation and glycan structures in DENV virulence has yet to be reported with evidence of attenuated phenotypes in symptomatic mouse models. Given the impact of glycosylation in virus entry and virus fitness in mammalian cells, it is highly likely that deglycosylated DENV mutants will display reduced virulence in vivo. The ability of Celgosivir treatment, a bicyclic iminosugar that inhibits glycosylation through negatively binding to ER [alpha]-glucosidase II, to protect mice from a lethal DENV challenge indirectly demonstrates the importance of glycosylation in DENV virulence (Perry et al., 2013; Sayce et al., 2016; Warfield et al., 2016) . To date, out of the eight DENV potential glycoproteins (Table 1) , only E and NS1 proteins have been characterized from a glycosylation standpoint and not across all the DENV serotypes. Despite the biological importance of these structures being recognized, efforts to characterize the nature of the glycan structures in DENV have remained timid. There is for example little understanding of how glycosylation impacts DENV cell tropism where different glycan variants may influence binding of DENV to various host cell receptors and subsequent cell infection. Characterization of DENV glycoforms has been mainly performed in the mammalian cell line BHK-21 but no study has been conducted in more relevant primary mammalian cell types including Langerhans cells, monocytes, hepatocytes, and endothelial cells. Furthermore, in-depth characterization of the glycan structures using the latest glycomics technologies has yet to be reported for DENV. Associating the contribution of these glycans to the structure and ultimately function of the virion glycoproteins indeed requires glycomics and glycoproteomics. Such data need to be modeled using computational approaches as methods for crystallizing glycoproteins remains a complicated feat. Furthermore, the role of glycosylation is also very important to recognize in biotherapeutic strategies. While substantial efforts have been devoted to developing neutralizing antibodies against DENV, the potency of these antibodies is largely dictated by the accessibility of the epitope that they target which can be influenced by the glycosylated status of the protein (Smith et al., 2013) . Consistently, E protein glycosylation site has been reported to modulate the binding of neutralizing antibodies against a highly conserved, serotype cross-reactive epitope (Dejnirattisai et al., 2015) . These challenges have prompted substantial investment into elucidating the three-dimensional conformation of the protein-antibody complexes and more importantly how glycosylation contributes to the tertiary and quaternary arrangements of the different glycoproteins on the virion. This approach is critical for the development of new therapeutics with broader activity and increased efficacy. In conclusion, the DEN field as a whole would benefit greatly from in-depth understanding and characterization of the glycosylation patterns of DEN virions. With the recent technical advances in the fields of glycomics and glycoproteomics, this has become possible and will depend on productive interactions between glycobiologists and DEN virologists. SY, TN-K, and SA wrote the manuscript. PR provided suggestions and edited the manuscript. SA holds a research grant from the National Medical Research Council (NMRC/MOHIAFCAT2/007/2016) to develop dengue mouse models. 
paper_id= fffb268f02887d8680dc611f6fc0b20c489030cb	title= Emergence of novel coronavirus and progress toward treatment and vaccine	authors= Muhammad Muzamil Khan;Amna  Noor;Asadullah  Madni;Mudassir  Shafiq;D G Khan;Ghazi  Dera;  Khan;Headquarter  Hospital;Pakistan  Rawalpinid;Muzamil  Khan;	abstract= In late December 2019, a group of patients was observed with pneumonia-like symptoms that were linked with a wet market in Wuhan, China. The patients were found to have a novel coronavirus genetically related to a bat coronavirus that was termed SARS-CoV-2. The virus gradually spread worldwide and was declared a pandemic by WHO. Scientists have started trials on potential preventive and treatment options. Currently, there is no specific approved treatment for SARS-CoV-2, and various clinical trials are underway to explore better treatments. Some previously approved antiviral and other drugs have shown some in vitro activity. Here we summarize the fight against this novel coronavirus with particular focus on the different treatment options and clinical trials exploring treatment as well as work done toward development of vaccines. K E Y W O R D S bat coronavirus, chloroquine, remdesivir, SARS-CoV-2, vaccine 2 | TREATMENT OPTIONS FOR CORONAVIRUS Currently, no treatment is approved for SARS-CoV-2, but various already approved drugs are being tried in different clinical trials to 	body_text= Coronaviruses are a form of positive-strand non-segmented RNA viruses distributed among birds, mammals and humans that cause respiratory and neurological illnesses. 1 There are six different types that can cause disease among humans. Four of these (HKU1, 229E, NL63 and OC43) cause only the common cold in patients, 2 5 Due to the broad genetic variation and diversity of coronaviruses and higher chances of animal to human spread they are likely to emerge periodically in future. 6 During December 2019, an outbreak of pneumonia-like symptoms occurred in patients that were linked to the seafood market in Wuhan China. 7 Investigation identified a new strain of coronavirus called SARS-CoV-2. 8 The detection of this novel coronavirus is key to confirm the cases and proceed to treatment. In an early method for detecting SARS-CoV-2, the samples from bronchoalveolar-lavage fluids were collected, centrifuged to remove debris and inoculated onto human epithelial cells of airway origin. 9 About 20 000 sequences from each sample were obtained and genome matches showed more than 85% identity with SARS-like betacornavirus. Results were also obtained from real-time PCR, and isolated viruses were named SARS-CoV-2. 8 To further characterize SARS-CoV-2, the de-novo sequence was obtained by using nanopore sequencing and Illumina methods. The airway epithelial cell cultures were suitable for visualization and identification of the novel coronavirus. 9 check efficacy against this virus. The possible options include nucleoside analogs, interferon that can act as immune modulators and approved antimalarials having antiviral activities such as chloroquine and hydroxychloroquine. The nucleoside analogs such as ribavirin, favipiravir, galidesivir and remdesivir target RNA polymerase and block the synthesis of viral RNA. 10 Favipiravir was effectively used against influenza and has the potential to inhibit viral RNA synthesis and a new study also supports its activity against SARS-CoV-2. 11 Different clinical trials are underway where patients are being recruited to evaluate the efficacy of a combination of favipiravir and interferon-α 12 and a combination of favipiravir and baloxavir marboxil is being evaluated. 13 Ribavirin is a guanine derivative antiviral drug approved for the treatment of HCV and RSV, but it causes anemia at higher doses that limit its use and its efficacy against coronavirus is uncertain. 14 Remdesivir is an adenine derivative antiviral drug. It has activity against a variety of viral strains such as SARS and MERS and a recent study also supports its activity against SARS-CoV-2. 11 A recent patient in US with SARS-CoV-2 has shown recovery with intravenous administration of remdesivir. 15 Recently, Phase-III clinical trials have been started in USA to evaluate the efficacy of IV remdesivir as 200 mg OD or 100 mg OD for 9 days. 16 A recent RCT apparently reported no significant efficacy, but we await the published findings.  Small molecular weight drugs such as chloroquine have shown inhibitory effects against SARS-CoV-2 (EC50 = 1.14 μM in Vero E6 cells) and is under evaluation in open-label trials.. 15 Chloroquine (CQ) is a recognized anti-malarial drug but also has antiviral activity. The antiviral activity of chloroquine was first noticed in the late 1960s. 21 Chloroquine and its analog hydroxychloroquine both have inhibitory effects against various viruses including SARS, 22 enterovirus 23 and Zika virus. 24 Chloroquine inhibits the virus by increasing endosomal pH and so reducing viral cell fusion and also interferes with cellular receptor glycosylation. 25 The EC90 value of chloroquine against SARS-CoV-2 is 6.90 μM that can be achieved with administration of 500 mg dose. 26 Remdesivir and chloroquine have shown activity in in vitro studies and can easily be tested in patients with SARS-CoV-2. 15 In another recent study, Gao et al found that chloroquine phosphate reduced the symptoms of pneumonia in SARS-CoV-2 patients and shortening the duration of disease. 27 The guidelines for the treatment of SARS-CoV-2 were revised six times since its issuance on 15 January 2020. The recent guidelines also include IFN-α, remdesivir, ribavirin, ritonavir and chloroquine. The mode of administration of IFN-α is through inhalation at a dose of 5 million units diluted with water for injection. The dose of ritonavir is 100 mg BD for adults. Ribavirin may be given in combination with IFN-α or ritonavir at a dose of 500 mg BD or TDS. Chloroquine phosphate should be administered at a dose of 500 mg BD. Arbidol can be given three times a day at 200 mg. 28 Hydroxychloroquine (HCQ) is an approved disease-modifying antirheumatic drug that also has immunomodulatory effects and prevents organ damage. 29 HCQ alters endosomal pH and interrupts the biding between RNA/DNA and toll-like receptors that leads to suppression of TLR signaling. [30] [31] [32] Inside the cytoplasm, HCQ also interferes with the interaction between nucleic acid sensor cyclic GMP and cytosolic DNA. 33 These two mechanisms lead to increase production of IL-1, TNF and type-1 interferon. Such mechanism supports the idea that HCQ suppress the cytokine release storm (CRS) which is due to SARS-CoV-2 triggered overreaction of immune system. 34 In a recent study, HCQ was found to be more effective than chloroquine; a loading dose of 400 mg twice daily and maintenance dose of 200 mg twice a day is recommended for SARS-CoV2 infection. 35 The mechanism involves in antiviral role of HCQ and CQ is inhibition of receptor binding and fusion of cell membrane. These two are crucial steps that are required for cell entry of SARS-CoV-2. Chloroquine interferes with the glycosylation of ACE-2 (angiotensinconverting enzyme receptors) receptors that are considered as cellular receptors for SARS-CoV-2 and block the fusion of SARS-CoV-2 with host cell. Thus, the binding of virus is blocked and infection is prevented. The HCQ and CQ after entry into the cell tend to concentrate in lysosomes and endosomes. The SARS-CoV-2 use endosome as a tool for cellular entry. HCQ and CQ increase the pH of endosomes and create a negative influence on the binding of SARS-C0v-2 with endosomes. 25 Lysosomal protease helps in viral fusion with cell membrane. Increase lysosomal pH prevents the action of protease and fusion and replication of virus is blocked 36 . The mechanism of action of chloroquine and hydroxychloroquine is represented as (Figure 1 ). Cinanserin is an antagonist of serotonin receptors. It can inhibit chymotrypsin-like (3C-like) protease and has shown promising results against SARS coronavirus. 37 The 3C-protease was also investigated to be encoded in SARS-CoV-2. 38 Flavonoids such as chalcones, flavones and isoflavones produce antioxidant effects but they also have antiviral effects. 39 A study has found that flavonoids can inhibit the entry of hepatitis-C virus. 40 Some flavonoids have activity against MERS coronavirus, presumably due to inhibition of 3C-like protease. 41 Nitric oxide (NO) is a biological gas produced from arginine. NO after reaction with superoxide forms peroxynitrite which has cytotoxic bactericidal action. 43 Nitric oxide is also known to regulate airway function and reduce inflammation of airways. 44 The beneficial effect of NO in SARS patients was observed in some studies. 45 NO can also inhibit the synthesis of RNA and viral protein. 46 A study has found that organic nitric oxide could inhibit the replication cycle of SARS-CoV-2. 47 Αlpha-lipoic acid (ALA) is used in hepatic disorders and polyneuropathies. 48 ALA is an antioxidant that protects cells against oxidative stress by increasing the glutathione level. 49 Some studies support the role of angiotensin-converting enzyme (ACE) inhibitors. This is based on the hypothesis that ACE-2 serves as receptor for SARS-CoV-2. 54,55 So, ACE inhibitor could potentially compete for site binding and reduce the mortality and morbidity associated with SARS-CoV-2. 56 The various ongoing clinical trials are summarized in Table 1 . Vaccine provides immunity against a particular pathogen before exposure of that infectious agent. Several types of vaccines exist that can be nucleic acid based, live attenuated vaccine, subunit proteins or nanoparticle vaccines. 67 Different technologies are being utilized to F I G U R E 1 Mechanism of action of HCQ and CQ by blocking binding of virus with ACE-2 receptors and increasing endosomal pH and preventing fusion with the cell develop potential vaccine for SARS-CoV-2 including DNA and mRNAbased nanoparticles. Phase-I trials of potential vaccines focus on safety and immunogenicity, for example, against MERS. 68 Two of the clinical trials on MERS vaccine are expected to be complete by the end of 2020 69, 70 in Russia and one in Germany by December 2021. 71 The Inovo pharmaceutical company has tested its vaccine against MERS coronavirus funded by coalition for epidemic preparedness innovation (CEPI) using DNAbased technology and named as INO-4700. 72 The University of Oxford recombinant chimpanzee adenovirus has also begun Phase 1 randomized multicenter trials for intramuscular injection of vaccine ChAdOx1 against SARS-CoV-2. The 1100 participants have been divided into four groups and they will be observed for any serious adverse event for 6 months. The ongoing clinical trials for development of vaccines have been summarized in Table 2 and completed vaccine trials for SARS and   MERS viruses have been summarized in Table 3 . The fusion of coronavirus with a cell occurs after biding of S protein to a target receptor delivers viral nucleocapsid and initiates replication. The S protein causes syncytial formation at the receptor site. 85 A neutralizing antibody that can target the S protein on the surface of Clinical trials for various treatment options for SARS-CoV-2 Clinical Trial Registration Number Trial information Date Type References The efficacy of Newgen beta-gluten probiotic powder in patient with 2019-nCOV. 16 March 2020 Interventional 57 The efficacy of combination of Favipiravir + Tocilizumab in patient with pneumonia (COVID-19 The immune response was dose-independent and further trials needed to establish efficacy 84  
paper_id= fffb411c331ee56d877d8755e5b3ad22f7737996	title= Open Forum Infectious Diseases Case Report: Overwhelming Babesia Parasitemia Successfully Treated Promptly With RBC Apheresis and Triple Therapy With Clindamycin, Azithromycin, and Atovaquone	authors= Yijia  Li;Susan  Stanley;Julian A Villalba;Sandra  Nelson;Jeffrey  Gelfand;	abstract= Babesiosis with high-grade parasitemia is life-threatening, especially in asplenic hosts. We report an asplenic patient with parasitemia >50% who was successfully treated with prompt red blood cell apheresis and triple therapy with clindamycin + azit hromycin + atovaquone. This regimen may be an alternative to poorly tolerated clindamycin + quinine in severe cases. 	body_text= Babesiosis, a tickborne illness transmitted by the Ixodes scapularis tick, has been increasing dramatically in incidence in the past decade, especially in the New England area [1] . As per the Centers for Disease Control and Prevention (CDC), in the state of Massachusetts the incidence of babesiosis nearly tripled, from 3.1 per 100 000 people in 2011 to 8.6 per 100 000 people in 2017 [1] . Babesiosis has a wide range of clinical manifestations and severity, from subclinical infection to severe disease leading to multiorgan failure and death. The treatment regimens for babesiosis, especially for severe babesiosis, however, are largely based on small-scale studies and anecdotal data. The latest Infectious Diseases Society of America (IDSA) guideline recommends clindamycin plus quinine for severe disease in addition to red blood cell (RBC) exchange if parasitemia is ≥10% [2] . However, the evidence supporting this regimen is limited [3] . Quinine is associated with multiple adverse effects, which often limits its use. The combination of atovaquone and azithromycin, when compared with clindamycin plus quinine, has been shown to be similarly effective in non-life-threatening babesiosis, with far fewer severe adverse effects, in a small-scale (n = 58), nonblinded randomized trial with participants who had a median parasitemia level of only 0.5% [4] . An optimal quinine-free regimen for severe babesiosis remains to be determined. In this case report, we describe a case of an asplenic patient who survived severe babesiosis with parasitemia >50% receiving an induction regimen combining clindamycin, azithromycin, and atovaquone, in addition to RBC exchange, followed by azithromycin and atovaquone for maintenance therapy. The patient is a 68-year-old gentleman with a history of asplenia due to a motor vehicle accident 5 years earlier and hypertension who was admitted for fever and altered mental status. One month before admission, he experienced right-sided facial droop and received a diagnosis of Bell's palsy. He received a prednisone taper along with valacyclovir, and his symptoms resolved within 1 month. Lyme disease screening returned negative at that time. Six days before admission he started to experience fever, nausea, vomiting, and diarrhea. Nasopharyngeal swab for severe acute respiratory syndrome coronavirus 2 using a reverse transcription polymerase chain reaction (RT-PCR) assay was negative. Symptoms worsened, and he was found unresponsive at home. He was initially transported to an outside hospital; initial laboratory findings are summarized in Table 1 and are notable for white blood cell count (WBC) 21.9 ×1000/μL, hemoglobin 11.5 g/dL, platelet count (PLT) 31 ×1000/μL, total bilirubin 9.6 mg/dL, and creatinine 4.68 mg/dL (baseline level 0.90 mg/dL). Peripheral blood smear demonstrated 64.2% babesia parasitemia. He was given 1 dose of azithromycin and atovaquone and transferred to our hospital. His medical history was otherwise significant for hereditary spastic paraplegia. He lives in a wooded area in New England and enjoys hiking. He did not recall any tick bites and denied recent travel. He denied any history of recent blood transfusion. Laboratory findings from our hospital are summarized in Table 1 , notable for peripheral blood smear showing 51.6% of babesia parasitemia ( Figure 1 ). This potentially fatal high-grade parasitemia and end-organ involvement prompted immediate red blood cell (RBC) exchange. He was initially given 1 dose of clindamycin and quinine upon arrival. However, based on the experience of our center with severe babesiosis and frequent adverse effects with quinine, the treatment regimen was promptly changed to clindamycin (600 mg intravenously every 8 hours), azithromycin (500 mg intravenously every 24 hours), and atovaquone (750 mg by mouth every 12 hours). The patient underwent 2 RBC exchange sessions, with decline of his babesia percentage to 9.8% after the first session and to 1.8% after the second session. Clindamycin was continued for a total of 5 days and then discontinued. He remained on azithromycin (500 mg by mouth every 24 hours) and atovaquone, and by day 7 of admission his parasitemia burden decreased to <0.1%; other laboratory values are summarized in Table 1 . Babesia smear was negative on day 14 of his admission. He also received an empiric 14-day course of doxycycline, although his lyme serology and anaplasma PCR were negative. His hospitalization was complicated by renal failure requiring renal replacement therapy for 14 days. His mental status improved, along with hemolysisrelated laboratory values. He was discharged on day 22 of admission, remaining on atovaquone and azithromycin, both by mouth, and finished a course of treatment 14 days after parasitemia clearance. A B Figure 1 . A, Peripheral blood thin smear on the day of admission showing multiple infected red blood cells with intraerythrocytic, nonpigmented, pleomorphic, and pyriform ring-form trophozoites of Babesia spp. Vacuolated intraerythrocytic ring-form trophozoites and occasional clusters of clumped extracellular rings are also seen. The estimated parasitemia in 10 high-power fields was 51.6%. Occasional merozoite tetrad-forms can be found (Maltese cross-like tetrad). B, Peripheral blood thin smear on day 5 of this admission. The estimated parasitemia in 10 high-power fields was 0.35%. Scanned peripheral blood smears from day 1 (slide 1 thick smear and slide 2 thin smear) and day 5 can be found at https://learn. mghpathology.org/index.php/DZI20-180. In this case report, we described an asplenic patient who survived life-threatening severe babesiosis with an extraordinarily high-grade babesia parasitemia (>50%), treated with timely RBC exchange and an essentially quinine-free 3-drug regimen. The role of timely RBC exchange in this case needs to be highlighted, as it promptly decreased the overwhelming parasite burden (reviewed by Radcliffe et al. [5] ). However, there is no high-quality evidence to fully evaluate the utility of RBC exchange [2] . Based on a large case series of 34 cases, in which 7 patients underwent RBC exchange, a parasitemia level >10% was associated with the decision to perform RBC exchange, an arbitrary threshold unsupported by high-quality evidence. Clinicians would still need to be vigilant about post-RBC exchange parasitemia rebound [6] . Further clinical trials are warranted to fully evaluate the role of RBC exchange. While clindamycin and quinine are still recommended in the IDSA guideline [2] , a growing body of evidence suggests that a quinine-free regimen may work just as well in babesiosis [4] , even in severe cases [5, 7] . The rapid clinical improvement with RBC exchange in conjunction with this quinine-free regimen in this extraordinary level of parasitemia extends these observations. Of note, this patient did receive 1 dose of quinine upon arrival. Quinine has a half-life of 10-12 hours [8] , and this 1 dose was unlikely to contribute substantially to his treatment, as his parasitemia level was 10.0% after the first RBC exchange and 9.8% 12 hours before the second RBC exchange. Based on early animal model studies, we suggest that the efficacy of quinine in babesiosis treatment might be limited. As demonstrated by Rowin et al., quinine alone was not associated with parasitemia clearance in a hamster model. When compared with untreated controls (peak parasitemia levels 53%), the combination of clindamycin + quinine (peak 22%), compared with clindamycin alone (peak 27%), was only slightly better at reducing peak parasitemia levels, while parasitemia clearance after therapy ended in both treatment arms was complete [9] . Similarly, in another animal study comparing quinine, azithromycin, and quinine + azithromycin, quinine yielded the least antiparasitic effect compared with the other treatment group; when comparing the quinine + azithromycin group with the azithromycin only group, the parasitemia level was comparable at the end of the treatment course [10] . In light of this, we believe that the role of quinine in the recommended clindamycin + quinine combination adds considerable potential toxicity for minimal therapeutic gain. Thus, we would argue that atovaquone + azithromycin in place of quinine might be more efficacious against severe parasitemia with babesia, in addition to its superior side effect profile. Accumulating evidence from in vitro and animal models also suggests that agents including tafenoquine and clofazimine are potential candidates for severe babesiosis treatment [11] [12] [13] . Furthermore, we stress the critical importance of prompt RBC exchange in severe babesiosis to promptly decrease the parasitemia burden and halt further end-organ damage in the early phase of fulminant, lifethreatening disease. In summary, our report demonstrates that the combination of clindamycin, azithromycin, and atovaquone together with prompt RBC exchange is efficacious in life-threatening babesiosis. We advocate further clinical trials to compare clindamycin + quinine and clindamycin + azithromycin + atovaquone to provide better evidence supporting this quinine-free regimen in severe babesiosis. In addition, new agents such as tafenoquine and clofazimine could be other alternatives to quinine in babesiosis treatment, and further studies are warranted. 
paper_id= fffc2cc19b86ddf20cfda2934854909cac98667e	title= Low rate of COVID-19 pneumonia in kidney transplant recipients-A battle between infection and immune response?	authors= Maryam Ghaffari Rahbar;Mohsen  Nafar;Alireza  Khoshdel;Nooshin  Dalili;Alireza  Abrishami;| Ahmad Firouzan;Fatemeh  Poorrezagholi;Fariba  Samadian;Shadi  Ziaie;Somayeh  Fatemizadeh;Shiva  Samavat;	abstract= Abbreviations: ARBs, Angiotensin receptor blockers; CNI, calcineurin inhibitor; CRP, C-reactive protein; eGFR, estimated glomerular filtration rate; IVIG, intravenous immunoglobulin; LDH, lactate dehydrogenase; MDRD, modification of diet in renal disease; MMF/MPA, mycophenolate mofetil/mycophenolic acid; N/L ratio, neutrophil to lymphocyte ratio; PLT/ Lymph, platelet to lymphocyte ratio. Abstract Background: With COVID-19 pandemic, concerns about kidney transplant recipients are rising. However, the incidence, clinical course, outcome, and predictive factors of disease severity are obscured. Methods: We describe clinical and laboratory manifestations, radiologic findings, clinical course, and finally outcome of kidney transplant recipients with COVID-19 pneumonia. 	body_text= Following the outbreak of novel corona virus infection in the late December 2019 in China and its spread throughout the world, the disease was first reported in Iran on 20th of February 2020. Since then more than 85 000 have been diagnosed and 5297 had lost their lives in the country. (By April 22nd, 2020). According to the literature, older patients and those with comorbidities such as diabetes, hypertension, cardiovascular diseases, obesity, and immunosuppressed patients are at higher risk of morbidity and mortality. 1 Kidney transplant recipients are in a chronic immunosuppressive state, which theoretically might pose them to the higher risk of complications, uncommon presentations, and worse outcome comparing with general population. Nevertheless, the incidence, clinical course, and outcome of COVID-19 are not clear in this population. The aim of our study was to report clinical manifestations, laboratory findings, disease course, and outcome of 19 kidney transplant recipients who were admitted in Labbafinejad Medical Center from 20th February 2020 till 15th of April 2020. In Labbafinejad Medical Center, we perform around 200 kidney transplantation per year (nearly 60% of them from deceased donors). Of 2493 patients that are currently followed by active clinical or phone surveillance, 19 kidney transplant recipients were admitted with the diagnosis of COVID-19 from 20th February 2020 till 15th of April 2020. We described the clinical and laboratory manifestations, radiologic findings, clinical course, and outcome of these patients. All adult kidney transplant recipients who were admitted to our center were enrolled. Oro-or nasopharyngeal swabs were taken at presentation. Demographic data including age, gender, comorbidities, graft characteristics, and maintenance immunosuppressive therapy were extracted from patients' files. History of treatment for acute rejection episode during the past 12 months was registered. Clinical presentations and laboratory and radiologic findings were recorded. Laboratory assessments consisted of complete blood count, and differential cell count, serum creatinine, aspartate aminotransferase (AST) and alanine aminotransferase (ALT), serum albumin, ferritin, troponin, D-dimer, coagulation testing, C-reactive protein (CRP), lactate dehydrogenase (LDH), creatine kinase (CPK), and pH. Bacterial infections were ruled out by taking blood and urine cultures. IL-6 and fibrinogen were assessed. Allograft function was evaluated by creatinine-based MDRD equation of eGFR. Initial chest CT scans were interpreted by two independent radiologists. Predominant pattern of involvement, percentage of the lobar involvement, extension of involvement (Unilateral/ Bilateral), total lung score, and percentage of lung involvement were reported ( Figure 1 ). Clinical course, ICU admission, type of ventilatory support, vasopressor requirement, and the outcome of hospitalized COVID-19 patients were evaluated. Outcomes considered as admission to an intensive care unit, death, or discharge from the hospital. Patients were treated according to the protocol with antiviral treatment and immunosuppressive dose reduction (discontinuation of antimetabolites, CNI dose reduction in normoxemic patients and discontinuation among hypoxemic patients), prednisolone 20 mg daily or methylprednisolone 40 mg daily in normoxemia and hypoxemic patients, respectively, and IVIG 1-2 g/kg over 5 days in hypoxemic patients. 2 The antiviral therapy dosing was as followed: (a) hydroxychloroquine 200 mg twice daily in patients who were not treated with lopinavir/ritonavir, and 400 mg on day one in those treated with lopinavir/ritonavir; (b) lopinavir/ritonavir 400/100 mg twice daily; (c) favipiravir 1600 mg on day one and then 600 mg twice daily; (d) oseltamivir 75 mg twice daily (adjusted based on eGFR); and (e) ribaverin 1200 mg twice daily (adjusted based on eGFR). The duration of treatment was 7-10 days. Out of about 2500 kidney transplant recipients under follow-up in our center, only 19 patients were admitted with diagnosis of  Patients' demographic and clinical manifestations are demonstrated in Only three allograft recipients had been treated with plasmapheresis, IVIG, and rituximab for chronic active antibody-mediated rejection during the past 12-month prior to admission. Two of them experienced rejection within a month of COVID-19 infection and one, 6 months earlier. Maintenance immunosuppressive regimen was cyclosporin, MMF/MPA and prednisolone in 9 (47.4%), tacrolimus, MMF/MPA and prednisolone in 7 (36.8%), and sirolimus, MMF/MPA and prednisolone in 3 (15.8%). About 58% of our cases were treated with angiotensin receptor blockers (ARBs). Alive (N = 10) Age (mean ± SD), y 47. 6 ARBs, n (%) 11 (57.9) 5 (55.6) 6 (60.0) .84 Abbreviations: ARBs, angiotensin receptor blockers; eGFR, estimated glomerular filtration rate; MMF/MPA, mycophenolate mofetil/mycophenolic acid. In the study population, 53% had comorbidities (21% diabetes, 32% hypertension). The most common clinical manifestation at the onset of illness was fever (73.7%), followed by dry cough and dyspnea (68.4% and 52.6%, respectively). None of our cases had experienced gastrointestinal symptoms or anosmia. 89.5% of patients were hypoxemic (O2 saturation < 93% at ambient air) at presentation. On the day of admission, 84.2% of patients had lymphopenia which was defined as lymphocyte count less than 1100 per mm 3 .  Fifteen patients underwent lung CT scan early in the course of disease. The images were interpreted by two radiologists, independently (Table 3) . Two-thirds of patients had bilateral lung involvement with either ground-glass opacities or consolidation or both. None of our cases had cavitation, cystic changes, and lymphadenopathy. Three patients had pleural or pericardial effusion or both. Sixteen patients with low-dose CT scan were followed up with portable X-ray. Progression in lung involvement was found in serial CXRs.  Of total 19 patients, 10 (52.6%) were admitted to ICU, 9 (47.7%) of GG score, mean ± SD 6.9 (4.7) 9.  We analyzed those patients with positive COVID PCR by omitting four patients with only radiologic diagnosis. Comparing the demographic, clinical, laboratory, and radiologic findings of patients who survived and those who did not, the results were as followed. History of acute rejection during the past 12 months (P value .03), lower platelet count (P value .008), and positive D-dimer (P value .04) were still associated with poor outcome. Treatment with cyclosporine was more common among those who survived the course of disease (P value .02). Male gender and high LDH levels were more common in those who did not survive, although not statistically significant (P value .06). The degree of CT scan lung involvement was greater among non-survivors. The results are displayed in Table 5 . Nearly 73% of our cases had AKI at presentation, even before initiation of antiviral treatment. In various case series of patients with either transplanted or native kidneys, the incidence of AKI was about 40%. [9] [10] [11] The low incidence of disease could be on one hand due to respecting the social distancing rules by this group, or due to the fact that some levels of immunosuppression could protect them against the severe immunologic response, cytokine storm, and viral replica- 14, 15 Hence, the maintenance therapy with CNIs might play a protective role in transplanted patients. In this cohort, we did not have patients with history of transplantation of less than 2 years, as was the case with the Italian series. 16 The low rate of admission in transplant recipients specially in the first years of transplantation might be due to protective effects of immunosuppressive agents against cytokine storm or modification of immunity function. However, most centers including ours 2 stop the antimetabolite and significantly reduce the CNI level or even stop it, and keep the steroids, which was based on a low level of evidence. Despite discontinuation of CNI and antimetabolites in severe and hypoxemic cases and early initiation of antiviral treatment upon admission and treatment with IVIG (1-2 g/kg), the mortality rate was 45% in our center. This high mortality rate could be due to more severe disease among this population as noted in series of 20 patients in Italy, 16 Although transplant recipients seem to be affected less with severe type of disease (only 19 patients out of about 2500), which might be due to some levels of immunosuppression, patients who had a history of acute rejection and recent immunosuppressive intensification had worse outcomes. One of the major limitations of our study was the small sample size. A larger cohort in a multicenter study may help to drive a more solid conclusion. In conclusion, COVID-19 is a mysterious disease that presented as an infectious disease and evolved to a state of immunologic disturbances. Thereby, management of COVID-19 is a state of art, with initiation of antiviral therapy in early steps and thoughtful usage of immunosuppressive drugs. We acknowledge the efforts of our treatment and laboratory team in management of patients. Abbreviations: CC, consolidation; GG, ground-glass; MMF/MPA, mycophenolate mofetil/mycophenolic acid; N/L, neutrophil to lymphocyte ratio. 
paper_id= fffc847dd01347205e76429f665bc9dab096736b	title= Bayesian Synthetic Likelihood Estimation for Underreported Non-Stationary Time Series: Covid-19 Incidence in Spain	authors= David  Moriña;Amanda  Fernández-Fontelo;Alejandra  Cabaña;Argimiro  Arratia;Pedro  Puig;	abstract= The problem of dealing with misreported data is very common in a wide range of contexts for different reasons. The current situation caused by the Covid-19 worldwide pandemic is a clear example, where the data provided by official sources were not always reliable due to data collection issues and to the high proportion of asymptomatic cases. In this work, we explore the performance of Bayesian Synthetic Likelihood to estimate the parameters of a model capable of dealing with misreported information and to reconstruct the most likely evolution of the phenomenon. The performance of the proposed methodology is evaluated through a comprehensive simulation study and illustrated by reconstructing the weekly Covid-19 incidence in each Spanish Autonomous Community in 2020. arXiv:2104.07575v1 [stat.ME] 15 Apr 2021 	body_text= The Covid-19 pandemic that is hitting the world since late 2019 has made evident that having quality data is essential in the decision making chain, especially in epidemiology but also in many other fields. Many methodological efforts have been made to deal with misreported Covid-19 data, following ideas introduced in the literature since the late nineties [6, 4, 20, 1, 24, 10] . These proposals range from the usage of multiplication factors [22] to Markov-based models [5, 14] or spatio-temporal models [23] . Additionally, a new R [19] package able to fitting endemic-epidemic models based on approximative maximum likelihood to underreported count data has been recently published [12] . More recently, several approaches based on discrete time series have been proposed [7, 9, 8] although there is a lack of continuous time series models capable of dealing with misreporting, a characteristic of the Covid-19 data and typically present in infectious diseases modeling. In this sense, a new approach for longitudinal data not accounting for temporal correlations is introduced in [16] and a model capable of dealing with temporal structures using a different approach is presented in [15] . Synthetic likelihood is a recent and very powerful alternative for parameter estimation in a simulation based schema when the likelihood is intractable and, conversely, the generation of new observations given the values of the parameters is feasible. The method was introduced in ([25]) and placed into a Bayesian framework in [18] , showing that it could be scaled to high dimensional problems and can be adapted in an easier way than other alternatives like approximate Bayesian computation (ABC). The method takes a vector summary statistic informative about the parameters and assumes it is multivariate normal, estimating the unknown mean and covariance matrix by simulation to obtain an approximate likelihood function approximate likelihood function of the multivariate normal. There is an enormous global concern around 2019-novel coronavirus (SARS-CoV-2) infection in the last months, leading the World Health Organization (WHO) to declare public health emergency [21] . As a large proportion of the cases run asymptomatically [17] and mild symptoms could have been easily confused with those of similar diseases at the beginning of the pandemic, its reasonable to expect that Covid-19 incidence has been notably underreported. Consider an unobservable process Xt following an AutoRegressive-Moving Average (ARM A(p, r)) structure, defined by where t is a Gaussian white noise process with t ∼ N (µ , σ 2 ). In our setting, this process Xt cannot be directly observed, and all we can see is a part of it, expressed as Xt with probability 1 − ω q · Xt with probability ω The interpretation of the parameters in Eq. (2) is straightforward: q is the overall intensity of misreporting (if 0 < q < 1 the observed process Yt would be underreported while if q > 1 the observed process Yt would be overreported). The parameter ω can be interpreted as the overall frequency of misreporting (proportion of misreported observations). To model consistently the spread of the disease, the expectation of the innovations t in 1 is linked to a simplified version of the well-known compartimental Susceptible-Infected-Recovered (SIR) model. At any time t ∈ R there are three kinds of individuals: Healthy individuals susceptible to be infected (S(t)), infected individuals who are transmitting the disease at a certain speed (I(t)) and individuals who have suffered the disease, recovered and cannot be infected again (R(t)). As shown in [8] , the number of affected individuals at time t, A(t) = I(t) + R(t) can be approximated by , being β the infection rate, γ the recovery rate and N the size of the susceptible population. At any time t the condition S(t) + I(t) + R(t) = N is fulfilled. The expression 3 allow us to incorporate the behaviour of the epidemics in a realistic way, defining The Bayesian Synthetic Likelihood (BSL) simulations are based on this model and the chosen summary statistics are the mean, standard deviation and the three first coefficients of autocorrelation of the observed process. Parameter estimation was carried out by means of the BSL [3, 2] package for R [19] . Taking into account the posterior distribution of the estimated parameters, the most likely unobserved process is reconstructed, resulting in a probability distribution at each time point. The prior of each parameter is set to be uniform on the corresponding feasible region of the parameter space and zero elsewhere. The performance and an application of the proposed methodology are studied through a comprehensive simulation study and a real dataset on Covid-19 incidence in Spain on this Section. A thorough simulation study has been conducted to ensure that the model behaves as expected, including AR(1), M A(1) and ARM A(1, 1) structures for the hidden process Xt defined as where t ∼ N (µ , σ 2 ) . The values for the parameters α, θ, q and ω ranged from 0.1 to 0.9 for each parameter. Average absolute bias, average interval length (AIL) and average 95% credibility interval coverage are shown in Table 1 . To summarise model robustness, these values are averaged over all combinations of parameters, considering their prior distribution is a Dirac's delta with all probability concentrated in the corresponding parameter value. For each autocorrelation structure and parameters combination, a random sample of size n = 1000 has been generated using the R function arima.sim, and the parameters m = log(M * ) and β have been fixed to 5 and 0.4 respectively. Several values for these parameters were considered but no substantial differences in the model performance were observed related to the value of these parameters or sample size, besides a poorer coverage for lower sample sizes, as expected.  The betacoronavirus SARS-CoV-2 has been identified as the causative agent of an unprecedented world-wide outbreak of pneumonia starting in December 2019 in the city of Wuhan (China) [21] , named as Covid-19. Considering that many cases run without developing symptoms or just with very mild symptoms, it is reasonable to assume that the incidence of this disease has been underregistered. This work focuses on the weekly Covid-19 incidence registered in Spain in the period (2020/02/19-2020/12/15) excluding the two autonomous cities Ceuta and Melilla, with very low incidences during all considered time period. It can be seen in Figure 1 that the registered data (turquoise) reflect only a fraction of the actual incidence (red). The grey area corresponds to 95% probability of the posterior distribution of the weekly number of new cases (the lower and upper limits of this area represent the percentile 2.5% and 97.5% respectively), and the dotted red line corresponds to its median. In the considered period, the official sources reported 1,819,982 Covid-19 cases in Spain (excluding Ceuta and Melilla), while the model estimates a total of 3,055,550 cases (only 59.56% of actual cases were reported). These work also shows that while the frequency of underreporting is extremely high for all regions (values close to 1) except Pais Vasco, the intensity of this underreporting is not uniform across the considered regions, as shown in Table 2 . It can be seen that Andalucía is the CCAA with highest underreporting intensity (q = 0.42) while La Rioja is the region where the estimated values are closest to the number of reported cases (q = 0.58). Figure 2 shows the evolution of the registered (turquoise) and estimated (red) weekly number of Covid-19 cases in Spain in the period 2020/02/19-2020/12/15, excluding the autonomous cities of Ceuta and Melilla.  Although it is very common in biomedical and epidemiological research to get data from disease registries, there is a concern about their reliability, and there have been some recent efforts to standardize the protocols in order to improve the accuracy of health information registries (see for instance [13, 11] ). However, as the Covid-19 pandemic situation has made evident, it is not always possible to implement these recommendations in a proper way. The analysis of the Spanish Covid-19 data shows that in average only about 60% of the cases in the period 2020/02/19-2020/12/15 were reported. Having accurate data is key in order to provide public health decision-makers with reliable information, which can also be used to improve the accuracy of dynamic models aimed to estimate the spread of the disease [26] and to predict its behavior. The proposed methodology can deal with misreported (over-or under-reported) data in a very natural and straightforward way, and is able to reconstruct the most likely hidden process, providing public health decision-makers with a valuable tool in order to predict the evolution of the disease under different scenarios. The simulation study shows that the proposed methodology behaves as expected and that the parameters used in the simulations, under different autocorrelation structures, can be recovered, even with severely underreported data. 
paper_id= fffc88be66eb39823fc9b50f0683e06a3038c9fe	title= A fractional-order compartmental model for predicting the spread of the Covid-19 pandemic	authors= T A Biala;A Q M Khaliq;	abstract= We propose a time-fractional compartmental model (SEI A I S HRD) comprising of the susceptible, exposed, infected (asymptomatic and symptomatic), hospitalized, recovered and dead population for the Covid-19 pandemic. We study the properties and dynamics of the proposed model. The conditions under which the disease-free and endemic equilibrium points are asymptotically stable are discussed. Furthermore, we study the sensitivity of the parameters and use the data from Tennessee state (as a case study) to discuss identifiability of the parameters of the model. The non-negative parameters in the model are obtained by solving inverse problems with empirical data from California, Florida, Georgia, Maryland, Tennessee, Texas, Washington and Wisconsin. The basic reproduction number is seen to be slightly above the critical value of one suggesting that stricter measures such as the use of face-masks, social distancing, contact tracing, and even longer stay-at-home orders need to be enforced in order to mitigate the spread of the virus. As stay-at-home orders are rescinded in some of these states, we see that the number of cases began to increase almost immediately and may continue to rise until the end of the year 2020 unless stricter measures are taken. 	body_text= Fractional differential equations (FDEs) are used to model complex phenomena such as such as the modeling of memory-dependent phenomena (DiGuiseppe et al. [1] , Baleanu et al. [2] , Podlubny [3] ), mechanical properties of materials (Caputo and Mainardi [4] ), anomalous diffusion in porous media (Fomin et al. [5] , Metzler and Klafter [6] ), groundwater flow problems (Cloot and Botha [7] , Iaffaldano et al. [8] ), and control theory (Podlubny [9] ), among others. They serve as a generalization of the integer-order differential equations and give more degree of freedom for modeling of biological and physical processes. FDEs have been applied in biological tissues [10] , DNA sequencing [11] , Pine Wilt disease [12] , lung tissue mechanics and models [13] , harmonic oscillators [14] , Dengue fever [15] , measles [16] , human liver [17] , diffusion processes [18] , SEIR models [19] . Infectious disease outbreaks are one of the main causes of deaths in human. Their dynamics and spread are modeled and studied before the introduction of vaccines. The novel coronavirus began in December 2019 in China. Over the last few months, it has spread rapidly leading to over 400,000 deaths across the globe. The first occurrence in the United States was seen around mid January in Washington [20] and has spread across America with over 100,000 deaths and 1.5 million infected. The pandemic has disrupted the day-to-day activities of the human life with over six million jobs lost in the United States. Several actions and measures have been taken by the federal, state and local governments to mitigate the spread of the pandemic. The most prominent measures taken include social distancing, testing, use of face-masks and contact tracing. It is important to model this pandemic in order to better understand the spread and dynamics as well as address the challenges of the pandemic. In short, mathematical models are important to guide the decisions of health and government officials. The goal of this study is to examine and analyze the spread of the pandemic using a modification of the Susceptible-Exposed-Infected-Recovered (SEIR) model with a time-fractional derivative. The use of fractional derivatives in the model stems from the fact that the spread of infectious diseases depends not only on the current state but also on its past states (history or memory dependency). Additionally, time-fractional order models reduce errors resulting from neglect of parameters in models. We shall focus on some selected states in the US. We note that models that consider the US as a whole may be misleading and have limited applicability as different states have different economical and political impacts on the pandemic. For example, while some states such as Maryland, New Jersey, New York, Connecticut, among others, enforced the use of masks in public places and longer stay-at-home order [21] , other states do not enforce these measures thereby allowing for a possibility of increase of infected individuals in such states. There have been several models for the study of the pandemic. Lu et al. [22] considered a fractional-order SEIHDR model which incorporates intercity movements. Liu et al. [23] studied the dynamics of the pandemic by considering asymptomatic and symptomatic infected populations separately. Wu et al. [24] studied domestic and international spread of the pandemic by using different data sets. Zhao and Chen [25] discussed the dynamics of the pandemic by considering the Susceptible, unquarantined infected, quarantined infected and Confirmed infected (SUQC) model and parametrize the intervention effect of control measures. Zhang et al. [26] considered a fractional SEIR model with different order of the time-fractional derivative for each of the different population being studied. The remainder of the paper is organized as follows: In section 2, we give some preliminary results and definitions from fractional Calculus. Furthermore, we discuss the formulation of the model by considering seven compartments: Susceptible-Exposed-Asymptomatic infected-Symptomatic infected, Hospitalized, Recovered and Dead populations (SEI A I S HRD). Section 3 details the properties and theoretical analysis of the model. Section 4 discusses the parameter sensitivity and identifiability analysis. In section 5, we applied the model to observed data for some selected states in the US. In particular, this section details solving several inverse problems for parameter estimation and computation of the basic reproduction number. Finally, we give some concluding remarks in Section 6. In this section, we present some preliminary results and definitions in fractional calculus. which converges in the right half of the complex plane Re(z) > 0. Definition 2.2. [27] For any t > 0, the Caputo-fractional derivative of order α, (n < α ≤ n − 1) of a function f (t) is defined as The Mittag-Leffler function which generalizes the exponential function for fractional calculus is defined as Definition 2.4. [28] A point x * is said to be an equilibrium point of the system Definition 2.5. [22] An equilibrium point x * of the system t0 D α t x(t) = f (t, x(t)), x(t 0 ) > 0 is said to be asymptotically stable if all he eigenvalues of the Jacobian matrix J = ∂f /∂x, evaluated at the equilibrium point, satisfies |arg(λ i )| > απ 2 , where λ i are the eigenvalues of J. The model discussed in this work is a modification of the SEIR model having three additional compartments. We consider a SEI A I S HRD compartmental model which comprises of the susceptible, exposed, infected (asymptomatic and symptomatic), hospitalized, recovered and dead population. We assume that the natural death and birth rates are the same. We further assume that deaths in the S and R compartments are due to natural deaths and deaths in the other compartments are as a result of the pandemic. The schema below shows the transmission flow of the model. Thus, the model consists of the following system of ODEs The system has the associated initial data The total population is N which is further divided into S(t), E(t) , I A (t), I S (t), H(t) and R(t), b 1 is the natural birth rate, β A and β S are the transmission rates to the susceptible population from the asymptomatic and symptomatic populations, respectively. 1/σ is the incubation period of an exposed individual, η denotes the fraction of the exposed population that becomes asymptomatic after the incubation period and the remaining (1 − η) of the population are symptomatic. γ 1 and γ 2 are the infectious rates of an asymptomatic and a symptomatic individual, respectively. ρ is the recovery rate through hospitalization and, µ 1 , µ 2 , µ 3 and µ 4 are the mortality rates of the exposed, asymptomatic, symptomatic and hospitalized populations, respectively. We note that the death population in the model comprises of deaths during exposure µ 1 E(t), infectious period (µ 2 I A and µ 3 I S ) and hospitalization µ 4 H, and assume that deaths due to other natural occurrences are negligible for this populations. We note that the parameters of the model are non-negative and have dimensions given by 1/time α . This observation was originally noted in Diethelm [15] . To alleviate this difference in dimensions, we replace the parameters (except η) with a power α of new parameters to obtain the new system of equations (1) In this section, we discuss the properties of the model beginning with the existence, uniqueness, non-negativity and boundedness of solutions of the model (1) . For simplicity in analysis, we reduce the system (1) to since D is a linear combination of populations in some of the other compartments. Theorem 3.1. There exist a unique solution to the system (2) and the solution is non-negative and bounded for any given initial data Proof. See Appendix A. 3.1. Stability Analysis 3.1.1. Computation of the basic reproduction number R 0 We shall use the next generation matrix originally proposed by Diekmann et al. [29] and further elaborated on by van den Driesche and Watmough [30] and Diekmann et al. [31] to determine R 0 . According to system (2), the diseasefree equilibrium point (DFE) is (N, 0, 0, 0, 0, 0). Now, consider the four compartments Y = (Y 1 , Y 2 , Y 3 , Y 4 ) = (E, I A , I S , H) containing the infected individuals and let Y * be the DFE point. Since the DFE point exists and is stable (shown in the next section) in the absence of any disease, then the linearized equation at the DFE is is the rate of appearance of new infections in compartment i and V i (Y ) is the rate of transfer of infections to and from compartment i. We further define Then ρ(F V −1 ) is the basic reproduction number R 0 , where ρ(x) is the spectral radius of x and F V −1 is the next generation matrix. For the system (2), and the basic reproduction number is given as . (3) Lemma 3.1. The fractional system (2) has at most two equilibrium points Theorem 3.2. The DFE point is locally asymptotically stable if R 0 < 1 and . Proof. See Appendix B. Proof. See Appendix C. We discuss the sensitivity and identifiability of the parameters with respect to the proposed model. The sensitivity analysis (SA) deals with the significance or importance of the parameters in the model. In particular, it finds the most influential parameters that drives the dynamics of the model. It also describes the extent to which parameter changes affects the result of the methods or models with the goal of identifying the best set of parameters that describes the process or phenomena in question. There are several SA methods which are broadly classified as local and global methods. In this work, we shall focus on the Morris screening method (local method) and Sobol analysis method (global method). The Morris screening method is a local sensitivity measure that makes use of the first order derivative of an output function y = f (θ) = f (θ 1 , · · · , θ p ) with respect to the input parameter θ. It measures the effect of the output when the input variable is perturbed one at a time around a nominal value. It serves as a first check, in most analysis, in screening parameters for identifiability. The method evaluates elementary effects [32, 33, 34] with the ith parameter through the forward perturbation Morris [35] proposed two sensitivity measures, the mean (µ) and the standard deviation (σ) of the elementary effects. For non-monotonic models, µ may lead to a very small value due to cancellation effects. For this reason, Campolongo et al. [36] proposed the use of absolute values for evaluating the mean. In order to obtain a dimension-free sensitivity, we prefer the use of the sensitivity measure δ given in Brun et al. [37] as where N is the number of sample points and . A common practice in the literature [38, 39, 34] is to plot the indices δ againstσ, the standard deviation. We see from Fig. 2 (a) that the fractional order α has the highest influence on the model output over time. The transmission rates for the symptomatic and asymptomatic populations and, the fractional parameter η are the next most influential parameters in the model. This is further corroborated by Fig. 2(b) . The four parameters (α β S , β A and η) denoted by red squares are the most important parameters, that is they have the largest δ andσ values. The parameters (b 1 , ρ, µ 1 , µ 2 , µ 3 and µ 4 ) represented by the blue squares have the least influence on the model output and can be considered unimportant. The other parameters represented by the green squares have more influence than the parameters represented by the blue squares. One major setback of the Morris screening test for sensitivity analysis is the consideration of each parameter individually and independently of the other parameters. In real applications, this is not true as parameters have collinearity and dependencies on one another.  The Sobol method is a variance-based sensitivity analysis method which unlike the Morris screening method takes into account the effect of the relationship between each parameters of the model. It uses the decomposition of variance to calculate Sobol's sensitivity indices: first and total order sensitivity measures. The basic idea of the Sobol's method is the decomposition of the model output function y = f (θ 1 , · · · , θ p ) into summands of increasing dimensionality, that is where V i is the partial variance of the contribution of the parameter θ i and V i,··· ,s is the partial variances caused by the interaction of the parameters (θ 1 , · · · , θ s ) for s ≤ p. The first order sensitivity index measures the main effect of parameter θ i on the model output; that is the partial contribution of θ i to the variance V (y).The index [40, 41] is defined as The larger this index, the more sensitive the parameter is to the model output [40, 41] . Using the law of total variances [41, 34] , the index can also be expressed as and where V θi (E θ∼i (y|θ i )) is the partial variance caused by θ i and E θ∼i (y|θ i ) is the mean of the model output calculated by using all the values of the other parameters θ ∼i (except θ i ) and V (y) is the total variance. The total sensitivity indices [42] measures the effects of parameter θ i and the interaction with the other parameters. It is defined as The total variance, V (y), for this index is given as The mean and the variance can be evaluated using quasirandom sampling method [43, 34] and are given as where A and B are two independent parameter sample matrices of dimensions N × p. We shall use the python SALib package [44] to compute the first and total order variance indices. Fig. (3) shows that the fractional order β S has the highest interaction with the other parameters. These results are consistent with the results in the Morris screening test as important parameters of the test show high interaction with the other parameters.  The concept of identifiability is dependent on sensitivity. It entails the selection of the subset of parameters of a model having little or no collinearity and uncertainty, and which can be identified uniquely from a given set of observed data or measurements. In other words, it answers the question "Can the available data be described by the model and the selected subset of parameters?". There are several techniques or tests for parameter identifiability. Most of the tests are based on the Fisher information matrix (FIM) F = χ T χ where χ = ∂y/∂θ for a model output function y. Cobelli and Di Stefano [45] showed that a sufficient condition for identifiability is the non-singularity of FIM. Burth et al. [46] proposed an iterative estimation process which implements a reducedorder estimation by finding parameters whose axis lie closest to the direction of FIM. The associated parameter values are then fixed at prior estimates during the iterated process. Brun et al. [37] studied parameter identifiability using two indices; a parameter importance ranking index δ and a collinearity index γ K which depends on the smallest eigenvalues of submatrices of χ T χ corresponding to the parameter subset K. Cintrón-Arias et al. [47] explained the need for a good parameter subset for identifiability to satisfy the full rank test. They further introduced two indices; the selection score and the condition number of χ T χ. The smaller these indices the lesser the collinearity and uncertainty in the parameter values of the subset. Finally, they used the coefficient of variation index to examine the effect of parameters in the parameter subset. In this work, we shall use the test proposed by Cintrón-Arias et al. in identifying the parameters. The algorithm can be summarized in the following steps Algorithm 1 Algorithm for Parameter subset Selection [47] 1: Perform a combinatorial search for all possible parameter subsets. Let S p = {θ = (λ 1 , λ 2 , · · · , λ p ) ∈ R p λ k ∈ I and λ k = λ m ∀ k, m = 1, · · · , p}, where I = {b 1 , β A , β S , σ, γ 1 , γ 2 , ρ, µ 1 , µ 2 , µ 3 , µ 4 , η, α}. 2: Select parameter subsets that pass the full rank test; that is and Σ(θ) = σ 2 0 χ T (θ)χ(θ) −1 ∈ R p . Calculate the condition number κ(χ(θ)) for each parameter subset θ ∈ Θ p . The smaller the values of κ(χ(θ)) and ϑ(θ), the lower the uncertainty possibilities in the estimate. To discuss the results in this section, we shall use the state of Tennessee as a case study to understand parameter identifiability. Furthermore, we used the following values obtained using a random search algorithm as the nominal parameter set θ 0 for the model: b 1 = 8.0316e-07, β A = 1.0000e-10, β S = 1.2312e+00, σ = 9.9618e-01, γ 1 = 2.6628e-02, γ 2 = 1.0000e+00, ρ = 6.1740e-03, µ 1 = 3.3491e-03, µ 2 = 1.0000e-10, µ 3 = 3.0859e-04, µ 4 = 2.1222e-05, η = 1.5683e-01, α = 1.0000e+00 and the nominal error variance σ 0 = 10. We further divide the parameters into three groups according to their importance rankings discussed in the previous section: where S 1 and S 3 are the most and least influential parameter sets, respectively, while S 2 contains more influential parameters than S 3 . We display some selections of the parameter subsets of size p in Table 1 where we have chosen the subsets with the smallest score values. The entries in Table 1 are ordered with respect to the selection score ϑ(θ) for each subset of same cardinality. A high selection score and condition number for a parameter subset indicates substantial collinearity and linear dependence, and thus is poorly identifiable even if the parameter subsets contains S 1 , that is contains the set of most influential parameters. We observe that most of the selections in Table 1 contains at least one element in each of the groups listed above. This shows that while parameter importance ranking is crucial in identifying parameters that drives the dynamics of a model, it does not have substantial effect in identifiability. Identifiability depends on proper selection of subsets including parameters in each of the three groups above that describes the measurement or data. To have an idea of the variations of the condition number and the selection score, we give a plot of these values for p = 2 in fig. 4 (with logarithmic scales). Good parameter combination in fig. 4 corresponds to values in the lower left corner of the figure where the values, ϑ(θ) and κ(χ(θ)), are relatively small. p Parameter Subsets κ(χ(θ)) ϑ(θ) 13 (b 1 , β A , β S , σ, γ 1 , γ 2 , ρ, µ 1 , µ 2 , µ 3 , µ 4 , η, α) 9.530e+03 8.481e+01 12 (b 1 , β A , β S , σ, γ 1 , γ 2 , ρ, µ 1 , µ 3 , µ 4 , η, α) 9.513e+03 6.147e+00 11 (b 1 , β S , σ, γ 1 , γ 2 , ρ, µ 1 , µ 3 , µ 4 , η, α) 4.814e+03 2.479e-04 10 (b 1 , β S , σ, γ 1 , γ 2 , ρ, µ 1 , µ 4 , η, α) 3.696e+03 3.852e-07 (b 1 , β S , σ, γ 1 , γ 2 , ρ, µ 3 , µ 4 , η, α) 3.685e+03 4.143e-06 9 (β S , σ, γ 1 , γ 2 , ρ, µ 1 , µ 4 , η, α) 3.411e+03 2.766e-07 (b 1 , β S , σ, γ 1 , γ 2 , ρ, µ 4 , η, α) 3.311e+03 2.824e-07 7 (β S , γ 1 , γ 2 , ρ, µ 4 , η, α) 2.718e+03 5.344e-08 (β S , γ 1 , γ 2 , ρ, µ 1 , η, α) 2.947e+03 5.388e-08 5 (γ 1 , γ 2 , ρ, µ 4 , α) 6.171e+01 2.006e-09 (β S , γ 1 , ρ, µ 4 , α) 6.389e+01 2.006e-09 (γ 1 , ρ, µ 4 , η, α) 6.989e+01 2.010e-09 4 (γ 1 , ρ, η, α) 6.258e+01 3.458e-10 (γ 1 , γ 2 , ρ, α) 5.289e+01 3.491e-10 (β S , γ 1 , ρ, α) 5.349e+01 3.496e-10 3 (γ 2 , ρ, α) 5.140e+01 5.431e-11 (β S , ρ, α) 5.203e+01 5.850e-11 (ρ, η, α) 6.224e+01 9.109e-11 To further analyze the parameter identifiability of the model, we consider the parameters subsets: θ 1 = (b 1 , β A , β S , σ, γ 1 , γ 2 , ρ, µ 1 , µ 2 , µ 3 , µ 4 , η, α), θ 2 = (b 1 , β S , σ, γ 1 , γ 2 , ρ, µ 1 , µ 3 , µ 4 , η, α), θ 3 = (β S , σ, γ 1 , γ 2 , ρ, µ 1 , µ 4 , η, α), θ 4 = (β S , γ 1 , γ 2 , ρ, µ 4 , η, α), θ 5 = (γ 1 , γ 2 , ρ, µ 4 , α), θ 6 = (γ 1 , γ 2 , ρ, α), such that θ i+1 ⊂ θ i , i = 1, · · · , 6. The choice of these parameter subsets are due to their relative small condition numbers and selection scores. In other to create synthetic data, we assume the nominal parameter subsets and error variance (given at the beginning of this section) to be the true parameter vectors and true variance. Furthermore, we add random noise to the model output as follows: Y j = z(t j , θ 0 ) + σ 0 N (0, 1), j = 1, · · · , N. We solve seven inverse problems for each of the parameter subsets θ i , i = 1, · · · , 7. We analyze the result using the coefficient of variation and standard error [47] given as From Table 2 , it is seen that the parameters (µ 2 , µ 3 ) ⊂ θ 1 have standard errors that is approximately 20 times their estimates. This shows substantial uncertainty in these parameter values and any parameter subset containing these parameters may result in illogical parameter estimation from observations. The standard errors of β S , γ 1 , ρ, µ 3 , µ 4 , η, α in θ 2 show improvements and implies lower linear dependence and collinearity for parameters in θ 2 than those in θ 1 . Thus, a substantial improvement in uncertainty quantification is seen from θ 1 to θ 2 . Further improvements are observed for each of the other parameter subsets as more parameters are removed. For instance, with the removal of β S and η in θ 4 , it seen that the standard error for γ 1 and γ 2 dropped from 4% and 1% to approximately 0.07% and 0.003%, respectively, of their estimates. Other improvements in θ 5 include α where the standard error is reduced to at least one-tenth of its standard error in θ 4 . We note that there is no substantial gain in the removal of µ 4 from θ 5 and γ 1 in θ 6 as seen in Table 2 . Parameter identifiability might be misleading without the investigation of the residual of the model [47] . The Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC) indices make use of residuals to determine the quality of models in the presence of a given set of data. Table 3 shows the AIC and BIC estimates for each parameter set θ i , i = 1, · · · , 7. It is seen that the best improvements occur from θ i to θ i+1 for i = 2, 3. Thus, the best case scenario of uncertainty quantification obtained for this analysis is that of θ 4 . Finally, we present results in Table 4 obtained from solving the inverse problem for θ 4 using the data given in [48] . The remaining parameters are fixed using the nominal parameters above and T * is the number of days used in the simulation Seven inverse problems for California, Florida, Georgia, Maryland, Texas, Washington and Wisconsin were solved to estimate the full parameter sets of the model. As seen in fig. 5 , the fits are reasonably good even for states like Tennessee and Wisconsin whose current infected population begins to flatten. Table 5 shows the fit parameter sets for each of the states with T * being the [52] . The incubation period 1/σ α is seen to be relatively small, around 1-2 days. The average length of active infection 1/γ α 1 and 1/γ α 2 of the asymptomatic and symptomatic population, respectively, is seen to be around 1-27 days which is in line with suggested range of days in the literature [53, 54] . The recovery rate via hospitalization is seen to be very small for California, Florida, Georgia and Washington where the data records little or no recovery at all for infected patients. The death rates µ α 1 , µ α 2 , µ α 3 , µ α 4 from the exposed, asymptomatic infected, symptomatic infected and hospitalized compartments are seen to be relatively small ranging from 0-0.01 day −1 . With a µ α 4 value of around 0, Florida and Georgia has no death from the hospitalized compartments. This is reasonable since ρ α is also practically 0 indicating no recovery for this states as seen in the data (not shown here). Table 6 shows the basic reproduction number R 0 values computed for our model using (3) and the parameters listed in Table 5 . The epidemic is expected to continue indefinitely if R 0 > 1 as predicted for all states. This suggest that stricter measures such as the use of masks in public places, social distancing, contact tracing and even longer stay-at-home orders need to be enforced in order to eradicate the epidemic. Fig. 5 shows plot of the infected (asymptomatic plus symptomatic), recovered and deaths. We see that shape of most curves are similar except for Wisconsin and Tennessee where the infected population begin to flatten over time. In particular, spikes of the curves in fig. 5 begin around late March or early April 2020 and continues to rise. Fig. 6 shows the simulation results of the model till July 2021. States like California, Florida, Maryland, Tennessee and Wisconsin are seen to have their peaks (infected) around June or July 2020 and then reducing over time before the end of the year. Texas is seen to have peaks in November 2020. Georgia and Washington have similar results except that the peak only drops to a certain level indicating that the epidemic will linger in these states for longer periods. In fig. 7 , we show the cumulative infected and hospitalized population. This plot also shows that while the infected population flatten over time for states like California, Florida, Maryland, Tennessee, Texas and Wisconsin, it increases for Georgia and Washington. If drastic measures are not taken, the model suggest that many will be infected, at some point, before the end of 2021 in these states.  A fractional-order compartmental model is proposed to study the spread and dynamics of the Covid-19 pandemic. We studied the properties of the model and discussed the parameters of the model in the context of sensitivity and identifiability. We solve several inverse problems to estimate the fit parameters of the model using data from John Hopkins University [55] for some selected states in the US. The basic reproduction number R 0 of the model was computed and shows that the states considered have R 0 values slightly greater than the critical value of one. The model suggests that stricter or aggressive measures need be enforced in order to slow the spread of the virus.  Thus, F satisfies the local Lipschitz conditions with respect to X which proves the uniqueness and boundedness of solution to (2) . Next we show the non-negativity of solutions. At first, we consider moving along the S-axis, that is E(0) = I A (0) = I S (0) = H(0) = R(0) = 0 and 0 < S(0) = S 0 ≤ N , then S whose solution is given as In a similar manner, moving along each of the other respective axis (that is all initial conditions are zeros except for the axis being considered), it is easy to show that Therefore, all axis are non-negative invariant. Now, if the solution of the system is positive in the E − I A − I S − H − R plane, then let S(t * ) = 0, E(t * ) > 0, I A (t * ) > 0, I S (t * ) > 0, H(t * ) > 0 and R(t * ) > 0 for some t * such that S(t) < S(t * ). But 0 D α t S | t=t * = b α 1 N > 0 in this plane. Using the mean value theorem for Caputo-fractional derivative for some τ ∈ [t * , t), we see that S(t) > S(t * ). This contradicts our previous statement. Similar arguments can be used for each of the remaining population variables. Appendix B. Proof of Theorem 3.2 Proof. The Jacobian matrix evaluated at the DFE point is given as The eigenvalues of the Jacobian matrix are given as λ 1 = λ 2 = −b α 1 , λ 3 = −(ρ α + µ α 4 ) and the roots of the equation . To show stability, we apply the Routh-Hurwitz criterion. Clearly A > 0 and C > 0 if R 0 < 1. It is easy to show that AB > C if K < 1 which completes the proof. Appendix C. Proof of Theorem 3.3 Proof. The Jacobian matrix evaluated at the EE point is given as The eigenvalues of the Jacobian matrix are λ 1 = −b α 1 , λ 2 = −(ρ α + µ α 4 ) and the roots of the equation z 4 + A 1 z 3 + A 2 z 2 + A 3 z + A 4 . By Applying the Routh-Hurwitz criterion, the EE is stable if Data for cumulative confirmed cases, recovered and deaths were obtained from the Tennessee Department of Health (TDH) and Johns Hopkins University (JHU) Center for Systems Science and Engineering which are made available to the public on TDH's website [48] and Github [55], respectively. The raw files are converted into Panda data frames and stored for ease of access. For the files obtained from [55], the data are sorted according to counties, so we aggregate the cases for each of the recorded counties to obtain the total number of cases in each day for the states. For the total number of population for each states, we used the data obtained from the US Census Bureau [57]. All numerical simulations were done in python using our numerical scheme [58] from which we obtain the solution of the proposed model at each time step as 1. Predictor: S p = S j + τ α Γ(1 + α) F 1 (t j , S j , E j , I A,j , I S,j , H j , R j , D j ) +H 1,j E p = E j + τ α Γ(1 + α) F 2 (t j , S j , E j , I A,j , I S,j , H j , R j , D j ) +H 2,j I A,p = I A,j + τ α Γ(1 + α) F 3 (t j , S j , E j , I A,j , I S,j , H j , R j , D j ) +H 3,j I S,p = I S,j + τ α Γ(1 + α) F 4 (t j , S j , E j , I A,j , I S,j , H j , R j , D j ) +H 4,j H p = H j + τ α Γ(1 + α) F 5 (t j , S j , E j , I A,j , I S,j , H j , R j , D j ) +H 5,j R p = R j + τ α Γ(1 + α) F 6 (t j , S j , E j , I A,j , I S,j , H j , R j , D j ) +H 6,j D p = D j + τ α Γ(1 + α) F 7 (t j , S j , E j , I A,j , I S,j , H j , R j , D j ) +H 7,j 2. Corrector: α F 1 (t j , S j , E j , I A,j , I S,j , H j , R j , D j ) + F 1 (t j+1 , S p , E p , I A,p , I S,p , H p , R p , D p ) +H 1,j , E j+1 = E j + τ α Γ(2 + α) α F 2 (t j , S j , E j , I A,j , I S,j , H j , R j , D j ) + F 2 (t j+1 , S p , E p , I A,p , I S,p , H p , R p , D p ) +H 2,j , I A,j+1 = I A,j + τ α Γ(2 + α) α F 3 (t j , S j , E j , I A,j , I S,j , H j , R j , D j ) + F 3 (t j+1 , S p , E p , I A,p , I S,p , H p , R p , D p ) +H 3,j , I S,j+1 = I S,j + τ α Γ(2 + α) α F 4 (t j , S j , E j , I A,j , I S,j , H j , R j , D j ) + F 4 (t j+1 , S p , E p , I A,p , I S,p , H p , R p , D p ) +H 4,j , H j+1 = H j + τ α Γ(2 + α) α F 5 (t j , S j , E j , I A,j , I S,j , H j , R j , D j ) + F 5 (t j+1 , S p , E p , I A,p , I S,p , H p , R p , D p ) +H 5,j , R j+1 = R j + τ α Γ(2 + α) α F 6 (t j , S j , E j , I A,j , I S,j , H j , R j , D j ) + F 6 (t j+1 , S p , E p , I A,p , I S,p , H p , R p , D p ) +H 6,j , D j+1 = D j + τ α Γ(2 + α) α F 7 (t j , S j , E j , I A,j , I S,j , H j , R j , D j ) + F 7 (t j+1 , S p , E p , I A,p , I S,p , H p , R p , D p ) +H 7,j , where F 1 (t j , S j , E j , I A,j , I S,j , H j , R j , D j ) = b α 1 N − S j N (β α A I A,j + β α S I S,j ) − b α 1 S j , F 2 (t j , S j , E j , I A,j , I S,j , H j , R j , D j ) = S j N (β α A I A,j + β α S I S,j ) − (σ α + µ α 1 )E j , F 3 (t j , S j , E j , I A,j , I S,j , H j , R j , D j ) = ησ α E j − (γ α 1 + µ α 2 )I A,j , F 4 (t j , S j , E j , I A,j , I S,j , H j , R j , D j ) = (1 − η)σ α E j − (γ α 2 + µ α 3 )I S,j , F 5 (t j , S j , E j , I A,j , I S,j , H j , R j , D j ) = γ α 1 I A,j + γ α 2 I S,j − (ρ α + µ α 4 )H j , F 6 (t j , S j , E j , I A,j , I S,j , H j , R j , D j ) = ρ α H j − b α 1 R j , F 7 (t j , S j , E j , I A,j , I S,j , H j , R j , D j ) = µ α 1 E j + µ α 2 I A,j + µ α 3 I S,j + µ α 4 H j , 
paper_id= fffce0b80f087df12f26cae985c0eda494a1190f	title= Title: Psychological distress among older persons during COVID-19 pandemic in a low-and middle-income country setting: The case of Thailand Psychological distress among older persons during COVID-19 pandemic in a low-and middle-income setting: The case of Thailand	authors= Paolo  Miguel;Manalang  Vicerra;	abstract= Objective: The Coronavirus Disease 2019 (COVID-19) situation in Thailand was controlled with various social measures. Much of the information covered in the media and in studies mostly focus on the public health and economic aspects of the pandemic. This study aimed to explore the psychological well-being of older people which is important especially in ageing society categorised as a low-and middle-income country due to the limits of economic and healthcare resources. The Impact of COVID-19 on Older Persons in Thailand, an online survey, taken across nine provinces within the five regions of the country. Participants: Information were collected from 1,230 adults aged at least 60 years old. If an older person was illiterate, unable to access the internet, or with a disability preventing one from responding to the survey, an intermediary who resides in the community conducted the survey interview. The analysis focused on the worries of older adults and the factors associated with psychological distress experienced during the pandemic using logistic regression analysis. : Majority of people aged at least 60 years experienced psychological distress during COVID-19. Employment loss (OR=1.08, 95% CI 0.78 to 1.38), inadequate income (OR=1.77, 95% CI 1.28 to 2.44), and debt incursion (OR=2.74, 95% CI 1.57 to 4.80), were detrimental to psychological well-being. The negative changes in the perception of their health status (OR=1.92, 95% CI 1.23 to 2.99) and decreased life satisfaction (OR=1.49, 95% CI 0.45 to 1.87), 	body_text= Observing the concerns of the older population is important for introducing policies that can alleviate their precarious status in terms of financial and health statuses. -This is a cross-sectional study of the prevalence of psychological distress among older adults related to the COVID-19 pandemic. -The factors associated with psychological distress were explored including sociodemographic characteristics, health conditions, and financial situation. -Older adults aged at least 60 years across five regions of the country were included in the analytic sample. -Self-reported health status was the main information for this study. 19) was declared a public health emergency by the World Health Organization (WHO) on 30 January 2020 [1] . The experience of each country was different from the detection of the first case, the speed of the infection rates, and the onset of measures to curb the spread of the disease among the population. The measures relating to restrictions to mobility and economic activity had immediate psychological impacts on the young and adult populations of different countries including China [2] , Italy [3] , and Belgium, Canada, and France [4] . The older population is particularly vulnerable during this pandemic situation because of their increased risks to disability and diseases [5] . Their psychological well-being expressed through loneliness has also been shown to be a health risk and contributes to higher mortality [6] . In relation to this , becoming physically and socially distant by sheltering in place can also affect the mental and social well-being of older persons [7, 8] . It is notable that the impacts of the pandemic are heterogeneous across societies. The effects of the pandemic with regard to the health and well-being of populations in low-and middle-income countries (LMIC) remain uncertain due to differences in resources and the status of vulnerable populations [9, 10] . In Thailand, the first positive case was identified in 12 January 2020. From that point, the situation was monitored by the government and on 22 March, lockdown measures were enacted. This restricted the mobility of the population and the activities of businesses. The public health situation had gravely affected the life in the country especially the people at advanced ages. Thailand has an ageing society whereby 19% of its total population are aged at least 60 years old [11] . There were indications that non-communicable diseases were increasing in recent decades such as the case of diabetes and obesity [12] . Although this is the context, the life expectancy of people in these older ages had been increasing over the years [13] and a substantial proportion of them remain engaged in economic activities [14] . This study is on the well-being of the older population of Thailand. The protection of the health and safety of the public is primary especially in times of public health emergencies but psychological health and well-being need attention. Lifestyle and culture-specific factors including co-residence with family members and socialisation had been found to be associated with improved psychological well-being among older Thais [15] . Psychological distress has to be analysed in the context of the pandemic as an unprecedented event where the situation can be detrimental given the financial, social, and health concerns of older Thais. There has been a study on the anxiety experienced due to COVID-19 but it concerned the general population of Bangkok [16] . The psychological distress of older people across the regions of Thailand has yet to be explored. This study utilised the data from the Impact of COVID-19 on Older Persons in Thailand survey conducted in July 2020 by the College of Population Studies, Chulalongkorn University. To our knowledge, this survey was the first to specifically focus on the impacts of the consequences of social policies and actions during the COVID-19 pandemic on the older population. The survey ultimately gathered information on the population aged 60 years old and above regarding their socioeconomic status, living arrangement, physical and psychological health, and daily activities before and during the COVID-19 outbreak. It also inquired about the older persons' sources of information, knowledge and preventive practices regarding COVID-19, as well as the source and types of assistance and support they had received to alleviate its impact. arrangement was also included where the categories were living alone,   with spouse and children if any, and living with other people other including relatives, caretakers, and those with non-familial relations. Several socioeconomic characteristics were also part of the analytic model. Education attainment was categorised into three: those with lower than primary education (0 to 3 years of education); people with primary level (4 to 6 years); and individuals with higher than primary level. Employment status was measured in reference to the period before COVID-19 had occurred. Respondents were asked if they had work 12 months before the situation and if they subsequently continued to work during the COVID-19 period. Three categories were created which are no work even before COVID-19, continued working during the situation, and the loss of employment during the pandemic. Average annual income information was also collected and categorised dichotomously into those who had less than 30,000 Baht and those who had earned at least 30,000 Baht in the year before COVID-19. Debt status was also considered similarly such that its reference point included the period before COVID-19 creating three categories: no debt, had debt before COVID-19, and debt incursion during the outbreak. Income adequacy was also incorporated in this study whereby the respondents were asked if their income during the pandemic was adequate or otherwise. The respondents were also asked if their spending was impacted during the outbreak. The expenditures included were food, water and electricity utilities; job expense (i.e. fuel cost), COVID-19 related costs for face masks etc.; medicine and medical supplies, and child/grandchild-related expenses (i.e. online learning fees and living expenses during the school 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60   F  o  r  p  e  e  r  r  e  v  i  e  w  o  n  l  y   break). A dichotomous variable was created for impacted expenditure where having at least one affected spending area would be in the unity category. Socialisation disruptions and health perception changes were also added in the analysis. Disrupted activities were measured as having at least one area of daily routine affected. Four mobility activities were in the survey: leaving the house to run errands and to buy groceries; keeping medical appointments, and attending religious ceremonies. Socialisation activities were meeting with family and relatives, meeting friends, and participating in social activities. In terms of changes in health, respondents were queried if they experienced changes in the following during the outbreak: vision, hearing, mobility, communication, memory, and personal care. Having one change was classified as the outcome category in the variable. Finally, older individuals in the survey were asked if they had worse self-rated health and life satisfaction during the COVID-19 social restriction period. Characteristics of the analytic sample were presented. There was also other descriptive information about the sample during the pandemic that was included to provide the context of the older persons in Thailand. For the main analysis, the logistic regression model analysis was utilised to determine the factors associated with psychological distress during COVID-19. The present study utilised secondary data. No patients nor the public was directly involved in the design, conduct, or reporting plans of the research. The financial status of older people in Thailand was precarious. Before the March 2020, many older people in the sample cited that they were working as a means to generate their main income (Figure 1) . At the similar level (40%), people were also receiving government subsidies especially the old-age allowance. Some were receiving support from their families and fewer were under the pension system and had saving. [Insert Figure 1 about here.] The source of main income among the respondents changed during the pandemic. Those who indicated work as means to generate income halved while those who received subsidies increased to 56%. Uncertainties compounded during the COVID-19 pandemic as observed in the list of concerns of older people ( Figure 2 ). Few had concerns with their living situation. More were fretful with regard to health and financial issues. Respondents were primarily concerned with having themselves or their families infected with the disease (41%). Following this is the concern on their financial situation (28%). [Insert Figure 2 about here.] In Table 2 , the associated factors were observed based on the logit model analysis. The oldest old (80 years and above) was observed to have higher prevalence of psychological distress than those in the 60-69-year age group. Living in the rural areas, being married, having higher levels of education attainment, being employed even during the pandemic, and having higher level of income were observed to have negative association with psychological distress.  Psychological well-being among older persons is an important aspect to study but is often lacking in the literature particularly in the context of developing economies [17] . The context of the COVID-19 pandemic also brings to the forefront certain issues that the older population encounter in relation to social and economic aspects. It was observed in this study that socioeconomic factors as education and income levels were associated with psychological distress. The perception of adequacy and the effect on debt and expenditures were also detrimental to the mental well-being of older people. The negative perceptions they have on their health status were also observed to be negatively associated with psychological distress. The concern on financial status among older people and its effects on lower well-being had been observed in a study in Sweden [18] . As shown in the results of the current study, financial concerns were also experienced by many older Thais. Uncertainties surround people because of the situation where economic sectors had halted during the lockdown measures to control the outbreak. The susceptibility of people in Thailand to market shocks had been previously studied [19] . It was found that a major factor to this is that most adults, including older people, engaged in the formal labour sector especially in the urban areas [20] . As economic output declined due to the pandemic, a great number of the population regardless of age had lost their employment. This may have an impact on another source of income among older Thais is support from their families, particularly if they have children [21] . Although more among the older people received support from their family members, the amount being received by the older people may have been affected as the loss of employment for all adults across the country was conspicuous. The avenues to have income were curtailed because of COVID-19. The trend of employment in the 1990s to the 2000s had been noted whereby about more than a third of the [22] . It is a necessity for some older Thais to participate in the labour force as many had worked in the informal sector during their younger years leading to a lack of pension [23] . The loss of this source of income has great effect on the financial capacity of older individuals and caused concern. As mentioned above, most older Thais lack pension. The old-age allowance system was established to provide social protection [23] . Poorer perception of health status was also found here to be negatively associated with psychological distress. Self-rated health, self-reported health manifestations including worse vision and hearing among others; and dissatisfaction with their lives were observed among most in the study sample. Anxiety and concern had been observed before with regard to the influenza A(H1N1) where adults thought of their vulnerabilities to infection among other aspects of a pandemic [25] . The similar worry and fear had been noted during COVID-19 among adults in Turkey [26] . Older Thais had concerns of infection, the experience of sheltering in place, and the stress of having lesser spending have influence on how they perceive their own health state and life satisfaction. There are limitations to this study. Firstly, psychological distress is based on self-reported feelings among the respondents. The measure does not represent a medical diagnosis of substantial proportion of its population was vulnerable to psychological distress which is a part of overall well-being. Health and socioeconomic vulnerabilities were observed to contribute to the distress. This study can provide insights on the vulnerabilities of the older population. A support system would be advantageous during outbreaks. Social protection also has to be strengthened even in times different from a pandemic situation. This will provide security for circumstances that can result to financial shocks. This research was made possible by the Ratchadaphisek Somphot Postdoctoral Fund from the Graduate School, Chulalongkorn University. This work was done with support from the Population, Family Dynamics, and Social Policy research unit also at Chulalongkorn University. The author initiated and completed this study solely.  None declared. No patient nor the public was directly involved in the design, conduct, or reporting plans of the research. Not required. Data are available upon reasonable request. The data is with the College of Population Studies, Chulalongkorn University, Bangkok, Thailand. Participants: Information was collected from 1,230 adults aged at least 60 years old. If an older person was illiterate, unable to access the internet, or had a disability preventing them from responding to the survey, an intermediary residing in the community conducted the survey interview. The analysis focused on the worries of older adults and the factors associated with psychological distress experienced during the pandemic using logistic regression analysis.  Observing the concerns of the older population is important for introducing policies that can alleviate their precarious financial and health statuses. -This study adds evidence on the psychological effect of COVID-19 pandemic in older adults. -The theme of the psychological well-being of older adults is often overlooked, especially during health emergencies in the context of low-and middle-income countries. -This study offers insights into the effect of the pandemic on the resources of older adults who need to continue working despite the situation. -Self-reported health status was the main measure used in this study, and therefore causation cannot be established. and France [2] . Although entire populations were affected negatively by the situation, the effects were greater among the older population aged 60 years or older in terms of anxiety and depressive risks [3, 4] . Physically distancing through sheltering in place was identified as among the factors associated with higher reports of anxiety and depression at the community level across several countries [5, 6] . This unexpected circumstance required sudden lifestyle adjustments which are difficult to manage especially among older adults. It is important to note these changes, because the psychological well-being of older persons is related to comorbidities that increase mortality risks [3, 7] . Notably, the impacts of the pandemic are heterogeneous across societies. The effects of the pandemic with regard to the health and well-being of populations in low-and middleincome countries (LMIC) remain uncertain due to differences in resources and the status of vulnerable populations [8, 9] . In Thailand, the first positive case was identified on 12 January 2020. From that point on, the government monitored the situation, and on 22 March, they enacted lockdown measures. This restricted the mobility of the population in their communities and the activities of businesses. The public health situation gravely affected life in the country, especially for those of advanced age. Thailand is an ageing society in which 19% of the total population is at least 60 [11] . Despite this context, life expectancy had been increasing over the years [12] , and a substantial proportion of older people remained engaged in economic activities [13] . The pandemic has affected the need for continued gainful employment among adults, and this extends to the older population, further exacerbating the situation. This study focuses on the psychological well-being of the older population of Thailand during the COVID-19 pandemic. Studies have been done regarding other aspects of health, such as the adoption of preventive behaviors in older people [14] and the level of physical activity among both young and older adults [15] . The protection of public health and safety is of primary importance especially during public health emergencies, but psychological health and well-being also need attention. One study examined anxiety experienced due to COVID-19, but it concerned only the general population of Bangkok [16] . The psychological distress of older people across the regions of Thailand has yet to be explored. This study utilized the data from the Impact of COVID-19 on Older Persons in Thailand survey conducted in July 2020 by the College of Population Studies, Chulalongkorn University [17] . This survey was the first to specifically focus on the impacts of social policies and actions during the COVID-19 pandemic on the older population in the country. The aim was to gather information on the population aged 60 years old and older regarding their socioeconomic status, living arrangement, physical and psychological health, and daily activities before and during the COVID-19 outbreak. The survey questionnaire was reviewed by national and international Data were collected in July 2020 when national lockdown measures had mostly been relaxed, but avoidance of face-to-face interactions was still recommended by governmental agencies; therefore, an online survey method was deemed appropriate. The survey employed a multistage, proportionate-to-size probability design with geographic and administrative stratification. The sample consisted of 1,230 respondents aged 60 and over living 11 urban communities and 7 rural villages in nine provinces across five regions of the country including Bangkok. A local resident in the community acted as an intermediary for respondents who required assistance because they were illiterate, had no access to a smart phone or the internet, or had difficulty navigating the survey platform. This study aimed to observe psychological well-being, using the experience of psychological distress as the measure, of older people in Thailand. It was based on the survey question, "During the COVID-19 outbreak, how frequently did you experience the following symptoms or feelings?" The symptoms were loss of appetite, no hope in life, unhappy, sad, and lonely. The response options for the question were never, sometimes, and always. The responses 'sometimes' and 'always' were combined to become the outcome category for having psychological distress. The total score for the index was five (5) . From this, the dependent variable was transformed into a dichotomous category whereby the outcome category included those who had at least one symptom. The items used to create the index for psychological distress were tested and showed an appropriate level of reliability (Cronbach's alpha = 0.81). 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60   F  o  r  p  e  e  r  r  e  v  i  e  w  o  n  l  y   8 Selected demographic factors were included in the analysis, namely age, gender, residence, and marital status. Living arrangement was also included, where the categories were living alone, living with a spouse and children, if any, and living with other people including relatives, caretakers, and other non-family members. Several socioeconomic characteristics were also part of the analytic model. Educational attainment was categorized into three groups: those with lower than primary education (0 to 3 years of education); people with a primary education (4 to 6 years); and individuals with higher than a primary education. Employment status was measured in reference to the period before COVID-19 began. Respondents were asked if they had worked in the 12 months before the outbreak began and if they continued to work during the COVID-19 period. Three categories were created, namely, no work before COVID-19, continued work during the situation, and loss of employment during the pandemic. Average annual income information was also collected and categorized dichotomously into those who earned less than 30,000 Baht and those who had earned at least 30,000 Baht in the year before COVID-19. Debt status was considered similarly such that its reference point included the period before COVID-19, creating three categories: no debt, in debt before COVID-19, and incurred debt during the outbreak. Income adequacy was also incorporated into the study as the respondents were asked if their income during the pandemic was adequate or inadequate. The respondents were also asked if their spending was impacted during the outbreak. The expenditures included were food, water and electricity utilities; job expenses (e.g., fuel costs); COVID-19 related costs for items such as face masks; medicine and medical supplies; and child/grandchild-related expenses (e.g., online learning fees and living expenses during the 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60   F  o  r  p  e  e  r  r  e  v  i  e  w  o  n  l  y   9 school break). A dichotomous variable was created for impacted expenditure, where having at least one affected spending area would place the respondent in the unity category. Socialization disruptions and changes in health perception were also included in the analysis. Having at least one area of daily routine affected counted as a disrupted activity. The survey asked about four mobility activities: leaving the house to run errands, buy groceries, keep medical appointments, and attend religious ceremonies. Socialization activities included meeting with family and relatives, meeting friends, and participating in social activities. In terms of changes in health, respondents were asked if they had experienced changes in the following during the outbreak: vision, hearing, mobility, communication, memory, and personal care. Having at least one change was classified as the outcome category in the variable. Finally, the survey asked older individuals if they had worse self-rated health and life satisfaction during the COVID-19 social restriction period. The distribution of the sample's characteristics is presented in Table 1 . The difference between the respondents who experienced psychological distress during the pandemic and those who did not was tested using a chi-square (χ 2 ) test. For the multivariate analysis, a bivariate logistic regression model analysis was utilized to determine the factors associated with psychological distress during COVID-19. This regression model is appropriate because the outcome was measured dichotomously to differentiate between the presence and absence of the experience of psychological distress. Multicollinearity was tested by estimating the variance inflation factor. The results show that there was minimal multicollinearity among the 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60   F  o  r  p  e  e  r  r  e  v  i  e  w  o  n  l  y 10 independent variables included in the analyses. All the analyses were performed using Stata 13.1 (StataCorp, Texas, USA). The present study utilized secondary data. Neither patients nor the public were directly involved in the design, conduct, or reporting plans of the research. About 57% of individuals in the sample experienced psychological distress during the COVID-19 pandemic (Table 1) . Respondents in urban areas reported more psychological distress than those in rural areas. A similar reporting of higher distress was found among those who lived alone and those with lower levels of education. A higher prevalence of psychological distress was further observed among those who had lost their employment during COVID-19 (66%, 310 respondents), had inadequate income (68%, 533 respondents), and incurred debt during the outbreak (81%, 87 respondents). Those who experienced affected expenditures, disruptions in socialization, and lower health and well-being statuses also suffered a higher prevalence of psychological distress. The financial status of older people in Thailand was already vulnerable before the pandemic. Before March 2020, many older people in the sample cited that they were working as a means to generate their main income (Figure 1) . At a similar level (40%), people were also receiving government subsidies, especially the old-age allowance. Some were receiving support from their families, and fewer were under the pension system and had savings. [Insert Figure 1 about here.] The main source of income among the respondents changed during the pandemic. Those who indicated they worked as means to generate income decreased by half, while those who received subsidies increased to 56%. Uncertainties compounded during the COVID-19 pandemic as shown in the list of concerns of older people ( Figure 2 ). Few had concerns with their living situation. More were fretful with regard to health and financial issues. Respondents were primarily concerned about themselves or their families becoming infected with the disease (41%). Following this was concern about their financial situation (28%). [Insert Figure 2 about here.] In Table 2 , the associated factors were observed based on the logit model analysis. The oldest group (80 years and above) was observed to have a higher prevalence of psychological distress than those in the 60-to 69-year-old age group. Living in a rural area, being married, 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60   F  o  r  p  e  e  r  r  e  v  i  e  w  o  n  l  y 15 Psychological well-being in older people is an important subject to study but is often overlooked in the literature, particularly in the context of developing economies [18] . The context of the COVID-19 pandemic also brings to the forefront certain issues that the older population encounters in relation to social and economic factors. This study observed that socioeconomic factors, such as education and income levels, were associated with psychological distress. The perception of income inadequacy and the pandemic's effect on debt and expenditures were also detrimental to the mental well-being of older people. Negative perceptions of one's health status were also observed to be negatively associated with psychological distress. Concerns about financial status among older people and its negative effects on well-being were previously observed in a study in Sweden [19] . As shown in the results of the current study, financial concerns have also been experienced by many older Thais. Uncertainties surround people because of the halting of various economic sectors during the lockdown measures to control the outbreak. The susceptibility of people in Thailand to market shocks has also been previously studied [20] . A major contributing factor was that most adults, including older people, are engaged in the formal labor sector, especially in urban areas [21] . As economic output declined due to the pandemic, a large portion of the population, regardless of age, lost their employment. This may have had an impact on another source of income among older Thais, namely support from their families, particularly if they have children [22] . Although many older people continued to receive support from their family members, the amount received may have been affected by the conspicuous loss of employment for all adults across the country. 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60   F  o  r  p  e  e  r  r  e  v  i  e  w  o  n  l  y   16 Avenues to earn income were curtailed because of COVID-19. The employment trend of the 1990s to the 2000s, during which more than a third of the older population of Thailand was in the labor force, has been noted [23] . It is a necessity for some older Thais to participate in the labor force, as many worked in the informal sector during their younger years, leading to the lack of a pension [24] . The loss of this source of income has a great effect on the financial capacity of older individuals and has thus caused concern. As mentioned above, most older Thais lack a pension. The old- The amount received was prone to insufficiency, as it was fixed and did not respond to changing inflation rates or the current poverty threshold [25] . A poorer perception of health status was also found to be negatively associated with psychological distress. Self-rated health, self-reported health manifestations, including worse vision and hearing, among others, and dissatisfaction with their lives were observed among most respondents in the study sample. Anxiety and concern have been observed before with regard to the influenza A(H1N1) outbreak, during which adults thought about their vulnerabilities to infection among other aspects of a pandemic [26] . A similar worry and fear has been noted during COVID-19 among adults in Turkey [27] . Older Thais had concerns about infection, the experience of sheltering in place, and the stress of having less to spend, all of which influenced how they perceived their own health state and life satisfaction. There are limitations to this study. Firstly, psychological distress is based on respondents' self-reported feelings. The measure does not represent a medical diagnosis of respondents' psychological state. Secondly, the study is based on a cross-sectional survey; therefore, causation was not established. Observations of the development of individuals' socioeconomic and health states through a longitudinal study would offer advantages, particularly to determine changes before, during, and after the COVID-19 pandemic. Despite these limitations, this study provides insights on the vulnerabilities of the older population. A support system would be advantageous during outbreaks. Social protection also has to be strengthened even in times outside of a pandemic situation. This would provide security in the event of circumstances that may result in financial shocks. The COVID-19 pandemic has been unprecedented for Thailand because of the measures implemented with regard to distancing restrictions. Although the public health policies enacted were necessary from the viewpoint of the government to curb the transmission of the disease, they affected people's social and economic lives, especially those considered vulnerable, such as older adults. This study observed that the precariousness of older adults' financial statuses was associated with the experience of psychological distress. The situation is continuing to evolve, and the impact in the longer term has to be considered.  PMMV initiated the study. The survey was carried out by WP. The initial version of the paper was written by PMMV with guidance from WP. Revisions were provided by WP. Both authors approved both the original and revised versions to be published. This research received no specific grant from any funding agency in the public, commercial or not-for-profit sectors. None declared. Not required. Data are available upon reasonable request. The data is housed at the College of Population Studies, Chulalongkorn University, Bangkok, Thailand. Percentage distribution of wage sources among adults in the study sample. 1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  
paper_id= fffd0e85683afdd2ff25aeb0e5c98000ee03a474	title= Biomechanics and Mathematical Modelling Solutions lab	authors= Durbar  Roy;Sophia M ;Abdur  Rasheed;Prasenjit  Kabi;Abhijit  Sinha;Roy  ;Rohit  Shetty;Saptarshi  Basu;	abstract= Non-invasive medical diagnostics demonstrate a propensity for droplet generation and should be studied to devise risk mitigation strategies against the spread of the SARS-CoV-2 virus. We investigate the air-puff tonometry, which uses a short-timed air-puff to applanate the human eye in a bid to detect the early onset of glaucoma by measuring the intraocular pressure. The air-puff consists of a vortex trailed by a high-speed jet. High-speed imaging of the eye during a typical tonometry measurement reveals a sequence of events starting with the interaction between the tear layer and the air puff leading to an initial sheet ejection. It is immediately followed by the trailing jet applanating the central corneal section, causing capillary waves to form and interact with the highly 3D transient expanding sheet. Such interaction with the capillary waves and the surrounding airfield due to the trailing jet causes the expanding sheet to undergo bag breakup, finger formation by Rayleigh Taylor instability and further break up into subsequent droplets by Rayleigh Plateau instability, which eventually splashes onto nearby objects, potentially forming fomites or aerosols which can lead to infections. The complex spatiotemporal phenomenon is carefully documented by rigorous experiments and corroborated using comprehensive theoretical analysis. 	body_text= the virus SARS-CoV-2 proliferates through droplets of saliva and discharge of the nose. It happens through coughing, talking, breathing and sneezing of a COVID-infected person. Many studies related to coughs and sneezing in both the clinical/medical community and the scientific/engineering community like the works of Scharfman et al. , Bourouiba et al., to name a few (1, 2) studied the mechanisms involved. Scharfman et al., for the first time, showed that the breakup of droplets happens outside the respiratory tracts in violent exhalation processes. They further showed various kinds of droplet breakup mechanisms like sheet breakup, bag breakup, and ligament breakup during such processes. Bourouiba et al. also studied the fluid dynamics of violent expiratory events like coughing and sneezing using high-speed scattering imaging techniques. They discovered that the flow is highly turbulent and multiphase. Further, a theoretical analysis of pathogen bearing droplets interacting with the turbulent momentum puff was conducted. The effect of respiratory droplets on the spread of Covid-19 was modelled by Chaudhari et al (3) . The virus transmission can be minimized by adequately understanding and studying the various causes of droplet generation from an infected individual and finding solutions to protect everyone from such droplets. This work deals with a non-evasive medical procedure called non-contact tonometry. Tonometry is a technique used by ophthalmologists to measure intraocular pressure that gives a good idea of whether any patient is suffering from glaucoma or not. Further intraocular pressure also helps to diagnose various other eye-related conditions that may be helpful to the specialist(4). The first most accurate IOP measuring device was the Maklakoff applanation tonometer (5) . These early tonometers were the contact type tonometer. Medical practitioners and researchers modified and developed the tonometer through several generations increasing the reliability and accuracy of the device. The new breed of tonometers like McKay-Marg tonometer, Tono-Pen, pneumatonometer, and air puff tonometer to name a few have gained popularity among medical practitioners (6) . This work uses an air-puff type tonometer, which is a non-contact type. Many studies have been conducted on the working principle and the development of non-contact tonometers. Ophthalmologists have conducted many clinical studies about IOP, its variation with time, its effect on different eye-related medical conditions like glaucoma, corneal stiffness, and thickness, to mention a few. Some work related to the corneal deflection in response to an air puff tonometry has also been studied using experimental and computational techniques (7) . The complicated structure of the human eye makes theoretical modeling quite challenging (8) . Scientists have conducted preliminary clinical studies decades ago and showed that droplets maybe generated during non-contact tonometry (9) . Some clinical studies shows that the SARS-CoV-2 virus can be present in human tears like the work of Sadhu et al. (10) . Such aerosols, may lead to the transmission of the virus undetected. The literature on the fluid mechanics of tonometry is quite sparse. The spatiotemporal scales involved are quite small, and hence resolving the phenomena is quite challenging. The air puff emanating of a non contact tonometer is essentially a vortex ring. There have been some studies on vortex ring impact on thin liquid films (11) , but the resulting fluid mechanics due to vortex ring interaction with a human eye tear film has never been probed in detail. Previously we have carried out some preliminary studies related to non-contact tonometry (12) . This work provides some rigorous comprehensive insights into non-contact tonometry using experimental and theoretical investigations performed on real human subjects. High-speed imaging and visualizations were carried out to decipher the underlying fluid dynamics in sufficient details for dry and watery eye conditions. Fig. 1 provides an overview of the entire time series of events during non-contact tonometry. Different phases have been identified on the basis of the order of events (according to temporal history.) The phenomenon starts with an air puff (a leading vortex with a trailing jet) Corneal deflection (C) Initial sheet expansion and no capillary waves (B) Droplets Ejection(F) Fig. 1 . Different Phases involved during the non-contact tonometry from a fluid dynamical perspective has been shown. Phase A represents the air puff generation from the nozzle and its propagation. Phase B consists of the initial sheet expansion due to sudden pressure reduction due to the approaching leading vortex. Phase C denotes the central corneal deflection due to the trailing jet. Phase D shows the detection and propagation of capillary waves on the tear film surface. Phase E shows the later sheet expansion, and Phase F shows the droplet breakup regime due to Rayleigh Plateau and its propagation under the influence of its linear momentum, gravity, and air resistance. ejected from the nozzle head of the tonometer. The leading vortex travels towards the eye, increasing the local air velocity field adjacent to the eye, which reduces the local pressure outside the tear layer. The transient pressure reduction causes the tear film accumulated at the bottom portion of the eye to be ejected as a sheet especially in the case of watery eye. Meanwhile, the trailing jet impacts the cornea causing a detectable deformation of the eye. The deformation leads to detectable capillary waves on the eye surface, which travel outwards in the form of concentric rings (13) . The initial sheet is reinforced by the capillary waves, expand and undergoes bag breakup (14) and Rayleigh Taylor instabilities (15) (16) (17) (18) . The Rayleigh Taylor instability causes finger-like structure to form and grow, which further undergoes Rayleigh Plateau breakup (16) (17) (18) into droplets. The bag breakup and Rayleigh Plateau breakup produces droplets of various sizes and velocity. These droplets can then travel in various directions under the effect of its linear momentum, acceleration due to gravity, and air resistance. The details of each phase have been described in the results and discussion sections and also illustrated in Fig. 1 . The main aim of this work is on the fluid dynamics rather than on the corneal deflection since a lot of literature is already available (7, 8) . We study the interaction between the air puff and the human eye and the associated fluid dynamics at different Spatio-temporal scales in sufficient detail. Further, to assess the spread of the COVID-19 virus, the size distribution and the velocity distribution of the droplets that are generated during the non-contact tonometry procedure are reported, which may help the opthalmologist and eye care practitioners to follow safety protocols while undertaking these procedures. 1 To whom correspondence should be addressed. E-mail: sbasu@iisc.ac.in  The schematic of the experimental setup is shown in Fig.  2 . The experiments were conducted on four different human subjects of various age groups and gender (one female and three males). High-speed images were acquired using Photron Mini UX100 and a high-intensity light source Veritas Mini Constellation at 2000 frames per second with a spatial resolution of 512 pixels × 320 pixels. The images were taken from different views like side view, orthographic views, oblique views, inclined views, and back views to capture as much information as possible since the fluid dynamics involved are highly transient and three-dimensional in nature. The tonometer's nozzle was kept at a working distance of approximately 11mm from the human eye, which is the standard protocol for NCT200 to give a correct reading of intraocular pressure. The high-intensity light source was placed in different strategic locations to give the desired lighting for different views. The initial vortex and jet were observed by smoke flow visualization and scattering techniques. The trailing jet was also visualized using alumina particles at various frames per second using scattering techniques. However, a dead goat eye was placed in the place of the real human subject for smoke flow visualization in order to characterize the air puff coming out of the tonometer and to understand the roles of the leading vortex and the trailing jet. Experiments were conducted on two different eye conditions: dry and wet. Dry eye conditions were studied so that we have a comprehensive understanding of the corneal deflection and observe whether any droplets are ejected in dry conditions. The wet eye conditions were simulated using eye drops that were prescribed by an ophthalmologist. Some industrial brands of eye drops that were used are Refresh Tears, Lubrex, Trehalube, Systrane Ultra. The drops were used on the human subjects just before the tonometry measurement to mimic watery eye condition. The images were acquired with the Photron FastCam Viewer (software package 4.0.3.4.) and the raw images were processed using a combination of open-source tools, ImageJ (19) and python programming language (20) . Preprocessing of the raw images were performed using FFT Bandpass Filter (21) and by using CLAHE (Contrast Limited Adaptive Histogram Equalization) (22) . The significant structures were filtered down to 40 pixels, and small structures up to 3 pixels were used for the FFT Bandpass Filter. For the CLAHE, the block size used was 127 pixels, histogram bins of 256, and a maximum slope of 3 were used. The various kinematic parameters involved were then tracked, and the results were post-processed using Python programming language (20) . The raw data of the kinematic parameters were converted into probability density function using kernel density estimation. Fig. 1 . demarcates the various phases of the tonometry process from a fluid dynamical perspective based on timescale. Phase A consists of the leading vortex approaching the human eye, and covering a working distance of approximately 11mm in approximately 2.6ms. Phase B starts with the initial sheet ejection and ends till the the effect of capillary waves is felt on the initial sheet. This phase lasts for approximately 3ms. Phase C begins with the corneal deflection due to the trailing jet impinging the cornea and this phase lasts up to 7.5ms. Phase D starts when the first detectable capillary waves are seen on the surface of the eye and this phase lasts up to 3ms. Phase E consists of all the interesting fluid mechanical phenomena related to sheet expansion, Rayleigh Taylor waves, bag formation, finger like structure formation. This phase lasts for approximately 5ms. The last phase F consists of the disintegration into the droplets due to Rayleigh plateau breakup. This phase lasts for approximately 3.5ms. This phase and beyond deals with the droplets of various size and velocity propagating under the combined effect of the initial linear momentum of the droplet, the acceleration due to gravity and effects of air-resistance. All the phases are discussed below in their respective subsections. Vortex ring approaches the eye (Phase A). The non-contact tonometry procedure starts with the ejection of an air puff, which is essentially a leading vortex followed by a trailing jet from the tonometer nozzle. The position of the leading vortex was tracked from the smoke flow visualization images. The leading vortex propagates before it interacts with the tear film of the human eye. Fig. 3 depicts the air puff characteristics. Fig. 3 (a) shows the smoke flow visualization of the leading propagating vortex and the trailing jet. The vortex propagation was tracked by measuring the horizontal position x of the vortex core (shown as a green dot), from a predefined reference line (shown as a red dotted line) which is the exit plane of the tonometer nozzle, as a function of time. Two other parameters, r1 and r2 were also tracked to understand the vortex's radial dynamics. r1 gives an estimate of the vortex's outer envelope, and r2 gives us an idea of the internal length scale of the propagating vortex. Fig. 4 (a) shows the vortex's propagation characteristics as it approaches an obstacle, which was essentially a goat's eye. Fig. 4 (b) shows the temporal evolution of r1 and r2. r1 and r2 increases linearly with time, showing a small expansion of the leading vortex. Both r1 and r2 are increasing at the same rate since the slope is approximately identical, as can be observed from Fig. 4(b) . The sudden jumps at approximately 2ms in both Fig. 4 (a) and 4(b) denote the time at which the leading vortex starts interacting with the eye. The leading vortex takes approximately 3ms to cover a working distance of approximately 11mm for the human subjects. The leading vortex average velocity scale is approximately Vvortex∼5m/s. As the leading vortex approaches the human eye, the velocity field locally adjacent to the eye increases with time, which causes a sudden reduction in the local pressure. The effect of this reduced pressure is different for dry and wet eye conditions. For dry eye, the tear distribution is uniform across the entire corneal surface, and the interaction adhesive force between the tear film and the cornea is quite strong. Hence no sheet-like structures are seen in the dry eye condition, and hence no droplets are ejected out. However, for a teary eye condition as simulated using eye drops for the experiments, the excess tear is located in the lower part of the cornea. Hence the tear film thickness is not uniform, as was the case for dry eye. The tear film has a distribution that is thicker at the bottom. Due to the higher thickness of the film, the adhesive force between the cornea and the film is smaller compared to the dry eye conditions. Hence the reduced outer field pressure causes fluid sheet ejection in the case of a wet eye. initial sheet ejection velocity can be modeled using an in-viscid analysis because the effect of viscosity can be neglected. The theoretical analysis of the sheet expansion is provided in Phase E. Corneal deflection (Phase C). The leading vortex is followed by a trailing jet. The trailing jet upon impact causes a central corneal deflection. Fig. 6 shows the entire image sequence of the corneal deflection process. The entire deformation process has an overall time scale of approximately 7.5ms. Fig. 7 shows the central corneal deflection curve as a function of time. The corneal deflection increases to a maximum and then returns to the undeformed state. The curve peaks at approximately 4ms, and the maximum corneal deflection varies from 0.6mm to 0.9mm. The scatter present in the displacement curve is due to the different intraocular pressure of the eye. The intraocular pressure is like the blood pressure of an individual, which is biological in origin and is dependent on many factors and varies within a band for an individual. Some of the factors are the time of the day, conditions before and after drinking water, to name a few (23) . To exactly model the corneal deflection, the anatomy of the cornea has to be understood adequately (8) . Some literature is available to model the deflection of the cornea in extensive detail (7) . This work focuses on the corneal deflection qualitatively only. Capillary waves (Phase D). The impact of the trailing jet causes a central corneal deflection along with the detectable capillary waves. Capillary waves are caused due to pressure variation on the top surface of the tear film. The pressure variations form ripples on the free surface of the liquid, which travels outwards. These ripples produce film thickness variation of the corneal tear film. The ripples, also known as capillary waves, are an outcome of the interplay between the external pressure (due to the trailing jet and its associated airflow field) and the air-water surface tension, which acts as a restoring force. The waves/ripples generated on the tear film are approximately symmetrical about the central spot where the trailing jet impacted the cornea. A radial symmetry exists at least on a local small Spatio-temporal scale, and hence a one-dimensional model is a good enough approximation to decipher the relevant velocity scales involved. The dispersion relations for a pure capillary waves (1-D relation)is given by Where ω is the growth rate of a disturbance, σaw is the airwater surface tension, ρa is the density of air, ρw is the density of water, k = 2π λ is the wavenumber which is inversely proportional to the wavelength of the capillary waves. A pure capillary wave's growth rate is directly proportional to the wavenumber raised to three-half powers. This implies that smaller wavelengths have higher growth rates compared to larger ones. The growth rate also depends on the material properties. ω is directly proportional to the square root of the air-water surface tension. This implies that higher surface tension liquids will have higher growth rates. ω is also inversely proportional to the square root of the sum of air density and water density. The phase velocity is the velocity with which capillary waves of a given wavenumber/wavelength propagates and is given as the ratio of the growth rate of the disturbance to the wavenumber of the ripples. v phase = ω k = σaw ρa + ρw k 1/2 [2] and the group velocity is given as the first derivative of the growth rate with respect to wavenumber. The group velocity helps to calculate the velocity of an envelope which contains waves of various wavelengths. Equation [2] and [3] have the same variation with wavenumber and the dependence is square root of wavenumber. This implies that short wavelengths have higher velocity of propagation compared to long wavelengths. Fig. 8(a) depicts the timeline of the propagation of the capillary waves. The peak of a capillary wave was tracked from a fixed reference datum and the average velocity of propagation was calculated using the forward difference approximation of the first derivative of position from displacement time relation. We did some experiments to verify the generality of the idea of capillary waves on cadaver goat eye. The time sequence of the resulting capillary waves on the goat eye is shown in Fig.  8 (b). The phase velocities and group velocities are plotted as a function of wavelength as depicted in Fig. 9 , respectively, using equation [2] and [3] , respectively. The experimentally measured values are shown in Fig. 9 , which are of the same order of magnitude as that obtained theoretically. The order of magnitude is approx V ∼1.1m/s, which corroborates well with the theoretical framework. Dynamics of Sheet Expansion. As the leading vortex approaches the eye, the local velocity field just outside of the tear film increases. This leads to a very thin boundary layer during the first 2ms, which is of the order of approximately 0.0242mm. Outside this length scale, viscous effects can be neglected, and hence the sheet expansion can be modeled using inviscid theory. Hence using Eulers equation we have: [4] where ρw denotes the density of water, V sheet is the sheet ejection velocity, dt denotes the elementary timescale, p denotes pressure and r is an characteristic length scale characterizing the outer envelope of the sheet. Figure 10 (a) shows the schematic of the sheet expansion. pin is the initial pressure inside the tear film at static condition. The initial tear film thickness is r(t = 0) = F0. Applying scaling analysis on the Eulers equation we have Assuming pin p * , which is the initial pressure inside the static tear layer before the interaction with the air-puff. A pressure difference exists across the interface due to air-water surface tension and the tear film's curvature at the lower part of the eye in the static condition. However, due to an increasing outer velocity field in the air medium, the outer pressure pout in the airfield reduces by an amount given by the Bernoulli's equation kinetic energy per unit volume term pin − pout σaw r + 1 2 ρairV 2 air [7] Substituting equation [7] in equation [5] we have, Equation [8] depicts the sheet's acceleration and is essentially a form of Newtons' second law of motion. This equation is one of the central results of the current work. The first term on the right-hand side is the acceleration due to surface tension and the curvatur of the interface. The second term is the acceleration due to the air velocity field. The sheet velocity is given by the first derivative of r, V sheet = dr dt . Substituting the value of V sheet in terms of first derivative of r, we have The above equation has been converted into two first order equations and are solved numerically. The first order derivatives were discretized using backward euler method. The initial conditions are the position initial condition X1(t = 0) and the velocity initial condition X2(t = 0). The position initial condition is given as where F0 denotes the initial film thickness. The velocity initial condition is given by where a = 2σaw ρw which is a constant and is twice the ratio of air-water surface tension to the density of water , b = ρa ρw V 2 air which is also a constant , F0 = initial film thickness, r = X1 andṙ = V sheet = X2. Figure 10 (b) shows the numerical solution of equation [8] and the experimental comparison of the sheet expansion. The constants used for the numerical solution are σ∼0.072N/m, ∆t∼ = 2×10 −6 s, ρw∼10 3 kg/m 3 , ρair∼1kg/m 3 Vair∼1m/s The theoretical estimates corroborates well with the experimental data. Phase E consists of a lot of exciting fluid dynamical events, as is shown in Figure 11 . Fig. 12(a) represents the sheet expansion in millimeters as a function of time in milliseconds. The experimental data are marked with red dots, the experimental fit with a dashed-dotted line, and the theoretical curves predicted by the numerical solution of equation [8] by solid lines. The theoretical model predicts the sheet expansion within 1 percent accuracy. The red shaded region shows a wide variation of initial expansion velocity due to different amount of tear layer thickness. The blue shaded region represents the later sheet expansion due to the effect of the trailing jet and its corresponding air velocity field. The initial sheet ejection leads to a more complex sheet expansion due to the trailing jet, corneal deflection, and the related capillary waves. This expansion of the sheet is shown by the blue curves. This rapid sheet expansion and acceleration cause small disturbances in the flow field to amplify and grow. The disturbances are in the azimuthal direction since each part of the sheet expands in a plane locally but in all directions. So while the sheet expands, the lighter fluid pushes the heavier fluid, and a Rayleigh Taylor instability is triggered, which amplifies small perturbations to grow into finger like structures. The disturbances that are triggered would have a predominant azimuthal symmetry if the flow field was less vigorous. However, due to other transient structures, the symmetry is broken in many places. However, the local symmetry is preserved like the distance between the fingers right λ at the point of initial growth. This can be measured in a statistical sense. Fig. 11(b) shows the sheet expansion and the associated timescale as a sequence of snapshots. Simultaneously due to the highly transient and 3D nature of the relative flow with respect to the expanding sheet, bag-like structures are formed in the expanding sheet, which undergoes breakup into daughter droplets. Rayleigh Taylor Instability of the sheet. The initial perturbations are mostly in the azimuthal direction. The wave propagation and the fluid flow is in the radial direction. The initial perturbations are due to small fluctuations in the velocity and pressure field. Therefore we can apply linear stability analysis in the azimuthal direction (1D) to get an idea of the distance between the fingers and the growth rate of the initial disturbance into the formation of fingers. The general Rayleigh Taylor dispersion (16, 17) relation for 1D and assuming wave propagation direction to be same as fluid flow direction which is radial in our case we have Equation [14] depicts that the disturbance growth rate is a complex valued function. The real part of the growth rate has contributions from three terms I, II, III. The first term I signifies the kinetic energy of one fluid with respect to the other. The second term II signifies the apparent gravity felt by the expanding sheet. The third term III is the contribution due to the surface tension. where V is the absolute value of the relative flow velocity of air with respect to the sheet and g ef f is the apparent gravity experienced by the sheet. Re(ω) = (I + II − III) 1/2 [18] where Re(ω) denotes the real part of the complex valued function which is physically the initial perturbations growth rate. Re(ω) 2 = (I + II − III) [19] Equation [19] shows that the kinetic energy and the apparent gravity contribute positively to the growth of the disturbance whereas surface tension has a negative effect on the growth of the disturbance and tries to suppress the growing disturbance. For extremum conditions (in this case it is a maxima) we have the derivative of the real part of the square of the growth rate with respect to the wavenumber to be zero.  The solution of equation [20] will give us the wavenumber whose real part of the growth rate is the maximum. This solution will provide the wavenumber of the fastest growing wave. is generated from equation [25] , and the peaks of the two distribution are very close to each other and hence corroborates the experimental and theoretical data to very high accuracy. Fig. 14(b) shows the probability density function for the finger formation timescale τRT in ms. The Rayleigh-Taylor time scale τRT is the reciprocal of Re(ω). The green shaded region denotes the theoretical estimates of the density function, which predicts the scale correctly. The theoretical density function for τRT is calculated by using equation [25] in equation [18] . Equation [25] gives us the wavenumber at which the dispersion curve peaks. The final phase of all the successive events is the disintegration of the finger-like structures into droplets by Rayleigh Plateau instability. The finger/ligament-like structures formed by the Rayleigh Taylor mechanism undergoes ligament breakup forming droplets at the tip. Fig. 15 (a) depicts the ligament breakup mechanism showing how the ligament disintegrates into droplets. Rayleigh Plateau Instability. The ligament undergoes a stretching and successively undergoes tip breakup. This stretching mechanism essentially happens in 1D locally, which approximately coincides with the sheet expansion in the radial direction. The dispersion relation from 1D Rayleigh Plateau stability analysis is given by (16, 17 ) Where I0 is the Modified Bessel function of the first kind (0th order), I1 is the Modified Bessel function of the first kind (1st order), t0 is the initial thickness of the ligament, τRP = 1 ωmax is the timescale associated with the growth rate. L t 0 > π is the ligament breakup criterion where L is the length of the ligament, and t0 is the thickness of the ligament. All the ligament that pinches off into droplets satisfies the length to width aspect ratio condition, i.e., the aspect ratio of the ligament has to be greater than π for the ligament that breakup. This is shown in Fig. 15(b) . This figure shows a complete set of 60 runs. If the aspect ratio of the ligament is greater than π, it breakups into droplets. On the contrary, if the aspect ratio is less than π, the ligament would stretch without breakup. Fig. 16(a) shows the theoretical dispersion curve obtained from the linear stability theory using the ligament thickness obtained from the probability density function experimentally. The distribution was experimentally obtained by assuming that the rim surrounding the expanding sheet has the same thickness as the ligament on an order of magnitude scale. This gives us an upper limit on the ligament thickness. The black curve represents a thickness of approximately 0.22mm, and the red curve represents a thickness of 0.43mm. Both curves peak at the same value of wavenumber, and the corresponding wavelength is approximately 3.93mm. However, the growth rate and the associated time scales are different. τRP is greater for the thicker ligament than the thinner ones. Fig. 16 (b) depicts the experimental and theoretical probability distribution function. The theoretical and the experimental curves peak at the same value of τRP corroborating the linear stability analysis. The droplets from ligament breakups have a multitude of size distribution and velocity distribution. A bounding rectangle is used to calculate the shape of the droplet. The major axis and minor axis of the droplets were calculated from the bounding rectangle's width and height. The droplets were tracked, and the velocities were calculated from the derivatives of the position. Fig. 17 summarizes the shape and the velocity of the droplets. Fig. 17 (a) characterizes the shape of the droplets. The droplets ejected out have a wide range of size distribution ranging from 0.2mm − 3mm. The droplets, in general, are elliptical in nature. The minor-axis to the majoraxis ratio of the droplets are approximately 0.8 on average. The droplets, on average, are elongated along the horizontal axis. The droplet velocities are also distributed over a range. The components of the droplet velocities are shown in Figure  17  This work addresses the fluid dynamics involved during an ophthalmologic measurement technique called tonometry used to measure the intraocular pressure (IOP) of the human eye. The mechanisms involved during tonometry were studied experimentally and theoretically in sufficient detail for dry eye and watery eye conditions using real human subjects. The experimental observations were supported by a comprehensive theoretical understanding of all the mechanisms that are involved. High-speed imaging was used to capture the intricate transient three-dimensional fluid mechanical processes that are involved. The images were acquired from different views like side, orthographic, to name a few, for a better understanding of the three-dimensional nature of the flow features. The tonometer ejects an air puff from the nozzle, which is kept at approximately 11mm from the eye. The air puff has been characterized using high-speed scattering and shadowgraphy techniques. It has been found that the air puff has two essential features, a leading vortex followed by a trailing jet. The leading vortex initially approaches the eye, which causes an increase in the air velocity field locally, which leads to a local pressure reduction causing an initial sheet ejection in the case of a wet eye. The wet eye is simulated by using eye drops before undergoing the tonometry measurement technique. While the sheet ejects out of the eye, the trailing jet hits the cornea causing a deflection, which in turn creates capillary waves on the surface of the eye. The sheet expands in two phases, first the ejection due to the leading vortex. Then it receives a secondary kick from the effect of the trailing jet. During the first phase of the sheet expansion, small disturbances lead to Rayleigh Taylor waves to form and grow. Due to the highly transient and three-dimensional nature of the flow field bag like structures begin to appear, it undergoes the bag breakup mode of disintegration. Further, the Rayleigh Taylor modes lead to finger like structure to form, which undergo Rayleigh Plateau break up into droplets. The droplet's size distribution and velocity were experimentally determined from the highspeed images. A complete theoretical framework relevant to the current case was developed and was compared with the corresponding experimental data. 
paper_id= fffd4aa319552fa6d648f4aad961db01ab14f15e	title= Thyroid sequelae of COVID-19: a systematic review of reviews	authors= Pierpaolo  Trimboli;Chiara  Camponovo;· Lorenzo Scappaticcio;Giuseppe  Bellastella;Arnoldo  Piccardo;Mario  Rotondi;	abstract= The coronavirus disease 2019 caused by the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) has the potential to cause multi-organ effects including endocrine disorders. The impact of COVID-19 on the thyroid gland has been described but several aspects have to be clarified. The systematic review was conceived to achieve more solid information about: 1) which thyroid disease or dysfunction should be expected in COVID-19 patients; 2) whether thyroid patients have a higher risk of SARS-CoV-2 infection; 3) whether the management has to be adapted in thyroid patient when infected. The literature was searched by two authors independently. A 5-step search strategy was a priori adopted. Only reviews focused on the relationship between thyroid and COVID-19 were included. The last search was performed on February 21 st 2021. Two-hundred-forty-seven records was initially found and nine reviews were finally included. The reviews identified several potential thyroid consequences in COVID-19 patients, such as thyrotoxicosis, low-T3 syndrome and subacute thyroiditis, while no relevant data were found regarding the potential impact of COVID-19 on the management of patients on thyroid treatment. The present systematic review of reviews found that: 1) patients diagnosed with COVID-19 can develop thyroid dysfunction, frequently non-thyroidal illness syndrome when hospitalized in intensive care unit, 2) having a thyroid disease does not increase the risk for SARS-CoV-2 infection, 3) thyroid patients do not need a COVID-19-adapted follow-up. Anyway, several factors, such as critical illness and medications, could affect thyroid laboratory tests. 	body_text= In March 2020, WHO declared the pandemic of the novel coronavirus disease 2019 (COVID-19) caused by the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) [1] . Since then, about 111 million cases and 2.5 million deaths from SARS-CoV-2 were reported until February 2021 [2] . What has been shocking for the whole medical scientific community is that SARS-CoV-2 has a wide spectrum of clinical severity, ranging from a-or pauci-symptomatic presentation to disease-specific mortality [3] . COVID-19 has the potential to cause upper respiratory tract, pulmonary and systemic inflammation, determining multi-organ dysfunction especially in frail patients [4] . These latter are also at higher risk of COVID-19 specific death. Among the various clinical effects of SARS-CoV-2, the endocrine ones have been investigated. Importantly, diabetic patients were proven to be at higher risk for infection and poorer prognosis when infected [5] . Moreover, treatment modifications are required as a consequence of COVID-19 in patients affected by endocrine diseases, such as diabetes, adrenal insufficiency, hypo-or hyper-natraemia [5, 6] . In addition, the impact of COVID-19 on thyroid gland has been described by direct or indirect effect [7] . In the thyroid field many original papers, several case reports, and some recommendations by endocrine societies on the impact of COVID-19 have been published. Furthermore, a not negligible number of narrative and systematic review articles on this topic have been conducted. Overall, although several clues would suggest a possible relationship between thyroid gland and SARS-CoV-2, and even if the presence of the receptor for SARS-CoV-2 entry (i.e., angiotensin-converting enzyme 2 [ACE-2]) in thyroid cells has been proven [8, 9] , which thyroid consequences can be observed in patients diagnosed with COVID-19 remains to be fully clarified. The present study was conceived to provide more solid information about three specific items: 1) which thyroid disease or dysfunction can be expected in COVID-19 patients; 2) whether thyroid patients have a higher risk of SARS-CoV-2 infection; 3) whether the management has to be adapted in thyroid patient when infected by SARS-CoV-2. Therefore, it was designed a systematic review of reviews published on this topic, the conclusions of the retrieved articles were summarized, and the summary of findings discussed. The systematic review was conducted according to the methodology proposed by Aromataris et al. [10] . The literature was searched by two authors independently (PT and LS). A 5-step search strategy was a priori adopted: 1) sentinel studies were sought in PubMed using multiple combinations of the following keywords: thyroid, SARS-CoV-2, coronavirus, COVID, COVID-19; 2) keywords and MeSH terms were identified in PubMed; 3) PubMed, CEN-TRAL, Scopus, and Web of Science were searched; 4) narrative reviews, systematic reviews and meta-analyses with potential to be eligible were identified; 5) reviews focused only on the relationship between thyroid and COVID-19 were directly included in the study while those focusing on the general endocrine impact by COVID-19 were screened and included only when having a large section dedicated to thyroid. The last search was performed on February 21 st 2021. To find additional studies and expand the search, the references of the articles retrieved were also screened. For each included article the following information was extracted by two authors independently (CC and LS): authors, country, date of the last literature search, date of publication, journal, type of review (i.e., narrative review, systematic review, systematic review with meta-analysis), context of the review (i.e., thyroid field or endocrinology), aim of the review, conclusions of the authors. Also, the type of thyroid sequelae (i.e., diseases or dysfunction) were extracted. By the search strategy, a number of 247 records was initially found. Among these, according to the above criteria, 238 studies were excluded since they were not review articles, and nine reviews were finally included [7, [11] [12] [13] [14] [15] [16] [17] [18] (Fig. 1) . The nine included reviews were conducted by European, American and Asian authors. The date of the last literature search was reported in three studies and varied from September to December 2020. Seven studies were narrative review and two were systematic review. Both objectives and conclusions of the nine reviews were clearly reported. However, since the largest part of the retrieved reviews was narrative, their pre-defined patientcentered questions (e.g., PICOS, participants, interventions, comparators, outcomes, and study design) were not reported. Table 1 summarizes the main features of the reviews included in the present systematic review.  The aim and the main conclusions of the nine reviews were schematically reported in the Table 2 . Briefly, the reviews identified several potential thyroid consequences in COVID-19 patients, while no relevant data were found regarding the potential impact of COVID-19 on the clinical/therapeutic management of patients with thyroid diseases. Patients with SARS-CoV-2 infection can develop thyrotoxicosis or low-T3 syndrome and should be at risk to develop subacute thyroiditis (SAT). Regarding the rate of these complications/ dysfunctions, no significant data were found in the reviews included in the present systematic review. Furthermore, having a thyroid disease should not be considered a risk factor for SARS-CoV-2 infection. The summary of findings of the present systematic review was reported in Table 3 to facilitate the presentation of the results. With the beginning of COVID-19 pandemic, we faced a tsunami of publications. This is true also for the endocrine and thyroid field. However, after one year since the WHO declaration of pandemic, we should need taking stoke of the situation. We have several clues that SARS-CoV-2 can have effects on the thyroid gland. High levels of expression of ACE2 receptor and transmembrane protease serine 2 have been found in thyroid cells [8, 9] , abnormal immune responses and cytokine storm associated to COVID-19 may induce thyroid gland inflammation [9, 19] , and both direct and indirect mechanisms might affect hypothalamic-pituitary-thyroid axis [20] [21] [22] [23] [24] . Based on the above evidences, and according to the sparse knowledge on this topic, a systematic review was designed and a number of nine reviews focused on the relationship between thyroid and SARS-CoV-2 were retrieved. Unfortunately, among the reviews included seven, were narrative reviews that do not represent an evidence-based information as the systematic ones well. Regarding the first question of the present review, the pooled findings from the included reviews allow us to conclude as follows. A subject diagnosed with COVID-19 can: 1) develop thyroid dysfunction due to direct and indirect damage on the thyroid gland; 2) have a low-T3 syndrome if hospitalized due to severe COVID-19 infection; 3) eventually develop SAT. In the nine reviews we did not find clear data allowing to establish the real prevalence of these scenarios. Concerning the other two questions we raised with this review, we can conclude that having a thyroid disease does not represent a risk for infection of SARS-CoV-2 and does not require specific recommendations. The COVID-19 pandemic has dramatically influenced our daily life and has abruptly shifted the medical research. Furthermore, we have faced a huge medical information, also in the thyroid field, that was often conflicting with each other [25] . Following these considerations, the results from the present paper should be discussed with caution and with the aim to provide further insights from a clinical point of view. It is recognized that patients with severe systemic disease generally show altered thyroid laboratory tests [26] . Also, There are no data suggesting that thyroid patients are at higher risk of COVID-19. In patients severely affected by COVID-19, changes in thyroid function may relate to a 'sick euthyroid' syndrome, but there may be specific thyroid-related damage which requires further investigation Gorini [12] To summarize the main findings on thyroid and COVID-19 and define research lines aimed at patient care and effective public health measures Thyroid disease is not a risk factor for the development of COVID-19, and a higher prevalence of thyroid disease has not been found in patients with COVID-19. Questioning patients about the presence of thyroid disease at the time of hospitalization with subsequent follow-up might be useful in patients with multiple diseases to avoid thyrotoxicosis first and hypothyroidism later resulting from SAT Caron [13] To discuss the diagnosis and the management of patients presenting with thyrotoxicosis, thyroid-associated orbitopathy and hypothyroidism in the context of SARS-CoV-2 infection Routine assessment of thyroid function in the acute phase for COVID-19 patients requiring intensive care is useful, as they frequently present thyrotoxicosis related to SARS-CoV-2, and during convalescence to diagnose and adapt levothyroxine replacement treatment in patients with primary or central hypothyroidism Scappaticcio [7] To explore the impact of COVID-19 on the thyroid gland SAT mainly occurs during or soon after mild COVID-19. Thyrotoxicosis without neck pain (possibly in the context of the nonthyroidal illness syndrome) could characterize more severe and critical cases of COVID-19 pneumonia. Some clues of the hormonal changes (i.e., low T3 and TSH concentrations) and overt thyrotoxicosis to be regarded as predictors of poor outcome of COVID-19 are already emerging Kumari [14] To explore the potential role of prevailing thyroid disorders in SARS-CoV-2 infection Thyroid dysfunction may have considerable risk in aggravating the infection and spread of SARS-CoV-2, and it is closely associated with aging Chen [15] To review the potential interaction between COVID-19 and the thyroid gland, including thyroid pathology, function and diseases. To explore the potential harmful effects of COVID-19 drugs on the thyroid SAT caused by SARS-CoV-2 has been reported. The alteration of thyroid tests is common among COVID-19 patients. However, there is no pathological evidence of thyroid injury caused by SARS-CoV-2. Some anti-COVID-19 agents may cause thyroid injury or affect its function Speer [16] To explore and compare the impact of SARS-CoV and SARS-CoV-2 on the thyroid gland No increase of prevalence of pre-existing thyroid disorder in SARS-CoV and SARS-CoV-2 patients was found. However, routine screening of thyroid function at least in COVID-19 patients requiring hospitalization is suggested, because subacute thyroiditis might be a late complication Croce [17] To summarize studies regarding thyroid function alterations in patients with COVID-19 Non-thyroid illness syndrome is the most consistently observed alteration of thyroid function parameters. SARS-CoV-2 may also infect the thyroid producing typical (painful) or atypical (painless) subacute thyroiditis Piticchio [18] To discuss the relationship between COVID-19 infection and the endocrine glands and compare it with SARS-CoV A possible damage of endocrine system in COVID-19 patients should be investigated in both COVID-19 acute phase and recovery to identify both early and late endocrine complications that may be important for patient's prognosis and well-being after infection it is known that patients in the intensive care unit typically present with decreased tri-iodothyronine, low thyroxine, and normal range or slightly decreased TSH (i.e., non-thyroidal illness syndrome or low-T3 syndrome), even if no proof exists for causality of this association [27] . Furthermore, it is known that thyroid hormones levels can hold a role to predict a worse prognosis in these patients [27] . The larger part of studies investigating thyroid function in COVID-19 patients enrolled critically ill subjects. Then, it should be not surprising that also COVID-19 patients present with this thyroid hormone profile, which should not be specifically ascribed to COVID-19. A certain role for systemic inflammation might also be envisaged. Indeed, the cytokine storm occurring during SARS-CoV-2 infection often results in an uncontrolled inflammatory response that is detrimental to host cells [17, 22] . As far as the thyroid gland is concerned, it should be highlighted that several models, including the chronic low-level inflammation associated with aging, the so-called "inflammaging" [28] , and the oxidant/antioxidant imbalance associated with thyroid chronic inflammation [29] , proved to have a role in the pathogenesis of thyroid diseases. From this point of view, there is potential to consider a thyroid role in the long-term effects of COVID-19. In addition, it has to be taken into account that the evaluation of thyroid function in COVID-19 patients can be influenced by a number of the medications received [30] . Heparin has been largely used in COVID-19 patients and it is known to interfere in free thyroid hormones assays. In fact, heparin liberates lipoprotein lipase from the vascular endothelium, and blood samples from heparin-treated patients have increased lipoprotein lipase activity generating non-esterified fatty acids (NEFA). Free thyroid hormone assays, especially those with prolonged incubation periods, are affected since NEFA displace T4 and T3 from binding proteins, causing spuriously high values [31] . Patients with severe COVID-19 can develop a systemic inflammatory response and corticosteroids have been largely used because of their powerful anti-inflammatory effects. However, corticosteroids can influence thyroid laboratory tests by several pathways: act on hypothalamicpituitary control on the thyroid reducing TSH, reduce levels of thyroxine-binding globulin and increase free T4, and inhibit thyroid hormone activation (i.e., conversion from T4 to T3). Finally, it cannot be excluded that COVID-19 patients have been exposed to multiple chest iodinated contrast-enhanced computed tomography which can obviously determines a transient thyrotoxicosis. Some considerations on the possible occurrence of SAT in patients experiencing COVID-19 should be drawn. Indeed, these descriptions mainly rely on case reports [7] . As a consequence, there is no definite proof about this possible COVID-19 complication, and the true frequency of SAT among COVID-19 is unknown. SAT might occur a few weeks after a viral infection of the upper respiratory tract, and then it might be a late complication of COVID-19. Here we systematically reviewed all reviews published on the relationship between thyroid and COVID-19. The nine reviews we included in our study were published between June 2020 and February 2021 and more recent original studies were not included. Then, some conclusions we achieved might be changed. Regarding the possible need to adapt the management of thyroid patients when infected by SARS-CoV-2, even if we did not find strong information, it has to be cited the recent bulletin by European Thyroid Association [32] quoting that no consequences have to be expected in patients receiving treatment for hypo-or hyperthyroidism [33, 34] . The present systematic review has some limitations to be disclosed. First, published data on the relationship between thyroid and SARS-CoV-2 are limited and sparse. Second, we found almost only narrative reviews that do not represent an evidence-based information. Third, the COVID-19 pandemic is still going on and data we found in the reviews should be revised in the next studies. In conclusion, the present systematic review of reviews found that: 1) patients diagnosed with COVID-19 can develop thyroid dysfunction, frequently non-thyroidal illness syndrome when hospitalized in intensive care unit, 2) having a thyroid disease does not increase the risk for SARS-CoV-2 infection, 3) thyroid patients do not need a COVID-19-adapted follow-up. Anyway, it has to be taken into account that several COVID-19 intrinsic factors, such as critical illness and medications, could affect thyroid laboratory tests. Since the rapid worldwide diffusion of SARS-CoV-2 and its variants, it is highly important to annotate that the summary of findings of the present study might change in the next future. Funding Open Access funding provided by Università della Svizzera italiana. The authors declare that they have no conflict of interest. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/. 
paper_id= fffdd7ae648facf9fb878162ced64cb30c71bf39	title= 	authors= 	abstract= 	body_text= In December 2019, after the cases of pneumonia of unknown cause began to appear in Wuhan, China, a new SARS CoV-2 was detected as a causative agent. This virus, which started to spread rapidly afterward, was named COVID-19 and was declared a pandemic by the World Health Organization (WHO) [1] . So far, more than 110 million confirmed cases and over 2,500,000 deaths have been reported [2] . There have been many studies in the literature on the main symptoms, treatment, and prevention methods of this disease, which transmits from person to person, and causes severe damage to many systems, especially the respiratory system [3] . Research continues on this disease, which does not have a proven, reliable treatment. With the COVID-19 outbreak, many countries have taken severe and swift measures to prevent the disease's spread and protect their citizens. In this process, COVID-19 patients became a priority, and all arrangements were made for the appropriate management of COVID-19 patients. This change brought along some problems. The most crucial issue has been the indirect effect of the COVID-19 outbreak on potentially life-threatening conditions [4] . During the pandemic period, the decrease in the number of referrals of patients with acute coronary syndrome and acute stroke was noted [5] [6] [7] . A similar reduction was found in the admissions of trauma patients [8] . Emergency departments (EDs) have been the site of first presentation of emergency patients during the pandemic period as in the pre-pandemic period. In our country, all presentations of COVID-19 suspected patients were made to EDs until pandemic outpatient clinics were become functional. After COVID-19 outpatient clinics were put into use, EDs continued to serve non-critical patients in addition to critically ill patients (both COVID and non-COVID). During the COVID-19 outbreak, COVID and non-COVID patients continued to receive care from EDs. During the pandemic period, there have been changes in ED visits as in other clinics. However, there are limited studies in the literature evaluating the changes in ED visits. Also, there are no studies on the COVID-19 outbreak's impact on critical and non-critical patient visits and emergency consultations together. This study aims to determine the effect of the COVID-19 outbreak on ED visits and emergency consultations according to the triage levels. This retrospective observational study was conducted in the ED of a tertiary training and research hospital in Turkey's most populous city, İstanbul. Approximately 350,000 patients present annually to the ED, where the study was conducted. It is the primary center for percutaneous coronary intervention and thrombolytic therapy for cardiac and neurological emergencies. Additionally, it is a level three trauma center and a referral center for oncological emergencies. The Ministry of Health assigned the hospital a "pandemic hospital" for COVID-19 suspected patients after the first COVID-19 case in the country was diagnosed in the second week of March. It serves approximately 1000 new COVID-19 suspected patients daily with all clinics, wards, intensive care units, and ED. Additionally, non-COVID-19 patients continued to take care from the hospital. The study started after the approval of the local ethics committee (Approval ID: 2020/365). Emergency patient data, recorded between April 1, 2020 and May 31, 2020, were included in the study. The daily count of ED visits during the study period and the count of the consultations requested from ED were recorded. The data were obtained from electronic medical records. The patient visits and consultations in the same months of the previous year (April 1, 2019 -May 31, 2019) were included as a control group to reflect the pre-pandemic period and to make the comparison reliable. Patients referred from another medical center and consultations requested from the same department (reconsultations) were excluded from the study. In total, there were 26,974 patients and 10,080 consultations in the pandemic group and 53,968 patients and 14,553 consultations in the pre-pandemic (control) group. The flow diagram of the study is shown in Figure  1 . Patients were triaged to red, yellow and green areas by a registered triage nurse who had previously received triage training organized by the Health Ministry. Life-threatening situations require rapid (within minutes), aggressive approach, and immediate, simultaneous evaluation/treatment. Cases that have the possibility of a life-threatening condition, risk of limb loss, and significant morbidity. Conditions with middle and long-period symptoms and potential of severity (Patients with abnormal respiratory rate, pulse, blood pressure, oxygen saturation, body temperature; patients who need medical treatment, and patients with a subjective pain score of 80% of the maximum score). The patients whose general condition is stable and who can be treated on an outpatient basis. Patients with simple health problems were not at risk of morbidity or life-threatening situation by waiting for 1-4 hours. Examples of cases used in triage color coding are given in Appendix 1. All patients were first evaluated in the triage area and were transferred to areas (red, yellow, or green area) suitable for their severity levels. The researchers calculated the examination and consultation rates of patients who presented to these areas according to the total count of patient examinations to the ED during the pandemic and pre-pandemic period. The total count of examinations represented the total patients' count seen by an emergency physician in the ED. Leaving without being seen (LWBS) count was not included in the calculations not to affect the results. The patient examination and consultation rates were calculated according to the following formula: Patient examination rate = Count of patients / Total count of examinations x 100 Patient consultation rate = Count of consultations / Total count of examinations x 100 Count of patients: Patient counts which were presented to the ED red, yellow, and green areas. Count of consultations: Consultation counts requested from ED red, yellow, and green areas. Total count of examinations: Total count of patients who were seen by an emergency physician (Total count of ED visits -Leaving without being seen count). The study's primary outcome is the change in emergency visits according to the triage levels during the COVID-19 outbreak. Secondary outcomes are the change in the total emergency visits during the COVID-19 outbreak, the frequency of consultations requested from the ED, and the count of LWBS patients. Numerical variables were represented as mean ± standard deviation or median (interquartile range). Categorical variables were presented as absolute values and percentages. Kolmogorov-Smirnov and Shapiro-Wilk tests were used to evaluate the distributions. Independent groups were assessed using the independent t-test and the Mann-Whitney U test. The relationship between categorical variables was assessed using the chi-square test. Spearman or Pearson test was used in correlation analysis according to the distribution of the data. The statistical significance level was set as p < 0.05. Statistical analysis of the data was performed with IBM SPSS version 23 program (IBM Corp., Armonk, NY). During the study period, 53,968 emergency visits were recorded between April and May 2019, while 26,974 patients were recorded in the same months of 2020. Approximately 50% reduction in total ED visits and 30% reduction in emergency consultations were detected. ED visits and consultations requested from ED are shown in Table 1 . A significant decrease was detected in all triage levels of visits during the COVID-19 outbreak (p < 0.001). Consultation counts in the red and yellow triage levels were significantly decreased (p < 0.001). However, there was no significant difference in the consultation counts in the green triage level (p = 0.967). While the count of consultations for all other clinics decreased or did not change, a significant increase was found in the count of infectious diseases consultations requested from ED (p < 0.001). All LWBS patients were at the green triage level in both pandemic and pre-pandemic groups. The changes in emergency visits and emergency consultations are shown in Table 2 . Forensic science 0 (0-0) a 0 (0-0) a 0.081* Psychiatry 0 (0-1) a 0 (0-0) a <0.001* A moderate-strong correlation was found between the total count of ED examinations, the count of patient visits at all triage levels, and LWBS patients' count. The correlations between the total count of examinations, the count of emergency visits, and consultation counts are shown in Table 3 . When the patient examination rates were evaluated, a significant increase was found in the red and yellow triage levels, while a significant decrease was found at the green triage level. A significant increase was found in the consultation rates requested from all triage levels. According to the consulting departments, a significant increase was found in consultation rates of infectious diseases, anesthesiology, and cardiology departments (p <0.001). There was a significant decrease in consultation rates of the neurology department (p <0.001). The changes in examination and consultation rates are shown in Table 4 . The changes in the consultation rates of the clinics are shown in Figure 2 .  In this study, a significant decrease was detected in the emergency visits of all triage levels and emergency consultations. In our study, it was found that the rates of examinations and consultation of patients with red and yellow triage levels increased significantly. In this situation, we can say that the critical patients presented to the ED more than non-critical patients during COVID-19 outbreak. In a multicenter study by Hoyer et al., they found a reduction in stroke and transient ischemic attack cases in some centers [9] . In the study conducted by Mafham et al., a decrease in all acute coronary syndrome types was found during the COVID-19 period [5] . Lange et al. stated in their report that during the COVID-19 period, hospital admissions decreased in life-threatening situations [4] . Similarly, in our study, a significant decrease was detected in the visits of critically ill patients at the red triage level and the consultations from the red area. Although there is a significant decrease in the count of critical patients, the rate of critical patients has increased within the total emergency visits in our study. Also, emergency consultation rates of critically ill patients have increased within the total emergency consultations. Nationwide information and clarification are needed for the public. It should be emphasized that non-COVID patients are also provided medical care in appropriate areas during the pandemic period. Additionally, appropriate triage scores could use to select the critical patients who need more resources and medical care [10] . A significant decrease was detected in the count of visits and visit rates at the green triage level. In this case, it can be argued that non-emergency patient visits decreased significantly during the COVID-19 period. Although the count of LWBS decreased, an increase in the LWBS rate was detected. The reason for this proportional increase may be the potential risk of contact with COVID-19 patients. It can be said that the count of non-emergency visits decreased during the pandemic period, and they tended to leave the ED before the examination at a higher rate. When the consulted departments were evaluated, there was a significant decrease in consultations requested from most departments, while an approximately nine-fold increase in infectious disease consultations. This increase can be considered as a natural consequence of the COVID-19 outbreak. COVID-19 cases have been reported in a broad clinical spectrum, from mild to severe conditions [11] . It has been reported that COVID-19 can cause severe diseases and the need for intensive care admission, especially in elderly patients [11] . The increase of critical patients due to COVID-19 infection also affected the count of anesthesiology consultations in our study. Although there was no significant difference in anesthesiology consultations in our study, it was found that the anesthesiology consultation rates reached approximately two times higher compared to the pre-pandemic period. The reason for this increase could be the proportional increase of COVID-19 critical patients in the red triage level within the total ED visits. Toniolo et al. stated that severe cardiac cases decreased during the pandemic period [12] . Similarly, a significant decrease was found in the patients' count consulted to the cardiology department in our study. However, a significant increase in cardiology consultation rates was detected. This increase can be accepted due to the increase in the proportion of patients with red triage level and the higher need for the cardiac evaluation of the critical patients. Besides, the cardiac effects of both the COVID-19 virus itself and the drugs used for treatment may have caused an increase in cardiology consultation rates. A more decrease was detected in the count of patients consulted to the neurology department than the cardiology department consultations. Schwarz et al. also found a reduction in acute coronary syndrome and cerebrovascular disease cases similar to our study [13] . The reservation of some of the neurology and coronary intensive care beds in our hospital for COVID-19 intensive care patients has decreased the count of patients with neurological and cardiac diagnoses brought to the ED by the emergency medical service. As a result of this situation, the count of patients consulted in both departments has decreased. Gastrointestinal and internal problems associated with COVID-19 have been reported in the literature [14, 15] . While the count of internal medicine and gastroenterology consultations decreased, the consultation rates did not change. COVID-19-related gastrointestinal and internal pathological conditions can be shown as the reason for similar consultation rates. The studies conducted by Mitkovic [17, 18] . Similarly, in our study, a significant reduction was found in the count of visits to the red and yellow areas where surgical emergencies and trauma patients were presented. A similarly significant decrease was found in patients' count consulted to the general surgery and orthopedics departments. Similar to the reduction of the count of orthopedics consultations, the consultation rates within total consultations were decreased too. The general surgery department was the most consulted clinic in the pre-pandemic period, and it was also the second most consulted clinic during the pandemic period. General surgery consultation counts decreased significantly during the pandemic period. The decrease in total ED visits may cause this situation. However, there was no significant change in general surgery consultation rates. Although the total count of neurosurgical consultations decreased, it increased proportionally. Also, there was no significant change in the count of thoracic surgery consultations during the pandemic period, but the thoracic surgery consultation rates increased. Our hospital continued to serve as a tertiary healthcare institution and a trauma center during the pandemic. Besides, ours is the only hospital in our region with a full-time thoracic surgeon on staff. These situations could cause the absence of significant decreases in critical patients' proportional values and some of the departments' emergency consultations requested from ED. In a multicenter study conducted in Spain, researchers found an approximately 60% decrease in emergency surgery cases and an approximately 50% prolongation between the onset of symptoms and the presentation to the ED during the pandemic period [18] . The reasons for this change may be the restrictions and the fear experienced by the patients in many countries as in our country. Although there are restrictions across the country, our country's emergency medical service has continued to serve patients with its increased capacity. The emergency call center has also helped emergency patients who have demanded to reach hospitals with their means by issuing permits during the restriction period. Therefore, fear plays a more significant role rather than the restrictions on the decrease in patient presentations. The reduction in all triage level visits in our study may also be due to fear. Also, Mantica et al. stated that an increasing COVID-19 daily mortality might affect ED admissions [19] . A moderate-strong correlation was found between the total examination count and all triage level patients and LWBS patients in our study. This relationship can be shown as the reason for a similar decrease in emergency visits and emergency consultations. Similarly to Babu et al.'s study, the ophthalmology consultation counts and consultation rates were decreased during the pandemic period in our study [20] . The reason for this decrease may be to protect the social distance between patients and physicians because the ophthalmology examinations were face to face and closer than other examinations. The first limitation of our study is a single-center study. Since the data on the count of emergency surgery and cardiac interventions performed during the pandemic period were not included in the study, no discussion could be made on this issue. Since the demographic information, clinical follow-up processes, and clinical outcomes of the patients were not included in the study, the relationship between the count of visits and consultations could not be evaluated. No comments have been made regarding the small number of consultation requests in the pre-pandemic period. During the COVID-19 pandemic, emergency visits at all triage levels decreased. While the rate of critical patient visits increased, non-emergency visits decreased. The total count of consultations requested from the emergency department decreased. Anesthesiology and cardiology consultation rates increased; neurology, orthopedics, and ophthalmology consultation rates decreased; and internal medicine and general surgery consultation rates did not change within total emergency consultations. Necessary measures should be taken, and public information should be provided to ensure that all patients can safely present to hospitals and be evaluated, especially non-COVID critical patients. All kinds of simple symptoms in the patient whose general condition and vital signs are stable Patients with severe shortness of breath in which the auxiliary respiratory muscles participate in respiration or whose pulse oximetry value is <90% if it can be observed. In compliance with the ICMJE uniform disclosure form, all authors declare the following: Payment/services info: All authors have declared that no financial support was received from any organization for the submitted work. Financial relationships: All authors have declared that they have no financial relationships at present or within the previous three years with any organizations that might have an interest in the submitted work. Other relationships: All authors have declared that there are no other relationships or activities that could appear to have influenced the submitted work. 
paper_id= fffe3c4121406d153309ca035dcb62190c56a968	title= The necessity of examining patients' social behavior and teaching behavior change theories: curricular innovations induced by the COVID-19 pandemic	authors= Danette Waller Mckinley;Saeideh  Ghaffarifar;	abstract= During the COVID-19 pandemic, despite many widespread calls for social distancing, recommendations have not been followed by some people and the high rate of non-compliance has significantly affected lives all around the world. It seems that the rate of non-compliance with the recommendations among medical students has been as high as the rest of the other youth. In the time that students are removed from clinical environments and most physician teachers are strained in providing services to patients, medical students can be trained in interdisciplinary behavior change counseling programs and they can be employed in delivering virtual consultations to the patients referred to medical centers. In this quick review, we provide an argument regarding the importance of integrating the topic of patients' social history into the undergraduate medical curriculum and the necessity of teaching theories of behavior change to medical students. Hypotheses are proposed that focus on the importance of integrating behavioral and social sciences into the medical curriculum and to teach theories or models of behavior change to students. Health professions educators can design and implement interventions to teach hypothesized models of behavioral change to medical students and evaluate the effectiveness of those interventions. The impacts of such educational interventions on increasing people's compliance with recommendations to improve public health can be evaluated as well. 	body_text= For many years, training medical students to be competent in the cultural and social aspects of health care has been emphasized by scholars and institutions around the world. Integrating behavioral and social sciences (BSS) into the American undergraduate medical curriculum was recommended by the Institute of Medicine (IOM) in 2004 [1] and guiding the "strategic development of an educational institution" by "social, economic, cultural and environmental determinants of health" was recommended by Boelen and Woolard in 2009 [2] . In the last decade, different strategies and methods have been employed to teach BSS to medical students. Medical students are involved in behavior counseling with patients. Considering the areas featured by Healthy People 2030 [3] , social determinants of health (SDoH) are situated among other topics into the undergraduate medical curriculum in some universities. In order to reduce the rate of premature deaths and increase the human beings' quality of life, medical students are educated to pay special attention to the role of social factors affecting health [4] . Many single or multi-institutional interventions, with different degrees of impact, are implemented to train medical students to advice and advocate behavior change in patients. Despite much invaluable advancement in medical education, concerns have been expressed that medical students are not adequately prepared to be competent in behavior-change counseling. A systematic review on "behavior change counseling curricula for medical trainees" published in 2012 reported that student satisfaction and/ or knowledge, not their performance (behavior) are considered as outcome measures in most studies [5] . It can be inferred from the findings of this review that present medical curricula lead to only an increase in learners' knowledge and satisfaction and may not be able to change the actual performance of students. The shortcomings of the present curricula in changing the actual performance of medical students may be exacerbated during the COVID-19 pandemic. Studies indicate that despite many widespread calls for social distancing during this recent pandemic, many regulations and recommendations have not been followed by some people in the world. Such non-compliance has affected the quality of life of many people around the world. The rate of premature deaths among the population has unfortunately increased. In the U.S, 39.8% of participants of an online survey were non-compliant. Eighteen-to thirty-one-year-olds were less likely to follow orders of social distancing (compliance rate = 52.4%) [6] . Given many concerns in the time of COVID-19, most medical students have been removed from clinical environments and they are not directly involved in COVID patients' care. The pandemic provides an opportunity to advance medical education through "active curricular innovations" [7] . Contemplating the shortcomings of the present curricula in teaching the role of BSS in changing health-related behaviors, revision of the undergraduate medical education curriculum, with a special focus on patients' social behavior history, seems necessary. In 1951, the role BSS can play in medical education was elaborated. Later, opportunities to enhance BSS training in medicine were provided by many universities in the US. The first behavioral science department in a medical school was established in the University of Kentucky in 1959. Since then, repeated efforts have been made to integrate the social sciences into medicine, but several barriers have persisted [8] . That is why, in 2004, IOM recommended including BSS in medical curricula, in six different domains. These domains include health policy and economics; social and cultural issues in health care; mind-body interactions in health and disease; physician role and behavior; patient-physician interaction and patient behavior [1] . Focusing specifically on patient behavior, as one of the six BSS domains recommended by IOM, this review is intended to look at the extent to which examining patients' social behavior and teaching behavior change theories are included in undergraduate medical curriculum. To conduct this quick review, bibliographic databases including MEDLINE (via Ovid), PubMed, ERIC (via EBSCO) and Cochrane Library (via Ovid) were searched. Using keywords: "medical education", "medical curriculum", "medical training"; "social science", "behavioral science", "social behavior", articles published through June 2020 were searched. A combination of controlled vocabulary, free text keywords and their synonyms were used in search strategy. The search results were limited to the publications after 2000 and those which were focused on undergraduate medical education. Our search strategy of Ovid MEDLINE(R) is provided in Additional file 1. One thousand five records were identified through database searching. Duplicates were removed and 886 records were screened. Fifty-nine full texts were reviewed for eligibility. Records, which were not about BSS integration into undergraduate medical education and those which were not published in English were excluded.10 articles were found based on ancestry searching and forward tracing as well. In all, 21 studies relevant to BSS instruction for medical students were reviewed. The characteristics of the included studies are presented in Table 1 . The characteristics of the included studies are presented in Table 1 . The reviewed studies were summarized to elaborate the importance of examining patients' social behavior and the required changes in the curriculum; and to identify lessons learned. About half of all reasons for mortality and morbidity in the United States are correlated with social and behavioral factors [9] . Behavioral medicine has been introduced as "some of medicine's most powerful tools for reversing dangerous health trends in the U.S." [10] . It seems that synthesizing the micro biomedical perspectives with macro public health ones is an urgent need to provide optimal health for patients. That is why it is believed that "all health is global health" and "all medicine is social medicine" [11] . In a content analysis study, in which medical faculty members' perspectives on the necessity of incorporating BSS into the medical curriculum were explored, six themes were emerged. The results of this study showed that integrating BSS into medical curriculum leads to facilitating health behavior changes [12] . Routinely, students in most medical schools do not explore their patients' social behavior history. In order to deliver quality care, they are encouraged to gather data by conducting a history interview of patients' problems in full. In practice, however, they just do their best to ask enough questions to document their patients' past and present history of illness, as well as family and drug histories. Indeed, taking and analyzing the history of patients' social behavior usually plays a small role in medical students' diagnostic and therapeutic reasoning. For instance, participating students in Peterson and colleagues' study were exposed to a reformed curriculum at Oregon Health and Science University and were educated about BSS during their first 2 years at medical school. They reported that they were not able to internalize and use BSS values in the second half of their medical training. Indeed, despite being taught about approaches to guide patients' behavior, students were unable to translate the training received into their practice [8] . The COVID-19 pandemic highlighted the necessity of integrating the topic of taking and analyzing patients' social history into the undergraduate medical curriculum. A curricular change focused on taking and analyzing patients' social behavior as a crucial competency along with more opportunities to practice and rehearse this competency can be provided to medical students. The more they practice, the more they recognize the role of the patients' social behaviors in improving health outcomes. Such training has been shown to improve students' understanding and attitudes towards patients with chronic disease [13] . By so doing, the likelihood of change in medical students' attitude towards the role of social behaviors in ensuring health of communities will increase and they could become more motivated to adopt preventive behaviors in counseling their patients. Most importantly, students who have received the necessary training in analyzing patients' social behaviors can be employed in implementation of health education programs in the time of pandemics. So, while volunteering in many health education initiatives, medical students can participate in caring for patients through tele-health environments [14] .  In all, reviewing the 21 published studies up to June 2020 revealed the fact that in recent decades, "the line between public health and medicine has blurred" [11] . Behavioral medicine, after surgery and pharmacology, has been the third major revolution in improving human health; however, highly effective behavioral treatments are under -recognized or rejected. BSS is poorly appreciated in medical curricula [15] . The biomedical mindset of physicians has not been fully compatible with BSS; and BSS education has had its own problems globally. Although the Liaison Committee on Medical Education (LCME) and other accreditation bodies in the world have emphasized the need to consider BSS in accreditation of medical schools, incorporation of BSS in the medical curriculum has been variable at many schools around the world [12] and social medicine is taught at less than 10 % of medical schools that are accredited by the LCME [11] . It is stated that BSS education should be multidisciplinary and requires systematic attention to cover BSS in the curriculum in order to increase students' ability to diagnose patients in a broad social context [12] . Most publications have reported the importance and necessity of integrating BSS into the medical curriculum, and there are very few studies in which BSS integration has been described based on the different components of the curriculum (including teachers' characteristics; appropriate educational context and BSS teaching and assessment methods) [12] . There is no agreement on when to integrate BSS into the medical curriculum and the content that needs to be integrated [10] . What is clear is that teaching BSS to medical students improves their ability to appreciate the impact of behavior on patients' wellness and health [16] and to detect the possible reasons of patients' suboptimal health outcomes [11] . Given the ever-increasing burden of medical schools, static number of teaching hours, explosive production of the content and incredible growth of disciplines, it seems that proper content for integration should be identified through a valid and reliable process [9] . It is recommended that every decision, about the topics, teaching and assessment methods, funding, valuing and management methods of teaching material should be established in "the context of competing disciplines" [9] . Improving the educational context as much as choosing the most appropriate content for integration can be crucial in increasing the clinical application of knowledge by students. For this reason, "no content without context" has been considered as the basis for integrating BSS into the pre-clerkship curriculum at University of California Los Angeles (UCLA). The instructional planning at UCLA has been performed by collaboration of students. Their innovations in repeating the intended content in interdisciplinary blocks, in flipping the courses, in creating cross-disciplinary case vignettes, in incorporating innovated Problem Based Learning (PBL) strategies, and in integrative summative assessments has prepared students to enter clinical wards with a systematic approach for discovering patients' problems [17] . At University of California San Francisco, "need -toknow" content was identified through a multi-model procedure and BSS content was addressed in six different domains through a four-year integrated curriculum. The domains included "mind-body interactions; patient behavior; the doctor's role and behavior; doctor-patient interactions; social and cultural issues in health care, and health policy and economics" [9] . The protocols written for training BSS cannot be taught by lecturing alone [15] . The content of BSS has been innovatively integrated into the preclinical medical curriculum across 77 cases in a PBL curriculum, at UCLA during a seven-week period [10] . BSS at Southern Illinois University is taught for firstyear medical students in 11.5 weeks. Initiating each session with a clinical case, mentorship activities, "tutorgroup and resource sessions", recording the learning issues into an electronic filing system, performance-based midterm and summative examinations based on the compromised learning issues and remediation units are among the initiatives to create a positive learning experience in their hybrid curriculum [18] . After a reform with multidisciplinary approach in 2007 at Harvard Medical School (HMS), first year medical students are required to attend an integrated "Social Medicine and Global Health (ISM)" course. During the course, they learn to provide solutions to the clinical cases, which are developed by a team of physicians and other health care professionals. Along with ISM, students make home visits and write reflective assignments. Hidden curriculum has been highly considered in designing this integrated curriculum at HMS. In Harvard's reform, in which the BSS has been integrated to preclinical medical curriculum, provision of interested faculty members with "curriculum time and support" have been considered essential to develop such invaluable courses" [11] . Teaching BSS alone does not warrant application of behavior change theories by medical students to visit patients. Standardized and valid evaluation systems and tools are required to ensure applying BSS and retaining humanistic attitudes by students. Since April 2015, assessment of BSS has been included in the third part of the Medical College Admission Test (MCAT) in order to evaluate candidates' understanding of the influence of behavior on wellbeing and health; and to enhance the medical culture as a bio-psychosocial enterprise [19] . Assessments are better to focus on clinical applications of BSS to stimulate students' deep learning [17] . So, to properly assess the knowledge of BSS, Multiple Choice Questions (MCQs) and Short Answer Questions (SAQs) alone are not enough [20] . Newer assessment methods need to be investigated to evaluate students' deep learning and applied knowledge. At Lancaster Medical School in the United Kingdom, students' applied knowledge of BSS is being evaluated by Scenario-based assessment, which has been aligned with PBL curriculum [20] . As it is inferred from international scholars' experiences and overviews, it is recommended to start BSS training from first years of studying at a medical school (preclinical years) [10, 11, 19] . Longitudinal integration of BSS into the medical curriculum and interweaving BSS to a "case-based, clinically oriented medical curriculum" is recommended as well [12, 18] . To provide COVID-19 behavior change theories-oriented specific training, Massive Open Online Courses (MOOCs) can be designed [21] and offered to medical students, as electives, on every available online learning management system. Courses can be flipped and students' online individual learning activities can be joined with online virtual classes. MD-PhD teachers can facilitate students' case-based discussions and supervise students' online activities and provide them with constructive feedback. The engaged students can be provided with the feedback from their fellow learners as well. According to the IOM call, BSS integration should address six different domains including social and cultural issues in health care, patient-physician interaction and patient behavior [1] . Students should be taught about communicating with specific vulnerable populations such as people with low health literacy. Medical students should be instructed to avoid using medical terms to interview patients with low health literacy. Instead, they should be able to use some plain alternatives. They should focus on only a few main key messages. They should speak slowly. Using a teach-back method, they should ask their patients to repeat their instructions back to them in their own words. They can also develop some written materials, with an easy-to-understand and picture-based format [22] and send them to their patients via social networks. In case of a language barrier, employing interpreter services, which are provided by tele -communicators, recruiting multilingual students and non-verbal communication [23] will implicitly improve the physician-patient relationships and quality of care. For medical students to be able to have a comprehensive interview with patients, they need to have previously been taught behavior change theories. Because, teaching the principles of interviewing patients without having a basic knowledge of behavior change theories is similar to teaching gene therapy to learners who have not previously learned the basics of genetics [16] . In order to involve students in behavior change counseling programs and to employ them in delivering virtual consultations, it is necessary to deepen their knowledge of the factors which predict patients' social behavior and train them how to obtain patients' social behavior history. In this regard, serious curricular development in terms of teaching behavior change theories to medical students is an imperative requirement. Expanding medical trainees' knowledge about theories of behavior change and empowering them to make good use of theories in predicting patients' non-compliant behaviors will increase the effectiveness of behavior change consultations [24] . Medical students, who have previously been taught behavior change theories, will be able to have a comprehensive interview with COVID patients. For instance, when an individual does not perceive him/herself susceptible to the COVID-19 and its harmful consequences, a well-trained, knowledgeable student will scrutinize his/ her preventive behaviors by specific questions based on the various constructs of behavior change theories, including HBM. By doing so, he/she will realize the low perceived susceptibility of that person about COVID and will consequently provide tailored and specific education for that person to adopt preventive health behaviors. Indeed, identifying the determinants of each individual's behavior to adopt preventive behaviors requires a great deal of time and thorough understanding of behavior change theories. During the pandemics such as that of COVID 19, when teaching physicians are at the forefront of disease control, medical students in their preclinical years have to spend most of their time at home. These times are great opportunities to ask medical students to make online calls with patients and their families in order to sharpen their consultation skills by managing comprehensive interviews with patients. That is why it is believed that during the COVID-19 pandemic, behavior change consultations by students who have mastered the theories of behavior change can decrease the rate of non-compliance with COVID-19 social distancing and save the lives of many people, including health care providers. By so doing, the burden of diagnosis and treatment in the health care systems will be decreased. "In the U.S., where the crisis is expected to cost nearly 3% of its GDP" [25] , the economic rippled effects of the COVID-19, which have profoundly affected governmental expenditures for health purposes, will be neutralized. There are many other theories and models, developed for health-related behaviors, which students need to be familiar with in order to explain and predict people's health behaviors. According to the systematic review on behavior change counseling [5] , the Trans Theoretical Model (TTM) is the most common model of behavior change, which is taught in behavior change counseling programs in medical schools. The TTM, like many other behavior change theories and models, has its own limitations and cannot explain all health behaviors alone. The Health Belief Model (HBM) [26] , the Theory of Planned Behavior (TPB) [27] and Social Cognitive Theory (SCT) [28] are those that address changes in behavior at the individual level. Explaining and analyzing the high rate of noncompliance with social distancing in the time of the COVID-19 can help medical students learn the constructs of the HBM and plan for disease prevention in society. Studying the constructs of TPB, medical students learn that intention is the most important predictor of people's behavior. They also learn that in order to change people's intention about a behavior, such as social distancing, they should consult people to change their attitude toward that behavior, their subjective norms and their perceived behavioral control [26] . Mastering SCT, medical students will be able to analyze personal, behavioral and environmental determinants of people's thoughts and actions. In this regard, educational researchers can employ SCT to analyze the role of social media in "informing, enabling, motivating, and guiding" people to comply social distancing recommendations in the time of COVID-19 [27] . Using various constructs of behavior change theories or models, medical students can analyze the high rate of non-compliance with recommendations during the COVID-19 pandemic. Some hypotheses, which may explain the high rate of non-compliance with recommendations during the COVID-19 pandemic, are presented in Table 2 . These hypotheses have been proposed merely to emphasize the importance of integrating behavioral and social sciences into the medical curriculum and to teach theories and models of behavior change to students. These are some examples based on some constructs of the HBM, TPB and SCT. Undoubtedly, the list of these hypotheses can be further elaborated based on the other constructs of HBM, TPB and SCT, or the constructs of the other theories and models that were not mentioned in this paper. Before any planning to highlight the patients' social behavior history in medical education, reviewing the barriers to the integration of the social sciences into the medical curriculum can be helpful. In a survey, educators from all medical schools in the UK reported on the barriers they experienced in teaching the social sciences. They reported that there is often a lack of qualified staff to teach and translate BSS to medics. Many senior educators are not interested and committed to integrate BSS in medical curricula. They do not encourage the essential culture modification. Competent social educators are not given priority in faculty recruitments. Medical training is predominantly compatible with a biomedical mindset. Medical curricula and timetable are usually crowded and there is not enough space for formal integration of BSS. Assessments do not fit with students' performance and do not drive their true learning [29] . Barriers to integration of BSS in the undergraduate medical curriculum are explored in a systematic review in 2016 as well [30] . It is interesting that all these barriers have remained similar during the decades. Although strategies to overcome these barriers, which are recommended in this review and other studies, have focused on BSS domains other than patient behavior, reviewing those strategies will help in planning for any curricular innovation in terms of teaching behavior change theories in undergraduate medical curriculum and examining patients' social behavior. BSS can be effectively integrated in undergraduate medical curricula if BSS in undergraduate medical curriculum moves from "nice to know" to "need to know" [29, 31] , and the approach of medical schools in teaching BSS changes from a luxury and trial and error state to formal and continuous training [31] . By so doing, integration should be considered as a key to teach BSS within an already crowded curriculum [29] and theoretical frameworks should be provided for "meaningful integration of extended knowledge domains into curriculum" [32] . Basic science teaching should be extended and supported with BSS too [32] . To integrate BSS in undergraduate medical curricula, teaching BSS should not be limited to the first half of studying at medical schools. It should be integrated throughout the years of study at medical schools. Supplements should also be offered later in clinical years [8, 30, 31] . BSS should be consistently included in both clinical and basic courses [30] . Moreover, adequately experienced and qualified BSS teachers should be employed and involved in teaching BSS at medical schools [29, 31] , and BSS-specialized departments should be established within medical schools. In this way, cooperation between BSS specialists and medical doctors will reduce disagreement between medical doctors and BSS professionals [25] . To deliver effective BSS integration and training, medical teachers should be sensitized with proper knowledge and competencies to teach BSS. In addition, faculty development training courses for clinician teachers are recommended so that teachers employ BSS in their daily training [31] . It is also recommended that BSS courses are supervised by BSS specialists [30] . Furthermore, BSS training should be dynamically delivered by mutual collaboration of clinical, biomedical science and behavioral science departments [33] . Training MD-PhDs, with combined knowledge and experience in both medicine and health education, can significantly increase social accountability of medical schools in this regard [34] . BSS integration and training will be done effectively if attending physicians, residents and senior students are role models for employing BSS in clinical settings [8] , and if a complete list of learning objectives are decided in order to acquire clinical skills related to BSS [30, 31] . It is also recommended that innovative methods to teach BSS and stimulate students' learning should be developed [30, 31] so that students can transfer the new content in learning and transfer the learnings out to the new ones [32] . Based on the existing literature, delivering BSS by just "discrete lectures" or "stand-alone additional sessions" should be avoided [32] . Educational materials and modules should be developed and made available to all students so that students can use them in a completely flexible manner as needed [31] and the content of BSS should be reflected in student assessment and certifying examinations [30, 31] . In order to accelerate the progression of integration of BSS into medical curricula, working parties are better to be formed from different school, university or country levels [31] . Of course, prior to any planning to improve integration of BSS in the medical curriculum, hidden curricula at medical schools should be scrutinized in order to explore "institutional policies, evaluation activities, resource-allocation decisions and institutional slang" [29, 31] . BSS content should also be translated to practical points so that the content can be tied to in learners and teachers daily practice [8, 29, 30] . When integrating BSS in curricula, it is important to note that the culture change at medical schools is better to be promoted by senior teachers [29] . Students' early exposure to community and clinical settings best planned so that they realize and appreciate the applicability of BSS in biomedicine [8, 29] . During such early exposures, medical students should be provided with opportunities to gain rewarding insight to patients' social behavior so that their thinking can be strengthened in early years of studying at medical school [33] . To increase the impact of integration, it is important to note that formal and regular feedback from students who have been trained with reformed curricula should be taken [31] , and incentive policies, such as rewards/awards for effective projects or strategies of integrating BSS into medical curricula should be applied [30] . In this regard, sufficient fiscal support should be available to move BSS training into "the mainstream of medical education" [30] . In order to enhance students' motivation to learn and practice BSS, competitive environments should be developed [30] , and teaching BSS content should be prioritized in terms of students and social needs [30] . Educators and researchers can design and implement interventions based on teaching behavioral change theories or models to medical students and evaluate the effectiveness of those interventions in changing medical students' practice, their patients' health behavior and health outcomes in society. The impacts of such educational interventions on increasing people's compliance with behavioral changes such as social distancing recommendations can be evaluated as well. 
paper_id= fffe3dd5faa2c670cea81cc619740e608894550b	title= Conditionality Analysis of the Radial Basis Function Matrix	authors= Martinčervenka  ;B  ;Václav  Skala;	abstract= The global Radial Basis Functions (RBFs) may lead to illconditioned system of linear equations. This contribution analyzes conditionality of the Gauss and the Thin Plate Spline (TPS) functions. Experiments made proved dependency between the shape parameter and number of RBF center points where the matrix is ill-conditioned. The dependency can be further described as an analytical function. 	body_text= Interpolation and approximation of scattered data is a common problem in many engineering and research areas, e.g. Oliver et al. [1] use interpolation (kriging) method on geographical data, Kaymaz [2] finds usage of this technique in structural reliability problem. Sakata et al. [3] model wing structure with an approximation method, Joseph et al. [4] even create metamodels. The RBF methods are also used in the solution of partial differential equations (PDE) especially in connection with engineering problems. To solve interpolation and approximation problems, we use two main approaches: -Tesselated approach -it requires tesselation of the data domain (e.g. Delaunay triangulation) to generate associations between pairs of points in the tesselated cloud of points. Some algorithms were developed (Lee et al. [5] show two of them, Smolik et al. [6] show a fast parallel algorithm for triangulation of large datasets, Zeu et al. [7] recently use tesselation for seismic data etc.) for triangulation and tesselation. Even though it seems simple, tesselation is a slow process in general 1 . -Meshless approach -a method based on RBFs can be used, which does not require any from of tesselation. Hardy [8] shown that the complexity of this approach is nearly independent to the problem dimensionality, therefore it is a better alternative to tesselation in higher dimensions. On the other hand, RBF methods require solving a system of linear equations which leads to some problems as well. There are several meshless approaches e.g. Fasshauer [9] implements some of the meshless algorithms in MATLAB, Franke [10] compares some interpolation methods of the scattered data. Conditionality of the matrix of a linear system of equation is a key element to determine whether the system is well solvable or not. RBF research was recently targeted: -to find out RBF applicability for large geosciences data, see Majdisova [11] , -to interpolate and approximate vector data, see Smolik [12] , -to study robustness of the RBF data for large datasets, see Skala [13, 14] . -to find out optimal variable shape parameters, see Skala [15] . This research is aimed to find optimal (or at least suboptimal) shape parameters of the RBF interpolation. This contribution describes briefly analysis of some of the most commonly used RBFs and determines its problematic shape parameters, causing ill-conditionality of the equation system matrix. The basic idea behind the RBF approach is the partial unity approach, i.e. summing multiple weighted radial basis functions together to obtain complex interpolating function. The Fig. 1 presents two RBFs (marked by red color) forming an interpolating final function (blue one). The RBF approach was introduced by Hardy [8] and modified in [16] . Since then, this method has been further developed and modified. Majdisova et al. [17] and Cervenka et al. [18] proposed multiple placement methods. There are also some behavioural studies of the shape parameters, e.g. searching the optimal ones from Wang et al. [19] , Afiatdoust et al. [20] or using different local shape parameters from Cohen et al. [21] , Sarra et al. [22] , Skala et al. [15] . This contribution analyzes the worst cases of the RBF matrix conditionality in order to avoid bad shape parameters, therefore the bad shape parameters can be avoided. The RBF interpolation is defined by Eq. 1, where h (x i ) is the resulting interpolant, N is the number of RBFs, λ i is a weight of the i-th RBF, ϕ is the selected RBF and r ij is a distance between points x i and x j . The points x j are all the points on the sampled original function, where the function value is known. The RBF approximation is slightly different, see Eq. 2. The notation is the same as above, however, x j are replaced by reference points ξ j , j = 1, . . . , M. Some arbitrary (sufficiently small M N ) number of points from the data domain are taken instead. More details can be found in Skala [23] . In both cases, i.e. approximation and interpolation, the equations can be expressed in a matrix form as: In the interpolation case, the matrix A is a square matrix, while in the approximation case, the matrix A is rectangular and the result is an overdetermined system of linear equations. In this case, we do not obtain exact values for the already calculated reference points ξ j . There are many RBFs and still new ones are being proposed e.g. Menandro [24] . In general, we can divide the RBFs into two main groups, "global" and "local" ones, see Fig. 3 and Fig. 2 . -Global RBFs influence the interpolated values globally. The matrix A will be dense and rather ill-conditioned. Typical examples of the global RBF are the Gaussian, the TPS or the inverse multiquadric RBFs. -Local RBFs have limited influence to a limited space near its centre point (hypersphere, in general). The advantage of the local RBFs is that they lead to a sparse matrix A. RBFs belonging to this group are called "Compactly Supported" RBFs (CS-RBFs, in short). Global RBFs are functions, which influence is not limited and its value may be nonzero for each value in its domain. The well-known ones are the Gaussian or the TPS functions. However, there are other functions, see e.g. Table 1 or Lin et al. [25] . Mentioned functions are illustrated in Fig. 2 .  The CS-RBF or compactly supported radial basis function is a function limited to a given interval. Some of CS-RBFs are presented on Fig. 3 . Generally, these functions are limited to an interval (usually r ∈ 0, 1 ) otherwise the value equals zero. These functions are defined by Eq. 4, where P (r) is a polynomial function, r is the distance of two points and q is a parameter.  It should be noted that some new CS-RBFs have been recently defined by Menandro [24] . Assuming a linear system of equations Ax = b, the condition number of the matrix A describes how the result (vector b) can change when the input vector x is slightly modified. This number describes sensitivity to changes in the input vector. We aim for the lowest possible sensitivity, in order to get reasonable results. In terms of linear algebra, we can define conditionality of a normal matrix A using eigenvalues λ i ∈ C 1 as: where κ (A) is the condition number of the normal matrix A, |λ max (A)| is the highest absolute eigenvalue of the matrix A and |λ min (A)| is the lowest absolute eigenvalue of the matrix. The higher the value κ (A) is, the more sensitive the matrix A is, meaning that κ (A) = 1 is the best option, forcing all eigenvalues λ to have the same value. It is worth noting that the conditionality is closely related to the matrix determinant. In the case when the determinant is zero, we have at least one eigenvalue equaling zero, so the conditionality will be infinite, see Eq. 6. This is only a brief introduction to the matrix conditionality. Details can be found in e.g. Ikramov [27] or Skala [14] , some experimental results can be found in Skala [28] . In the RBF approximation problem, we normally have two main issues to deal with -selecting number of RBFs and its global shape parameter. To obtain a robust solution, the matrix A of the linear system of equations should not be illconditioned. We did some experiments to show how the condition number of the matrix A depends on the number of RBFs (N ) used and a shape parameter (α or β, see below). To make things easier, all RBFs have been distributed uniformly on x ∈ 0, 1 interval and have the same constant shape parameter. The Gaussian RBF is defined by Eq. 7. It is the unnormalized probability density function of a Gaussian distribution centred at zero and with a variance of 1 2α . Variable r denotes the distance from its centre points and α is the shape parameter. ϕ (r, α) = e −αr 2 (7) Figure 4 presents dependence of matrix conditionality on Gaussian RBF shape parameter α and number of uniformly distributed RBF reference points. A hyperbolic function (Eq. 8) was used to fit extremal points of each curve ( Table 2 ).  The plot at Fig. 5 describes the situation. These curves describe number of RBFs N and shape parameter α when the matrix is ill-conditioned.  The Thin Plate Spline (TPS) radial basis function is defined by the Eq. 9. The TPS was introduced by Duchon [29] and used for RBF approximation afterwards. Variable r is the same as in the Gaussian RBF -the distance from its centre point and parameter β is the shape parameter. The Fig. 6 presents a result for a simulated experiment to the recent Gaussian RBF case using the TPS function instead. There is only one curve which has a hyperbolic shape similar to the Gaussian RBF case. The Fig. 7 also represents the curve, when the matrix A is close to singular. The Table 3 presents dependency of the β exp shape parameter for different N as an function when the matrix A is significantly ill-conditioned. We obtained a hyperbolic function from the graph on Fig. 7 (coefficients are rounded to 2 decimal places). The Table 3 presents the shape parameters β calc evaluated for small numbers of RBF functions according to Eq. 10. The experimental results presented above led to a question, how the results are related from the analytical side. This led to the validation of experiments with two analytical results described in this section. Let us calculate values of the TPS shape parameter β for N = 3 and N = 4 in a way that the matrix A will be ill-conditioned (κ (A) = +∞). It should be noted that the multiplicative constant 1 2 is ommited in the Eq. 11 as it has no influence to the conditionality evaluation. In the first case, i.e. N = 3, the RBF matrix A has the form (using equidistant distribution of RBF center points): Let us explore singularity of the matrix A 3 , when det (A 3 ) = 0, the determinant will have the form: log βr 2 4 log β4r 2 log βr 2 0 log βr 2 4 log β4r 2 log βr 2 0 = 0 As r = 0 for all pairs of different points, lim r→0 r 2 log r 2 = 0 and equidistant point distribution. For the sake of simplicity, we substitute q = log βr 2 , a = log 4 and use formula log (ab) = log a + log b so we get: In the experiments, we used interval x ∈ 0, 1 and with three points (0, 0.5, 1). The distance between two consecutive points r is 0.5, which led to β = 1. This exact value we obtained from experiments as well (see Table 3 ). In the second case, i.e. N = 4, a similar approach has been taken. In this case the matrix A 4 is defined as: r 2 log βr 2 (2r) 2 log β4r 2 (3r) 2 log β9r 2 r 2 log βr 2 0 r 2 log βr 2 (2r) 2 log β4r 2 (2r) 2 log β4r 2 r 2 log βr 2 0 r 2 log βr 2 (3r) 2 log β9r 2 (2r) 2 log β4r 2 r 2 log βr 2 0 Similarly as in the case for N = 3, we can write the det (A 4 ) and declare the matrix singular if: log βr 2 4 log β4r 2 9 log β9r 2 log βr 2 0 log βr 2 4 log β4r 2 4 log β4r 2 log βr 2 0 log βr 2 9 log β9r 2 4 log β4r 2 log βr 2 0 = 0 Using the substitutions q = log βr 2 , a = log 4 and b = log 9, we obtain: This can be further expressed as: (4 (q + a)) 4 + q 4 + q 2 (9 (q + b)) 2 = −2q 3 (9 (q + b)) − 2q (4 (q + a)) 2 (9 (q + b)) − 2q 2 (4 (q + a)) This leads to the cubic equation: As we have four points distributed uniformly on the interval x ∈ 0, 1 , the distance between two adjacent nodes is r = 1 3 . Now, using the real root of the Eq. 19, i.e. q = −2.2784, we can estimate the shape parameter β as follows: It should be noted, that if irregular point distribution is used, i.e. using Halton points distributions, the ill-conditionality get slightly worse. In this paper, we discussed some properties of the two well-known RBFs. We find out that there are some regularities in the shape parameters, where the RBF matrix is ill-conditioned. Our experiments proved that there are no global optimal shape parameters from the RBF matrix conditionality point of view. In the future, the RBF conditionality problem is to be explored for higher dimension, especially for d = 2, d = 3 and in the context of partial differential equations. 
paper_id= fffed7a598720ae39065dc5f084375a1202daa03	title= Journal Pre-proof Multiple sclerosis, neuromyelitis optica spectrum disorder and COVID-19: A pandemic year in the Czech Republic Multiple sclerosis, neuromyelitis optica spectrum disorder and COVID-19: A pandemic year in the Czech Republic	authors= Dominika  Stastna;Ingrid  Menkyova;Jiri  Drahota;Aneta  Mazouchova;Jana  Adamkova;Radek  Ampapa;Marketa  Grunermelova;Marek  Peterka;Eva  Recmanova;Petra  Rockova;Matous  Rous;Ivana  Stetkarova;Martin  Valis;Marta  Vachova;Ivana  Woznicova;Dana  Horakova;  Affiliations;Kralovske  Vinohrady;Hradec  Hospital;Hradec  Kralove;  Kralove;  Czechia;Assoc  Dana Horakova;	abstract= Publication type: Original research article Word count: 3243; abstract count: 371 Number of figures/tables: 6 Number of references: 24 Highlights • Majority of MS patients had a mild COVID-19 course • The COVID-19 incidence in MS patients seems to be similar to the general population • Higher age and BMI were associated with worse COVID-19 course • Anti-CD20 therapy and high-dose glucocorticoids were associated with pneumonia • NMOSD patients seem to be at higher risk of pneumonia and death because of COVID-19 Abstract When the novel coronavirus disease 2019 (COVID-19) appeared, concerns about its course in patients with multiple sclerosis (MS) and neuromyelitis optica spectrum disorder (NMOSD) arose. This study aimed to evaluate the incidence, severity and risk factors of the more severe COVID-19 course among MS and NMOSD patients. From March 1, 2020, to February 28, 2021, 12 MS centres, representing 70% of the Czech MS and NMOSD population, reported laboratory-confirmed COVID-19 cases via the Czech nationwide register of MS and NMOSD patients (ReMuS). The main outcome was COVID-19 severity assessed on an 8-point scale with a cut-off at 4 (radiologically confirmed pneumonia) according to the World Health Organisation´s (WHO) COVID-19 severity assessment. We identified 958 MS and 13 NMOSD patients, 50 MS and 4 NMOSD patients had pneumonia, 3 MS and 2 NMOSD patients died. The incidence of COVID-19 among patients with MS seems to be similar to the general Czech population. A multivariate logistic regression determined that higher body mass index (BMI [OR 1.07, 95% CI, 1.00-1.14]), older age (OR per 10 years 2.01, 95% CI, 1.41-2.91), high-dose glucocorticoid treatment during the 2 months before COVID-19 onset (OR 2.83, 95% CI, 0.10-7.48) and anti-CD20 therapy (OR 7.04, 95% CI, were independent variables associated with pneumonia in MS patients. Increase odds of pneumonia in anti-CD20 treated MS patients compared to patients with other diseasemodifying therapy (same age, sex, BMI, high-dose glucocorticoid treatment during the 2 months before COVID-19 onset, presence of pulmonary comorbidity) were confirmed by propensity score matching (OR 8.90, 95% CI, 3.04-33.24). Reports on COVID-19 infection in patients with NMOSD are scarce, however, data available up to now suggest a high risk of a more severe COVID-19 course as well as a higher mortality rate among NMOSD patients. In our cohort, 4 NMOSD patients (30.77%) had the more severe COVID-19 course and 2 patients (15.39%) died. The majority of MS patients had a mild COVID-19 course contrary to NMOSD patients, however, higher BMI and age, anti-CD20 therapy and high-dose glucocorticoid treatment during the 2 months before COVID-19 onset were associated with pneumonia. Based on this study, we have already started an early administration of anti-SARS-CoV-2 monoclonal antibodies and preferential vaccination in the risk group of patients. 	body_text= When COVID-19 appeared in Wuhan in December 2019 and started to spread rapidly all around the world, concern about its course in patients with autoimmune diseases including MS and neuromyelitis optica spectrum disorder (NMOSD) arose. The effect of immunomodulatory drugs and other possible risk factors on the course of this new enemy had to be evaluated as soon as possible. The results would influence clinical decisions that would have a long-term impact due to the chronicity of the diseases and the long-lasting effects of some treatments. Already reported data indicates that the majority of MS patients have a mild course. Risk factors associated with worse clinical severity seem to be similar to that of the general population Mares and Hartung, 2020; Möhn et al., 2020; Salter et al., 2021; Sharifian-Dorche et al., 2021; Sormani et al., 2021) . However, the effect of immunomodulatory therapies on the course of COVID-19 has not been satisfactorily elucidated yet. As for NMOSD, reports on COVID-19 infection are scarce, but the available data (Alonso et al., 2021; Ciampi et al., 2020; Creed et al., 2020; Fan et al., 2020; Louapre et al., 2020b; Sahraian et al., 2020; Zeidan et al., 2020a) indicates a high proportion of hospitalized patients and a high mortality rate among these patients. The aim of this study was to evaluate the incidence, severity and risk factors of the more severe course of COVID-19 among MS and NMOSD patients when the Czech Republic is one of the countries with the highest number of COVID-19 infected people per capita. The large homogenous cohort based on the Czech nationwide registry of MS and NMOSD patients (ReMuS) may help to clarify COVID-19 characteristics among MS and NMOSD patients and help with clinical decisions about prevention, treatment and vaccination. This was a multicenter, retrospective, observational cohort study of patients with MS or NMOSD with laboratory-confirmed infection by severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2). The data were collected via registry ReMuS from March 1, 2020 , to February 28, 2021 ReMuS was established by the Endowment Fund IMPULS ("NF IMPULS," n.d.). The guarantor of expertise of this registry is the Section for Neuroimmunology and Liquorology of the Czech Neurological Society. The estimated number of MS patients in the Czech Republic is 20,000, ReMuS represents all 15 Czech MS centres and approximately 80% of Czech MS patients. The ReMuS is based on informed consent, thus it is possible to use retrospective data for scientific and research purposes without requiring new approvals. All patients signed an informed consent form for inclusion in ReMuS. Inclusion criteria for analysis were (1) MS or NMOSD, (2) a COVID-19 diagnosis based on a positive result of a SARS-CoV-2 polymerase chain reaction test (PCR) or positive antigen test or positive serological test and (3) known outcome of acute illness (return to normal activities or end of self-isolation in asymptomatic cases; or death). Data were collected by healthcare professionals retrospectively from the first contact until an outcome, taking as baseline the day of symptoms appearance or positive laboratory test (if earlier). The collected data set was based on the Global Data Sharing initiative COVID-19 core data set (Peeters et al., 2020) and the experience of MS clinicians and epidemiologists. Data on demographics, MS, immunomodulatory treatment, COVID-19 course, comorbidities and basic laboratory parameters were obtained (Table 1, Table 5 ). The primary endpoint was the participant´s clinical status at the most severe point of their COVID-19 course (on an 8-point ordinal scale), referred to as the COVID-19 severity score, where 1 indicated the asymptomatic patient; 2 the symptomatic patient; 3 patient with suspected pneumonia defined by both dry cough and shortness of breath; 4 patient with radiologically confirmed pneumonia (chest X-ray/CT scan); 5 need of supplemental oxygen; 6 need of non-invasive ventilation or high flow oxygen therapy (HFOT); 7 need of invasive ventilation or extracorporeal membrane oxygenation (ECMO); and 8 death. The whole analysis was applied to MS patients. Cohort characteristics were summarized using mean (SD) for continuous variables and frequencies (%) for categorical variables. Data were compared between patients without radiologically confirmed pneumonia  corresponding to COVID-19 severity scores 1-3) and patients with radiologically confirmed pneumonia, resp. COVID-19 severity scores of 4 or more (more severe COVID-19). Our "more severe COVID-19 course" corresponded to a "moderate", "severe" and "very severe COVID-19 course" according to the World Health Organisation (WHO) definition (World Health Organisation, 2020). We did not use any data imputations. Comparison between groups was made using t-test and χ2 as appropriate. Two-sided P values were considered statistically significant at .05. Univariate logistic regression models were performed on relevant identified variables to assess their association with COVID-19 outcome (mild vs more severe COVID-19). Because of the prior comparison between mild and more severe COVID-19 groups as well as prior reports of an association between B-cell depleting antibodies with an increased risk of worse COVID-19 courses (Sormani et al., 2021) , we categorized disease-modifying treatment (DMT) into 3 groups: rituximab and ocrelizumab (anti-CD20), no current DMT and other DMT (reference). Subsequently, a multivariate logistic regression model was performed to determine which variables were independently associated with more severe COVID-19. Variable selection was based on univariate logistic regression. Associations were reported using odds ratios (ORs) and 95% CIs. Additional analyses were focused on the anti-CD20. We performed 1:2 propensity score matching. Patients treated with anti-CD20 were matched with patients treated with other DMT by determined variables from a previous multivariate logistic regression model. Association with more severe COVID-19 was reported using OR and 95% CI. We also investigated the effect of the duration from the last anti-CD20 infusion and the effect of the time since therapy started. Sensitivity analyses were run by repeating all analyses using a leave one out procedure, rerunning the analysis excluding one of the largest MS centres (Prague 2, Prague 5, Teplice) at a time. Data analyses were performed in R version 4.0.2. Due to the small sample size of the NMOSD patient cohort, we used just descriptive statistics for them. As of February 28, 2021, 958 MS laboratory-confirmed COVID-19 patients with known infection outcome and 13 NMOSD patients were reported. The characteristics of MS patients are shown in Table 1 . Briefly, almost half of the patients were reported by MS centres in Prague (47.60%), mean age was 43.36 years (SD 10.94), most patient´s Expanded Disability Status Scale (EDSS) was lower than 4 (75.42%) and the mean MS duration was 12.52 years (SD 11.25) . In a majority of patients, COVID-19 infection was confirmed by PCR (89.46%). Approximately one third of patients had one or more comorbidities. Not all data were available for all individuals, but we did not use any data imputations. We could not have classified 34 patients as mild or more severe COVID-19 because of their unknown pneumonia status. The estimated number of MS patients in the Czech Republic is 20,000. At the time of this analysis, ReMuS contained information about 13,471 patients from 12 participating MS centres with at least one visit to an MS centre in the previous 12 months, thus our patient base represented almost 70% of Czech MS patients. As of February 28, 2021, 1004 laboratoryconfirmed COVID-19 patients with MS were reported (ongoing COVID-19 included). That means approximately 7.45% of MS patients had COVID-19. At the same time, there were 1,240,302 laboratory-confirmed COVID-19 cases in the Czech Republic (11.59% of the Czech population) (Komenda M. et al., 2021) ,(Český statistický úřad). The incidence of COVID-19 among patients with MS seems to be similar (or lower) to the general Czech population. The number of patients with particular COVID-19 symptoms is shown in Table 1 . The most common symptom was fatigue. New or worsening neurological symptoms were reported in 122 (13.25%) patients, 26 (21.31%) of them had a consequent relapse treated with glucocorticoids. All symptoms, as well as consequent relapse, were more common among patients with more severe COVID-19, except for sore throat, nasal congestion and loss of smell or taste. The distribution of COVID-19 severity scores according to the different DMT in MS patients is presented in Table 2 . A total of 50 patients (5.41%) had a more severe COVID-19. Supplemental oxygen was necessary for 9 patients (0.98%), non-invasive ventilation or HFOT for 12 (1.30%), invasive ventilation for one (0.11%) and 3 patients died (0.33%). Risk factors for more severe COVID-19 in univariate analysis are reported in Table 3 . Variables associated with increased risk of more severe COVID-19 in the univariate logistic regression model that were considered for retention in the multivariate logistic regression model are reported in Table 4 . From the variables with significant mutual correlations, the one with the highest predictive value was always selected for the multivariate analysis. Among selected variables, older age (OR per 10 years 2.01, 95% CI, 1.41-2.91), higher BMI (OR 1.07, 95% CI, 1.00-1.14), anti-CD20 therapy (OR 7.04, 95% CI, 3.10-15.87) and high-dose glucocorticoid treatment during the 2 months before COVID-19 onset (OR 2.83, 95% CI, 0.10-7.48) were shown to be independent variables associated with a more severe COVID-19 course. In 1:2 propensity score matching ( Figure) , patients treated with anti-CD20 compared to the same sex, age, presence of pulmonary comorbidity, BMI and high-dose glucocorticoid treatment during the 2 months before COVID-19 onset, patients treated with other DMT had an almost 9-fold increased odds of a more severe COVID-19 course (OR 8.90, 95% CI 3.04-33.24, p<.00001). Exploratory analyses revealed no association between COVID-19 severity and the time passed since the last anti-CD20 infusion as well as the duration of anti-CD20 treatment. Sensitivity analyses run by repeating all the analyses using a leave one out procedure rerunning the analysis excluding one of the largest MS centres one at a time, showed risk factors that were consistent with the entire cohort.  We present a large homogenous cohort of MS and NMOSD patients with COVID-19 collected via the Czech nationwide registry ReMuS. Our patient base represents almost 70% of Czech MS patients. The data collection was initiated when the first case of COVID-19 was spotted in the Czech Republic on March 1, 2020, the majority of patients was, however, infected from October 2020 to January 2021 (88.67%). The Czech Republic was one of the countries with the highest number of COVID-19 infected people per capita, but at that time the availability of COVID-19 laboratory tests was quite sufficient. Unlike the previous studies Salter et al., 2021; Sormani et al., 2021) we didn´t have to include suspected COVID-19 cases. That´s the probable reason we have a higher proportion of asymptomatic patients. We also didn´t use any data imputations and all values were obtained from healthcare professionals. As opposed to previous publications, our data analysis was based on the WHO definition We also included only laboratory-confirmed cases. In older patients with higher EDSS, there is a lower probability of undergoing a laboratory test due to their limited mobility. Some patients also may have behaved more cautiously and adhered more strictly to public health recommendations due to their chronic illness. Furthermore, the mortality rates are not consistent among publications probably because of the low numbers of deaths. Second, we identified independent risk factors for COVID-19 in multivariate analysis. Similar to data from other MS registries and the general population, we confirmed higher age and BMI as independent risk factors of more severe COVID-19 (Goumenou et al., 2020; Louapre et al., 2020a; Möhn et al., 2020; Salter et al., 2021; Sharifian-Dorche et al., 2021; Sormani et al., 2021) . In agreement with the COViMS (Salter et al., 2021) , Musc-19 (Sormani et al., 2021) and with data in other autoimmune diseases (Gianfrancesco et al., 2020) , we confirmed that high-dose glucocorticoid treatment during the 2 months before COVID-19 onset is associated with a worse disease course. This was not unexpected, as glucocorticoids affect the immune system, reducing responsiveness to infections. However, the 4 largest publications Möhn et al., 2020; Salter et al., 2021; Sormani et al., 2021) are not entirely consistent on the question of the effect of immunomodulatory therapies on the course of COVID-19, especially anti-CD20. The Covisep registry , as well as the German review of 873 patients (Möhn et al., 2020) , reported no association of anti-CD20 therapies with worse COVID-19 outcomes. Its small sample size and inconsistency may have limited the ability to detect associations. On the other hand, the increased frequency of more severe COVID-19 in people treated with anti-CD20 compared to other DMT was observed in Musc-19 (Sormani et al., 2021) as well as in our study. In our cohort, 81 MS patients were treated with antiCD20 and 16 of them contracted pneumonia. We proved that assumption via multivariate logistic regression and propensity score matching. Nevertheless, none of the antiCD20 treated MS patients died. The most common adverse events in patients treated with anti-CD20 therapy are viral respiratory infections. Anti-CD20 therapy binds to CD20 on the surface of B-cells, effectively depleting them, and interferes with antibody development. Therefore, B-cell depletion could potentially compromise antiviral immunity, including the development of SARS-CoV-2 antibodies. (Mehta et al., 2020; Syed, 2018) Serological studies following COVID-19 infection as well as COVID-19 vaccination in patients treated with anti-CD20 therapy will be crucial to determine the characteristics of the immune response to COVID-19 to help with clinical decisions about treatment and vaccination. We did not observe a link between time from last infusion of anti-CD20 or duration of anti-CD20 treatment. This can be due to the small sample size, but it is consistent with the idea that the immunological effect of anti-CD20 treatment may last longer than 6 months. We should also mention the higher proportion of patients with consequent relapse after COVID-19 infection among those with a more severe COVID-19 course in our cohort (Table   1 ). Longitudinal studies are needed to evaluate the long-term consequences of COVID-19, relapse rate included. Information on COVID-19 and pregnancy is still scarce, even more so when it comes to pregnancy in MS patients. Pregnant women do not appear to be at greater risk of COVID-19 infection. While the risk of critical care appears increased relative to the general population, there does not appear to be a significantly higher mortality rate. (Yam et al., 2020) However, data available up to now suggest a high risk of a more severe COVID-19 course as well as a higher mortality rate among NMOSD patients. Limitations of this analysis include low representation of children as well as older patients without DMT, with comorbidities and long MS duration, who are not followed in ReMuS. In older patients with higher EDSS, there is also less probability to undergo a laboratory test because of their limited mobility. Not all patients underwent a CT scan or chest X-ray so more severe cases could be underestimated. Otherwise, there can be an improved outcome in comparison to the general population due to greater adherence to public health recommendations because of MS diagnosis. Finally, although the multivariate analysis and propensity score matching adjusted for the effect of DMT on COVID-19 severity for the main confounding factors, we cannot exclude that some residual confounding can partly explain the observed associations. Overall, this study confirms that a majority of MS patients have a mild COVID-19 course. Anti-CD20 therapy, high-dose glucocorticoid during the 2 months before COVID-19 onset and risk factors known in the general population such as higher BMI and age were associated with more severe COVID-19 course. These findings are in agreement with previous studies as well as knowledge about infections in MS patients. It will be important to look at longitudinal and serological studies to evaluate the long-term consequences of COVID-19 and the characteristics of the immune response in MS patients. Contrarily, it seems that NMOSD patients are at higher risk of worse COVID-19 course, even though the COVID-19 reports are scarce. The results from this analysis are very important for clinical practice. Based on this study, we have already started an early administration of anti-SARS-CoV-2 monoclonal antibodies and preferential vaccination in the group of patients treated by anti-CD20. We continue to collect this data and plan to publish it soon. 
paper_id= ffff2fb21d4880b63a7cf3849cecc45bc596f4f0	title= Comparison of SARS-CoV-2 detection in nasopharyngeal swab and saliva Running title SARS-CoV2 in nasopharyngeal and saliva sample	authors= Sumio  Iwasaki;Shinichi  Fujisawa;Sho  Nakakubo;Keisuke  Kamada;Yu  Yamashita;Tatsuya  Fukumoto;Kaori  Sato;Satoshi  Oguri;Keisuke  Taki;Hajime  Senjo;Kasumi  Hayasaka;Satoshi  Konno;Mutsumi  Nishida;Takanori  Teshima;	abstract= We prospectively compared the efficacy of PCR detection of SARS-CoV-2 between paired nasopharyngeal and saliva samples in nine COVID-19 patients. SARS-CoV-2 was detected in saliva in 8 of 9 (89%) patients and in all 11 samples taken within 2 weeks after disease onset. Viral load was equivalent at earlier time points but declined in saliva than nasopharyngeal samples. PCR negativity was also concordant in all 27 saliva samples from 24 patients between nasopharyngeal and saliva samples. These results suggest that saliva is a reliable noninvasive alternative to nasopharyngeal swabs and facilitate widespread PCR testing in the face of shortages of swabs and protective equipment without posing a risk to healthcare workers. 	body_text= Rapid detection of the novel coronavirus SARS-CoV-2 is critical for the prevention of outbreaks coronavirus disease 2019 in communities and hospitals. The diagnosis of COVID-19 is made by PCR testing of samples collected by nasopharyngeal or oropharyngeal swabs, with the nasopharyngeal route being the standard with a sensitivity for the virus in the range of 52-71% [1] [2] [3] [4] [5] . However, swab sample collection requires specialized medical personnel with protective equipment and poses a risk of viral transmission to healthcare workers. Although sputum specimen is a noninvasive alternative, sputum production is seen in only 28% of COVID-19 patients 6 . The angiotensin converting enzyme 2 (ACE2) is the main host cell receptor for SARS-CoV-2 entry to the human cell 7, 8 . ACE2 is highly expressed on the mucous of oral cavity, particularly in epithelial cells of the tongue 9 . These findings explain mechanisms that the oral cavity is high risk for SARS-CoV-2 infection, transmission occurs through saliva before the onset of symptoms, and the impairment of the sense of taste 10 . Thus, it is reasonable to use saliva as a diagnostic sample, and recent studies have shown that SARS-CoV-2 is detected in saliva [11] [12] [13] . Moreover, Wyllie et al. demonstrated the saliva to be more sensitive for SARS-CoV-2 detection patients than nasopharyngeal swabs 14 . However, few studies compared viral load between nasopharyngeal and saliva samples. We herein compared the diagnostic value of saliva and nasopharyngeal samples using prospectively collected paired samples. Nasopharyngeal swab samples and saliva samples were simultaneously collected from patients suspicious of COVID-19 and from patients who were referred to our hospital with the diagnosis of COVID-19. This study was approved by the Institutional Ethics Board and informed consent was obtained from all patients. All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint this version posted May 19, 2020. . https://doi.org/10.1101/2020.05.13.20100206 doi: medRxiv preprint Nasopharyngeal samples were obtained by using FLOQSwabs (COPAN, Murrieta, CA, USA). The swab was passed through the nostril until reaching the posterior nasopharynx and slowly removed while rotating. The swabs were placed in the saline. Saliva samples were self-collected by the patients except one patient, in whom saliva was collected by swab due to inability of self-collection, and spit into a sterile PP Screw cup 50 (ASIAKIZAI Co., Tokyo, Japan). 200 L Saliva was added to 600 L PBS, mixed vigorously, then centrifuged at 20,000 x g for 5 minutes at 4 o C, and 140 µl of the supernatant was used as a sample.  We used the paired t-test to compare data. All P-values were 2-sided. We used Pearson's correlation to assess the relation between time from symptom onset and viral load and CT value. We performed analyses using Prism software version 6 (GraphPad, La Jolla, CA). P-value of 0.05 was used as the cutoff for statistical significance. Thirty-three patients were enrolled in this study, including 9 patients with COVID-19 and 24 COVID-19 suspicious patients. Most of COVID-19 patients had mild to moderate disease. Median age of COVID-19 patients was 70.5 years-old, ranging from 30 to 97 years-old. COVID-19 patients were admitted to our hospital after a diagnosis was made by nasopharyngeal samples. In COVID-19 patients, median day of sampling was 10 days (range, 7-19 days) after symptom onset. SARS-CoV-2 was detected in all 9 patients in nasopharyngeal samples and in 8/9 (89%) patients in saliva samples (Table 1 ). In one patient who showed saliva negativity, samples were taken 19 days after symptom onset. The mean ± SD of the viral load were 6.1 ± 1.3 and 4.2 ± 1.5 log10 gene copies/ml in nasopharyngeal and saliva samples, respectively, and significantly lower in saliva samples All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint this version posted May 19, 2020. . https://doi.org/10.1101/2020.05.13.20100206 doi: medRxiv preprint (P=0.018). When looking at relation of viral load and duration from symptom onset to sampling, viral load was equivalent between the two samples at earlier time points but declined in saliva at later time points ( Figure 1A) . In a real-time PCR assay, cycle threshold (CT) value is defined as the number of cycles required for the fluorescent signal to cross the threshold. The mean ± SD of the CT values were 24.2 ± 4.4 and 30.4 ± 4.9 in nasopharyngeal and saliva samples, respectively, and significantly higher in saliva samples (P=0.018). The CT values were equivalent between the two samples at earlier time points but higher in saliva at later time points ( Figure 1B ). All patients were treated with favipiravir 15 and underwent PCR examination when symptoms were relieved to determine the timing of discharge. Figure 2  We prospectively compared SARS-CoV-2 detection between nasopharyngeal samples and saliva samples in 9 patients with COVID-19. The virus was detected in saliva in 8/9 (89%) patients. Detection rate was consistent to other studies: 11/12 (92%) patients and 20/23 (87%) patients in Hong Kong 11, 13 , 25/25 (100%) in Italy 12 , and 36/38 (95%) in New Haven 14 . In a screening clinic in Australia, 39/622 (6.3%) patients had PCR positive nasopharyngeal swabs and 33/39 (85%) had virus in saliva 16 . Taken together, these results consistently support the use of saliva as an effective alternative to nasopharyngeal swabs for diagnosis and screening of COVID-19. It has been shown that salivary viral load peaks at onset of symptoms and is highest during the first week and subsequently declines with time 11, 13, 14 . Our results were consistent to these data; the virus was detected in all the saliva samples taken within 2 weeks after symptom onset. PCR cannot distinguish whether the virus is alive or dead. Recently, Korean researchers suggested that particle of the dead virus could persist in the nasopharynx and resulted in "false positivity" (http://www.koreaherald.com/view.php?ud=20200429000724). Interestingly, in our results, PCR results tended to become negative much quicker in saliva All rights reserved. No reuse allowed without permission. (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint this version posted May 19, 2020. . https://doi.org/10.1101/2020.05.13.20100206 doi: medRxiv preprint than in the nasopharynx, suggesting that dead virus particle in mouth is more efficiently cleaned by saliva. Saliva could be a better tool to determine virus clearance in COVID-19 patients. To our knowledge, two studies compared viral load between nasopharyngeal and saliva samples. The viral load was higher in saliva than in nasopharyngeal samples in one study 14 , whereas sensitivity determined by CT values was lower in saliva in one study 16 . Our results showed that the viral load was equivalent at earlier time points but lower in saliva than in nasopharyngeal samples at later points. Although it is difficult to compare viral concentration in saliva itself and in transport media of the swab samples, there are several possibilities to explain the inconsistency. First, salivary viral load appears to be highest during the first week after symptom onset and subsequently declines with time 11, 13, 14 . Our study did not include samples taken during the first week after onset. Second, differences in sampling methods may have contributed to the discrepant results. Specifically, the 1ml volume of our saliva samples in our study were much smaller in our study with compared to a third full cup of saliva collected in the Yale study. There was inconsistency of nasopharyngeal swab samples in the Yale study 14 , while our data showed less inconsistency, probably due to swab sampling performed exclusively by only three pulmonologists. Last, all patients in our study were treated with favipiravir 15 , which may be associated with rapid virus clearance from the oral cavity. Although our study has several limitations due to the small number of samples and the lack of samples within the first week of symptom onset, there have been few prospective studies to date comparing the two samples. Given the large benefits of saliva collection that does not require health worker specialists and protective equipment, our results together with recent studies support the use of saliva as a noninvasive alternative to nasopharyngeal swabs to greatly facilitate widespread PCR testing. Data are shown as mean ± SD (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint this version posted May 19, 2020. . https://doi.org/10.1101/2020.05.13.20100206 doi: medRxiv preprint (which was not certified by peer review) is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this preprint this version posted May 19, 2020. . https://doi.org/10.1101/2020.05.13.20100206 doi: medRxiv preprint 
paper_id= ffff4330337614d8a69bf518f7b60652b37b14cf	title= Preexisting Conditions That Kill Us	authors= Mona  Hanna-Attisha;Erik D Olson;	abstract= To protect human life, science and public health need to guide public policy. We call for an end to the anti-science, anti-prevention, and anti-regulatory policies that have resulted in countless preexisting conditions and deaths. Reactive responses are not a substitute for primary prevention; we must invest in environmental and public health protections. 	body_text= I T STARTED innocently enough-with a lowgrade fever and whole-body aches. It progressed to a deep cough and shortness of breath. A week later, no amount of oxygen, or support on the ventilator, was enough to save her life. Like thousands in this country, she died alone. Official cause of death: acute respiratory distress syndrome. For others who do not survive COVID-19, the cause may be renal failure, cytokine storm, multiorgan collapse, or Kawasaki disease. But is that really the cause? Laying blame on the Trump administration is easy. The president tragically failed to address the pandemic's threat. In another context, this would be a criminal act of manslaughter. 1 But even if he could be convicted, the bungled federal response to our modern plague is not the work of one individual. It was caused by preexisting conditions, by the decades-long assault on science and public health. As a frontline physician and a public health lawyer, we both have witnessed the chronic neglect that allowed our country to rise to the top of the global list in infections and death rate. Where do we start? From the emblematic injustice of the Flint water crisis 2 to the incessant attacks on environmental health protections, insidious and largely invisible assaults on science and public health have stolen the potential of generations of children and already filled thousands of body bags. Well-financed lobbying campaigns, from the sugar to asbestos industries, have eroded government investment and action in key areas for decades. Polluters have used scientists-for-hire, corporate law firms, and PR gurus to develop a stunningly effective playbook for sowing doubt and suspicion about independent science. 3 Scientists who documented threats from lead, arsenic, other toxic chemicals, tobacco, and climate change have been silenced and undermined, often suffering the retaliatory fate of brave whistleblowers. This has left our environmental protection and public health agencies at all levels of government in a constant state of disinvestment. With the unchecked use of innumerable poisons in the food we eat, the air we breathe, and the water we drink-all too often disproportionately burdening poor and minority populationsthe groundwork for our country's unrelenting COVID-19 fatalities was laid. Years of neglect and disinvestment were not enough, though. The Trump administration took the suspicion of science to its most extreme, moving even farther away from basing decisions on facts and studies. Instead of the health and protection of the population, it leans entirely on ideological dogma, political motivations, and the pecuniary interests of corporate supporters. The list of nefarious acts is long and scandalous. Along with ignoring climate science, the current administration is disregarding evidence showing that aggressive action is needed on lead poisoning, refusing to address asbestos, opposing action on widespread toxic "forever chemicals" called PFAS, and rejecting the Environmental Protection Agency's (EPA's) own scientists' recommendations to ban chlorpyrifos-an insecticide that damages children's brains. [4] [5] [6] [7] And as the death toll rises and pandemic rages on, the EPA is brazenly racing to enact another industry-favorable proposal-opposed by leading medical and scientific societies-that would block EPA from considering many epidemiological studies that show adverse health effects published in scientific journals. 8, 9 And just this May, indifferently turning a blind eye to the potential of our nation's children, the EPA refused to impose limits on the brain-damaging water contaminant perchlorate. 7 These anti-science, anti-prevention, and antiregulatory policies-decades in the making and worsening radically under this administration-are not just wrongheaded. They are literally killing us. Approximately 200 000 people die annually in the United States from weak air quality standards. 10 And multitudes of Americans-the numbers are grossly underestimated-die of cancer each year provoked by exposure to pollution. 11 Now, in the midst of the COVID-19 pandemic, we are able to see, finally, the human consequences of government maleficence-in the overcapacity hospitals, unjust rationing of testing, unequal health care, the pages of obituary listings, and the dead bodies stuffed into rented freezer trucks outside medical centers and overflowing county morgues. The United States can now claim the worst performance of the developed world. Was it really COVID-19 that ended the lives of so many Americans? Our rich and talented society could not even muster an adequate response to a foreseeable crisis. Playing catch-up on this scale is not possible; no amount of a reactive response can compensate for proactive prevention. In the face of the demands of the pandemic, our hollowed-out public health agencies are being asked to perform herculean feats. In the face of so much sickness and death, the anti-science misinformation campaigns still continue. Many Americans deny the severity of the pandemic and happily flaunt public health recommendations-with mounting threats and personal attacks against experts such as Dr Fauci and Dr Bright, who are trying to keep us alive. This is more maleficence-and will only increase the number of deaths and the pandemic's duration. Postmortem examinations of this crisis are not too far off. We will need to dig deep and dissect the true cause of the pandemic. We cannot save all the Americans we have lost, but we can commit ourselves to understanding how it came about. We can begin to accept that the outcome was not an accident. It was predictable and preventable. Next, we must begin recognizing and undoing the preexisting conditions. Science tells us that many preventable catastrophes are on the horizon-from rampant antibiotic-resistant infections and contaminated drinking water to the potentially horrific impact of climate change. These are threats that only scientific study, long-range planning, and competent government can prevent and protect us from. As we begin to emerge from this crisis and prepare for the next ones, that is exactly what will save us, and what we need to demand: science-driven leadership, robust public health protection, and a responsive, forward-looking government that has our best interests at heart. The only way to be optimistic about our future is to be ready for it. 
paper_id= ffff73d17bc392ee68f3f16ef37d25579cb99322	title= Patterns of activity rhythms of invasive coypus Myocastor coypus inferred through camera-trapping	authors= Emiliano  Mori;· Alley Andreoni;Francesco  Cecere;Matteo  Magi;· Lorenzo Lazzeri;	abstract= Studies on activity rhythms are pivotal for the management of invasive alien species, as they provide basic insights into species basic ecology and may increase the success of control programs. The coypu Myocastor coypus, introduced from South America for fur farms, has become one of the most invasive rodents in Europe. Introduced coypus may affect crop productions, as well as natural vegetation and the breeding success of wading birds. In this study, we examined activity data collected through intensive camera-trapping in three Italian areas, including two natural areas in Northern and Central Italy, and a suburban area in Central Italy. Coypus were mostly diurnal in areas characterised by low predator pressure and, at night, they are mostly active in bright moonlight. Conversely, where predators, human pressure or numerical control programmes are present, coypus remarkably shift their behaviour towards crepuscular and night hours. In these last areas, nocturnal activity increased as moonlight decreased, possibly to reduce predation risk or encounters with humans. Where winter temperature are low, diurnal habits may have developed as a physiological adaptation and a strategy to preserve energy, potentially achieving a cost/effective thermal balance. 	body_text= Invasive alien species represent one of the main causes of the current global biodiversity crisis (Genovesi 2011; Bellard et al. 2016) . Biological invasions may shape and modify native biodiversity communities, by means of impacts including competition, ecological alterations, economic and health costs (Mack et al. 2000; Mazza et al. 2014) . A detailed knowledge on the spatiotemporal behaviour of invasive alien species is pivotal to implement the success of removal operations Capizzi 2020) . Accordingly, assessing when alien species are active may help timing operations of numerical control, so to increase success of management interventions (Mazza et al. 2019; Andreoni et al. 2020) . Camera-trapping have been widely reported to be a reliable tool to assess vertebrate activity rhythms (Ridout and Linkie 2009; Monterroso et al. 2013; Torretta et al. 2017; Mori et al. 2020a) , as they provide similar results to radio-tracking (Lashley et al. 2018 ) when at least 30-50 detections are available (Ridout and Linkie 2009 ). Among invasive vertebrates, the coypu Myocastor coypus (Molina, 1782) is a large semiaquatic rodent native to the subtropical regions of South America (Woods et al. 1992; Carter and Leonard 2002) . In the XX century, the coypu has been widely traded and introduced outside of its native range for fur-farming (Carter and Leonard 2002) . In Italy, first coypus in the wild were observed in the 1960s and it now occurs in all regions, but for Aosta valley, including major islands (Sicily, where no observation are available since 3 years, and Handling editor: Adriano Martinoli. The online version of this article (https ://doi.org/10.1007/s4299 1-020-00052 -8) contains supplementary material, which is available to authorized users. Sardinia: Bertolino and Cocchi 2018) . On one hand, impacts and spatial behaviour of this species are widely known. Coypus select waterways with weak streams and dense riparian vegetation (Woods et al. 1992; D'Adamo et al. 2000; Roviani et al. 2020 ), but they may also colonise polluted habitats and urban areas too (Meyer et al. 2005; Corriale et al. 2006) . Where introduced, they may affect native environments by (1) feeding on natural vegetation (Prigioni et al. 2005; Marini et al. 2013 ) and crops (D'Adamo et al. 2000; Panzacchi et al. 2007 ), (2) crashing the eggs of waterbirds using floating nests as resting sites Bertolino et al. 2012) and (3) burrowing in riverbanks (Carter and Leonard 2002) . Furthermore, a sanitary impact by this rodent is also recorded, as it represents a reservoir for several zoonoses (Mènard et al. 2001; Nardoni et al. 2011; Zanzani et al. 2016 ). On the other hand, data on activity rhythms of this species are scanty, thus potentially limiting the success of killings for numerical control. Coypus are reported to be mostly nocturnal, but with some diurnal activity in cold season and where supplemental food is provided by humans, according to studies in captivity (Lomnicki 1957; Gosling 1979) , based on indirect signs of presence (Gosling et al. 1980; Chabreck 1982) , on very few radio-tagged individuals (N = 6: Palomares et al. 1994; Meyer et al. 2005) , or for a low amount of time (e.g., 7 days in winter: Palomares et al. 1994) . In Italy, the coypu is excluded from protected species (National Law 157/1992). Plans of numerical control of this large rodent are widespread in Northern and Central Italy, mostly involving direct captures with cages or direct killing through firearms (Bertolino and Cocchi 2018) . Human activity and predator presence are reported to influence activity rhythms of prey species, including rodents (Yoneda 1983; Mohr et al. 2003; Patergnani et al. 2010) . In our work, we assessed the seasonal variation of patterns of activity rhythms of the coypu in three areas of Italy characterised by contrasting habitat composition, through intensive camera-trapping. We predicted that (1) activity of coypu would change seasonally, (2) coypus would be mostly crepuscular and nocturnal in our study areas, (3) diurnal behaviour would be more evident in suburban areas, where predation risk is suggested to be the lowest and where humans are known to feed coypus, with respect to rural ones. During the Italian national lockdown due to SARS-CoV-2 pandemic outbreak, we carried out a survey on the cameratrapping Facebook group "Fototrappolaggio Naturalistico" (https ://www.faceb ook.com/group s/21657 42184 24779 , N followers = 9765) and we asked whether someone has camera-trapped a good amount of coypu records (N ≥ 30 per season: Lashley et al. 2018) , to draw patterns of activity rhythms. Successful camera-trapping of semiaquatic mammals may be difficult because of their lower body temperature, which may fail in triggering camera sensors (Lerone et al. 2015 , for the Eurasian otter Lutra lutra). Amongst 28 respondents, only three provided us with a relevant amount of videos from three study areas. Other 12 carried out camera-trapping to assess presence and densities of other species (e.g., the wolf Canis lupus, the wild boar Sus scrofa, small carnivores) and deleted most of their coypu videos. We obtained data from three sites. Ludovici et al. 2012 ). The thermal regime shows a winter minimum in January (2 °C) and a summer maximum in July (22 °C), with a mean annual temperature of 13 °C. Precipitations show instead an annual average of 829 mm, with a peak (about 95 mm) in October (Agapito Ludovici et al. 2012) . Camera trapping in 30 ha of this area is ongoing since May 2015, with three camera traps placed along water mirrors (camera trap models: ScoutGuard ® SG550-12mHD, ScoutGuard ® SG2060-X and BG962-30W), kept constantly active throughout the year. Apart from a single house within the study area, the straight distance Site 1-nearest settlement (including sheds, barns and industrial areas) was 539 m, the one Site 1-nearest residential area (village or town) was 1900 m. At least 50-60 coypus occurred in the study area at the time of field work. 2. Site 2 (Bagno a Ripoli, province of Florence, C Italy: 43.74°N-11.31°E, 125-139 m a.s.l., 26 ha), along the Ema river banks in the Florence metropolitan area. This site is mostly covered with cultivations (73%), followed by deciduous woodland (12%), irrigation canals (10%) and human settlements (5%). The thermal regime shows a winter minimum in January (2 °C) and a summer maximum in August (29 °C), with a mean annual temperature of 14.5 °C. Precipitations show instead an annual average of 870 mm, with a peak (about 115 mm) in November (cf. Giuntini 2019). Camera trapping in this area has been ongoing since January 2019, with three camera traps (Multipir 12 ® , Scouting Camera) placed on trails along irrigation ditches and kept constantly active throughout the year. The straight distance Site 2-nearest settlement (including sheds, barns and industrial areas) was 150 m, as well as the one Site 2-nearest residential area. A total of 25-30 coypus were counted in this area at the time of our field work. 3. Site 3 (Villa Ceccolini, province of Pesaro-Urbino, C Italy: 43.86°N-12.85°E, 62-75 m a.s.l., 4 ha). This site is located in a rural area, and includes a small hygrophilous deciduous woodland (78% of the study area), with a pond (16%) surrounded by riparian vegetation (4%), and forest glades (2%). The thermal regime shows a winter minimum in February (3 °C) and a summer maximum in July (24 °C), with a mean annual temperature of 13 °C. Precipitations show instead an annual average of 828 mm, with a peak (about 99 mm) in October (Farina and Cavitolo 2016) . Camera trapping in this area has been ongoing since January 2020, with two camera traps (Apeman ® , H70) kept constantly active. The straight distance Site 3-nearest settlement (including sheds, barns and industrial areas) was 202 m, the one Site 3-nearest residential area (village or town) was 1110 m. About 8-10 coypus were present in the pond at the time of our field work. In all study sites, no control programme for the coypu was currently ongoing. In Site 2, some coypu individuals were killed through firearms in 2018. The main predators of the coypu are the red fox Vulpes vulpes, which occurs on all the study sites, and the grey wolf Canis lupus, present in Sites 2 and 3 (Berzi 2015; Supplementary File 1), which mostly prey on coypu in winter (Ferretti et al. 2019 ). As to Site 1 and Site 2, we have data for all the four seasons, whereas we have only winter data for Site 3. In all sites, camera-traps were opportunistically placed on the coypu trail around ponds, ditches and rivers. Cameratrap stations were separated from one-another by at least 500 m in straight line, so to reduce pseudo-replication bias (O'Connell et al. 2011) , so to cover the entire water bodies hosting coypus in the three study areas. Accordingly, with this inter-camera distance, we put the higher number of possible camera traps in each study area. Cameras were placed at ~ 130-150 cm from the ground level and were activated 24 h/day, to take one video every time an animal passed in front of the camera trap. Checks occurred once every 10 days to download data and replace empty batteries. We detected no significant spatiotemporal bias in sampling across different seasons (Rayleigh test, Z = 13.25, p = 0.08) (O'Connell et al. 2011 ). As to Site 1, we detected no significant difference in circadian rhythms across years and across the same seasons in different years (Mardia-Watson-Wheeler tests: 0.018 < W < 0.159; p > 0.10). Thus, we pooled together all coypu videos occurring in the same seasons in different years for our analyses (Mori et al. 2020a) . We defined the night as the period included between 1 h after sunset and 1 h before sunrise (Carnevali et al. 2016 ). We divided the study period of each study area (Site 1, 2015-2020; Site 2, 2019; Site 3, 2020) into four astronomical seasons (spring, summer, winter and autumn). Analyses were carried out on both total and seasonal scales. We defined activity outside burrows as the cumulate period animals spend outside dens, regardless of their behaviour (Meredith and Ridout 2014; Lashley et al. 2018) . For all coypu videos, we reported the date and the solar hour of capture (directly shown on the video) on a dataset. The use of solar hour allows a better evaluation of activity patterns as, differently from "legal hour", it is defined by the position of the sun in the sky regardless to the local time which varies among seasons. We limited pseudoreplication bias by counting as one "independent event" all coypu videos taken by the same camera trap in less than 30 min (Monterroso et al. 2014; Torretta et al. 2017) . When more than one coypu video was recorded by the same camera trap in ≤ 30 min, we kept in our dataset only one record, placed in the mid-time between the first and the last video. We used the software R (version 3.6.1., R Foundation for Statistical Computing, Wien, Austria: www.cran.r-proje ct.org), package overlap (Meredith and Ridout 2014) to assess activity rhythms and patterns of inter-seasonal temporal overlap. We estimated the coefficient of overlapping (Δ) between temporal activity patterns of all pairwise season combinations in the same study area. We also calculated overlap among activity patterns in the same season in different study sites. The coefficient of overlapping ranges between 0 (no overlap) and 1 (total overlap: Linkie and Ridout 2011; Meredith and Ridout 2014) . We calculated the Δ 4 estimator when the smallest sample of each pairwise comparison was > 75 records, Δ 1 if at least one of the sample of each pairwise comparison was < 75 records (Linkie and Ridout 2011; Meredith and Ridout 2014) . The 95% confidence intervals (hereafter, 95% CI) of the coefficient estimator were estimated using 10,000 bootstrap replicates (Dias et al. 2019 ). Overlap was "intermediate" with Δ included between 0.50 and 0.75 was considered as, "high" with Δ > 0.75, "very high" with Δ > 0.90 (Monterroso et al. 2014; Mazza et al. 2019) . The Mardia-Watson-Wheeler W-test (MWW test, U 2 ) was used to compare inter-seasonal overlap between different study sites, and same-season overlap between each site pair (Monterroso et al. 2014) ; it was calculated through the R package circular (Lund et al. 2017) . We also tested whether sky brightness affected the activity of the coypu by classifying surveyed nights according to moon phases and epact: (1) epact days = 0-3, 26-29; (2) epact days = 4-6, 21-25; (3) epact days = 7-9, 17-20; (4) epact days = 10-16 (Mori et al. 2014) . We performed a chi-squared test on the numbers of videos recorded during each of these moonlight periods, to assess if they were uniform throughout epact days (Mori et al. 2014 (Mori et al. , 2020a . In our analyses, we included a total of 447 independent records from Site 1 (autumn, N = 174; winter, N = 78; spring, N = 137; summer, N = 58), 188 from Site 2 (autumn, N = 42; winter, N = 60; spring, N = 43; summer, N = 43), 72 from Site 3 (winter only: Supplementary File 2) , for a total of 5198 camera trap-nights. Annual patterns of temporal rhythms at Sites 1 and 2 showed that the peak of coypu activity occurred in crepuscular hours (Site 1), at night in suburban ones (Site 2). Overlap of annual active patterns between Sites 1 and 2 was intermediate, i.e., 60% (Δ 4 = 0.60, 95% CIs 0.57-0.71). Inter-seasonal overlaps of activity rhythms ranged between intermediate (Δ = 0.55) to high values (Δ = 0.81) in Site 1 (Fig. 1) and Site 2 (Fig. 2) . Particularly, in Site 1, the highest overlap occurred between winter and spring, i.e., when most of the watercourses are frozen and coypus are mostly diurnal. In Site 2, overlap levels were always high, except for pairwise comparison involving autumn, showing intermediate overlap levels. We detected no significant difference in pairwise seasonal overlaps between study sites (MWW tests: 0.0417 < U 2 < 0.1528, p > 0.05). Same-season overlaps of temporal activity across study sites (Fig. 3 ) ranged between 49% (Δ = 0.49) and 77% (Δ = 0.77). The overlap winter-winter between Sites 2 and 3 was significantly higher than the overlaps winter-winter between both sites and Site 1 (MWW test: U 2 = 0.2452, p > 0.03). Coypus avoided darkest nights in spring and summer, as well as on the total survey period, at Site 1. Conversely, this rodent avoided bright full moon nights at Sites 2 and 3 only in winter; in other seasons, its activity rhythms did not depend on moon phases (Table 1 ). Our work showed the result of a pilot study on the activity rhythms of the coypus in three study areas characterised by different environmental features. We are aware of the limits related to small sample sizes (i.e., three areas, with at max three camera traps/site), but first results are suggestive and in line with behaviour of other rodent species with changing predation pressure and human activity (Yoneda 1983; Mohr et al. 2003) . Differently from radio-tracking, camera-trapping is intrinsically limited by the imperfect detectability of animals (Burton et al. 2015) . However, a number of recent works has shown that it may provide reliable estimates of activity rhythms (Meredith and Ridout 2014) , even when local abundance of species is not available (Monterroso et al. 2013; Lashley et al. 2018; Chen et al. 2019) . This is quite a common issue with species living also in concealed habitats (Ridout and Linkie 2009 ), e.g., the coypu. In our work, coypus showed a great ecological plasticity: they may have adapted their temporal behaviour to seasons within the same site and to local environmental conditions in different sites, thus confirming our prediction (1). This is suggested by the low overlap in annual activity rhythms between the natural site (Site 1) and the suburban one (Site 2). As well, sameseason overlaps between these areas were low, furtherly supporting the hypothesis of different local adaptations. In the natural area, coypus showed an activity peak at dusk throughout the year. A diurnal peak occurred in winter, i.e., when nights are the coldest (up to − 6 °C: cf. Agapito Ludovici et al. 2012) , and coypus compensate by foraging during daylight. In our study area, diurnal activity decreased in spring and almost disappeared in summer and autumn. The local absence of large wild predators (e.g., the wolf and the golden jackal Canis aureus: Woods et al. 1992 ), of control plans and of human disturbance may have favoured the coypu activity in daylight hours. This result confirmed data by Davis and Jenson (1960) and by Gosling et al. (1980) , who showed that coypu may become diurnal in cold months. Presumably, this adaptation to daylight hours is an adaptive response to limit energy loss and to maintain an adequate food intake at low environmental temperatures (Gosling 1979) . Freezing of the surface of water bodies at night may prevent coypus to feed on aquatic vegetation, i.e., the staple of its diet (Prigioni et al. 2005; Marini et al. 2013) . Accordingly, food provided to captive coypus throughout the 24 h cycle restored the crepuscular and nocturnal behaviour of this large rodent (Lomnicki 1957; Gosling 1979) . Meyer et al. (2005) also suggested that coypu diurnal behaviour may represent an adaptation to urban environments in Germany, where humans tend to feed this large rodent. Radio-tagged coypus in urban areas in Germany were never detected between midnight and sunrise (Meyer et al. 2005) . Conversely, in our suburban study area, activity of the coypu peaked after sunset. Crepuscular and nocturnal behaviour are in line with previous studies in natural environments (i.e., far from human settlements) both in the native and in the introduced range (Chabreck 1982; Palomares et al. 1994) , as well as with studies on other large rodents (i.e., porcupines Hystrix spp.: Corsini et al. 1995; Fattorini and Pokerhal 2012) , including semiaquatic ones (i.e., the musk rat Ondatra zibethicus and the beaver Castor fiber: Svihla and Svihla 1931; MacArthur 1980; Swinnen et al. 2015; Pontarini et al. 2019 ). In our suburban study area, the nocturnal behaviour is maintained by the coypu also in winter, as temperatures of this area rarely go below 0 °C (Giuntini 2019), which may not affect the commonest locomotor pattern of this species (Wood et al. 1992) . Moreover, differently from urban location by Meyer et al. (2005) , our study site was located in the peripheral quarters of a metropolitan area which counts for over 950 human inhabitants/km 2 . Human disturbance and a highly busy beltway may have forced coypus to rest in their den setts in daylight hours, being more active at night. We are aware that several individuals have been killed with firearms in 2018 in this area (i.e., one-two years before our camera trapping study), which might have increased coypu activity in darkest hours (Gosling et al. 1980) . In fact, in Great Britain, during the national coypu eradication programme, these rodents were mostly killed during daylight (Gosling et al. 1980) . Furthermore, the wolf, i.e., the main predator of the coypu in Italy (Ferretti et al. 2019 ) is present in this area, as also confirmed by three records in our camera-trap survey, and this may have helped bringing coypus to be more active when their detectability is the lowest, i.e., in darkest hours. Similarly, despite we have only winter data, coypus were strictly nocturnal also in a natural area (Site 3) located at the same latitude of the suburban one (i.e., similar winter temperatures), characterised by the presence of the wolf. We suggest that human pressure and predator presence have created a sort of "landscape of fear" for the coypu, which has adapted a mostly crepuscular/nocturnal behaviour in our suburban area to limit encounters with humans and to lower its detectability by natural predators (Tolon et al. 2009; Laundrè et al. 2010; Brivio et al. 2017 ). This would also reflect the nocturnal behaviour of the coypu in its native range (Palomares et al. 1994) , where a number of potential predators occurs (e.g., the jaguar Panthera onca, the puma Puma concolor, the ocelot Leopardus pardalis, the oncilla L. tigrinus and six species of grey foxes Lycalopex spp.). Despite most predators being nocturnal in Europe (Mori et al. 2020b) , which may suggest an increase in encounter rate with nocturnal coypus, this large rodent has also shown another temporal adaptation. Moon phases influence the ranging behaviour of mammal species, with predators being mostly effective when some light occurs (e.g., the grey wolf: Theuerkauf et al. 2003) , as their visual detection of prey is increased (Cozzi et al. 2012 ; Monterroso et al. 2013) . As a response, prey species (e.g., rodents) tend to shift their ranging movements towards close or dark sites/nights (Alkon and Saltz 1988; Fattorini and Pokheral 2012; Upham and Hafner 2013) . In Central Italy, the coypu is mostly preyed by the grey wolf in cold months (Ferretti et al. 2019) , which may explain why this large rodent avoided bright moonlight only in winter, both in Site 2 and in Site 3. Consistently, in a natural area where the wolf is not present, besides diurnal behaviour, nocturnal bouts are concentrated in brightest moonlight, with coypus avoiding darkest nights. This fits with the behaviour of a mainly nocturnal semiaquatic rodent, the European beaver Castor fiber: this species, at night, select bright moonlight nights in absence of predators to better forage (Swinnen et al. 2015) . Thus, our prediction (2) and (3) were only partially fulfilled, as other environmental constraints we did not consider in our analyses may shape the local behaviour of the coypu. Eradication of the coypu could be possible no more for peninsular Italy; therefore, control programmes are ongoing to limit impacts on native biodiversity. Effectiveness of management strategies in the long-term could not circumvent ecological information on the target species, including patterns of activity rhythms, also to limit costs (cf. Bertolino et al. 2005; Bertolino and Viterbi 2010) . The great ecological plasticity allowed coypus to adapt to a number of different environmental conditions ranging from natural ponds, to swamps, to riverine habitats in urban areas. Success of coypu control is related to local population density, social behaviour and spatiotemporal ecology, which should, therefore, be previously assessed. 
paper_id= ffff79e335491d459180a337e572a19ece47a885	title= Estimating Risk of Mechanical Ventilation and Mortality Among Adult COVID-19 patients Admitted to Mass General Brigham: The VICE and DICE Scores Running Title: Mortality and ventilation risk in COVID-19	authors= Christopher J Nicholson;;  Luke Wooster;;  Haakon;H  Sigurslid;Rebecca F Li;Wanlin  Jiang;Wenjie  Tian;Christian L Lino Cardenas;Rajeev  Malhotra;‡  ;	abstract= Background: Risk stratification of COVID-19 patients upon hospital admission is key for their successful treatment and efficient utilization of hospital resources. Objective: To evaluate the risk factors associated with ventilation need and mortality. Design, setting and participants: We established a retrospective cohort of COVID-19 patients from Mass General Brigham hospitals. Demographic, clinical, and admission laboratory data were obtained from electronic medical records of patients admitted to hospital with laboratoryconfirmed COVID-19 before May 19 th , 2020. Using patients admitted to Massachusetts General Hospital (MGH, derivation cohort), multivariable logistic regression analyses were used to construct the Ventilation in COVID Estimator (VICE) and Death in COVID Estimator (DICE) risk scores. The primary outcomes were ventilation status and death. Results: The entire cohort included 1042 patients (median age, 64 years; 56.8% male). The derivation and validation cohorts for the risk scores included 578 and 464 patients, respectively. We found seven factors to be independently predictive for ventilation requirement (diabetes mellitus, dyspnea, alanine aminotransferase, troponin, C-reactive protein, neutrophil-lymphocyte ratio, and lactate dehydrogenase), and 10 factors to be predictors of in-hospital mortality (age, sex, diabetes mellitus, chronic statin use, albumin, C-reactive protein, neutrophil-lymphocyte ratio, mean corpuscular volume, platelet count, and procalcitonin). Using these factors, we constructed the VICE and DICE risk scores, which performed with C-statistics of at least 0.8 in our cohorts. Importantly, the chronic use of a statin was associated with protection against death due to COVID-19. The VICE and DICE score calculators have been placed on an interactive website freely available to the public (https://covid-calculator.com/). : medRxiv preprint mechanisms. Nat Rev Drug Discov. 2005;4(12):977-987. 	body_text= The number of global confirmed cases of the novel coronavirus disease 2019 (COVID-19) passed 20 million in August, with over 700,000 deaths. 1 The U.S. has surpassed any other country in the number of total deaths and case rates continue to rise with some hospitals utilizing nearly 100% of available ICU beds. Specific information regarding the patient risk factors that associate with mortality from COVID-19 remain limited, and methods to accurately predict severity of disease at the time of hospital presentation are lacking. [2] [3] [4] [5] [6] Using data from a Chinese cohort, an algorithm was recently developed that predicts critical illness (a composite of ICU admission, ventilation needs and death) in hospitalized COVID-19 patients. However, the applicability of this algorithm to predict outcomes in a US population, which has distinct disease risk profiles, remains unknown. 7, 8 Further, it is crucial for health care providers to be able to stratify risk for the most important clinical outcomes in COVID, namely mechanical ventilation need and mortality. Knowing the risk of these outcomes will allow the most optimal allocation of healthcare resources on admission to the hospital and identify those that will require the most intense care. Given the United States has reported over a quarter of global deaths due to COVID-19 and is currently in the midst of a profound wave of infections, new information on the factors that influence risk of severe outcomes is greatly needed. This study describes the details of patients with laboratory confirmed COVID-19 admitted to Mass General Brigham hospitals in Boston, Massachusetts. Specifically, we described the baseline comorbidities, presenting clinical tests and outcomes of hospitalized COVID-19 patients, explored the risk factors associated with mechanical ventilation . CC-BY-NC-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review) preprint The copyright holder for this this version posted September 17, 2020. . https://doi.org/10.1101/2020.09.14.20194670 doi: medRxiv preprint requirements and in-hospital death, and developed risk models to more effectively predict severe outcomes in patients from the United States. . CC-BY-NC-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review) preprint The copyright holder for this this version posted September 17, 2020. . https://doi.org/10.1101/2020.09.14.20194670 doi: medRxiv preprint This study included consecutive adult patients with laboratory-confirmed COVID-19 infection who were admitted for illness related to A confirmed case of COVID-19 was defined by a positive result on a reversetranscriptase-polymerase-chain-reaction (RT-PCR) assay of a specimen collected on a nasopharyngeal swab. Patients were defined as COVID-19 positive if they had a positive test, or if they had a negative test, but repeat testing was positive. Only laboratory-confirmed cases of those that were sufficiently ill to require hospital admission were included. Clinical outcomes were monitored up to and including July 20, 2020, the final date for follow-up. We excluded children (those younger than 18 years of age) from the study. Epidemiological, demographic (self-reported), clinical laboratory, and treatment data were obtained first from the Research Patient Data Registry, a centralized clinical data registry directed by the Mass General Brigham network. Outcome data, including discharge, ICU, and ventilation status, and home medication data were extracted from electronic medical records . CC-BY-NC-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review) preprint The copyright holder for this this version posted September 17, 2020. . https://doi.org/10.1101/2020.09.14.20194670 doi: medRxiv preprint (EPIC) using a standardized data collection form. All laboratory tests and radiologic assessments, including plain chest radiography, were performed at the discretion of the treating physician. Only laboratory tests performed on or within 24 hours of hospital admission were included in the analyses. Patients were assessed for the presence of hypertension, diabetes mellitus, coronary artery disease, chronic obstructive pulmonary disease (COPD), chronic kidney disease (CKD), and a history of cancer. These covariates were selected based relevance to COVID-19 in previously published analyses. 2, 3, [5] [6] [7] 9 Outcome definitions and data analysis The primary endpoints for our analyses were need for mechanical ventilation and inhospital death. For our statistical analyses, we excluded patients that requested, upon admission, to be treated with comfort measures only (CMO). Patients that were identified as "do not intubate" (DNI) on admission were excluded from analyses that assessed risk factors for mechanical ventilation requirements. Discharged patients were defined as those who were discharged to home, nursing homes, or a rehabilitation facility. All analyses were performed using R studio (version 1.2.5033) or Stata (version 13.0). Continuous variables were reported as mean (SD) unless otherwise noted and categorical variables were reported as n (%). The Fisher's exact test, Student's t-test or Mann Whitney test were used to measure differences between groups, where appropriate. Univariate logistic regression was used to determine if a clinical factor was associated with the need for mechanical ventilation or with in-hospital mortality. Validation was performed by applying the regression coefficients and estimating probabilities. . CC-BY-NC-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review) preprint The copyright holder for this this version posted September 17, 2020. . https://doi.org/10.1101/2020.09.14.20194670 doi: medRxiv preprint We used a multivariable logistic regression model to determine variables that would be included in our predictive risk score algorithms for mechanical ventilation needs (Ventilation in COVID Estimator [VICE] score) and death (Death in COVID Estimator [DICE] score). To do so, we used a derivation cohort including patients admitted to MGH only. For variables that had a univariate P-value of <0.05 or a P-value <0.05 after adjusting for age and sex in this cohort, we performed a multivariable logistic regression analysis with a backwards stepwise approach. Of variables that were highly correlative (e.g. eGFR and creatinine, with r > 0.7), we only included the one with the lowest p-value in univariate analysis. We used this method to determine risk factors associated with (i) mechanical ventilation requirements and (ii) in-hospital mortality, which allowed construction of risk score predictors for each outcome. We assessed the accuracy of the risk score models using the area under the receiver-operator characteristic curve (AUC or C-statistic). We first assessed our risk score in patients that were admitted to MGH only (our derivation cohort), and then validated it using patients admitted to BWH, NWH, BWFH, and NSMC (our validation cohort). . CC-BY-NC-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review) preprint The copyright holder for this this version posted September 17, 2020. . https://doi.org/10.1101/2020.09.14.20194670 doi: medRxiv preprint Demographic and clinical characteristics of the cohort 1137 adults were admitted to Mass General Brigham hospitals with COVID-19 symptoms before May 19, 2020. Patients that were treated with comfort measures only (CMO) on arrival (n=95) to the hospital were excluded from the study. As described in Supplemental Table 3 , CMO patients were on average older and had a higher level of cancer diagnoses than patients included in the final study. Interestingly, however, only 17% of these patients died in hospital by the end of the study. After this exclusion, we included 1042 patients (578 from MGH, 269 from BWH, 125 from BWFH, 60 from NWH, and 10 from NSMC) in our final analyses ( Table 1) . The median age for these patients was 64 (IQR 53-75), ranging from 18 to 99 years old, and the majority (57%) were male. Among the 1042 patients, 438 (42%) identified as white, 187 (17.9%) as black, 113 (10.8%) as Hispanic, and 37 (3.6%) as Asian. One hundred and seventy-seven (17%) patients did not identify with any of these racial backgrounds and 90 (8.6%) patients had no racial background recorded. As of July 20th, 2020, among the 1042 patients admitted to hospital, 829 (79.6%) were discharged, 211 (20.2%) died in hospital, and 2 (0.2%) remained in hospital. Three quarters of the patients had at least one comorbidity. The most common comorbidities were hypertension (56.4%), diabetes mellitus (42.5%), and hyperlipidemia (38.1%). 86% of patients who died had at least one comorbidity. The median length of stay was 10 (IQR: 6-21, up to 98) days. The median length of ICU stay (n=449) and ventilation time (n=400) were 15 (7-23) days and 13 (7-22) days, respectively. The median survival time amongst those that died was 11 days (IQR = 6-19; Range = 1-71). Only 47 patients who were admitted to the ICU received no mechanical ventilation. Ninety-one patients were identified as "do not intubate" (DNI) on admission. As . CC-BY-NC-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review) preprint The copyright holder for this this version posted September 17, 2020. . https://doi.org/10.1101/2020.09.14.20194670 doi: medRxiv preprint shown in Supplemental Table 3 , DNI patients were on average older and had a higher prevalence of pre-existing conditions than patients included in the final study. Of the 449 patients who were admitted to the ICU, 400 patients (89.1%) required mechanical ventilation. One hundred and thirty-six (34%) mechanically ventilated patients died. With regards to home medications, 511 (49%) were on a statin, 318 (30.5%) on aspirin, and 315 (30.2%) on a RAAS inhibitor. In separate univariate analyses, clinical factors that were associated with need for mechanical ventilation and associated with in-hospital mortality were identified (detailed in Table 1 and Table 2 ). Many variables on admission were consistently predictive of both ventilation need and mortality, including male sex, diabetes mellitus, lower levels of albumin and eGFR, and elevated absolute neutrophils, anion gap, activated partial thromboplastin time, blood urea nitrogen, C-reactive protein, creatinine, D-dimer, eGFR, plasma glucose, neutrophil to lymphocyte ratio, procalcitonin, and troponin T (high sensitivity). Troponin predicted both need for mechanical ventilation and in-hospital mortality when assessed as a continuous variable or as a dichotomous variable with the threshold of >10 ng/mL used to indicate the presence of myocardial injury. However, it was striking that many factors were only associated with only one of either mechanical ventilation need or in-hospital mortality. Those that met significance for ventilation need only included dyspnea and loss of consciousness on presentation, x-ray abnormality on admission, and elevated alanine aminotransferase (ALT), aspartate aminotransferase (AST), direct bilirubin, erythrocyte sedimentation rate (ESR), ferritin, fibrinogen, lactate dehydrogenase (LDH), mean corpuscular hemoglobin (MCH), and white blood cell count (WBC). The variables that were predictive of mortality only included age, coronary artery disease (CAD), hypertension, hyperlipidemia, chronic kidney disease, chronic . CC-BY-NC-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review) preprint The copyright holder for this this version posted September 17, 2020. . https://doi.org/10.1101/2020.09.14.20194670 doi: medRxiv preprint obstructive pulmonary disease, statin use, aspirin use, lower levels of hemoglobin and platelets, and elevated levels of creatinine, mean corpuscular volume (MCV), NT-ProBNP, and increased prothrombin time, and red cell distribution and width (RDW). It was notable that age was not a significant predictor of whether a patient would require mechanical ventilation. To investigate this further, we determined rates of mortality and need for mechanical ventilation per decade of life. As anticipated, older age was associated with an increase in mortality rate (Figure 1a) . However, other than patients in the youngest age groups, the percentage of hospitalized COVID-19 patients requiring mechanical ventilation was similar in each decade of life (Figure 1b) . Of those that were ventilated, there is a clear correlation between age and risk of death. Indeed, of patients in the oldest group (>84 years of age), only 15% survived if mechanical ventilation was required (Figure 1c) . Interestingly, young patients were as likely as patients of advanced age to require long durations on ventilation (Figure 2 ). In fact, 78% of ventilated patients between ages 18 and 44 were intubated for longer than 6 days, and 45% were intubated for longer than 14 days. Multivariable logistic regression models were used in order to develop risk scores to predict important clinical outcomes in COVID-19, namely the need for mechanical ventilation and in-hospital mortality. As detailed in the above section, we found many variables, including age, that were distinctly associated with both the need for ventilation and for mortality. We therefore constructed separate risk scores for ventilation requirement (VICE=Ventilation In COVID Estimate) and death (DICE=Death In COVID Estimate) based on multivariable logistic . CC-BY-NC-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review) preprint The copyright holder for this this version posted September 17, 2020. . https://doi.org/10.1101/2020.09.14.20194670 doi: medRxiv preprint regression models. We divided our overall study into separate derivation (MGH, n=578) and validation (BWH, NWH, BWFH, NSMC; n=464) populations based on the hospital of admission. Our derivation cohort of 578 patients (Supplement Table 1 is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review) preprint The copyright holder for this this version posted September 17, 2020. . https://doi.org/10.1101/2020.09.14.20194670 doi: medRxiv preprint increase; 95% CI 0.64-0.92, P=0.004), and procalcitonin (log2-transformed, OR 1.20; 95% CI 1.03-1.40, P=0.017). Of note, use of an angiotensin converting enzyme inhibitor (ACEI) or angiotensin receptor blocker (ARB) was not associated with a difference in outcome both in univariate or multivariable analysis. The VICE and DICE risk scores were constructed based on coefficients from the multivariate logistic regression models. We used the following formula to calculate the probability (p) and 95% confidence intervals: ! = (23) and β 4 can be found in the caption to Table 3 (Figure 3b ) rates was observed with increasing quintiles of VICE and DICE scores, respectively, in our cohort. In patients falling within the highest quintile of DICE score, mortality was 58% compared to 2% in the lowest quintile. Previous reports have highlighted a marked increase in risk of developing severe illness in COVID-19 in Asians and African Americans. [10] [11] [12] [13] We found it interesting, therefore, that we . CC-BY-NC-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review) preprint The copyright holder for this this version posted September 17, 2020. . https://doi.org/10.1101/2020.09.14.20194670 doi: medRxiv preprint did not observe this in our univariate analyses (Table 1) , with the data even pointing to worse mortality rates in white patients. We therefore investigated if patients from white backgrounds were in poorer health on admission to hospital by analyzing their DICE scores. Indeed, we observed that white patients in our cohort were at significantly higher risk of in-hospital death on admission by the DICE score than Black or Hispanic patients (Supplement [0.03-0.39]), although this did not meet statistical significance. After adjusting for DICE score, race was no longer a predictor of in-hospital mortality. We also considered whether there were a disproportionate number of minority patients in the DNI and/or CMO groups that were excluded from our analyses, therefore explaining the discordant results with the published literature. As shown in Supplemental Table 3 , this does not appear to be explained by the CMO population, which had a higher percentage of white patients than any other background. There was also a large percentage of white patients in the DNI group that could have confounded the mortality calculations. However, even when excluding these patients from mortality analyses, Hispanic patients still had a significantly lower risk of death in our study. is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review) preprint The copyright holder for this this version posted September 17, 2020. . https://doi.org/10.1101/2020.09.14.20194670 doi: medRxiv preprint validation cohort. The lower AUC for COVID-GRAM in our population may be due to geographical differences in COVID-19 presentation and outcomes in China compared to Boston. . CC-BY-NC-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review) preprint The copyright holder for this this version posted September 17, 2020. . https://doi.org/10.1101/2020.09.14.20194670 doi: medRxiv preprint This study reports on the in-hospital outcomes of sequentially hospitalized patients with COVID-19 in the Boston area. We identified many independent risk factors for mortality in this population, including older age, male sex, preexisting diabetes mellitus, thrombocytopenia, hypoalbuminemia, and higher levels of inflammatory and infectious biomarkers including procalcitonin, CRP, and neutrophil to lymphocyte ratio. Interestingly, we also found that chronic statin use was associated with a lower risk of death, perhaps supporting the anti-inflammatory and immunomodulatory benefits of statins in this disease. [14] [15] [16] Notably, there was some but not complete overlap in the factors that predicted ventilation needs, with preexisting diabetes and elevated CRP and neutrophil to lymphocyte ratio being included in both risk score models. Factors that uniquely and independently predicted ventilation requirement included dyspnea on presentation and elevated lactate dehydrogenase, ALT, and troponin. Age was not a significant predictor of ventilation need, perhaps dispelling the belief that COVID-19 only severely affects the elderly. Recent evidence suggests that young people are driving the recent surge in coronavirus cases in many states. While there is clear evidence that young patients are less likely to die from COVID-19, young hospitalized patients regularly require the use of ventilators for extended periods of time, thus stressing a system that is already in short supply of ICU beds. Recognizing the difference in variables that independently predict need for ventilation and death in COVID-19 is greatly important, especially as current risk score calculators combine ICU admission, ventilation needs and mortality into one endpoint. 7 The use of two independent risk scores predicting ventilation need and death allows health care systems to not only more precisely prognosticate individual patient outcomes but also better predict demand for mechanical ventilators and ICU beds. . CC-BY-NC-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this this version posted September 17, 2020. . https://doi.org/10.1101/2020.09.14.20194670 doi: medRxiv preprint In our study, we demonstrated that excessive levels of inflammatory and infectious markers, such as CRP and procalcitonin, were associated with an increased risk of death. Given the relationship between cardiometabolic disease and death in COVID-19, it is possible that these patients are more vulnerable to the aggressive inflammatory response induced by the virus. 17 Of note, it has recently been suggested that biological, rather than chronological, aging is a stronger predictor of all-cause mortality related to COVID-19. 18 Therefore, underlying agerelated cardiovascular dysfunction may increase the risk of a hyperinflammatory response that augments the effects of COVID-19 in these individuals. 17,19,20 Interestingly, elevated troponin levels, a marker of cardiac injury, predicted increased need for mechanical ventilation in multivariable analysis whereas chronic statin use was associated with reduced in-hospital mortality, further underscoring the strong link between underlying cardiovascular disease and worse outcomes with COVID-19. 8,21-29 Importantly, we developed two novel prediction models to calculate risk of hospitalized COVID-19 developing severe outcomes. These models were effective at predicting risk of ventilation need and death in both our derivation and validation cohorts. Of note, the two cohorts showed considerable variability in several factors, including sex and age, demonstrating our risk scores may be accurate in distinct populations from the United States. We also observed that the factors used to construct each risk score were different, demonstrating that it is beneficial to consider risks of ventilation need and mortality separately. Interestingly, and in contrast to previous reports, we did not observe worse outcomes in COVID-19 patients from minority ethnic backgrounds [10] [11] [12] [13] In fact, we actually presented data suggesting worse mortality rates in white patients, even when removing patients with a DNI status. However, we found no difference in . CC-BY-NC-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this this version posted September 17, 2020. . https://doi.org/10.1101/2020.09.14.20194670 doi: medRxiv preprint outcomes after adjusting for individual DICE scores. These findings suggest that variations in outcomes by racial group may be explained by differences in underlying risk factor profiles, although socioeconomic status was not assessed in our study and could be another determinant of outcome. In addition, the patients included were inpatients only, and inequitable access to healthcare (including hospital admission) appears to contribute to the disproportionate impact of COVID-19 on patients from racial minorities in the United States. 30 The data from our study was unable to quantify any racial disparities in this other important area. The variables that we found to predict mechanical ventilation requirement and mortality are either readily available or routinely measured upon admission to the hospital. We anticipate that clinicians will easily be able to implement the DICE and VICE scores to stratify risk in admitted patients. In situations where hospital resources are plentiful, clinicians could use both scores to identify which patients are most likely to develop severe illness, and plan accordingly to monitor higher risk patients more intensely. However, under the most critical of circumstances where ventilators are in short supply, clinicians may require aid in triage and ventilator utilization. For example, ventilators may be prioritized for those patients who are at most risk for ventilation need (high VICE score) while still having a relatively lower risk of death (assessed by the DICE score). One of the main strengths of the study was our ability to collect comprehensive data on and follow more than 99% of the patients from our cohort from admission to the primary endpoint, either discharge or death, with a large fraction of patients (18%) remaining in the hospital for longer than 28 days. Also, data were obtained by detailed medical record review rather than reliance on billing codes. Given the different variables that predict ventilation needs . CC-BY-NC-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this this version posted September 17, 2020. . https://doi.org/10.1101/2020.09.14.20194670 doi: medRxiv preprint and mortality, we believe another strength was the construction of distinct risk scores. One potential limitation is the modest sample sizes in both our derivation and validation cohorts. Although the exclusion of CMO patients was justified in the current study, this may have resulted in underestimating the impact of malignancy on inpatient mortality. Further, we aimed to focus on admission findings in determining outcomes and hence our risk scores do not include the effects of different treatment regimens. This study identified baseline patient characteristics and admission laboratory values that associate with critical illness and in-hospital death in patients with COVID-19. In this investigation, we developed and validated risk score calculators to predict mechanical ventilation need and in-hospital death in COVID-19 patients. These risk scores could potentially aid clinicians to better stratify risk in COVID-19 patients and optimize patient care and resource utilization in the surge of infections we are facing worldwide. . CC-BY-NC-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. The copyright holder for this this version posted September 17, 2020. . https://doi.org/10.1101/2020.09.14.20194670 doi: medRxiv preprint CJN and RM conceived, designed, and planned the study with input from LW and HHS. WT, WJ, RHL, HHS, and LW extracted the data from RPDR and EPIC. LW and HHS designed the code that was used to extract data from RPDR and analyze the data. CJN, HHS and LW performed the data analysis and designed the figures. CJN, LW and RM interpreted the data. CJN and RM drafted the manuscript with critical input from CLLC, LW, HHS and RHL. All authors interpreted the data and made significant contributions to manuscript editing. . CC-BY-NC-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review) preprint The copyright holder for this this version posted September 17, 2020. . Kuo . CC-BY-NC-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review) preprint The copyright holder for this this version posted September 17, 2020. . CC-BY-NC-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review) preprint The copyright holder for this this version posted September 17, 2020. . https://doi.org/10.1101/2020.09.14.20194670 doi: medRxiv preprint . CC-BY-NC-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review) preprint The copyright holder for this this version posted September 17, 2020. . https://doi.org/10.1101/2020.09.14.20194670 doi: medRxiv preprint Patients coded for "do not intubate" were excluded from analyses for ventilation status. P values are for univariate logistic regression analyses. COPD = chronic obstructive pulmonary disorder; LOC = loss of consciousness; RAAS = renin angiotensin-aldosterone system; OR = odds ratio; CI = confidence interval; SD = standard deviation. * Calculated as a percentage of each group (race or smoking status). Otherwise, values were calculated as a percentage of each outcome group (ventilation or discharge). Odds ratios in the ethnicity and smoking categories were calculated relative to white patients or never smokers, respectively. ∞ Odds ratio calculated for every 10-year increase in age. . CC-BY-NC-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review) preprint The copyright holder for this this version posted September 17, 2020. . https://doi.org/10.1101/2020.09.14.20194670 doi: medRxiv preprint c . CC-BY-NC-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review) preprint The copyright holder for this this version posted September 17, 2020. . https://doi.org/10.1101/2020.09.14.20194670 doi: medRxiv preprint CC-BY-NC-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review) preprint The copyright holder for this this version posted September 17, 2020. . https://doi.org/10.1101/2020.09.14.20194670 doi: medRxiv preprint CC-BY-NC-ND 4.0 International license It is made available under a is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review) preprint The copyright holder for this this version posted September 17, 2020. . https://doi.org/10.1101/2020.09.14.20194670 doi: medRxiv preprint 
